<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w ktÃ³rym nasi inÅ¼ynierowie dzielÄ… siÄ™ wiedzÄ… oraz case study z wybranych projektÃ³w w firmie - w formie artykuÅ‚Ã³w, podcastÃ³w oraz eventÃ³w."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="15"/><link rel="preload" href="/_next/static/css/f90f7240cef95199c2b1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f90f7240cef95199c2b1.css" data-n-g=""/><link rel="preload" href="/_next/static/css/bab806b57e265845b5ec.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bab806b57e265845b5ec.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/webpack-189c53927ffd3caf09c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework-0441fae7fd130f37dee1.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-547dee26f92077ae29b6.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-f9c1c7bd15b9b2e730cb.js" as="script"/><link rel="preload" href="/_next/static/chunks/805-39af1b772818e37efb44.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/index-fe725e13004e1b56d414.js" as="script"/></head><body><div id="__next"><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__2vWRp m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="OtwÃ³rz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__n5PN5"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__15JNc"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">O nas</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro to jedna zÂ najbardziej zaawansowanych technologicznie firm wÂ naszej czÄ™Å›ci Europy. Allegro to rÃ³wnieÅ¼ ponad 1000 specjalistÃ³w IT, rÃ³Å¼nych specjalizacji, rozwijajÄ…cych nasz serwis. Unikatowa skala iÂ zÅ‚oÅ¼onoÅ›Ä‡ problemÃ³w, ktÃ³re rozwiÄ…zujemy na co dzieÅ„, dajÄ… nam moÅ¼liwoÅ›Ä‡ rozwoju przy bardzo rÃ³Å¼norodnych projektach. Allegro Tech to miejsce, wÂ ktÃ³rym nasi inÅ¼ynierowie dzielÄ… siÄ™ wiedzÄ… oraz case study zÂ wybranych projektÃ³w w firmie â€“ wÂ formie artykuÅ‚Ã³w, podcastÃ³w oraz eventÃ³w.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html" title="Splitting data that does not fit on one machine using data partitioning"><img width="388" src="images/post-headers/default.jpg" alt="Splitting data that does not fit on one machine using data partitioning" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html" title="Splitting data that does not fit on one machine using data partitioning" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Splitting data that does not fit on one machine using data partitioning</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">dzieÅ„ temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/sharding">#<!-- -->sharding</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bigdata">#<!-- -->bigdata</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/dba">#<!-- -->dba</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/distributed">#<!-- -->distributed</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">The following article is an excerpt from Software Mistakes and Trade-offs book.
In real-world big data applications, the amount of data that we need to storeâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:2"><img alt="Tomasz Lelek" src="https://blog.allegro.tech/img/authors/tomasz.lelek.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Jon Skeet" src="https://blog.allegro.tech/img/authors/jon.skeet.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/tomasz.lelek">Tomasz Lelekâ€¦</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html" title="CSS Architecture and Performance in Micro Frontends"><img width="388" src="images/post-headers/default.jpg" alt="CSS Architecture and Performance in Micro Frontends" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html" title="CSS Architecture and Performance in Micro Frontends" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">CSS Architecture and Performance in Micro Frontends</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">13 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/webperf">#<!-- -->webperf</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/frontend">#<!-- -->frontend</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/perfmatters">#<!-- -->perfmatters</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/css">#<!-- -->css</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Itâ€™s been over 5 years since the introduction of the article describing the ongoing transformation of Allegroâ€™s frontend architecture â€” an approach that was laterâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Mateusz Krzeszowiak" src="https://blog.allegro.tech/img/authors/mateusz.krzeszowiak.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/mateusz.krzeszowiak">Mateusz Krzeszowiak</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html" title="Making API calls a seamless user experience"><img width="388" src="images/post-headers/default.jpg" alt="Making API calls a seamless user experience" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html" title="Making API calls a seamless user experience" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Making API calls a seamless user experience</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">21 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/javascript">#<!-- -->javascript</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/frontend">#<!-- -->frontend</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ajax">#<!-- -->ajax</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/restapi">#<!-- -->restapi</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/http">#<!-- -->http</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Almost every modern web application somehow interacts with a backend - be it loading data, doing background sync, submitting a form, or publishing the metrics.
Makingâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="PaweÅ‚ Wolak" src="https://blog.allegro.tech/img/authors/pawel.wolak.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/pawel.wolak">PaweÅ‚ Wolak</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html" title="One task â€” two solutions: Apache Spark or Apache Beam?"><img width="388" src="images/post-headers/default.jpg" alt="One task â€” two solutions: Apache Spark or Apache Beam?" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html" title="One task â€” two solutions: Apache Spark or Apache Beam?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">One task â€” two solutions: Apache Spark or Apache Beam?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o miesiÄ…c temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/Apache Spark">#<!-- -->Apache Spark</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/Apache Beam">#<!-- -->Apache Beam</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/BigFlow">#<!-- -->BigFlow</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/BigData">#<!-- -->BigData</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/GCP">#<!-- -->GCP</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Some time ago, our team faced the issue of moving an existing Apache Spark job from an on-premise Hadoop cluster to public cloud.
While working onâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Yevgeniya Li" src="https://blog.allegro.tech/img/authors/yevgeniya.li.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/yevgeniya.li">Yevgeniya Li</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html">przejdÅº do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz wiÄ™cej wpisÃ³w</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/rola_architekta_w_allegro" title="Rola architekta w Allegro"><img src="images/podcast.png" alt="Rola architekta w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/rola_architekta_w_allegro" title="Rola architekta w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Rola architekta w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o 2 miesiÄ…ce temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Od kodowania do tworzenia strategii technicznej... Jak wyglÄ…da rola architekta w Allegro? Ile takich osÃ³b pracuje w naszej firmie i dlaczego ta rola jest tak rÃ³Å¼norodna? Czym jest Andamio i jak rozwijamy naszÄ… platformÄ™ â€“ o tym wszystkim opowie Piotr Betkier â€“ InÅ¼ynier, Architekt Platformy Technicznej w Allegro oraz twÃ³rca piosenek o IT :)</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/rola_architekta_w_allegro">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/infrastruktura_Allegro" title="Infrastruktura Allegro"><img src="images/podcast.png" alt="Infrastruktura Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/infrastruktura_Allegro" title="Infrastruktura Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Infrastruktura Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">2 miesiÄ…ce temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak jest zbudowane Å›rodowisko uruchomienia aplikacji Allegro? Jak dziaÅ‚ajÄ… serwerownie firmy i ile ich potrzeba, a ktÃ³re elementy Allegro dziaÅ‚ajÄ… w chmurze publicznej? Jak przebiegaÅ‚a transformacja w Allegro i co zmieniaÅ‚o siÄ™ przez lata? Jak wzrost biznesu wpÅ‚ywa na wielkoÅ›Ä‡ infrastruktury i jak infrastruktura Allegro odczuÅ‚a przyjÅ›cie pandemii? O tym, a takÅ¼e o rozwoju liderÃ³w technologii w Allegro oraz o historii powstania dÅ¼ingla do naszych podcastÃ³w, opowie Piotr MichoÅ„ski - menadÅ¼er ZespoÅ‚Ã³w tworzÄ…cych infrastrukturÄ™ Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/infrastruktura_Allegro">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro" title="Praca architekta ekosystemu big data w Allegro"><img src="images/podcast.png" alt="Praca architekta ekosystemu big data w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro" title="Praca architekta ekosystemu big data w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Praca architekta ekosystemu big data w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiÄ…ce temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak wyglÄ…da praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespÃ³Å‚ odpowiedzialny za narzÄ™dzia i infrastrukturÄ™ dla przetwarzania danych? Kiedy moÅ¼emy mÃ³wiÄ‡ o duÅ¼ych danych i ile petabajtÃ³w przetwarza Allegro? SkÄ…d pochodzÄ… dane Allegro i dlaczego jest ich tak duÅ¼o oraz z jakiego powodu dopiero teraz przenosimy siÄ™ do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz â€“ Team Manager &amp; Platform Architect w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro" title="Od inÅ¼yniera do lidera w Allegro"><img src="images/podcast.png" alt="Od inÅ¼yniera do lidera w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro" title="Od inÅ¼yniera do lidera w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Od inÅ¼yniera do lidera w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiÄ…ce temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym jest Opbox i jakie wyzwania przed nim stojÄ…? Jak w Allegro angaÅ¼ujemy siÄ™ w rozwÃ³j kultury Open Source? Ile mamy projektÃ³w na GitHubie i jak Å›wiÄ™tujemy Hacktoberfest? W jaki sposÃ³b moÅ¼na rozwinÄ…Ä‡ siÄ™ od inÅ¼yniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek GaÅ‚ek, Team Leader w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro">PosÅ‚uchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz wiÄ™cej podcastÃ³w</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/278903176/" title="Allegro Tech Live #20: WydajnoÅ›Ä‡ Backendu" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #20: WydajnoÅ›Ä‡ Backendu"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/278903176/" title="Allegro Tech Live #20: WydajnoÅ›Ä‡ Backendu" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #20: WydajnoÅ›Ä‡ Backendu</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">okoÅ‚o miesiÄ…c temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live w 100% zdalna odsÅ‚ona naszych stacjonarnych meetupÃ³w Allegro Tech Talks. Zazwyczaj spotykaliÅ›my siÄ™ w naszych biurach, ale tym razem to my zagoÅ›cimyâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/278903176/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/278374635/" title="UX Research Confetti" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/278374635/" title="UX Research Confetti" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">okoÅ‚o 2 miesiÄ…ce temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">ğŸ‰ Niech rozsypie siÄ™ confetti wiedzy o badaniach UX! ğŸ‰ SzukaliÅ›my konferencji badawczej UX w Polsce i nie znaleÅºliÅ›myâ€¦ Dlatego Å‚Ä…czymy siÅ‚y z ekspertami zâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/278374635/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/278528964/" title="Allegro Tech Live Odcinek: #19   Co to znaczy byÄ‡ liderem i jak nim zostaÄ‡?" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live Odcinek: #19   Co to znaczy byÄ‡ liderem i jak nim zostaÄ‡?"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/278528964/" title="Allegro Tech Live Odcinek: #19   Co to znaczy byÄ‡ liderem i jak nim zostaÄ‡?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live Odcinek: #19   Co to znaczy byÄ‡ liderem i jak nim zostaÄ‡?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">2 miesiÄ…ce temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to w 100% zdalna odsÅ‚ona naszych stacjonarnych meetupÃ³w Allegro Tech Talks. Zazwyczaj spotykaliÅ›my siÄ™ w naszych biurach, ale tym razem to myâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/278528964/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/277852879/" title="Allegro Tech Live #18 PM w Allegro, jak do nas doÅ‚Ä…czyÄ‡ i czerpaÄ‡ radoÅ›Ä‡ z pracy" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #18 PM w Allegro, jak do nas doÅ‚Ä…czyÄ‡ i czerpaÄ‡ radoÅ›Ä‡ z pracy"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/277852879/" title="Allegro Tech Live #18 PM w Allegro, jak do nas doÅ‚Ä…czyÄ‡ i czerpaÄ‡ radoÅ›Ä‡ z pracy" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #18 PM w Allegro, jak do nas doÅ‚Ä…czyÄ‡ i czerpaÄ‡ radoÅ›Ä‡ z pracy</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiÄ…ce temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to w 100% zdalna odsÅ‚ona naszych stacjonarnych meetupÃ³w Allegro Tech Talks. Zazwyczaj spotykaliÅ›my siÄ™ w naszych biurach, ale tym razem to myâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/277852879/">SzczegÃ³Å‚y</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz wiÄ™cej wydarzeÅ„</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">UX Team Leader</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, Warszawa, KrakÃ³w</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999766022669-ux-team-leader?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Team Leader (Design System)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, PoznaÅ„</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999766022264-team-leader-design-system?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (Java/Kotlin) - Delivery Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, PoznaÅ„, KrakÃ³w</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999766022103-software-engineer-javakotlin-delivery-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Product Manager (Allegro Smart!)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, Warszawa</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999765816116-senior-product-manager-allegro-smart?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Research Engineer - Machine Learning (Computer Vision)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, KrakÃ³w, PoznaÅ„, ToruÅ„</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999765815469-research-engineer-machine-learning-computer-vision?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz wiÄ™cej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=6875319421723.989;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Splitting data that does not fit on one machine using data partitioning","link":"https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html","pubDate":"Tue, 10 Aug 2021 00:00:00 +0200","authors":{"author":[{"name":["Tomasz Lelek"],"photo":["https://blog.allegro.tech/img/authors/tomasz.lelek.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.lelek"]},{"name":["Jon Skeet"],"photo":["https://blog.allegro.tech/img/authors/jon.skeet.jpg"],"url":["https://blog.allegro.tech/authors/jon.skeet"]}]},"content":"\u003cp\u003eThe following article is an excerpt from \u003ca href=\"https://www.manning.com/books/software-mistakes-and-tradeoffs\"\u003eSoftware Mistakes and Trade-offs\u003c/a\u003e book.\nIn real-world big data applications, the amount of data that we need to store and process can be often counted in the hundreds of terabytes or petabytes. It is not feasible to store such an amount of data on one physical node. We need a way to split that data into N data nodes.\u003c/p\u003e\n\n\u003cp\u003eThe technique for splitting the data is called data partitioning. There are a lot of techniques to partition your data.\u003c/p\u003e\n\n\u003cp\u003eFor online processing sources (like a database), you may pick some ID, for example, user-ID, and store a range of users on a dedicated node. For example, assuming that you have 1000 user IDs and 5 data nodes, the first node can store IDs from 0 to 200, the second node can store data from 201 to 400, and so on. When picking the partitioning scheme, you need to be careful not to introduce the data skew. Such a situation can occur when most of the data is produced by one or a group of IDs that belongs to the same data node. For example, letâ€™s assume that the user ID 10 is responsible for 80% of our traffic and generates 80% of the data. Therefore, it will mean that 80% of the data is stored on the first data node, and our partitioning will not be optimal. In the worst case, this userâ€™s amount of data may be too big to store on the given data node. It is important to note that for online processing, the partitioning is optimized for reading or writing data access patterns.\u003c/p\u003e\n\n\u003ch2 id=\"offline-big-data-partitioning\"\u003eOffline big data partitioning\u003c/h2\u003e\n\n\u003cp\u003eWe will focus now on the offline, big data processing partitioning.\u003c/p\u003e\n\n\u003cp\u003eFor Big Data systems, we often need to store the historical data (cold data) for an â€œindefiniteâ€ amount of time. It is crucial to store the data for as long as we can. When the data is produced, we may not be aware of the business value that it can bring in the future. For example, we may save all userâ€™s request data with all the HTTP headers. When the data is saved, there may be no use case for these HTTP headers. In the future, however, we may decide to build a tool that profiles our users by the type of device (Android, iOS) that they use. Such information is propagated in the HTTP headers. We can execute our new profiling logic based on the historical data because we stored it in the raw data. It is important to note here that the data was not needed for a long period.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, we needed to store a lot of information and save it for later. Thus, our storage needs to contain a lot of data stored in cold storage. In Big Data applications, it often means that data is saved to a Hadoop distributed file system (HDFS). It also means that the data should be partitioned in a fairly generic way. We cannot optimize for read patterns because we cannot anticipate how those read patterns will look like.\u003c/p\u003e\n\n\u003cp\u003eBecause of these reasons, the most often used data partitioning scheme for big data offline processing is based on dates. Letâ€™s assume that we have a system that saves userâ€™s data on the /users file system path and clickstream data in the /clicks file system path. We will analyze the first data set that stores the userâ€™s data. We are assuming that the number of records that we store is equal to 10 billion. We started collecting the data in the year 2017, and itâ€™s been collected since then.\u003c/p\u003e\n\n\u003cp\u003eThe partitioning scheme that we pick is based on the date. It means that our partition identifier starts with the year. We will have 2017, 2018, 2019, and 2020 partitions. If we would have smaller data requirements, partitioning by year may be enough. In such a scenario, the file system path for our userâ€™s data would be /users/2017, /users/2018, and so on. It will be analogical for clicks: /clicks/2017, /clicks/2018, and so on.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img1.png\" alt=\"Figure 1\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"four-data-partitions\"\u003eFour data partitions\u003c/h2\u003e\n\n\u003cp\u003eBy using this partitioning, the userâ€™s data will have 4 partitions. It means that we can split the data into up to four physical data nodes. The first node will store the data for the year 2017, the second node for 2018, etc. Nothing prevents us from keeping all of those partitions on the same physical node when having four partitions. We may be ok with storing the data on one physical node as long as we have enough disk space. Once the disk space runs out, we can create a new physical node and move some of the partitions to the new node.\u003c/p\u003e\n\n\u003cp\u003eIn practice, such a partitioning scheme is too coarse-grained. Having one big partition for all yearâ€™s data is hard from both a read and write perspective. When you read such data and are interested only in events from a particular date, you need to scan the whole yearâ€™s data. Itâ€™s very inefficient and time-consuming. It is also problematic from the writing perspective because if your disk space runs out, there is no easy way to split the data further. You wonâ€™t be able to perform a successful write.\u003c/p\u003e\n\n\u003cp\u003eBecause of that reason, offline big data systems tend to partition the data in a more fine-grained fashion. The data is partitioned by year, month, and even day. For example, if you are writing data for the 2nd of January 2020, you will save the event into a /users/2020/01/02 partition. Such a partitioning gives you a lot of flexibility at the read side as well. If you wish to analyze events for a specific day, you can directly read the data from the partition. If you want to perform some higher-level analysis, for example, analyze the whole monthâ€™s data, you can read all partitions within a given month. The same pattern applies if you want to analyze a whole yearâ€™s data.\u003c/p\u003e\n\n\u003cp\u003eTo sum up, our 10 billion records will be partitioned in the following way:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img2.png\" alt=\"Figure 2\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"date-based-data-partitioning\"\u003eDate based data partitioning\u003c/h2\u003e\n\n\u003cp\u003eYou can see that the initial 10 billion records are partitioned into year/month, and finally, a specific date of the month. In the end, each dayâ€™s partition contains a hundred thousand records. Such an amount of data can easily fit into one machine disk space. It also means that we have 365/366 partitions per year. The upper number of data nodes on which we can partition the data is equal to the number of days * the number of years we store data. If your one-day data does not fit into one machine disk space, you can easily partition your data further by hours of the day, minutes, seconds, and so on.\u003c/p\u003e\n\n\u003ch2 id=\"partitioning-vs-sharding\"\u003ePartitioning vs sharding\u003c/h2\u003e\n\n\u003cp\u003eAssuming that we have our data partitioned by the date, we can split that data into multiple nodes. In such a scenario, we are putting a subset of all partition keys in a physical node.\u003c/p\u003e\n\n\u003cp\u003eOur userâ€™s data is partitioned into N partitions (logical shards). Letâ€™s assume that our partition granularity is a month. In that case, the data for the year 2020 has 12 partitions that can be split horizontally into N physical nodes (physical shards). It is important to note that N is less than or equal to 12. In other words, the maximum level of physical shards is 12. This architecture pattern is called sharding.\u003c/p\u003e\n\n\u003cp\u003eLetâ€™s assume that we have three physical nodes. In that case, we can say that our userâ€™s data for the year 2020 is partitioned into 12 partitions. Next, they are assigned to 3 shards (nodes). Each of the nodes stores 4 partitions for 2020 (12 partitions / 3 nodes = 4 partitions/node).\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img1.png\" alt=\"Figure 8.6. Sharding\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIn our diagram, the physical shard is the same as the physical node. The partition keys (logical shards) are distributed evenly to physical shards. In case a new node is added to a cluster, each physical shard needs to re-assign one of its logical shards to a new physical node.\u003c/p\u003e\n\n\u003cp\u003eThere are a variety of algorithms for shards assignments. They also need to handle shards re-distribution in case of adding or removing a node (failure or scale down). This technique is used by most big data technologies and data stores such as HDFS, Cassandra, Kafka, Elastic, etc., and they vary depending on the implementation.\u003c/p\u003e\n\n\u003cp\u003eAt this point, we know the basics of data partitioning. Next, we need to understand how the partitioning algorithms work in depth. This is essential to understand if we want to reason about the big data tools we are using to get business value. This is discussed at length in the book.\u003c/p\u003e\n\n\u003cp\u003eThis is discussed at length in the book. You can purchase it with a special discount \u003ca href=\"https://www.manning.com/books/software-mistakes-and-tradeoffs\"\u003ehere\u003c/a\u003e using the code \u003cstrong\u003eallegrotech35\u003c/strong\u003e.\u003c/p\u003e\n","contentSnippet":"The following article is an excerpt from Software Mistakes and Trade-offs book.\nIn real-world big data applications, the amount of data that we need to store and process can be often counted in the hundreds of terabytes or petabytes. It is not feasible to store such an amount of data on one physical node. We need a way to split that data into N data nodes.\nThe technique for splitting the data is called data partitioning. There are a lot of techniques to partition your data.\nFor online processing sources (like a database), you may pick some ID, for example, user-ID, and store a range of users on a dedicated node. For example, assuming that you have 1000 user IDs and 5 data nodes, the first node can store IDs from 0 to 200, the second node can store data from 201 to 400, and so on. When picking the partitioning scheme, you need to be careful not to introduce the data skew. Such a situation can occur when most of the data is produced by one or a group of IDs that belongs to the same data node. For example, letâ€™s assume that the user ID 10 is responsible for 80% of our traffic and generates 80% of the data. Therefore, it will mean that 80% of the data is stored on the first data node, and our partitioning will not be optimal. In the worst case, this userâ€™s amount of data may be too big to store on the given data node. It is important to note that for online processing, the partitioning is optimized for reading or writing data access patterns.\nOffline big data partitioning\nWe will focus now on the offline, big data processing partitioning.\nFor Big Data systems, we often need to store the historical data (cold data) for an â€œindefiniteâ€ amount of time. It is crucial to store the data for as long as we can. When the data is produced, we may not be aware of the business value that it can bring in the future. For example, we may save all userâ€™s request data with all the HTTP headers. When the data is saved, there may be no use case for these HTTP headers. In the future, however, we may decide to build a tool that profiles our users by the type of device (Android, iOS) that they use. Such information is propagated in the HTTP headers. We can execute our new profiling logic based on the historical data because we stored it in the raw data. It is important to note here that the data was not needed for a long period.\nOn the other hand, we needed to store a lot of information and save it for later. Thus, our storage needs to contain a lot of data stored in cold storage. In Big Data applications, it often means that data is saved to a Hadoop distributed file system (HDFS). It also means that the data should be partitioned in a fairly generic way. We cannot optimize for read patterns because we cannot anticipate how those read patterns will look like.\nBecause of these reasons, the most often used data partitioning scheme for big data offline processing is based on dates. Letâ€™s assume that we have a system that saves userâ€™s data on the /users file system path and clickstream data in the /clicks file system path. We will analyze the first data set that stores the userâ€™s data. We are assuming that the number of records that we store is equal to 10 billion. We started collecting the data in the year 2017, and itâ€™s been collected since then.\nThe partitioning scheme that we pick is based on the date. It means that our partition identifier starts with the year. We will have 2017, 2018, 2019, and 2020 partitions. If we would have smaller data requirements, partitioning by year may be enough. In such a scenario, the file system path for our userâ€™s data would be /users/2017, /users/2018, and so on. It will be analogical for clicks: /clicks/2017, /clicks/2018, and so on.\n\nFour data partitions\nBy using this partitioning, the userâ€™s data will have 4 partitions. It means that we can split the data into up to four physical data nodes. The first node will store the data for the year 2017, the second node for 2018, etc. Nothing prevents us from keeping all of those partitions on the same physical node when having four partitions. We may be ok with storing the data on one physical node as long as we have enough disk space. Once the disk space runs out, we can create a new physical node and move some of the partitions to the new node.\nIn practice, such a partitioning scheme is too coarse-grained. Having one big partition for all yearâ€™s data is hard from both a read and write perspective. When you read such data and are interested only in events from a particular date, you need to scan the whole yearâ€™s data. Itâ€™s very inefficient and time-consuming. It is also problematic from the writing perspective because if your disk space runs out, there is no easy way to split the data further. You wonâ€™t be able to perform a successful write.\nBecause of that reason, offline big data systems tend to partition the data in a more fine-grained fashion. The data is partitioned by year, month, and even day. For example, if you are writing data for the 2nd of January 2020, you will save the event into a /users/2020/01/02 partition. Such a partitioning gives you a lot of flexibility at the read side as well. If you wish to analyze events for a specific day, you can directly read the data from the partition. If you want to perform some higher-level analysis, for example, analyze the whole monthâ€™s data, you can read all partitions within a given month. The same pattern applies if you want to analyze a whole yearâ€™s data.\nTo sum up, our 10 billion records will be partitioned in the following way:\n\nDate based data partitioning\nYou can see that the initial 10 billion records are partitioned into year/month, and finally, a specific date of the month. In the end, each dayâ€™s partition contains a hundred thousand records. Such an amount of data can easily fit into one machine disk space. It also means that we have 365/366 partitions per year. The upper number of data nodes on which we can partition the data is equal to the number of days * the number of years we store data. If your one-day data does not fit into one machine disk space, you can easily partition your data further by hours of the day, minutes, seconds, and so on.\nPartitioning vs sharding\nAssuming that we have our data partitioned by the date, we can split that data into multiple nodes. In such a scenario, we are putting a subset of all partition keys in a physical node.\nOur userâ€™s data is partitioned into N partitions (logical shards). Letâ€™s assume that our partition granularity is a month. In that case, the data for the year 2020 has 12 partitions that can be split horizontally into N physical nodes (physical shards). It is important to note that N is less than or equal to 12. In other words, the maximum level of physical shards is 12. This architecture pattern is called sharding.\nLetâ€™s assume that we have three physical nodes. In that case, we can say that our userâ€™s data for the year 2020 is partitioned into 12 partitions. Next, they are assigned to 3 shards (nodes). Each of the nodes stores 4 partitions for 2020 (12 partitions / 3 nodes = 4 partitions/node).\n\nIn our diagram, the physical shard is the same as the physical node. The partition keys (logical shards) are distributed evenly to physical shards. In case a new node is added to a cluster, each physical shard needs to re-assign one of its logical shards to a new physical node.\nThere are a variety of algorithms for shards assignments. They also need to handle shards re-distribution in case of adding or removing a node (failure or scale down). This technique is used by most big data technologies and data stores such as HDFS, Cassandra, Kafka, Elastic, etc., and they vary depending on the implementation.\nAt this point, we know the basics of data partitioning. Next, we need to understand how the partitioning algorithms work in depth. This is essential to understand if we want to reason about the big data tools we are using to get business value. This is discussed at length in the book.\nThis is discussed at length in the book. You can purchase it with a special discount here using the code allegrotech35.","guid":"https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html","categories":["tech","performance","sharding","bigdata","dba","distributed"],"isoDate":"2021-08-09T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"CSS Architecture and Performance in Micro Frontends","link":"https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html","pubDate":"Thu, 29 Jul 2021 00:00:00 +0200","authors":{"author":[{"name":["Mateusz Krzeszowiak"],"photo":["https://blog.allegro.tech/img/authors/mateusz.krzeszowiak.jpg"],"url":["https://blog.allegro.tech/authors/mateusz.krzeszowiak"]}]},"content":"\u003cp\u003eItâ€™s been over 5 years since the introduction of the \u003ca href=\"https://blog.allegro.tech/2016/03/Managing-Frontend-in-the-microservices-architecture.html\"\u003earticle describing the ongoing transformation of Allegroâ€™s frontend architecture\u003c/a\u003e â€” an approach that was later formalized by the industry under the name of Micro Frontends. I think that after all this time we can safely say that this direction was correct and remained almost entirely unchanged in relation to the original idea. Still, some of the challenges foreseen in the publication soon became the reality. In this article I would like to focus on the CSS part of the whole adventure to tell you about how we manage consistency and frontend performance across over half a thousand components, and what it took us to get to where we stand today.\u003c/p\u003e\n\n\u003ch3 id=\"new-approach--new-challenges\"\u003eNew Approach â€” New Challenges\u003c/h3\u003e\n\n\u003cp\u003eHandling all the dependencies, libraries and visual compatibility when the entire website resides in a single repository is a challenge by itself. The level of difficulty increases even more, when there are hundreds of said repositories, each managed by a different team and tooling. When in such situation, one of the things that quickly become apparent is the need for some kind of guidelines around the look of various aspects of components being developed like color scheme, spacing, fonts etc. â€” those are exactly the reasons why the Metrum Design System came to life.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-07-29-css-architecture-and-performance-of-micro-frontends/metrum-design-system.jpg\" alt=\"Metrum Design System\" title=\"Metrum Design System\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIn its initial form â€” apart from visual examples and design resources â€” Metrum was providing reusable PostCSS mixins that every developer could install via separate npm packages and include in the component they were working on.\u003c/p\u003e\n\n\u003cdiv class=\"language-scss highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003e@import\u003c/span\u003e \u003cspan class=\"s1\"\u003e'node_modules/@metrum/button/css/mixins.css'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"nc\"\u003e.button\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003e@mixin\u003c/span\u003e \u003cspan class=\"nf\"\u003em-button\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"nl\"\u003ebackground-color\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"no\"\u003eblack\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cdiv class=\"language-html highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nt\"\u003e\u0026lt;button\u003c/span\u003e \u003cspan class=\"na\"\u003eclass=\u003c/span\u003e\u003cspan class=\"s\"\u003e\"button\"\u003c/span\u003e\u003cspan class=\"nt\"\u003e\u0026gt;\u0026lt;/button\u0026gt;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIf we try to evaluate that approach we could come up with following pros and cons:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003ePros\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eEasy to use â€” install mixin and include in componentâ€™s selector;\u003c/li\u003e\n  \u003cli\u003eMixins allow for sharing visual identity between components;\u003c/li\u003e\n  \u003cli\u003eDevelopers can use mixins in any version without depending on other parts of the page;\u003c/li\u003e\n  \u003cli\u003eEvery component ships with a complete set of styles.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003eCons\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eIncluding mixins introduces duplication of CSS rules between components used on the same page;\u003c/li\u003e\n  \u003cli\u003eMore files â€” every component brings at least one request for its styles;\u003c/li\u003e\n  \u003cli\u003eNo sharing of CSS â€” no cache reuse between pages built from different components;\u003c/li\u003e\n  \u003cli\u003eClashing of class names within the global namespace.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn summary, while being very flexible and easy to use, mixins-based approach was not ideal from a performance point of view. Every time when somebody wanted to use a button, input, link etc., they would have to include a mixin for it pulling the entire set of CSS rules to their stylesheet. This resulted in our users downloading unnecessary kilobytes during the first visit while bringing no caching benefit when navigating through other pages which in turn increased rendering times. We knew we could do better.\u003c/p\u003e\n\n\u003ch3 id=\"enter-css-modules\"\u003eEnter CSS Modules\u003c/h3\u003e\n\n\u003cp\u003eAfter a lot of brainstorming, a decision was made that the next step should involve Metrum making use of CSS Modules. While the technical aspects and usage were changing as the adoption grew, the main principles stayed the same up to this day. Currently, whenever any developer wants to assemble a new component out of Metrum building blocks, they can install desired packages, compose styles from them and declare used classes in their markup:\u003c/p\u003e\n\n\u003cdiv class=\"language-scss highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nc\"\u003e.button\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"na\"\u003ecomposes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003em-button\u003c/span\u003e \u003cspan class=\"n\"\u003efrom\u003c/span\u003e \u003cspan class=\"s1\"\u003e'@metrum/button'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"na\"\u003ecomposes\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003em-background-color-black\u003c/span\u003e \u003cspan class=\"n\"\u003efrom\u003c/span\u003e \u003cspan class=\"s1\"\u003e'@metrum/color'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cdiv class=\"language-javascript highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eimport\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"nx\"\u003estyles\u003c/span\u003e \u003cspan class=\"k\"\u003efrom\u003c/span\u003e \u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"s1\"\u003e./styles.css\u003c/span\u003e\u003cspan class=\"dl\"\u003e'\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003eexport\u003c/span\u003e \u003cspan class=\"k\"\u003edefault\u003c/span\u003e \u003cspan class=\"kd\"\u003efunction\u003c/span\u003e \u003cspan class=\"nx\"\u003erender\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"s2\"\u003e`\n        \u0026lt;button class=\"\u003c/span\u003e\u003cspan class=\"p\"\u003e${\u003c/span\u003e\u003cspan class=\"nx\"\u003estyles\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ebutton\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"\u0026gt;...\u0026lt;/button\u0026gt;\n    `\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThanks to the fact that all of our micro frontends run on Node.js, this approach can be used quite easily with the majority of tooling available. The only thing left to do is to collect all of the required Metrum stylesheets during render in our facade server called opbox-web and embed them on the page with the correct order. Ordering requirement is important, because we follow atomic design and more complicated components (molecules, organisms) are built using simpler ones (atoms). Lets see what all of those changes did to our list of tradeoffs:\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003ePros\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eStill easy to use â€” install package and compose desired classes in your component;\u003c/li\u003e\n  \u003cli\u003eSharing classes means sharing visual traits which was one of our goals;\u003c/li\u003e\n  \u003cli\u003eStyles for certain module only appear once per page if used in multiple components;\u003c/li\u003e\n  \u003cli\u003eEach Metrum stylesheet can be cached by the browser separately and reused on different pages;\u003c/li\u003e\n  \u003cli\u003eDevelopers can still use packages in any version without depending on other parts of the page.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003eCons\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAdditional logic has to be maintained that extracts needed Metrum stylesheets from components and adds them to the page once;\u003c/li\u003e\n  \u003cli\u003eAbove logic has to also take care of sorting so the order of styles is correct and we donâ€™t run into problems with cascade;\u003c/li\u003e\n  \u003cli\u003eMultiple versions of the same Metrum component may be needed on the page;\u003c/li\u003e\n  \u003cli\u003eMore and more requests have to be made as components transition to the new approach.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eJudging from the upsides the transition was worth it: despite higher maintenance effort we were finally able to share common CSS code between components, the amount of downloaded data as well as render times started decreasing. Unfortunately, after some time we started to see a worrying trend related to the number of embedded stylesheets. Prior to this change, it was roughly equal to the number of components used on the page. Afterwards, with additional Metrum modules, plus the fact that multiple versions of them may be needed, we ended up with as much as around 100 requests for render-blocking CSS.\u003c/p\u003e\n\n\u003cp\u003eUsually, when we bring up the issue of excessive number of requests people respond with â€œSo what? You have HTTP/2, right?â€ and yes, we do. Itâ€™s true that the user agent will reuse existing connections for multiple files but the limit of concurrent streams does exist, latency is still going to affect each one of them and the compression efficiency will be worse especially for those relatively small files like ours. We had to come up with yet another idea for improvement.\u003c/p\u003e\n\n\u003ch3 id=\"let-the-bundle-begin\"\u003eLet the Bundle Begin\u003c/h3\u003e\n\n\u003cp\u003eAs I touched briefly earlier, we have the opbox-web â€” a place thatâ€™s already responsible for extracting, sorting and embedding Metrum dependencies. We figured that instead of adding each of them separately, we could prepare predefined bundles that would serve as replacements. We did as planned and, after deployment on 6th of July 2020, achieved 15% improvement in FCP metric time, which means that our users saw the first render of content faster by almost half a second.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-07-29-css-architecture-and-performance-of-micro-frontends/fcp-after-metrum-bundle.png\" alt=\"FCP metric chart before and after deployment of Metrum bundle\" title=\"FCP metric chart before and after deployment of Metrum bundle\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eImprovement was satisfactory, but it came at a certain cost. From that time on we had to make sure all of the components used on a certain page share the same versions of Metrum modules supported by the bundle and I assure you, it was bothersome to say the least. Monitoring that nobody updated their dependency by accident was one thing (especially that we managed to automate it) but undergoing a process of actually wanting to do this was another. In addition, every time we failed within that area we had to bail and serve every stylesheet separately, preventing incorrect order of CSS and bringing performance to the previous low.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003ePros\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eCSS Modules usage stayed the same;\u003c/li\u003e\n  \u003cli\u003eFewer requests for critical resources resulted in noticeable improvement in FCP metric.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003eCons\u003c/strong\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eExtra work is needed to keep Metrum packages versions aligned;\u003c/li\u003e\n  \u003cli\u003eUpdating Metrum dependencies becomes much harder as it requires synchronization between all of the components on a certain page;\u003c/li\u003e\n  \u003cli\u003eAll of the above meant that we only managed to enable this feature on the most popular of routes.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe knew there was going to be additional effort to maintain this solution but the performance gains outweighed the cost at that time. It took almost a year of tedious work from multiple teams to keep the look and feel of Allegro up to date with newest changes, until we came up with another idea.\u003c/p\u003e\n\n\u003ch3 id=\"just-in-time-bundling\"\u003eJust In Time Bundling\u003c/h3\u003e\n\n\u003cp\u003eIn the beginning of 2021 another idea started to form. This time we wanted to unlock the agile nature of our Micro Frontends and their deployment. We came to the conclusion that it would be ideal if instead of serving bundles containing a predefined list of components, we could send one composed of just the files that were actually required to render a certain page. Collecting the list of CSS thatâ€™s needed was not the problem â€” we were generating the HEAD section after all â€” but generating these unique bundles, well that was something different.\u003c/p\u003e\n\n\u003cp\u003eFirst option we had to verify was the possibility to prepare all of the bundles beforehand so they can be picked and served from CDN. Sadly, taking into account that there are around 500 components, any of which can either be used as a building block of a certain page or not, gives us 2\u003csup\u003e500\u003c/sup\u003e combinations which is way more than we can handle. Even if we optimistically assumed that each stylesheet required around 50ms to generate, it would take us roughly 5x10\u003csup\u003e141\u003c/sup\u003e years to cover everything. Additionally, it would not only be a waste of time and storage (some components have higher possibility to be used than others) but also at least a portion of the work would have to be redone every time a component is updated, what can happen multiple times a day.\u003c/p\u003e\n\n\u003cp\u003eFinally, we went with a different approach by implementing a bundler microservice. Its operating principle can be explained in a few steps:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eWhen user makes a request for a page, our server collects a list of CSS files that would normally be added to the HEAD section;\u003c/li\u003e\n  \u003cli\u003eIt sends this list of files to our microservice asking for an URL to the corresponding bundle that contains them;\u003c/li\u003e\n  \u003cli\u003eThe microservice checks if it has the bundle in its cache:\n    \u003col\u003e\n      \u003cli\u003eIf it does, then bundle URL is immediately returned;\u003c/li\u003e\n      \u003cli\u003eOtherwise, it also responds right away with empty result, triggering bundle generation in the background;\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/li\u003e\n  \u003cli\u003eBased on the response from the microservice, the server either embeds separate CSS files as usual or replaces them with a single bundle.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThis is where we are now â€“ generating only what is actually needed and keeping the duration overhead minimal. A lot of thought and multiple iterations went into making it possible, so I think you can expect a completely separate article about this microservice in the future. Most important thing for us is that the trend of constant improvement for our users continues as confirmed by \u003ca href=\"https://developers.google.com/web/tools/chrome-user-experience-report/\"\u003eChrome UX Report\u003c/a\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-07-29-css-architecture-and-performance-of-micro-frontends/fcp-in-crux.png\" alt=\"FCP according to CrUX over last 10 months\" title=\"FCP according to CrUX over last 10 months\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\n\u003cp\u003eCSS architecture is one of the most important factors influencing performance, what makes ignoring it harder and harder as the page grows. Fortunately, our experience shows that even in higher-scale systems built using micro frontends, it is still possible to improve successfully. By solving problems of existing solutions and experimenting with new ideas we are able to constantly raise the bar of our metrics making browsing Allegro a better experience for our users month by month.\u003c/p\u003e\n","contentSnippet":"Itâ€™s been over 5 years since the introduction of the article describing the ongoing transformation of Allegroâ€™s frontend architecture â€” an approach that was later formalized by the industry under the name of Micro Frontends. I think that after all this time we can safely say that this direction was correct and remained almost entirely unchanged in relation to the original idea. Still, some of the challenges foreseen in the publication soon became the reality. In this article I would like to focus on the CSS part of the whole adventure to tell you about how we manage consistency and frontend performance across over half a thousand components, and what it took us to get to where we stand today.\nNew Approach â€” New Challenges\nHandling all the dependencies, libraries and visual compatibility when the entire website resides in a single repository is a challenge by itself. The level of difficulty increases even more, when there are hundreds of said repositories, each managed by a different team and tooling. When in such situation, one of the things that quickly become apparent is the need for some kind of guidelines around the look of various aspects of components being developed like color scheme, spacing, fonts etc. â€” those are exactly the reasons why the Metrum Design System came to life.\n\nIn its initial form â€” apart from visual examples and design resources â€” Metrum was providing reusable PostCSS mixins that every developer could install via separate npm packages and include in the component they were working on.\n\n@import 'node_modules/@metrum/button/css/mixins.css';\n\n.button {\n    @mixin m-button;\n    background-color: black;\n}\n\n\n\n\u003cbutton class=\"button\"\u003e\u003c/button\u003e\n\n\nIf we try to evaluate that approach we could come up with following pros and cons:\nPros\nEasy to use â€” install mixin and include in componentâ€™s selector;\nMixins allow for sharing visual identity between components;\nDevelopers can use mixins in any version without depending on other parts of the page;\nEvery component ships with a complete set of styles.\nCons\nIncluding mixins introduces duplication of CSS rules between components used on the same page;\nMore files â€” every component brings at least one request for its styles;\nNo sharing of CSS â€” no cache reuse between pages built from different components;\nClashing of class names within the global namespace.\nIn summary, while being very flexible and easy to use, mixins-based approach was not ideal from a performance point of view. Every time when somebody wanted to use a button, input, link etc., they would have to include a mixin for it pulling the entire set of CSS rules to their stylesheet. This resulted in our users downloading unnecessary kilobytes during the first visit while bringing no caching benefit when navigating through other pages which in turn increased rendering times. We knew we could do better.\nEnter CSS Modules\nAfter a lot of brainstorming, a decision was made that the next step should involve Metrum making use of CSS Modules. While the technical aspects and usage were changing as the adoption grew, the main principles stayed the same up to this day. Currently, whenever any developer wants to assemble a new component out of Metrum building blocks, they can install desired packages, compose styles from them and declare used classes in their markup:\n\n.button {\n    composes: m-button from '@metrum/button';\n    composes: m-background-color-black from '@metrum/color';\n}\n\n\n\nimport * as styles from './styles.css';\n\nexport default function render() {\n    return `\n        \u003cbutton class=\"${styles.button}\"\u003e...\u003c/button\u003e\n    `;\n}\n\n\nThanks to the fact that all of our micro frontends run on Node.js, this approach can be used quite easily with the majority of tooling available. The only thing left to do is to collect all of the required Metrum stylesheets during render in our facade server called opbox-web and embed them on the page with the correct order. Ordering requirement is important, because we follow atomic design and more complicated components (molecules, organisms) are built using simpler ones (atoms). Lets see what all of those changes did to our list of tradeoffs:\nPros\nStill easy to use â€” install package and compose desired classes in your component;\nSharing classes means sharing visual traits which was one of our goals;\nStyles for certain module only appear once per page if used in multiple components;\nEach Metrum stylesheet can be cached by the browser separately and reused on different pages;\nDevelopers can still use packages in any version without depending on other parts of the page.\nCons\nAdditional logic has to be maintained that extracts needed Metrum stylesheets from components and adds them to the page once;\nAbove logic has to also take care of sorting so the order of styles is correct and we donâ€™t run into problems with cascade;\nMultiple versions of the same Metrum component may be needed on the page;\nMore and more requests have to be made as components transition to the new approach.\nJudging from the upsides the transition was worth it: despite higher maintenance effort we were finally able to share common CSS code between components, the amount of downloaded data as well as render times started decreasing. Unfortunately, after some time we started to see a worrying trend related to the number of embedded stylesheets. Prior to this change, it was roughly equal to the number of components used on the page. Afterwards, with additional Metrum modules, plus the fact that multiple versions of them may be needed, we ended up with as much as around 100 requests for render-blocking CSS.\nUsually, when we bring up the issue of excessive number of requests people respond with â€œSo what? You have HTTP/2, right?â€ and yes, we do. Itâ€™s true that the user agent will reuse existing connections for multiple files but the limit of concurrent streams does exist, latency is still going to affect each one of them and the compression efficiency will be worse especially for those relatively small files like ours. We had to come up with yet another idea for improvement.\nLet the Bundle Begin\nAs I touched briefly earlier, we have the opbox-web â€” a place thatâ€™s already responsible for extracting, sorting and embedding Metrum dependencies. We figured that instead of adding each of them separately, we could prepare predefined bundles that would serve as replacements. We did as planned and, after deployment on 6th of July 2020, achieved 15% improvement in FCP metric time, which means that our users saw the first render of content faster by almost half a second.\n\nImprovement was satisfactory, but it came at a certain cost. From that time on we had to make sure all of the components used on a certain page share the same versions of Metrum modules supported by the bundle and I assure you, it was bothersome to say the least. Monitoring that nobody updated their dependency by accident was one thing (especially that we managed to automate it) but undergoing a process of actually wanting to do this was another. In addition, every time we failed within that area we had to bail and serve every stylesheet separately, preventing incorrect order of CSS and bringing performance to the previous low.\nPros\nCSS Modules usage stayed the same;\nFewer requests for critical resources resulted in noticeable improvement in FCP metric.\nCons\nExtra work is needed to keep Metrum packages versions aligned;\nUpdating Metrum dependencies becomes much harder as it requires synchronization between all of the components on a certain page;\nAll of the above meant that we only managed to enable this feature on the most popular of routes.\nWe knew there was going to be additional effort to maintain this solution but the performance gains outweighed the cost at that time. It took almost a year of tedious work from multiple teams to keep the look and feel of Allegro up to date with newest changes, until we came up with another idea.\nJust In Time Bundling\nIn the beginning of 2021 another idea started to form. This time we wanted to unlock the agile nature of our Micro Frontends and their deployment. We came to the conclusion that it would be ideal if instead of serving bundles containing a predefined list of components, we could send one composed of just the files that were actually required to render a certain page. Collecting the list of CSS thatâ€™s needed was not the problem â€” we were generating the HEAD section after all â€” but generating these unique bundles, well that was something different.\nFirst option we had to verify was the possibility to prepare all of the bundles beforehand so they can be picked and served from CDN. Sadly, taking into account that there are around 500 components, any of which can either be used as a building block of a certain page or not, gives us 2500 combinations which is way more than we can handle. Even if we optimistically assumed that each stylesheet required around 50ms to generate, it would take us roughly 5x10141 years to cover everything. Additionally, it would not only be a waste of time and storage (some components have higher possibility to be used than others) but also at least a portion of the work would have to be redone every time a component is updated, what can happen multiple times a day.\nFinally, we went with a different approach by implementing a bundler microservice. Its operating principle can be explained in a few steps:\nWhen user makes a request for a page, our server collects a list of CSS files that would normally be added to the HEAD section;\nIt sends this list of files to our microservice asking for an URL to the corresponding bundle that contains them;\nThe microservice checks if it has the bundle in its cache:\n    \nIf it does, then bundle URL is immediately returned;\nOtherwise, it also responds right away with empty result, triggering bundle generation in the background;\nBased on the response from the microservice, the server either embeds separate CSS files as usual or replaces them with a single bundle.\nThis is where we are now â€“ generating only what is actually needed and keeping the duration overhead minimal. A lot of thought and multiple iterations went into making it possible, so I think you can expect a completely separate article about this microservice in the future. Most important thing for us is that the trend of constant improvement for our users continues as confirmed by Chrome UX Report:\n\nSummary\nCSS architecture is one of the most important factors influencing performance, what makes ignoring it harder and harder as the page grows. Fortunately, our experience shows that even in higher-scale systems built using micro frontends, it is still possible to improve successfully. By solving problems of existing solutions and experimenting with new ideas we are able to constantly raise the bar of our metrics making browsing Allegro a better experience for our users month by month.","guid":"https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html","categories":["tech","webperf","frontend","performance","perfmatters","css"],"isoDate":"2021-07-28T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Making API calls a seamless user experience","link":"https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html","pubDate":"Wed, 21 Jul 2021 00:00:00 +0200","authors":{"author":[{"name":["PaweÅ‚ Wolak"],"photo":["https://blog.allegro.tech/img/authors/pawel.wolak.jpg"],"url":["https://blog.allegro.tech/authors/pawel.wolak"]}]},"content":"\u003cp\u003eAlmost every modern web application somehow interacts with a backend - be it loading data, doing background sync, submitting a form, or publishing the metrics.\nMaking API requests is not an easy task - we have to consider multiple outcomes and handle them properly. Otherwise, we might end up with confused users and\ndecreased conversion. Although the stakes are high, it is still very likely to encounter an application designed with only a happy path scenario in\nmind. The question is - how can we improve it?\u003c/p\u003e\n\n\u003ch3 id=\"make-the-request-state-visible\"\u003eMake the request state visible\u003c/h3\u003e\n\u003cp\u003eBack in the old days submitting a form would result in a full page reload. Until the page was ready, there was a clear sign that something was happening. If\nsomething went wrong typically there was an unstyled generic error message.\u003c/p\u003e\n\n\u003cp\u003eThis approach served its purpose very well - it was easy to tell that the page was still loading and it was easy to tell when there was an error. Then\n\u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/Guide/AJAX\"\u003eAJAX\u003c/a\u003e became popular, bringing with its benefits also certain drawbacks - it was up to the\nprogrammer to handle loading and error state indicators, which were often omitted. To prevent userâ€™s confusion, you should always remember about\nclearly presenting the request state to the user.\u003c/p\u003e\n\n\u003ch3 id=\"retry-failed-requests\"\u003eRetry failed requests\u003c/h3\u003e\n\u003cp\u003eAs mentioned above, errors do happen sometimes. Usually, users are faced with a message and an option to retry the request. This approach is far better than\nfailing silently and not informing the user, but still can lead to abandoned actions. Maybe we can do better than that?\u003c/p\u003e\n\n\u003cp\u003eWhat if instead of asking the user to retry the failed request, we could do it automatically? There is another follow-up question: is every request worth\nretrying? Imagine a server responding with status code \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\"\u003e403 (forbidden)\u003c/a\u003e,\n\u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\"\u003e422 (unprocessable entity)\u003c/a\u003e, or 4xx in general. These are the clientâ€™s faults and usually\nretrying those requests will yield the same result. Now letâ€™s consider 5xx status codes, which are often caused by temporary database unavailability or server\nresource exhaustion. Especially in multi-server environments, chances are that the next request will be rerouted to a healthy instance, resulting in a\nsuccessful response.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, in case of increased traffic, repeating failed requests instantly could possibly make things worse. In order to prevent that, it is a good\npractice to introduce an exponential delay between consecutive retry attempts. Another solution, more common in inter-service communication, is a circuit breaker\nmechanism which prevents further requests until service becomes available.\u003c/p\u003e\n\n\u003cp\u003eAlso, keep in mind that network conditions might not be stable, particularly on mobile devices. If there is no connection, instead of making pointless API\ncalls you could queue the requests and observe\n\u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/NavigatorOnLine/Online_and_offline_events\"\u003eonline/offline events\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eFinally, not every request is safe to retry. Sometimes when you receive 5xx, there is no guarantee that it has not been processed. Imagine retrying a request\nto make a money transfer from one account to another - handling it twice would be a disaster! In order to prevent these mistakes from happening, you have to\nmake sure your API is \u003ca href=\"https://developer.mozilla.org/en-US/docs/Glossary/Idempotent\"\u003eidempotent\u003c/a\u003e. This is usually achieved by using adequate HTTP methods\n(like \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PUT\"\u003ePUT\u003c/a\u003e) or passing an additional identifier with the request.\u003c/p\u003e\n\n\u003ch3 id=\"do-not-wait-forever\"\u003eDo not wait forever\u003c/h3\u003e\n\u003cp\u003eHave you ever wondered how long it takes before an API call times out due to no response from the server? If not, I have bad news for you -\n\u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/timeout\"\u003ethe default timeout value in XMLHttpRequest is 0\u003c/a\u003e, which basically means the browser will wait forever.\nWith \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\"\u003efetch\u003c/a\u003e there is no parameter responsible for timeouts - it relies on browser defaults (which\nis 300 seconds for \u003ca href=\"https://source.chromium.org/chromium/chromium/src/+/master:net/socket/client_socket_pool.cc;l=29\"\u003eChrome\u003c/a\u003e and\n\u003ca href=\"https://searchfox.org/mozilla-central/source/netwerk/protocol/http/nsHttpHandler.cpp#219\"\u003eFirefox\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe good news is that you can actually implement timeout functionality using\n\u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/AbortController\"\u003eAbortController\u003c/a\u003e:\u003c/p\u003e\n\n\u003cdiv class=\"language-javascript highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003econtroller\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nx\"\u003eAbortController\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003etimeout\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003esetTimeout\u003c/span\u003e\u003cspan class=\"p\"\u003e(()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nx\"\u003econtroller\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eabort\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e \u003cspan class=\"mi\"\u003e3000\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e \u003cspan class=\"c1\"\u003e// 3s timeout\u003c/span\u003e\n\n\u003cspan class=\"nx\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003e/api/something\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"na\"\u003esignal\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003econtroller\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003esignal\u003c/span\u003e \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ethen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"nx\"\u003eclearTimeout\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003etimeout\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e// process the response\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAnother useful application of AbortController is cancelling straggler requests (requests still in progress at a point when the response is no longer needed).\nThis is particularly useful in libraries like \u003ca href=\"https://reactjs.org/\"\u003eReact\u003c/a\u003e where receiving a response after unmounting a component results in an error:\u003c/p\u003e\n\n\u003cdiv class=\"language-javascript highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nx\"\u003euseEffect\u003c/span\u003e\u003cspan class=\"p\"\u003e(()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003econtroller\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nx\"\u003eAbortController\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\n  \u003cspan class=\"nx\"\u003efetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003e/api/something\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"na\"\u003esignal\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003econtroller\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003esignal\u003c/span\u003e \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003ethen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eresponse\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"c1\"\u003e// process the response\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\n  \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nx\"\u003econtroller\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003eabort\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e \u003cspan class=\"c1\"\u003e// cancel the request when un-mounting the component\u003c/span\u003e\n\u003cspan class=\"p\"\u003e},\u003c/span\u003e \u003cspan class=\"p\"\u003e[]);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3 id=\"be-optimistic\"\u003eBe optimistic\u003c/h3\u003e\n\u003cp\u003eAs funny as it may sound, there is an actual pattern called â€œoptimistic UIâ€. The idea behind it is very straightforward: given that most of the time an API\nrequest will result in a successful response, we can skip the â€œloadingâ€ part and go straight to a result stage. In the unlikely event of failure, we can still\nrollback our changes and inform the user about the error.\u003c/p\u003e\n\n\u003cp\u003eLetâ€™s consider a popular example of a counter (eg. Facebook/Twitter like button). For the sake of simplicity I will skip the actual request and return a\npromise after 1 second so that approximately 25% of requests will fail:\u003c/p\u003e\n\n\u003cdiv class=\"language-html highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eCounter value: \u003cspan class=\"nt\"\u003e\u0026lt;span\u003c/span\u003e \u003cspan class=\"na\"\u003eid=\u003c/span\u003e\u003cspan class=\"s\"\u003e\"counter\"\u003c/span\u003e\u003cspan class=\"nt\"\u003e\u0026gt;\u003c/span\u003e0\u003cspan class=\"nt\"\u003e\u0026lt;/span\u0026gt;\u003c/span\u003e \u003cspan class=\"nt\"\u003e\u0026lt;button\u003c/span\u003e \u003cspan class=\"na\"\u003eid=\u003c/span\u003e\u003cspan class=\"s\"\u003e\"button\"\u003c/span\u003e\u003cspan class=\"nt\"\u003e\u0026gt;\u003c/span\u003eIncrease\u003cspan class=\"nt\"\u003e\u0026lt;/button\u0026gt;\u003c/span\u003e\n\n\u003cspan class=\"nt\"\u003e\u0026lt;script\u0026gt;\u003c/span\u003e\n  \u003cspan class=\"kd\"\u003elet\u003c/span\u003e \u003cspan class=\"nx\"\u003ecounter\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n  \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003eupdateView\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003edocument\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003equerySelector\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003e#counter\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nx\"\u003einnerText\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nx\"\u003ecounter\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n  \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003emakeRequest\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nb\"\u003ePromise\u003c/span\u003e\u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"nx\"\u003eresolve\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nx\"\u003ereject\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003econst\u003c/span\u003e \u003cspan class=\"nx\"\u003eoutcome\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003eMath\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003erandom\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.25\u003c/span\u003e \u003cspan class=\"p\"\u003e?\u003c/span\u003e \u003cspan class=\"nx\"\u003eresolve\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nx\"\u003ereject\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"nx\"\u003esetTimeout\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003eoutcome\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1000\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\n  \u003cspan class=\"nb\"\u003edocument\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nx\"\u003equerySelector\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003e#button\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nx\"\u003eaddEventListener\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"s2\"\u003eclick\u003c/span\u003e\u003cspan class=\"dl\"\u003e\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"k\"\u003easync\u003c/span\u003e \u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"nx\"\u003ecounter\u003c/span\u003e\u003cspan class=\"o\"\u003e++\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"nx\"\u003eupdateView\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003etry\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"k\"\u003eawait\u003c/span\u003e \u003cspan class=\"nx\"\u003emakeRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"k\"\u003ecatch\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nx\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n      \u003cspan class=\"nx\"\u003ecounter\u003c/span\u003e\u003cspan class=\"o\"\u003e--\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n      \u003cspan class=\"nx\"\u003eupdateView\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n  \u003cspan class=\"p\"\u003e});\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e\u0026lt;/script\u0026gt;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eBy simply assuming the positive outcome we drastically reduce the amount of time spent on the interactions, making the whole experience more swift. However,\ndue to the risk of so-called \u003ca href=\"https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error\"\u003efalse positives\u003c/a\u003e this pattern should not\nbe applied to critical actions (for example, making a reservation). If you are further interested I recommend reading a\n\u003ca href=\"https://www.smashingmagazine.com/2016/11/true-lies-of-optimistic-user-interfaces/\"\u003emore comprehensive article about optimistic UI\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\u003cp\u003eIn this article, I presented the basics of requests handling. Some of these ideas (like retries and timeouts) also apply to the backend service-to-service\ncommunication. Additionally, there are other interesting techniques which I encourage you to study (eg. preconnecting, batching). In the end, in my opinion,\neverything that improves UX is worth a try.\u003c/p\u003e\n","contentSnippet":"Almost every modern web application somehow interacts with a backend - be it loading data, doing background sync, submitting a form, or publishing the metrics.\nMaking API requests is not an easy task - we have to consider multiple outcomes and handle them properly. Otherwise, we might end up with confused users and\ndecreased conversion. Although the stakes are high, it is still very likely to encounter an application designed with only a happy path scenario in\nmind. The question is - how can we improve it?\nMake the request state visible\nBack in the old days submitting a form would result in a full page reload. Until the page was ready, there was a clear sign that something was happening. If\nsomething went wrong typically there was an unstyled generic error message.\nThis approach served its purpose very well - it was easy to tell that the page was still loading and it was easy to tell when there was an error. Then\nAJAX became popular, bringing with its benefits also certain drawbacks - it was up to the\nprogrammer to handle loading and error state indicators, which were often omitted. To prevent userâ€™s confusion, you should always remember about\nclearly presenting the request state to the user.\nRetry failed requests\nAs mentioned above, errors do happen sometimes. Usually, users are faced with a message and an option to retry the request. This approach is far better than\nfailing silently and not informing the user, but still can lead to abandoned actions. Maybe we can do better than that?\nWhat if instead of asking the user to retry the failed request, we could do it automatically? There is another follow-up question: is every request worth\nretrying? Imagine a server responding with status code 403 (forbidden),\n422 (unprocessable entity), or 4xx in general. These are the clientâ€™s faults and usually\nretrying those requests will yield the same result. Now letâ€™s consider 5xx status codes, which are often caused by temporary database unavailability or server\nresource exhaustion. Especially in multi-server environments, chances are that the next request will be rerouted to a healthy instance, resulting in a\nsuccessful response.\nOn the other hand, in case of increased traffic, repeating failed requests instantly could possibly make things worse. In order to prevent that, it is a good\npractice to introduce an exponential delay between consecutive retry attempts. Another solution, more common in inter-service communication, is a circuit breaker\nmechanism which prevents further requests until service becomes available.\nAlso, keep in mind that network conditions might not be stable, particularly on mobile devices. If there is no connection, instead of making pointless API\ncalls you could queue the requests and observe\nonline/offline events.\nFinally, not every request is safe to retry. Sometimes when you receive 5xx, there is no guarantee that it has not been processed. Imagine retrying a request\nto make a money transfer from one account to another - handling it twice would be a disaster! In order to prevent these mistakes from happening, you have to\nmake sure your API is idempotent. This is usually achieved by using adequate HTTP methods\n(like PUT) or passing an additional identifier with the request.\nDo not wait forever\nHave you ever wondered how long it takes before an API call times out due to no response from the server? If not, I have bad news for you -\nthe default timeout value in XMLHttpRequest is 0, which basically means the browser will wait forever.\nWith fetch there is no parameter responsible for timeouts - it relies on browser defaults (which\nis 300 seconds for Chrome and\nFirefox.\nThe good news is that you can actually implement timeout functionality using\nAbortController:\n\nconst controller = new AbortController();\nconst timeout = setTimeout(() =\u003e controller.abort(), 3000); // 3s timeout\n\nfetch(\"/api/something\", { signal: controller.signal })\n  .then(response =\u003e {\n    clearTimeout(timeout);\n    // process the response\n  });\n\n\nAnother useful application of AbortController is cancelling straggler requests (requests still in progress at a point when the response is no longer needed).\nThis is particularly useful in libraries like React where receiving a response after unmounting a component results in an error:\n\nuseEffect(() =\u003e {\n  const controller = new AbortController();\n\n  fetch(\"/api/something\", { signal: controller.signal })\n    .then(response =\u003e {\n      // process the response\n    });\n\n  return () =\u003e controller.abort(); // cancel the request when un-mounting the component\n}, []);\n\n\nBe optimistic\nAs funny as it may sound, there is an actual pattern called â€œoptimistic UIâ€. The idea behind it is very straightforward: given that most of the time an API\nrequest will result in a successful response, we can skip the â€œloadingâ€ part and go straight to a result stage. In the unlikely event of failure, we can still\nrollback our changes and inform the user about the error.\nLetâ€™s consider a popular example of a counter (eg. Facebook/Twitter like button). For the sake of simplicity I will skip the actual request and return a\npromise after 1 second so that approximately 25% of requests will fail:\n\nCounter value: \u003cspan id=\"counter\"\u003e0\u003c/span\u003e \u003cbutton id=\"button\"\u003eIncrease\u003c/button\u003e\n\n\u003cscript\u003e\n  let counter = 0;\n\n  const updateView = () =\u003e document.querySelector(\"#counter\").innerText = counter;\n\n  const makeRequest = () =\u003e new Promise((resolve, reject) =\u003e {\n    const outcome = Math.random() \u003e 0.25 ? resolve : reject;\n    setTimeout(outcome, 1000);\n  });\n\n  document.querySelector(\"#button\").addEventListener(\"click\", async () =\u003e {\n    counter++;\n    updateView();\n\n    try {\n      await makeRequest();\n    } catch (e) {\n      counter--;\n      updateView();\n    }\n  });\n\u003c/script\u003e\n\n\nBy simply assuming the positive outcome we drastically reduce the amount of time spent on the interactions, making the whole experience more swift. However,\ndue to the risk of so-called false positives this pattern should not\nbe applied to critical actions (for example, making a reservation). If you are further interested I recommend reading a\nmore comprehensive article about optimistic UI.\nSummary\nIn this article, I presented the basics of requests handling. Some of these ideas (like retries and timeouts) also apply to the backend service-to-service\ncommunication. Additionally, there are other interesting techniques which I encourage you to study (eg. preconnecting, batching). In the end, in my opinion,\neverything that improves UX is worth a try.","guid":"https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html","categories":["javascript","frontend","ajax","restapi","http"],"isoDate":"2021-07-20T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"One task â€” two solutions: Apache Spark or Apache Beam?","link":"https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html","pubDate":"Mon, 28 Jun 2021 00:00:00 +0200","authors":{"author":[{"name":["Yevgeniya Li"],"photo":["https://blog.allegro.tech/img/authors/yevgeniya.li.jpg"],"url":["https://blog.allegro.tech/authors/yevgeniya.li"]}]},"content":"\u003cp\u003eSome time ago, our team faced the issue of moving an existing \u003ca href=\"https://spark.apache.org\"\u003eApache Spark\u003c/a\u003e job from an on-premise Hadoop cluster to public cloud.\nWhile working on the transition we came across another way to process data that is \u003ca href=\"https://beam.apache.org\"\u003eApache Beam\u003c/a\u003e. We were curious whether this tool had\nmore advantages in comparison to traditional Apache Spark. We wanted to find the answer relatively quickly with minimal effort. Hence, we built two projects to\nprocess the same data using these technologies. Below you can get to know the architecture of the jobs written in Apache Spark and Apache Beam.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGiven: 4 input tables (~2.5 TB/day).\u003c/li\u003e\n  \u003cli\u003eTask: join and clean data.\u003c/li\u003e\n  \u003cli\u003eResult: 4 output tables.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-28-1-task-2-solutions-spark-or-beam/bigdata-projects-architecture.png\" alt=\"scripting\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNote: Below I described our solution and used tools and technologies which do not pretend to be 100% the right approach so your results might be different.\u003c/p\u003e\n\n\u003ch3 id=\"programming-model\"\u003eProgramming model\u003c/h3\u003e\n\n\u003cp\u003eThe first comparison is about showing the way these technologies are built as capturing the general concept plays the first role in writing the code for data\nprocessing.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Based on the in-memory \u003ca href=\"https://en.wikipedia.org/wiki/MapReduce\"\u003eMapReduce\u003c/a\u003e model evolved from Hadoop MapReduce. This fact forces developers\n        to take care more of executors' memory, as mapping operations occur there. And you still need to remember about shuffling at the Join moment and how to\n        split data to make right repartitioning (too big data chunk causes overloading on machines, too small means more shuffling over the net). How did we tune\n        this job? The only way we knew how to do it was applying some \"a-la best practices\" Spark configurations, running the job with production data volume\n        (just imagine how it reflects machine resource cost!), looking at the metrics and crossing fingers that executors will stand. Finally after n-th attempt\n        it started working.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        It pretends to be a unified processing model. What does it mean? Apache Beam is based on so-called abstract pipelines that can be run on different\n        executors (you can even switch to the Spark execution environment). In our case we're using a DataFlow runner. This pipeline includes every stage of\n        processing starting from data fetching, transformation, ending with the result output. With these pipelines Apache Beam hides low-level things like\n        shuffling, repartitioning, etc. from the developer. Additionally we used \u003ca href=\"https://github.com/allegro/bigflow\"\u003eBigFlow\u003c/a\u003e open-source framework\n        developed internally in Allegro which is built to support Apache Beam data processing technology (simplifies building packages, configuration and\n        deployment processes).\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Apache Beam looks more like a framework as it abstracts the complexity of processing and hides technical details, and Spark is the\ntechnology where you literally need to dive deeper.\u003c/p\u003e\n\n\u003ch3 id=\"programming-languages-and-build-tools\"\u003eProgramming languages and build tools\u003c/h3\u003e\n\n\u003cp\u003eHere I do not want to spread hate and discuss which programming language is the best one for data processing, it is the matter of taste. I just would like to\nprovide some context of our teamâ€™s background to give you better understanding of our preferences: most of us are specialised in the software development in\nKotlin so we were a little bit biased against non JVM-based languages before starting this task.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        For the main processing part we chose Scala as it looks similar to our background in Java, although there is PySpark - version in Python. Python was\n        used only for composing a DAG file, which is basically a description of steps to be performed by \u003ca href=\"https://airflow.apache.org/\"\u003eAirflow\u003c/a\u003e\n        (the tool which automates running the job in the cluster).\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Natively available in Java and Python, but since we took advantage of the BigFlow framework, Python was used for everything.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Because of Scala, \u003ca href=\"https://www.scala-sbt.org\"\u003esbt\u003c/a\u003e is used for building the package and running the tests. This can be more beneficial for\n        Java developers who are familiar with Gradle: it uses the build.sbt configuration file, which like build.gradle declares dependencies, the Scala version\n        to use, etc.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        \u003ca href=\"https://packaging.python.org/key_projects/#setuptools\"\u003eSetuptools\u003c/a\u003e (tool for building Python packages) is run by script.py build script\n        provided by BigFlow. This framework also provides an additional mechanism for managing dependencies which is based on the standard\n        \u003ca href=\"https://github.com/jazzband/pip-tools\"\u003epip-tool\u003c/a\u003e. Additionally running tests with BigFlow CLI is more convenient and faster compared with sbt.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Scala is much closer to our team (mostly Java developers), especially at the beginning when we needed to get used to working with Python\nwithout static typing. Also Scala is more natural for Spark, so all upcoming features will be firstly supported for this programming language, educational\nresources and examples in Scala for Spark are more exhaustive.\u003c/p\u003e\n\n\u003ch3 id=\"batch-and-stream-data-processing\"\u003eBatch and stream data processing\u003c/h3\u003e\n\n\u003cp\u003eIn our projects we did not implement stream data processing, since we have batch processing only, but anyway it is probable that business requirements could\nchange so we must also consider how to do it in a better way.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Two different APIs for batch/stream data processing. You need to split data yourself by grouping by the time and it is not truly real-time processing,\n        as basically Spark divides the data stream into micro batches of X seconds called Dstreams, which is a sequence of RDDs under the hood.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Due to the unified model, processing is carried out in the same way for both batch and stream data. An additional bonus here is the useful feature for\n        windowing (a way to split data during stream processing), watermarks and triggers (handling events that come late or out-of-order).\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: although we didnâ€™t try it ourselves, weâ€™d bet on Apache Beam in this comparison. It looks more adapted for Streaming than Spark.\nAlthough there is a streaming extension since Spark 2.2 but libraries of streaming functions are quite limited. For us this means more efforts to apply.\u003c/p\u003e\n\n\u003ch3 id=\"testing\"\u003eTesting\u003c/h3\u003e\n\n\u003cp\u003eAlong with the development process these projects were getting more complicated - so we realized the unit testing is not enough to make us feel safe. So below\nare our researches how to ensure the jobs will be able to handle big load in a production environment.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:20%;\"\u003e\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%; text-align: center;\"\u003eUnit tests (local environment without access to real infrastructure, validation of business logic)\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e\n        \u003ca href=\"http://spark.apache.org/docs/latest\"\u003eEmbedded Spark\u003c/a\u003e\n        \u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e\n        \u003ca href=\"https://beam.apache.org/documentation/pipelines/test-your-pipeline/\"\u003eTestPipeline\u003c/a\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%; text-align: center;\"\u003eIntegration tests (validation of proper reads/writes from/to BigQuery)\u003c/td\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eNo official support for mocking BigQuery/Google Storage or embedded/test container version\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%; text-align: center;\"\u003eE2E tests (ensure job is executed, run in Cloud and has integration with GCP infrastructure)\u003c/td\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003e\n        Due to the missing implementation of testing the whole flow, we were running locally jobs to load production data, process them and store into BigQuery\n        tables for development purposes. This way we were ensuring introduced changes did not impact performance and both return identical results.\n        Note: recently BigFlow added a \u003ca href=\"https://github.com/allegro/bigflow/blob/master/docs/e2e_testing.md\"\u003esolution\u003c/a\u003e by setting real Dataflow/BigQuery\n        infrastructure while running E2E tests\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: a solution is to minimize the ratio of missing test coverage and isolate classes responsible for I/O operations to external storages\n(Google Storage and BigQuery) as much as possible. Big issue is to provide these dependencies while running tests inside CI pipeline but this is completely\nanother story. In both cases, it is not trivial task. So, for the sake of simplicity we were ensuring there are no performance issues and jobs are not broken by\nrunning them on a dev environment using the production data which also took us a lot of time.\u003c/p\u003e\n\n\u003ch3 id=\"local-run\"\u003eLocal run\u003c/h3\u003e\n\n\u003cp\u003eLocal run in our case does not mean 100% execution on the laptop: job is triggered from the local machine, but actually it has a place on the real cluster\nin Google Cloud.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Natively via Spark submit command. In our case we automated this by using Terraform: setting necessary infrastructure in GCP and running the job on the\n        cluster.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%; text-align: center;\"\u003e\n        BigFlow CLI\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: no absolute winner here, depends on your preferences. Beside this we found Terraform to be a fine separate tool to set up a local\nenvironment for running the job and could be used for Apache Beam job as well.\u003c/p\u003e\n\n\u003ch3 id=\"running-on-the-gcp\"\u003eRunning on the GCP\u003c/h3\u003e\n\n\u003cp\u003eOnce we are done with the local development, we are good to go to the Cloud! However, even here, not everything is so clear as we have two services to run our\njobs and we described our considerations about them below.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%; vertical-align: top;\"\u003e\n        \u003ca href=\"https://cloud.google.com/dataproc\"\u003eDataproc\u003c/a\u003e (Hadoop under the hood which is required to run Spark). As we do not need to have it running all the time, we deployed it for each job\n        execution, luckily we again have benefited from Terraform to do it. Also we used it to spin up the network, subnetwork, router and other things that are\n        needed to run the Spark job within Dataproc, so I would generally recommend it as a useful tool to automate things in GCP. Dataproc has autoscaling\n        feature, but it requires more actions: creating autoscaling policy in GCP and integrating it into the job. Moreover, to achieve good performance we\n        needed to play a lot with Spark configuration like tuning memory on workers, choosing appropriate number of shuffle partitions, executor instances and\n        so on.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        \u003ca href=\"https://cloud.google.com/dataflow\"\u003eDataflow\u003c/a\u003e. Due to its serverless nature we didnâ€™t need to set up a cluster each time we wanted to process data. The next big advantage\n        of Dataflow is the Shuffle service, which addresses the shuffle issue on Spark executors as it moves heavy operation out of the worker virtual machine\n        to the service backend. Besides, there is autoscaling out-of-the-box, Streaming engine for streaming pipeline support. Generally, Dataflow is supposed\n        to be a self-managed platform, so less effort is required to configure it compared to Dataproc.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Dataflow wins in terms of ease of the setup. However, there is one more drawback here: limitation to run on the public cloud. It was so\nconvenient for us to profit from Dataflow services that it would be hard to find appropriate substitution for them. Furthermore, BigFlow framework\npositions itself as a Python framework for data processing pipelines on GCP. So if we want to migrate to another platform, this would enforce us to\nconfigure another runner so that we are able to run the job properly.\u003c/p\u003e\n\n\u003ch3 id=\"monitoring\"\u003eMonitoring\u003c/h3\u003e\n\n\u003cp\u003eIn addition to the process itself, it is crucial to have an option to observe its workflow, collect metrics and have access to logs from the machines.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Familiar dashboard with Yarn metrics at our disposal - Spark UI is the same that we had in the Hadoop cluster on-premise. The inconvenience was that we\n        had to switch between different tabs in the browser. Another issue was with Spark logs UI view, as if you open it by clicking on the particular running job, you\n        can see only the part of them. The rest can be found in GCP Logger where you need to know how to build queries to fetch them. Also Dataproc does not\n        keep the history of metrics once the cluster is shut down. You need to spin up a separate Spark history server to collect them and then configure its\n        visualisation in GCP Monitoring.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Logs from workers and metrics are displayed on the same UI and are available even after the job is finished.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Dataflow beats Dataproc here and that is it. No comments are required.\u003c/p\u003e\n\n\u003ch3 id=\"cost\"\u003eCost\u003c/h3\u003e\n\n\u003cp\u003eLast but not least is the pricing of used GCP resources needed to process data like setting and running a cluster, storing data and queries execution in\nBigQuery, etc. Here we are most likely talking not only about the money, but also about the time. As it was basically a pilot project, we calculated the cost of\nnon-optimized jobs to see how much it is without significant tuning.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:20%;\"\u003e\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%;\"\u003eCost\u003c/td\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003e\n        Approximately on the same level, slightly in favour of Apache Spark\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%;\"\u003eTime\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e1.5h\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e1h\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: cost is almost the same, however we need to highlight that Spark job has much more space to optimize while Apache Beam job already\ncontains Dataflow optimizations out-of-the-box. For example: playing more with Spark configuration, experimenting with Dataproc workers number, etc, so\nprobably it would cost less and run faster if you know how to tune it properly.\u003c/p\u003e\n\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\n\u003cp\u003eAt the end weâ€™d like to say that Spark Job could be more beneficial if you know it well. Additionally, Dataproc requires advanced skills close to\nthe experienced DevOps engineer to organize all necessary infrastructure. But if you do not have an engineer with deep BigData experience in your team or you\nrun out of time and you are ready to pay a little bit more for out-of-the-box features, then Apache Beam + Dataflow is your choice. Also remember even if you\npay a little bit more, it means that you are saving developersâ€™ time spent on the Spark tweaking that may bring some value.\u003c/p\u003e\n\n\u003cp\u003eBelow you can find how weâ€™d estimate entry level for skills that were necessary for us to develop using the two technologies.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:20%;\"\u003e\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd rowspan=\"3\" style=\"width:20%;\"\u003eProgramming languages\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eScala (Intermediate)\u003c/td\u003e\n        \u003ctd rowspan=\"2\" style=\"width:40%; text-align: center;\"\u003ePython (Intermediate)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e\n        Python (Basic)\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003e\n        SQL (depends on your data and constraints in the executed query, generally it is recommended to load only necessary data)\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%;\" rowspan=\"8\"\u003eTechnology stack\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\" rowspan=\"2\"\u003eApache Spark (Advanced)\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eApache Beam (Intermediate)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eBigFlow (Advanced)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eGoogle Dataproc (Intermediate)\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eGoogle DataFlow (Intermediate) \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eGoogle Logging (Basic)\u003c/td\u003e\n        \u003ctd style=\"width:40%;\"\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eTerraform (Intermediate)\u003c/td\u003e\n        \u003ctd style=\"width:40%;\"\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eGoogle BigQuery (Basic)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eGoogle Composer to schedule jobs in GCP (Basic)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eGoogle Cloud Storage (Basic)\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cstyle type=\"text/css\"\u003e\n.post img{margin: 0 auto;display: block;}\ntd {\n  vertical-align: top;\n}\n@media (max-width: 400px) {\n  table {\n    font-size: 0.8em;\n  }\n}\n@media (max-width: 350px) {\n  table {\n    font-size: 0.6em;\n  }\n}\n\u003c/style\u003e\n\n","contentSnippet":"Some time ago, our team faced the issue of moving an existing Apache Spark job from an on-premise Hadoop cluster to public cloud.\nWhile working on the transition we came across another way to process data that is Apache Beam. We were curious whether this tool had\nmore advantages in comparison to traditional Apache Spark. We wanted to find the answer relatively quickly with minimal effort. Hence, we built two projects to\nprocess the same data using these technologies. Below you can get to know the architecture of the jobs written in Apache Spark and Apache Beam.\nGiven: 4 input tables (~2.5 TB/day).\nTask: join and clean data.\nResult: 4 output tables.\n\nNote: Below I described our solution and used tools and technologies which do not pretend to be 100% the right approach so your results might be different.\nProgramming model\nThe first comparison is about showing the way these technologies are built as capturing the general concept plays the first role in writing the code for data\nprocessing.\nApache Spark\n        Apache Beam\n    \nMapReduce model evolved from Hadoop MapReduce. This fact forces developers\n        to take care more of executors' memory, as mapping operations occur there. And you still need to remember about shuffling at the Join moment and how to\n        split data to make right repartitioning (too big data chunk causes overloading on machines, too small means more shuffling over the net). How did we tune\n        this job? The only way we knew how to do it was applying some \"a-la best practices\" Spark configurations, running the job with production data volume\n        (just imagine how it reflects machine resource cost!), looking at the metrics and crossing fingers that executors will stand. Finally after n-th attempt\n        it started working.\n        \n        \n        It pretends to be a unified processing model. What does it mean? Apache Beam is based on so-called abstract pipelines that can be run on different\n        executors (you can even switch to the Spark execution environment). In our case we're using a DataFlow runner. This pipeline includes every stage of\n        processing starting from data fetching, transformation, ending with the result output. With these pipelines Apache Beam hides low-level things like\n        shuffling, repartitioning, etc. from the developer. Additionally we used BigFlow open-source framework\n        developed internally in Allegro which is built to support Apache Beam data processing technology (simplifies building packages, configuration and\n        deployment processes).\n        \n    \nSummary: Apache Beam looks more like a framework as it abstracts the complexity of processing and hides technical details, and Spark is the\ntechnology where you literally need to dive deeper.\nProgramming languages and build tools\nHere I do not want to spread hate and discuss which programming language is the best one for data processing, it is the matter of taste. I just would like to\nprovide some context of our teamâ€™s background to give you better understanding of our preferences: most of us are specialised in the software development in\nKotlin so we were a little bit biased against non JVM-based languages before starting this task.\nApache Spark\n        Apache Beam\n    \nAirflow\n        (the tool which automates running the job in the cluster).\n        \n        \n        Natively available in Java and Python, but since we took advantage of the BigFlow framework, Python was used for everything.\n        \n    \nsbt is used for building the package and running the tests. This can be more beneficial for\n        Java developers who are familiar with Gradle: it uses the build.sbt configuration file, which like build.gradle declares dependencies, the Scala version\n        to use, etc.\n        \n        \n        Setuptools (tool for building Python packages) is run by script.py build script\n        provided by BigFlow. This framework also provides an additional mechanism for managing dependencies which is based on the standard\n        pip-tool. Additionally running tests with BigFlow CLI is more convenient and faster compared with sbt.\n        \n    \nSummary: Scala is much closer to our team (mostly Java developers), especially at the beginning when we needed to get used to working with Python\nwithout static typing. Also Scala is more natural for Spark, so all upcoming features will be firstly supported for this programming language, educational\nresources and examples in Scala for Spark are more exhaustive.\nBatch and stream data processing\nIn our projects we did not implement stream data processing, since we have batch processing only, but anyway it is probable that business requirements could\nchange so we must also consider how to do it in a better way.\nApache Spark\n        Apache Beam\n    \nSummary: although we didnâ€™t try it ourselves, weâ€™d bet on Apache Beam in this comparison. It looks more adapted for Streaming than Spark.\nAlthough there is a streaming extension since Spark 2.2 but libraries of streaming functions are quite limited. For us this means more efforts to apply.\nTesting\nAlong with the development process these projects were getting more complicated - so we realized the unit testing is not enough to make us feel safe. So below\nare our researches how to ensure the jobs will be able to handle big load in a production environment.\n\n        Apache Spark\n        Apache Beam\n    \nUnit tests (local environment without access to real infrastructure, validation of business logic)\n        \n        Embedded Spark\n        \n        \n        TestPipeline\n        \n    \nIntegration tests (validation of proper reads/writes from/to BigQuery)\n        No official support for mocking BigQuery/Google Storage or embedded/test container version\n    \nE2E tests (ensure job is executed, run in Cloud and has integration with GCP infrastructure)\n        \n        Due to the missing implementation of testing the whole flow, we were running locally jobs to load production data, process them and store into BigQuery\n        tables for development purposes. This way we were ensuring introduced changes did not impact performance and both return identical results.\n        Note: recently BigFlow added a solution by setting real Dataflow/BigQuery\n        infrastructure while running E2E tests\n        \n    \nSummary: a solution is to minimize the ratio of missing test coverage and isolate classes responsible for I/O operations to external storages\n(Google Storage and BigQuery) as much as possible. Big issue is to provide these dependencies while running tests inside CI pipeline but this is completely\nanother story. In both cases, it is not trivial task. So, for the sake of simplicity we were ensuring there are no performance issues and jobs are not broken by\nrunning them on a dev environment using the production data which also took us a lot of time.\nLocal run\nLocal run in our case does not mean 100% execution on the laptop: job is triggered from the local machine, but actually it has a place on the real cluster\nin Google Cloud.\nApache Spark\n        Apache Beam\n    \nSummary: no absolute winner here, depends on your preferences. Beside this we found Terraform to be a fine separate tool to set up a local\nenvironment for running the job and could be used for Apache Beam job as well.\nRunning on the GCP\nOnce we are done with the local development, we are good to go to the Cloud! However, even here, not everything is so clear as we have two services to run our\njobs and we described our considerations about them below.\nApache Spark\n        Apache Beam\n    \nDataproc (Hadoop under the hood which is required to run Spark). As we do not need to have it running all the time, we deployed it for each job\n        execution, luckily we again have benefited from Terraform to do it. Also we used it to spin up the network, subnetwork, router and other things that are\n        needed to run the Spark job within Dataproc, so I would generally recommend it as a useful tool to automate things in GCP. Dataproc has autoscaling\n        feature, but it requires more actions: creating autoscaling policy in GCP and integrating it into the job. Moreover, to achieve good performance we\n        needed to play a lot with Spark configuration like tuning memory on workers, choosing appropriate number of shuffle partitions, executor instances and\n        so on.\n        \n        \n        Dataflow. Due to its serverless nature we didnâ€™t need to set up a cluster each time we wanted to process data. The next big advantage\n        of Dataflow is the Shuffle service, which addresses the shuffle issue on Spark executors as it moves heavy operation out of the worker virtual machine\n        to the service backend. Besides, there is autoscaling out-of-the-box, Streaming engine for streaming pipeline support. Generally, Dataflow is supposed\n        to be a self-managed platform, so less effort is required to configure it compared to Dataproc.\n        \n    \nSummary: Dataflow wins in terms of ease of the setup. However, there is one more drawback here: limitation to run on the public cloud. It was so\nconvenient for us to profit from Dataflow services that it would be hard to find appropriate substitution for them. Furthermore, BigFlow framework\npositions itself as a Python framework for data processing pipelines on GCP. So if we want to migrate to another platform, this would enforce us to\nconfigure another runner so that we are able to run the job properly.\nMonitoring\nIn addition to the process itself, it is crucial to have an option to observe its workflow, collect metrics and have access to logs from the machines.\nApache Spark\n        Apache Beam\n    \nSummary: Dataflow beats Dataproc here and that is it. No comments are required.\nCost\nLast but not least is the pricing of used GCP resources needed to process data like setting and running a cluster, storing data and queries execution in\nBigQuery, etc. Here we are most likely talking not only about the money, but also about the time. As it was basically a pilot project, we calculated the cost of\nnon-optimized jobs to see how much it is without significant tuning.\n\n        Apache Spark\n        Apache Beam\n    \nCost\n        \n        Approximately on the same level, slightly in favour of Apache Spark\n        \n    \nTime\n        1.5h\n        1h\n    \nSummary: cost is almost the same, however we need to highlight that Spark job has much more space to optimize while Apache Beam job already\ncontains Dataflow optimizations out-of-the-box. For example: playing more with Spark configuration, experimenting with Dataproc workers number, etc, so\nprobably it would cost less and run faster if you know how to tune it properly.\nConclusion\nAt the end weâ€™d like to say that Spark Job could be more beneficial if you know it well. Additionally, Dataproc requires advanced skills close to\nthe experienced DevOps engineer to organize all necessary infrastructure. But if you do not have an engineer with deep BigData experience in your team or you\nrun out of time and you are ready to pay a little bit more for out-of-the-box features, then Apache Beam + Dataflow is your choice. Also remember even if you\npay a little bit more, it means that you are saving developersâ€™ time spent on the Spark tweaking that may bring some value.\nBelow you can find how weâ€™d estimate entry level for skills that were necessary for us to develop using the two technologies.\n\n        Apache Spark\n        Apache Beam\n    \nProgramming languages\n        Scala (Intermediate)\n        Python (Intermediate)\n    \nTechnology stack\n        Apache Spark (Advanced)\n        Apache Beam (Intermediate)\n    \nBigFlow (Advanced)\n    \nGoogle Dataproc (Intermediate)\n        Google DataFlow (Intermediate) \n    \nGoogle Logging (Basic)\n        \n    \nTerraform (Intermediate)\n        \n    \nGoogle BigQuery (Basic)\n    \nGoogle Composer to schedule jobs in GCP (Basic)\n    \nGoogle Cloud Storage (Basic)\n    \n\n\n.post img{margin: 0 auto;display: block;}\ntd {\n  vertical-align: top;\n}\n@media (max-width: 400px) {\n  table {\n    font-size: 0.8em;\n  }\n}\n@media (max-width: 350px) {\n  table {\n    font-size: 0.6em;\n  }\n}","guid":"https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html","categories":["tech","Apache Spark","Apache Beam","BigFlow","BigData","GCP"],"isoDate":"2021-06-27T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999766022669","name":"UX Team Leader","uuid":"139866c1-8ec8-4bd2-b5c5-e3381d6a1255","refNumber":"REF2739W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-08-10T11:53:58.000Z","location":{"city":"PoznaÅ„, Warszawa, KrakÃ³w","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572906","label":"UX - Research \u0026 Design"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"KrakÃ³w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"WiÄ™cej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"ToruÅ„","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"bf8669ea-7cc9-445c-9713-25ba37d96657","valueLabel":"IT - Management"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"BÅ‚onie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"WrocÅ‚aw","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572906","valueLabel":"UX - Research \u0026 Design"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"PoznaÅ„","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Team Leader, Leader, Lider, Kierownik, Manager, UX, Technologia, Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999766022669","creator":{"name":"Martyna Maziarska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999766022264","name":"Team Leader (Design System)","uuid":"186a251f-9306-4574-b92c-aeabd7ad1d2c","refNumber":"REF2816I","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-08-10T11:50:55.000Z","location":{"city":"Warszawa, PoznaÅ„","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572906","label":"UX - Research \u0026 Design"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"bf8669ea-7cc9-445c-9713-25ba37d96657","valueLabel":"IT - Management"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"WiÄ™cej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572906","valueLabel":"UX - Research \u0026 Design"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"PoznaÅ„","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Technologia, Technology, Team Leader, Leader, Lider, Manager, Kierownik, UX, UI, User Experience, User Interface, Design, Design System, Projektant, Frontend Development, Front-end Development, Frontend, Front-end, Art Director, Graphic"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999766022264","creator":{"name":"Martyna Maziarska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999766022103","name":"Software Engineer (Java/Kotlin) - Delivery Experience","uuid":"307b2bfa-848e-4a68-84e7-3b8b1c9a5e6d","refNumber":"REF2584V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-08-10T11:49:47.000Z","location":{"city":"Warszawa, PoznaÅ„, KrakÃ³w","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"KrakÃ³w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"WiÄ™cej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"ToruÅ„","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"743f6067-ce19-4a83-9a0d-10d49cd63004","valueLabel":"Delivery Experience"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"BÅ‚onie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"WrocÅ‚aw","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"PoznaÅ„","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999766022103","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999765816116","name":"Senior Product Manager (Allegro Smart!)","uuid":"62261af9-3a10-438a-ac35-12f0523faa9a","refNumber":"REF2915Q","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-08-09T12:49:26.000Z","location":{"city":"PoznaÅ„, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572889","label":"IT - Product/Project Management"},"function":{"id":"product_management","label":"Product Management"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"743f6067-ce19-4a83-9a0d-10d49cd63004","valueLabel":"Delivery Experience"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"WiÄ™cej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572889","valueLabel":"IT - Product/Project Management"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"PoznaÅ„","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"pm, smart"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999765816116","creator":{"name":"Paulina Partyka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999765815469","name":"Research Engineer - Machine Learning (Computer Vision)","uuid":"a4a77a84-1d39-4ee4-b65a-6588956fc41e","refNumber":"REF2880R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-08-09T12:46:46.000Z","location":{"city":"Warszawa, KrakÃ³w, PoznaÅ„, ToruÅ„","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"KrakÃ³w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"BÅ‚onie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"WrocÅ‚aw","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"WiÄ™cej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"PoznaÅ„","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"ToruÅ„","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999765815469","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1623957759000,"duration":7200000,"id":"278903176","name":"Allegro Tech Live #20: WydajnoÅ›Ä‡ Backendu","date_in_series_pattern":false,"status":"past","time":1624982400000,"local_date":"2021-06-29","local_time":"18:00","updated":1624994207000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":125,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278903176/","description":"Allegro Tech Live w 100% zdalna odsÅ‚ona naszych stacjonarnych meetupÃ³w Allegro Tech Talks. Zazwyczaj spotykaliÅ›my siÄ™ w naszych biurach, ale tym razem to my zagoÅ›cimyâ€¦","how_to_find_us":"https://youtu.be/VklKR_fO5OI","visibility":"public","member_pay_fee":false},{"created":1621842668000,"duration":100800000,"id":"278374635","name":"UX Research Confetti","date_in_series_pattern":false,"status":"past","time":1624456800000,"local_date":"2021-06-23","local_time":"16:00","updated":1624563213000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278374635/","description":"ğŸ‰ Niech rozsypie siÄ™ confetti wiedzy o badaniach UX! ğŸ‰ SzukaliÅ›my konferencji badawczej UX w Polsce i nie znaleÅºliÅ›myâ€¦ Dlatego Å‚Ä…czymy siÅ‚y z ekspertami zâ€¦","visibility":"public","member_pay_fee":false},{"created":1622474681000,"duration":5400000,"id":"278528964","name":"Allegro Tech Live Odcinek: #19   Co to znaczy byÄ‡ liderem i jak nim zostaÄ‡?","date_in_series_pattern":false,"status":"past","time":1623340800000,"local_date":"2021-06-10","local_time":"18:00","updated":1623349290000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":52,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278528964/","description":"Allegro Tech Live to w 100% zdalna odsÅ‚ona naszych stacjonarnych meetupÃ³w Allegro Tech Talks. Zazwyczaj spotykaliÅ›my siÄ™ w naszych biurach, ale tym razem to myâ€¦","how_to_find_us":" https://www.youtube.com/watch?v=8sLX0ExSq7E","visibility":"public","member_pay_fee":false},{"created":1619620661000,"duration":5400000,"id":"277852879","name":"Allegro Tech Live #18 PM w Allegro, jak do nas doÅ‚Ä…czyÄ‡ i czerpaÄ‡ radoÅ›Ä‡ z pracy","date_in_series_pattern":false,"status":"past","time":1620921600000,"local_date":"2021-05-13","local_time":"18:00","updated":1620932668000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":46,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/277852879/","description":"Allegro Tech Live to w 100% zdalna odsÅ‚ona naszych stacjonarnych meetupÃ³w Allegro Tech Talks. Zazwyczaj spotykaliÅ›my siÄ™ w naszych biurach, ale tym razem to myâ€¦","how_to_find_us":"https://youtu.be/WNOQJxPKweM","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wyglÄ…da rola architekta w Allegro? Ile takich osÃ³b pracuje w naszej firmie i dlaczego ta rola jest tak rÃ³Å¼norodna? Czym jest Andamio i jak rozwijamy naszÄ… platformÄ™ â€“ o tym wszystkim opowie Piotr Betkier â€“ InÅ¼ynier, Architekt Platformy Technicznej w Allegro oraz twÃ³rca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wyglÄ…da rola architekta w Allegro? Ile takich osÃ³b pracuje w naszej firmie i dlaczego ta rola jest tak rÃ³Å¼norodna? Czym jest Andamio i jak rozwijamy naszÄ… platformÄ™ â€“ o tym wszystkim opowie Piotr Betkier â€“ InÅ¼ynier, Architekt Platformy Technicznej w Allegro oraz twÃ³rca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wyglÄ…da rola architekta w Allegro? Ile takich osÃ³b pracuje w naszej firmie i dlaczego ta rola jest tak rÃ³Å¼norodna? Czym jest Andamio i jak rozwijamy naszÄ… platformÄ™ â€“ o tym wszystkim opowie Piotr Betkier â€“ InÅ¼ynier, Architekt Platformy Technicznej w Allegro oraz twÃ³rca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr MichoÅ„ski"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr MichoÅ„ski"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane Å›rodowisko uruchomienia aplikacji Allegro? Jak dziaÅ‚ajÄ… serwerownie firmy i ile ich potrzeba, a ktÃ³re elementy Allegro dziaÅ‚ajÄ… w chmurze publicznej? Jak przebiegaÅ‚a transformacja w Allegro i co zmieniaÅ‚o siÄ™ przez lata? Jak wzrost biznesu wpÅ‚ywa na wielkoÅ›Ä‡ infrastruktury i jak infrastruktura Allegro odczuÅ‚a przyjÅ›cie pandemii? O tym, a takÅ¼e o rozwoju liderÃ³w technologii w Allegro oraz o historii powstania dÅ¼ingla do naszych podcastÃ³w, opowie Piotr MichoÅ„ski - menadÅ¼er ZespoÅ‚Ã³w tworzÄ…cych infrastrukturÄ™ Allegro.","contentSnippet":"Jak jest zbudowane Å›rodowisko uruchomienia aplikacji Allegro? Jak dziaÅ‚ajÄ… serwerownie firmy i ile ich potrzeba, a ktÃ³re elementy Allegro dziaÅ‚ajÄ… w chmurze publicznej? Jak przebiegaÅ‚a transformacja w Allegro i co zmieniaÅ‚o siÄ™ przez lata? Jak wzrost biznesu wpÅ‚ywa na wielkoÅ›Ä‡ infrastruktury i jak infrastruktura Allegro odczuÅ‚a przyjÅ›cie pandemii? O tym, a takÅ¼e o rozwoju liderÃ³w technologii w Allegro oraz o historii powstania dÅ¼ingla do naszych podcastÃ³w, opowie Piotr MichoÅ„ski - menadÅ¼er ZespoÅ‚Ã³w tworzÄ…cych infrastrukturÄ™ Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr MichoÅ„ski","summary":"Jak jest zbudowane Å›rodowisko uruchomienia aplikacji Allegro? Jak dziaÅ‚ajÄ… serwerownie firmy i ile ich potrzeba, a ktÃ³re elementy Allegro dziaÅ‚ajÄ… w chmurze publicznej? Jak przebiegaÅ‚a transformacja w Allegro i co zmieniaÅ‚o siÄ™ przez lata? Jak wzrost biznesu wpÅ‚ywa na wielkoÅ›Ä‡ infrastruktury i jak infrastruktura Allegro odczuÅ‚a przyjÅ›cie pandemii? O tym, a takÅ¼e o rozwoju liderÃ³w technologii w Allegro oraz o historii powstania dÅ¼ingla do naszych podcastÃ³w, opowie Piotr MichoÅ„ski - menadÅ¼er ZespoÅ‚Ã³w tworzÄ…cych infrastrukturÄ™ Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wyglÄ…da praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespÃ³Å‚ odpowiedzialny za narzÄ™dzia i infrastrukturÄ™ dla przetwarzania danych? Kiedy moÅ¼emy mÃ³wiÄ‡ o duÅ¼ych danych i ile petabajtÃ³w przetwarza Allegro? SkÄ…d pochodzÄ… dane Allegro i dlaczego jest ich tak duÅ¼o oraz z jakiego powodu dopiero teraz przenosimy siÄ™ do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz â€“ Team Manager \u0026 Platform Architect w Allegro.","contentSnippet":"Jak wyglÄ…da praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespÃ³Å‚ odpowiedzialny za narzÄ™dzia i infrastrukturÄ™ dla przetwarzania danych? Kiedy moÅ¼emy mÃ³wiÄ‡ o duÅ¼ych danych i ile petabajtÃ³w przetwarza Allegro? SkÄ…d pochodzÄ… dane Allegro i dlaczego jest ich tak duÅ¼o oraz z jakiego powodu dopiero teraz przenosimy siÄ™ do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz â€“ Team Manager \u0026 Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wyglÄ…da praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespÃ³Å‚ odpowiedzialny za narzÄ™dzia i infrastrukturÄ™ dla przetwarzania danych? Kiedy moÅ¼emy mÃ³wiÄ‡ o duÅ¼ych danych i ile petabajtÃ³w przetwarza Allegro? SkÄ…d pochodzÄ… dane Allegro i dlaczego jest ich tak duÅ¼o oraz z jakiego powodu dopiero teraz przenosimy siÄ™ do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz â€“ Team Manager \u0026 Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz GaÅ‚ek"]},"title":"Od inÅ¼yniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz GaÅ‚ek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stojÄ…? Jak w Allegro angaÅ¼ujemy siÄ™ w rozwÃ³j kultury Open Source? Ile mamy projektÃ³w na GitHubie i jak Å›wiÄ™tujemy Hacktoberfest? W jaki sposÃ³b moÅ¼na rozwinÄ…Ä‡ siÄ™ od inÅ¼yniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek GaÅ‚ek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stojÄ…? Jak w Allegro angaÅ¼ujemy siÄ™ w rozwÃ³j kultury Open Source? Ile mamy projektÃ³w na GitHubie i jak Å›wiÄ™tujemy Hacktoberfest? W jaki sposÃ³b moÅ¼na rozwinÄ…Ä‡ siÄ™ od inÅ¼yniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek GaÅ‚ek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz GaÅ‚ek","summary":"Czym jest Opbox i jakie wyzwania przed nim stojÄ…? Jak w Allegro angaÅ¼ujemy siÄ™ w rozwÃ³j kultury Open Source? Ile mamy projektÃ³w na GitHubie i jak Å›wiÄ™tujemy Hacktoberfest? W jaki sposÃ³b moÅ¼na rozwinÄ…Ä‡ siÄ™ od inÅ¼yniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek GaÅ‚ek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true},"page":"/","query":{},"buildId":"J1aLTAmVL3rv4sUH077bM","isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-eef578260fd80f8fff94.js"></script><script src="/_next/static/chunks/webpack-189c53927ffd3caf09c3.js" async=""></script><script src="/_next/static/chunks/framework-0441fae7fd130f37dee1.js" async=""></script><script src="/_next/static/chunks/main-547dee26f92077ae29b6.js" async=""></script><script src="/_next/static/chunks/pages/_app-f9c1c7bd15b9b2e730cb.js" async=""></script><script src="/_next/static/chunks/805-39af1b772818e37efb44.js" async=""></script><script src="/_next/static/chunks/pages/index-fe725e13004e1b56d414.js" async=""></script><script src="/_next/static/J1aLTAmVL3rv4sUH077bM/_buildManifest.js" async=""></script><script src="/_next/static/J1aLTAmVL3rv4sUH077bM/_ssgManifest.js" async=""></script></body></html>