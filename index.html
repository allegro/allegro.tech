<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><link rel="author" href="humans.txt"/><script defer="" data-domain="allegro.tech" src="https://plausible.io/js/script.js"></script><meta name="next-head-count" content="15"/><link rel="preload" href="/_next/static/css/c4277531f90028a4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c4277531f90028a4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/6809bff34030a131.css" as="style"/><link rel="stylesheet" href="/_next/static/css/6809bff34030a131.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d73513da824d371.js" defer=""></script><script src="/_next/static/chunks/206-668a2bb90b2c9b6d.js" defer=""></script><script src="/_next/static/chunks/pages/index-2100ffa74b8edefd.js" defer=""></script><script src="/_next/static/cJo7-lX_UosNxZ78IKqnF/_buildManifest.js" defer=""></script><script src="/_next/static/cJo7-lX_UosNxZ78IKqnF/_ssgManifest.js" defer=""></script><script src="/_next/static/cJo7-lX_UosNxZ78IKqnF/_middlewareManifest.js" defer=""></script></head><body class="m-color-bg_desk"><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="/images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-color-bg_card m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://jobs.allegro.eu">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">About us</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://about.allegro.eu/who-we-are/at-a-glance">Allegro</a> is one of the most technologically advanced companies in our part of Europe. Allegro is also over 2300 IT specialists of various specializations, developing our e-commerce platform. The unique scale and complexity of the problems that we solve on a daily basis give us the opportunity to work on a wide variety of projects. Allegro Tech is a place where our engineers share knowledge and case studies from selected projects in the company – in the form of articles, podcasts and events.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html" title="A Mission to Cost-Effectiveness: Reducing the cost of a single Google Cloud Dataflow Pipeline by Over 60%"><img width="388" src="images/blogpost.png" alt="A Mission to Cost-Effectiveness: Reducing the cost of a single Google Cloud Dataflow Pipeline by Over 60%" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html" title="A Mission to Cost-Effectiveness: Reducing the cost of a single Google Cloud Dataflow Pipeline by Over 60%" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">A Mission to Cost-Effectiveness: Reducing the cost of a single Google Cloud Dataflow Pipeline by Over 60%</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 10 godzin temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/big data">#<!-- -->big data</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">In this article we’ll present methods for efficiently optimizing physical resources and fine-tuning the configuration of a Google Cloud Platform (GCP)
Dataflow pipeline in order to…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circular MuiAvatarGroup-avatar" style="z-index:1"><img alt="Jakub Demianowski" src="https://blog.allegro.tech/img/authors/jakub.demianowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/jakub.demianowski">Jakub Demianowski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/06/pragmatic-engineer-score.html" title="Engineering culture of Allegro &amp; Allegro Pay: Pragmatic Engineer Score"><img width="388" src="images/blogpost.png" alt="Engineering culture of Allegro &amp; Allegro Pay: Pragmatic Engineer Score" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/06/pragmatic-engineer-score.html" title="Engineering culture of Allegro &amp; Allegro Pay: Pragmatic Engineer Score" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Engineering culture of Allegro &amp; Allegro Pay: Pragmatic Engineer Score</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">9 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/engineering culture">#<!-- -->engineering culture</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/pragmatic engineer">#<!-- -->pragmatic engineer</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">One tech blog/newsletter gained traction and popularity for a couple of years now: Pragmatic Engineer.
Quoting author:
The #1 technology newsletter on Substack. Highly relevant for software…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circular MuiAvatarGroup-avatar" style="z-index:1"><img alt="Jakub Dropia" src="https://blog.allegro.tech/img/authors/jakub.dropia.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/jakub.dropia">Jakub Dropia</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/06/pragmatic-engineer-score.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/06/pragmatic-engineer-score.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/06/rest-service-client-design-testing-monitoring.html" title="REST service client: design, testing, monitoring"><img width="388" src="images/blogpost.png" alt="REST service client: design, testing, monitoring" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/06/rest-service-client-design-testing-monitoring.html" title="REST service client: design, testing, monitoring" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">REST service client: design, testing, monitoring</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">16 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kotlin">#<!-- -->kotlin</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/testing">#<!-- -->testing</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/integration tests">#<!-- -->integration tests</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/rest">#<!-- -->rest</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/wiremock">#<!-- -->wiremock</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">The purpose of this article is to present how to design, test, and monitor a REST service client.
The article includes a repository with clients written…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circular MuiAvatarGroup-avatar" style="z-index:1"><img alt="Piotr Klimiec" src="https://blog.allegro.tech/img/authors/piotr.klimiec.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/piotr.klimiec">Piotr Klimiec</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/06/rest-service-client-design-testing-monitoring.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/06/rest-service-client-design-testing-monitoring.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html" title="Unveiling bottlenecks of couchbase sub-documents operations"><img width="388" src="images/blogpost.png" alt="Unveiling bottlenecks of couchbase sub-documents operations" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html" title="Unveiling bottlenecks of couchbase sub-documents operations" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Unveiling bottlenecks of couchbase sub-documents operations</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około miesiąc temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/couchbase">#<!-- -->couchbase</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/sub-documents">#<!-- -->sub-documents</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bottlenecks">#<!-- -->bottlenecks</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">This story shows our journey in addressing a platform stability issue related to autoscaling, which, paradoxically, added some additional overhead instead
of reducing the load. A…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circular MuiAvatarGroup-avatar" style="z-index:1"><img alt="Tomasz Ziółkowski" src="https://blog.allegro.tech/img/authors/tomasz.ziolkowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/tomasz.ziolkowski">Tomasz Ziółkowski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/" title="O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali"><img src="images/podcast.png" alt="O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/" title="O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Na czym polega praca analityków w obszarze technologii w Allegro? Jakich narzędzi i technologii na co dzień używają osoby pracujące na tych stanowiskach? Jak efekty pracy analityków wpływają na naszą platformę, produkty i funkcjonalności? Czym zajmuje się Data Product Manager w Allegro Pay? Dlaczego monety są ważnym elementem ekosystemu Allegro? Posłuchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziałem Adrianny Napiórkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/" title="Programowanie - co liczy się w nim najbardziej?"><img src="images/podcast.png" alt="Programowanie - co liczy się w nim najbardziej?" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/" title="Programowanie - co liczy się w nim najbardziej?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Programowanie - co liczy się w nim najbardziej?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">5 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jaką ścieżkę trzeba przejść, aby dobrze programować? Gdzie zdobywać wiedzę, doświadczenie i szlifować swoje umiejętności? Ile czasu potrzeba aby nabrać doświadczenia i jak zadbać o swój dalszy rozwój? Na czym w praktyce polegają role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza się w naszych zespołach? Posłuchajcie nowego odcinka Allegro Tech Podcast z udziałem Rafała Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/" title="MBox: server-driven UI dla aplikacji mobilnych"><img src="images/podcast.png" alt="MBox: server-driven UI dla aplikacji mobilnych" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/" title="MBox: server-driven UI dla aplikacji mobilnych" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">MBox: server-driven UI dla aplikacji mobilnych</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">7 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym jest i jak powstał MBox: wewnętrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? Skąd wziął się pomysł na to rozwiązanie i na jakie bolączki odpowiada? Dlaczego zdecydowaliśmy się na budowanie tego typu rozwiązania in-house i z jakimi wyzwaniami mierzyliśmy się w procesie tworzenia? Co wyróżnia zespoły pracujące nad tym narzędziem i jak pracuje im się bez Product Ownera? Posłuchajcie siódmego odcinka Allegro Tech Podcast z udziałem Pauliny Sadowskiej i Tomasza Gębarowskiego - Managerów w obszarze Technical Platform Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/" title="O chatbotach i ich wpływie na Allegro"><img src="images/podcast.png" alt="O chatbotach i ich wpływie na Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/" title="O chatbotach i ich wpływie na Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O chatbotach i ich wpływie na Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">8 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieć w kontekście obszaru Customer Experience? W czym pomagają nam chatboty, jak je rozwijamy i dbamy o ich jakość? Kim są Allina oraz Albert i co mają wspólnego z automatyzacją? Za jakie rozwiązania otrzymaliśmy nagrodę hiperautomatyzacji? O tym wszystkim posłuchacie w odcinku z udziałem Rafała Gajewskiego - Managera w obszarze IT Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/301022703/" title="Allegro Tech Talks #43 - Wszystko o programie e-Xperience" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #43 - Wszystko o programie e-Xperience"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/301022703/" title="Allegro Tech Talks #43 - Wszystko o programie e-Xperience" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #43 - Wszystko o programie e-Xperience</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">29 dni temu<!-- -->, Allegro Office - Poznań (Nowy Rynek)</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-43/](https://app.evenea.pl/event/allegro-tech-talk-43/) Zapraszamy Was na #43 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej kawie☕, napojach🥤…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/301022703/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/298027809/" title="UX Research Confetti - IV edycja" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - IV edycja"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/298027809/" title="UX Research Confetti - IV edycja" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - IV edycja</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około miesiąc temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**🎉 Przedstawiamy 4. edycję UX Research Confetti - bezpłatną, polską konferencję poświęconą badaniom UX, organizowaną przez zespół badaczy z Allegro.** ✨ Konferencja odbędzie się w…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/298027809/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/300288303/" title="DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/300288303/" title="DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około 2 miesiące temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** **[https://app.evenea.pl/event/allegro-tech-talk-ddd/](https://app.evenea.pl/event/allegro-tech-talk-ddd/)** Dobrze Was widzieć! Allegro Tech to miejsce, w którym dzielimy się wiedzą, dobrymi praktykami i case study z różnych projektów prowadzonych w…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/300288303/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/300327359/" title="Allegro Tech Talks #42 - Kariera Product Managera" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #42 - Kariera Product Managera"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/300327359/" title="Allegro Tech Talks #42 - Kariera Product Managera" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #42 - Kariera Product Managera</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około 2 miesiące temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-42/ ](https://app.evenea.pl/event/allegro-tech-talk-42/) Zapraszamy Was na #42 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/300327359/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (.NET) - Product Team - Allegro Pay</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999994499625-software-engineer-net-product-team-allegro-pay?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Salesforce Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999994263391-senior-salesforce-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Salesforce Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999994260676-senior-salesforce-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Contractor Software Engineer (Java/Kotlin)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw, Poznań, Cracow</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999994187830-contractor-software-engineer-javakotlin?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (Java) - Platform Team</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999994187205-software-engineer-java-platform-team?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://jobs.allegro.eu">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"A Mission to Cost-Effectiveness: Reducing the cost of a single Google Cloud Dataflow Pipeline by Over 60%","link":"https://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html","pubDate":"Thu, 20 Jun 2024 00:00:00 +0200","authors":{"author":[{"name":["Jakub Demianowski"],"photo":["https://blog.allegro.tech/img/authors/jakub.demianowski.jpg"],"url":["https://blog.allegro.tech/authors/jakub.demianowski"]}]},"content":"\u003cp\u003eIn this article we’ll present methods for efficiently optimizing physical resources and fine-tuning the configuration of a Google Cloud Platform (GCP)\nDataflow pipeline in order to achieve cost reductions.\nOptimization will be presented as a real-life scenario, which will be performed in stages.\u003c/p\u003e\n\n\u003cp\u003eBefore we start, it’s time to introduce several avenues through which the cost of Big Data pipelines can be significantly reduced.\nThese include:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eCareful optimization of consumed physical resources, like choosing VM types with optimal CPU to memory ratio and a cost-effective CPU type.\u003c/li\u003e\n  \u003cli\u003eEnhancing the configuration of the data processing engine to maximize its efficiency.\u003c/li\u003e\n  \u003cli\u003eOptimizing input and output datasets. Not all data may need processing or perhaps, altering their structure could reduce the processing time.\u003c/li\u003e\n  \u003cli\u003eRefining storage strategies for input and output datasets. This is particularly beneficial if reading or writing speeds are suboptimal and demand improvements.\u003c/li\u003e\n  \u003cli\u003eStreamlining of our pipeline code and utilizing built-in optimization functionalities (for example broadcast joins and repartitioning in Apache Spark).\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThroughout this article we will focus solely on optimizing consumed physical resources (1st point) and enhancing configuration of the data processing engine (2nd point).\u003c/p\u003e\n\n\u003ch2 id=\"about-the-data-pipeline-being-optimized\"\u003eAbout the data pipeline being optimized\u003c/h2\u003e\n\n\u003cp\u003eThe data pipeline which will serve us as an example throughout this article is written in Apache Beam using Python SDK.\nThe pipeline runs on Google Cloud Dataflow processing engine.\nThe goal of the pipeline is to join a couple of tables (most of them are a terabyte+ in size), apply some transformations and produce a unified output table.\u003c/p\u003e\n\n\u003cp\u003eOverall processing cost of the full dataset is around $350 per day.\nIt results in roughly $10,500 per month, and $127,000 per year.\u003c/p\u003e\n\n\u003ch2 id=\"approach-to-cost-optimization\"\u003eApproach to cost optimization\u003c/h2\u003e\n\n\u003cp\u003eAt the beginning of the cost optimization let’s draft a couple of hypotheses:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ePhysical resources are underutilized.\u003c/li\u003e\n  \u003cli\u003ePhysical resources have not the best price-to-performance ratio.\u003c/li\u003e\n  \u003cli\u003eConfiguration of the Dataflow job is suboptimal and could be optimized.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eMy goal will be to check those hypotheses.\u003c/p\u003e\n\n\u003cp\u003eDuring the testing phase I’ll use a 3% subsample of input datasets. As a result I will be running tests with input size at ~100 GB level.\nThus, I’ll limit the cost of tests and significantly reduce their time. Final tests will be made on the full dataset, not on a limited subsample.\u003c/p\u003e\n\n\u003cp\u003eIn order to save time and resources I’ve made some speculative choices regarding what I should test during optimization.\nIn addition, I’ve decided not to test all the possible combinations of machine families, disk types and configuration options to save time.\nI will try to stick with the most promising choices and omit testing unpromising configurations.\u003c/p\u003e\n\n\u003ch2 id=\"hypothesis-testing-physical-resources-are-underutilized\"\u003eHypothesis testing: physical resources are underutilized\u003c/h2\u003e\n\n\u003cp\u003eIn our initial configuration we used the following type of worker machines:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eMachine type: \u003ca href=\"https://cloud.google.com/compute/docs/general-purpose-machines#n2_series\"\u003en2-standard-4\u003c/a\u003e (4 vCPU, 16 GB of memory)\u003c/li\u003e\n  \u003cli\u003eDisk size: 100 GB\u003c/li\u003e\n  \u003cli\u003eDisk type: HDD\u003c/li\u003e\n  \u003cli\u003eMax. worker nodes: 500\u003c/li\u003e\n  \u003cli\u003eAutoscaling algorithm: throughput based\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eI made a decision to focus on CPU and memory utilization first.\u003c/p\u003e\n\n\u003ch3 id=\"cpu-utilization\"\u003eCPU utilization\u003c/h3\u003e\n\u003cp\u003eI checked if CPU utilization was on an acceptable level, and it was.\u003c/p\u003e\n\n\u003cp\u003eThe following diagram from Dataflow UI presents CPU utilization on All Workers in terms of the CPU utilization\nfor all cores on a single App Engine flexible instance.\nSo it gives us an idea of how the CPU is utilized on each virtual machine.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-06-20-cost-optimization-data-pipeline-gcp/01_cpu_utilization_all_workers.png\" alt=\"CPU utilization on all worker nodes\" class=\"image-with-frame\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWe could also take a look at the same data presented in terms of statistical metrics.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-06-20-cost-optimization-data-pipeline-gcp/02_cpu_utilization_stats.png\" alt=\"CPU utilization statistics\" class=\"image-with-frame\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eFrom the given graph I could see that mean utilization of the CPU was around 85%, which is a good score.\nThe result is affected by two shuffle stages, when we need to send data around the cluster (usually network is a small bottleneck here).\nCPU tends to be idle while shuffling data using Dataflow \u003ca href=\"https://cloud.google.com/dataflow/docs/shuffle-for-batch\"\u003eShuffle Service\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eSo CPU resources are not underutilized. We use almost all of what we pay for.\u003c/p\u003e\n\n\u003ch3 id=\"memory-utilization\"\u003eMemory utilization\u003c/h3\u003e\n\n\u003cp\u003eIn the end I checked memory usage. I saw that we did not use all the memory which we were paying for.\nLet’s take a look at the following two graphs.\nThe first one shows maximal memory utilization among all the workers.\n\u003cimg src=\"/img/articles/2024-06-20-cost-optimization-data-pipeline-gcp/03_memory_utilization_max_usage.png\" alt=\"Memory utilization max usage\" class=\"image-with-frame\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe second one shows memory utilization statistics among all the worker nodes.\n\u003cimg src=\"/img/articles/2024-06-20-cost-optimization-data-pipeline-gcp/04_memory_utilization_summary.png\" alt=\"Memory utilization summary\" class=\"image-with-frame\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe first one presents average memory usage on a worker node, the second one presents overall memory usage among the whole cluster.\nWe clearly see that we only use around 50% of the memory. Bingo, we pay for memory that we do not use.\u003c/p\u003e\n\n\u003ch3 id=\"improving-memory-utilization\"\u003eImproving memory utilization\u003c/h3\u003e\n\n\u003cp\u003eUsually there are two ways of improving memory utilization:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eChange CPU to memory ratio on worker nodes.\u003c/li\u003e\n  \u003cli\u003eDecrease the amount of worker nodes.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eI’ve decided to change the CPU to memory ratio rather than decrease the number of worker nodes.\nI did not want to compromise on scalability and time needed to perform a job.\u003c/p\u003e\n\n\u003cp\u003eTest on a 3% subsample of input data has given the following cost of data processing:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003en2-standard-4: $9.48\u003c/li\u003e\n  \u003cli\u003en2-highcpu-8: $8.52 (~ 10% less than original price)\u003c/li\u003e\n  \u003cli\u003en2d-highcpu-8: $8.57 (~ 10% less than original price)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe saved 10% on adjusting CPU and memory ratio.\nIt results in around $12,700 of estimated savings per year (10% of $127,000 annual cost).\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eHypothesis\u003c/th\u003e\n    \u003cth\u003eSavings\u003cspan\u003e\u003csup id=\"fnref:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/span\u003e\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[1] Physical resources are underutilized\u003c/td\u003e\n    \u003ctd\u003e$12,700\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003ch2 id=\"hypothesis-testing-physical-resources-has-not-the-best-price-to-performance-ratio\"\u003eHypothesis testing: physical resources has not the best price-to-performance ratio\u003c/h2\u003e\n\n\u003cp\u003eI assumed that the current virtual machine type (n2-standard-4) has not the best price-to-performance ratio.\nTo check performance of different virtual machine types I used \u003ca href=\"https://cloud.google.com/compute/docs/benchmarks-linux\"\u003eCoreMark scores provided by Google Cloud itself\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eBased on CoreMark scores and official Google Cloud VM pricing, I prepared a table which would help me choose the VM type with the best price-to-performance ratio.\nThe most important column is “price per 1 mln points” — how much I need to pay on average to score 1 mln points.\nI used \u003ca href=\"https://cloud.google.com/compute/vm-instance-pricing\"\u003eofficial VM instance prices from Google Cloud site\u003c/a\u003e from region europe-west1.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eVirtual Machine Type\u003c/th\u003e\n    \u003cth\u003ePoints in ScoreMark\u003cspan\u003e\u003csup id=\"fnref:2\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:2\" class=\"footnote\" rel=\"footnote\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003c/span\u003e\u003c/th\u003e\n    \u003cth\u003ePrice per hour\u003cspan\u003e\u003csup id=\"fnref:3\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:3\" class=\"footnote\" rel=\"footnote\"\u003e3\u003c/a\u003e\u003c/sup\u003e\u003c/span\u003e\u003c/th\u003e\n    \u003cth\u003ePrice per 1 mln points\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en2-standard-4\u003c/td\u003e\n    \u003ctd\u003e66 833 pts\u003c/td\u003e\n    \u003ctd\u003e$0.21\u003c/td\u003e\n    \u003ctd\u003e$3.20\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en2-standard-8\u003c/td\u003e\n    \u003ctd\u003e138 657 pts\u003c/td\u003e\n    \u003ctd\u003e$0.43\u003c/td\u003e\n    \u003ctd\u003e$3.08\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en2d-standard-8\u003c/td\u003e\n    \u003ctd\u003e164 539 pts\u003c/td\u003e\n    \u003ctd\u003e$0.37\u003c/td\u003e\n    \u003ctd\u003e$2.26\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003ee2-standard-8\u003c/td\u003e\n    \u003ctd\u003e103 808 pts\u003c/td\u003e\n    \u003ctd\u003e$0.29\u003c/td\u003e\n    \u003ctd\u003e$2.84\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003et2d-standard-8\u003c/td\u003e\n    \u003ctd\u003e237 411 pts\u003c/td\u003e\n    \u003ctd\u003e$0.37\u003c/td\u003e\n    \u003ctd\u003e$1.57\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003cp\u003eAs we see, another hypothesis proved to be true. We’re not using the virtual machine type with the best price-to-performance ratio - T2D.\nWe’re using N2 machine type.\u003c/p\u003e\n\n\u003cp\u003eUnfortunately, at the time of writing, T2D machines do not provide CPU to memory ratios other than 3 GB per 1 vCPU.\nIt’s still better than 4 GB per 1 vCPU, but far from 1 or 2 GB per 1 vCPU.\nWe will check if T2D virtual machine type with 4 GB of memory per 1 CPU is cheaper than its counterparts.\u003c/p\u003e\n\n\u003ch3 id=\"moving-to-a-virtual-machine-type-with-better-price-to-performance-ratio\"\u003eMoving to a virtual machine type with better price-to-performance ratio\u003c/h3\u003e\n\n\u003cp\u003eI performed several tests on a small scale (3% subsample of input data) with T2D machine types. Let’s take a look at them.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003en2-standard-4 + HDD: $9.48\u003c/li\u003e\n  \u003cli\u003en2-highcpu-8 + HDD: $8.52 (~ 10% less than original price)\u003c/li\u003e\n  \u003cli\u003en2d-highcpu-8 + HDD: $8.57 (~ 10% less than original price)\u003c/li\u003e\n  \u003cli\u003et2d-standard-8 + HDD: $6.65 (~ 32% less than original price)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis way we decreased the estimated processing cost from $127,000 by $40,640 per year to $86,360 (by 32%)\u003csup id=\"fnref:1:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\nUnfortunately, we also introduced some possible underutilized resources (memory) by changing CPU to memory ratio.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eHypothesis\u003c/th\u003e\n    \u003cth\u003eSavings\u003cspan\u003e\u003csup id=\"fnref:1:2\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/span\u003e\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[1] Physical resources are underutilized\u003c/td\u003e\n    \u003ctd\u003e$12,700\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[2] Moving to a more cost-effective VM type\u003c/td\u003e\n    \u003ctd\u003e$27,940\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003cp\u003eTotal: $40,640 of estimated savings\u003csup id=\"fnref:1:3\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n\u003ch3 id=\"coming-back-to-optimization-of-virtual-machine-storage-type\"\u003eComing back to optimization of virtual machine storage type\u003c/h3\u003e\n\n\u003cp\u003eAs I found the most suitable virtual machine type, I was able to focus on choosing between SSD and HDD disk types.\nAs we all know, HDDs are much slower than SSDs, especially in terms of random read/write.\nFor processes where we do not heavily use storage I/O operations there’s no need to move to more expensive SSDs.\u003c/p\u003e\n\n\u003cp\u003eI decided to check if we should use cheaper and slower HDDs or more expensive and faster SSDs.\nI run the pipeline (3% of input data size) with HDD and SSD disks.\nHere are the results for different VM families:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003en2-standard-4 + HDD: $9.48\u003c/li\u003e\n  \u003cli\u003en2-highcpu-8 + HDD: $8.52 (~ 10% less than original price)\u003c/li\u003e\n  \u003cli\u003en2d-highcpu-8 + HDD: $8.57 (~ 10% less than original price)\u003c/li\u003e\n  \u003cli\u003et2d-standard-8 + HDD: $6.65 (~ 32% less than original price)\u003c/li\u003e\n  \u003cli\u003et2d-standard-8 + SSD: $5.64 (~ 41% less than original price)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThis way we decreased the estimated processing cost from $127,000 by $52,070 per year to $74,930 (by 41%)\u003csup id=\"fnref:1:4\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eHypothesis\u003c/th\u003e\n    \u003cth\u003eSavings\u003cspan\u003e\u003csup id=\"fnref:1:5\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/span\u003e\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[1] Physical resources are underutilized\u003c/td\u003e\n    \u003ctd\u003e$12,700\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[2] Moving to a more cost-effective VM type\u003c/td\u003e\n    \u003ctd\u003e$27,940\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[3] Changing VM disk type to SSD\u003c/td\u003e\n    \u003ctd\u003e$11,430\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003cp\u003eTotal: $52,070 of estimated savings\u003csup id=\"fnref:1:6\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n\u003ch2 id=\"hypothesis-testing-configuration-of-the-dataflow-job-is-not-optimal\"\u003eHypothesis testing: configuration of the Dataflow job is not optimal\u003c/h2\u003e\n\n\u003cp\u003eDataflow, in comparison to Apache Spark, leaves us with almost no configuration options to be changed.\nIt’s good because in Dataflow you get very decent out-of-the-box settings.\nThe single option which I wanted to tune was if we should use Shuffle Service.\nShuffle Service is a serverless tool that facilitates data shuffling around the cluster, thus relieving worker nodes from this task.\nAlso, node preemption is not so painful because Shuffle Service stores data in external storage independent of worker nodes.\nBut it comes at a price.\u003c/p\u003e\n\n\u003cp\u003eCost breakdown of processing 3% input dataset using virtual machine t2d-standard-8 with SSD disk is presented below\u003csup id=\"fnref:3:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:3\" class=\"footnote\" rel=\"footnote\"\u003e3\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eCost of CPU: $2.47\u003c/li\u003e\n  \u003cli\u003eCost of memory: $0.70\u003c/li\u003e\n  \u003cli\u003eCost of SSD disk: $0.16\u003c/li\u003e\n  \u003cli\u003eCost of shuffle service: $2.32\u003c/li\u003e\n  \u003cli\u003eOverall cost: $5.64\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThus, we see that the cost of the shuffle service plays an important role - it’s more than 40% of the overall cost. Let’s do an experiment and turn Shuffle Service off.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003en2-standard-4 + HDD: $9.48 (original configuration)\u003c/li\u003e\n  \u003cli\u003et2d-standard-8 + SSD: $5.64 (~ 41% less than original configuration)\u003c/li\u003e\n  \u003cli\u003et2d-standard-8 + SSD + no Shuffle Service: $3.95 (~ 58% less than original configuration)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBy turning off Shuffle Service we achieved a much lower cost.\nAs a bonus, our memory utilization increased to almost 100%, because we use worker nodes to perform a shuffle.\nSo we eliminated an underutilized T2D issue connected with a CPU to memory ratio.\nNode preemption is not a problem since we’re not utilizing preemptible VMs.\u003c/p\u003e\n\n\u003cp\u003eI must also add that turning off external shuffle service may not always result in lower cost.\nIt depends on many factors, and you should test it on your own data pipeline.\nAlso, you need to take into consideration that the job will usually require more resources (CPU, memory) once you turn off external shuffle service.\u003c/p\u003e\n\n\u003cp\u003eThis way we decreased the estimated processing cost from $127,000 by $73,660 per year to $53,340 (by 58%)\u003csup id=\"fnref:1:7\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\nSo it’s now less than half of the initial cost\u003csup id=\"fnref:1:8\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eHypothesis\u003c/th\u003e\n    \u003cth\u003eSavings\u003cspan\u003e\u003csup id=\"fnref:1:9\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/span\u003e\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[1] Physical resources are underutilized\u003c/td\u003e\n    \u003ctd\u003e$12,700\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[2] Moving to a more cost-effective VM type\u003c/td\u003e\n    \u003ctd\u003e$27,940\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[3] Changing VM disk type to SSD\u003c/td\u003e\n    \u003ctd\u003e$11,430\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e[4] Turning off Shuffle Service\u003c/td\u003e\n    \u003ctd\u003e$21,590\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003cp\u003eTotal: $73,660 of estimated savings\u003csup id=\"fnref:1:10\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n\u003cp\u003eNote: Why did we not use Dataflow \u003ca href=\"https://cloud.google.com/dataflow/docs/guides/flexrs\"\u003eFlexRS\u003c/a\u003e which could lower the processing price by combining preemptible and regular VMs?\u003c/p\u003e\n\n\u003cp\u003eWe did not test it due to how scheduling in FlexRS works.\nWhen you schedule a Dataflow FlexRS job you do not know the exact start time,\nthe only one promise from FlexRS is that the job will start within 6 hours (\u003ca href=\"https://cloud.google.com/dataflow/docs/guides/flexrs\"\u003edocumentation notes from Google Cloud website on that\u003c/a\u003e).\nOur data pipeline must start at specified time and having a 6 hour delay is not acceptable.\u003c/p\u003e\n\n\u003ch2 id=\"final-test-on-a-full-dataset\"\u003eFinal test on a full dataset\u003c/h2\u003e\n\n\u003cp\u003eMy last task was to test findings from subsampled input dataset (3%) tests on the full dataset (without subsampling).\nHere are the costs of processing a full dataset for a single day:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eConfiguration\u003c/th\u003e\n    \u003cth\u003eProcessing cost for one day on a full dataset\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003en2-standard-4 + HDD\u003c/td\u003e\n    \u003ctd\u003e$350.02\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003et2d-standard-8 + SSD + shuffle service turned off\u003c/td\u003e\n    \u003ctd\u003e$134.14 (~ 62% less than original price)\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003cp\u003eAs we see, the predicted gain from subsampling was achieved, and savings are even 3 pp higher than estimated.\nFor reference: we estimated, based on runs with 3% of input size, that we will achieve a roughly 58% cost reduction.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eInitial annual cost: $127,000\u003c/li\u003e\n  \u003cli\u003eEstimated annual cost after optimization: $48,260\u003c/li\u003e\n  \u003cli\u003eTotal estimated annual savings: $78,740\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003ePresented figures are only estimates based on a single run and extrapolated to the whole year.\nTo know the exact savings we will need to run the processing pipeline over a year, which hasn’t been done yet.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWe achieved excellent outcome without even touching the processing code.\nSpeculative approach provided good results.\nThere may still be some space for optimization, but within the timeframe I was given, I treat these results as first-rate and do not find any more reasons to further\noptimize the environment and configuration of the Dataflow job.\u003c/p\u003e\n\n\u003cp\u003eAlso, specified strategies do not have to lead to cost optimizations in other pipelines.\nAs every data pipeline is different, some changes which brought cost reduction in this example may result in increased processing cost in different data pipelines.\nWhat is most important in this article: how to approach cost optimization of a data pipeline, not which type of resources to choose.\u003c/p\u003e\n\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n  \u003col\u003e\n    \u003cli id=\"fn:1\" role=\"doc-endnote\"\u003e\n      \u003cp\u003ePresented figures are only estimates based on a single run (with only 3% of input data) and extrapolated to the whole year with the assumption that processing the whole dataset will result in the same relative savings as processing 3% of source data. \u003ca href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e \u003ca href=\"#fnref:1:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e2\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:2\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e3\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:3\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e4\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:4\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e5\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:5\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e6\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:6\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e7\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:7\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e8\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:8\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e9\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:9\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e10\u003c/sup\u003e\u003c/a\u003e \u003ca href=\"#fnref:1:10\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e11\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:2\" role=\"doc-endnote\"\u003e\n      \u003cp\u003eCoreMark results from \u003ca href=\"https://cloud.google.com/compute/docs/benchmarks-linux\"\u003eCoreMark scores provided by Google Cloud itself\u003c/a\u003e, retrieved on 04/05/2024. \u003ca href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:3\" role=\"doc-endnote\"\u003e\n      \u003cp\u003eOfficial prices taken from Google Cloud site, VM instance pricing in region europe-west1, retrieved on 04/05/2024. \u003ca href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e \u003ca href=\"#fnref:3:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e2\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n  \u003c/ol\u003e\n\u003c/div\u003e\n","contentSnippet":"In this article we’ll present methods for efficiently optimizing physical resources and fine-tuning the configuration of a Google Cloud Platform (GCP)\nDataflow pipeline in order to achieve cost reductions.\nOptimization will be presented as a real-life scenario, which will be performed in stages.\nBefore we start, it’s time to introduce several avenues through which the cost of Big Data pipelines can be significantly reduced.\nThese include:\nCareful optimization of consumed physical resources, like choosing VM types with optimal CPU to memory ratio and a cost-effective CPU type.\nEnhancing the configuration of the data processing engine to maximize its efficiency.\nOptimizing input and output datasets. Not all data may need processing or perhaps, altering their structure could reduce the processing time.\nRefining storage strategies for input and output datasets. This is particularly beneficial if reading or writing speeds are suboptimal and demand improvements.\nStreamlining of our pipeline code and utilizing built-in optimization functionalities (for example broadcast joins and repartitioning in Apache Spark).\nThroughout this article we will focus solely on optimizing consumed physical resources (1st point) and enhancing configuration of the data processing engine (2nd point).\nAbout the data pipeline being optimized\nThe data pipeline which will serve us as an example throughout this article is written in Apache Beam using Python SDK.\nThe pipeline runs on Google Cloud Dataflow processing engine.\nThe goal of the pipeline is to join a couple of tables (most of them are a terabyte+ in size), apply some transformations and produce a unified output table.\nOverall processing cost of the full dataset is around $350 per day.\nIt results in roughly $10,500 per month, and $127,000 per year.\nApproach to cost optimization\nAt the beginning of the cost optimization let’s draft a couple of hypotheses:\nPhysical resources are underutilized.\nPhysical resources have not the best price-to-performance ratio.\nConfiguration of the Dataflow job is suboptimal and could be optimized.\nMy goal will be to check those hypotheses.\nDuring the testing phase I’ll use a 3% subsample of input datasets. As a result I will be running tests with input size at ~100 GB level.\nThus, I’ll limit the cost of tests and significantly reduce their time. Final tests will be made on the full dataset, not on a limited subsample.\nIn order to save time and resources I’ve made some speculative choices regarding what I should test during optimization.\nIn addition, I’ve decided not to test all the possible combinations of machine families, disk types and configuration options to save time.\nI will try to stick with the most promising choices and omit testing unpromising configurations.\nHypothesis testing: physical resources are underutilized\nIn our initial configuration we used the following type of worker machines:\nMachine type: n2-standard-4 (4 vCPU, 16 GB of memory)\nDisk size: 100 GB\nDisk type: HDD\nMax. worker nodes: 500\nAutoscaling algorithm: throughput based\nI made a decision to focus on CPU and memory utilization first.\nCPU utilization\nI checked if CPU utilization was on an acceptable level, and it was.\nThe following diagram from Dataflow UI presents CPU utilization on All Workers in terms of the CPU utilization\nfor all cores on a single App Engine flexible instance.\nSo it gives us an idea of how the CPU is utilized on each virtual machine.\n\nWe could also take a look at the same data presented in terms of statistical metrics.\n\nFrom the given graph I could see that mean utilization of the CPU was around 85%, which is a good score.\nThe result is affected by two shuffle stages, when we need to send data around the cluster (usually network is a small bottleneck here).\nCPU tends to be idle while shuffling data using Dataflow Shuffle Service.\nSo CPU resources are not underutilized. We use almost all of what we pay for.\nMemory utilization\nIn the end I checked memory usage. I saw that we did not use all the memory which we were paying for.\nLet’s take a look at the following two graphs.\nThe first one shows maximal memory utilization among all the workers.\n\nThe second one shows memory utilization statistics among all the worker nodes.\n\nThe first one presents average memory usage on a worker node, the second one presents overall memory usage among the whole cluster.\nWe clearly see that we only use around 50% of the memory. Bingo, we pay for memory that we do not use.\nImproving memory utilization\nUsually there are two ways of improving memory utilization:\nChange CPU to memory ratio on worker nodes.\nDecrease the amount of worker nodes.\nI’ve decided to change the CPU to memory ratio rather than decrease the number of worker nodes.\nI did not want to compromise on scalability and time needed to perform a job.\nTest on a 3% subsample of input data has given the following cost of data processing:\nn2-standard-4: $9.48\nn2-highcpu-8: $8.52 (~ 10% less than original price)\nn2d-highcpu-8: $8.57 (~ 10% less than original price)\nWe saved 10% on adjusting CPU and memory ratio.\nIt results in around $12,700 of estimated savings per year (10% of $127,000 annual cost).\nHypothesis\n    Savings1\n  \n[1] Physical resources are underutilized\n    $12,700\n  \nHypothesis testing: physical resources has not the best price-to-performance ratio\nI assumed that the current virtual machine type (n2-standard-4) has not the best price-to-performance ratio.\nTo check performance of different virtual machine types I used CoreMark scores provided by Google Cloud itself.\nBased on CoreMark scores and official Google Cloud VM pricing, I prepared a table which would help me choose the VM type with the best price-to-performance ratio.\nThe most important column is “price per 1 mln points” — how much I need to pay on average to score 1 mln points.\nI used official VM instance prices from Google Cloud site from region europe-west1.\nVirtual Machine Type\n    Points in ScoreMark2\n    Price per hour3\n    Price per 1 mln points\n  \nn2-standard-4\n    66 833 pts\n    $0.21\n    $3.20\n  \nn2-standard-8\n    138 657 pts\n    $0.43\n    $3.08\n  \nn2d-standard-8\n    164 539 pts\n    $0.37\n    $2.26\n  \ne2-standard-8\n    103 808 pts\n    $0.29\n    $2.84\n  \nt2d-standard-8\n    237 411 pts\n    $0.37\n    $1.57\n  \nAs we see, another hypothesis proved to be true. We’re not using the virtual machine type with the best price-to-performance ratio - T2D.\nWe’re using N2 machine type.\nUnfortunately, at the time of writing, T2D machines do not provide CPU to memory ratios other than 3 GB per 1 vCPU.\nIt’s still better than 4 GB per 1 vCPU, but far from 1 or 2 GB per 1 vCPU.\nWe will check if T2D virtual machine type with 4 GB of memory per 1 CPU is cheaper than its counterparts.\nMoving to a virtual machine type with better price-to-performance ratio\nI performed several tests on a small scale (3% subsample of input data) with T2D machine types. Let’s take a look at them.\nn2-standard-4 + HDD: $9.48\nn2-highcpu-8 + HDD: $8.52 (~ 10% less than original price)\nn2d-highcpu-8 + HDD: $8.57 (~ 10% less than original price)\nt2d-standard-8 + HDD: $6.65 (~ 32% less than original price)\nThis way we decreased the estimated processing cost from $127,000 by $40,640 per year to $86,360 (by 32%)1.\nUnfortunately, we also introduced some possible underutilized resources (memory) by changing CPU to memory ratio.\nHypothesis\n    Savings1\n  \n[1] Physical resources are underutilized\n    $12,700\n  \n[2] Moving to a more cost-effective VM type\n    $27,940\n  \nTotal: $40,640 of estimated savings1\nComing back to optimization of virtual machine storage type\nAs I found the most suitable virtual machine type, I was able to focus on choosing between SSD and HDD disk types.\nAs we all know, HDDs are much slower than SSDs, especially in terms of random read/write.\nFor processes where we do not heavily use storage I/O operations there’s no need to move to more expensive SSDs.\nI decided to check if we should use cheaper and slower HDDs or more expensive and faster SSDs.\nI run the pipeline (3% of input data size) with HDD and SSD disks.\nHere are the results for different VM families:\nn2-standard-4 + HDD: $9.48\nn2-highcpu-8 + HDD: $8.52 (~ 10% less than original price)\nn2d-highcpu-8 + HDD: $8.57 (~ 10% less than original price)\nt2d-standard-8 + HDD: $6.65 (~ 32% less than original price)\nt2d-standard-8 + SSD: $5.64 (~ 41% less than original price)\nThis way we decreased the estimated processing cost from $127,000 by $52,070 per year to $74,930 (by 41%)1.\nHypothesis\n    Savings1\n  \n[1] Physical resources are underutilized\n    $12,700\n  \n[2] Moving to a more cost-effective VM type\n    $27,940\n  \n[3] Changing VM disk type to SSD\n    $11,430\n  \nTotal: $52,070 of estimated savings1\nHypothesis testing: configuration of the Dataflow job is not optimal\nDataflow, in comparison to Apache Spark, leaves us with almost no configuration options to be changed.\nIt’s good because in Dataflow you get very decent out-of-the-box settings.\nThe single option which I wanted to tune was if we should use Shuffle Service.\nShuffle Service is a serverless tool that facilitates data shuffling around the cluster, thus relieving worker nodes from this task.\nAlso, node preemption is not so painful because Shuffle Service stores data in external storage independent of worker nodes.\nBut it comes at a price.\nCost breakdown of processing 3% input dataset using virtual machine t2d-standard-8 with SSD disk is presented below3:\nCost of CPU: $2.47\nCost of memory: $0.70\nCost of SSD disk: $0.16\nCost of shuffle service: $2.32\nOverall cost: $5.64\nThus, we see that the cost of the shuffle service plays an important role - it’s more than 40% of the overall cost. Let’s do an experiment and turn Shuffle Service off.\nn2-standard-4 + HDD: $9.48 (original configuration)\nt2d-standard-8 + SSD: $5.64 (~ 41% less than original configuration)\nt2d-standard-8 + SSD + no Shuffle Service: $3.95 (~ 58% less than original configuration)\nBy turning off Shuffle Service we achieved a much lower cost.\nAs a bonus, our memory utilization increased to almost 100%, because we use worker nodes to perform a shuffle.\nSo we eliminated an underutilized T2D issue connected with a CPU to memory ratio.\nNode preemption is not a problem since we’re not utilizing preemptible VMs.\nI must also add that turning off external shuffle service may not always result in lower cost.\nIt depends on many factors, and you should test it on your own data pipeline.\nAlso, you need to take into consideration that the job will usually require more resources (CPU, memory) once you turn off external shuffle service.\nThis way we decreased the estimated processing cost from $127,000 by $73,660 per year to $53,340 (by 58%)1.\nSo it’s now less than half of the initial cost1.\nHypothesis\n    Savings1\n  \n[1] Physical resources are underutilized\n    $12,700\n  \n[2] Moving to a more cost-effective VM type\n    $27,940\n  \n[3] Changing VM disk type to SSD\n    $11,430\n  \n[4] Turning off Shuffle Service\n    $21,590\n  \nTotal: $73,660 of estimated savings1\nNote: Why did we not use Dataflow FlexRS which could lower the processing price by combining preemptible and regular VMs?\nWe did not test it due to how scheduling in FlexRS works.\nWhen you schedule a Dataflow FlexRS job you do not know the exact start time,\nthe only one promise from FlexRS is that the job will start within 6 hours (documentation notes from Google Cloud website on that).\nOur data pipeline must start at specified time and having a 6 hour delay is not acceptable.\nFinal test on a full dataset\nMy last task was to test findings from subsampled input dataset (3%) tests on the full dataset (without subsampling).\nHere are the costs of processing a full dataset for a single day:\nConfiguration\n    Processing cost for one day on a full dataset\n  \nn2-standard-4 + HDD\n    $350.02\n  \nt2d-standard-8 + SSD + shuffle service turned off\n    $134.14 (~ 62% less than original price)\n  \nAs we see, the predicted gain from subsampling was achieved, and savings are even 3 pp higher than estimated.\nFor reference: we estimated, based on runs with 3% of input size, that we will achieve a roughly 58% cost reduction.\nInitial annual cost: $127,000\nEstimated annual cost after optimization: $48,260\nTotal estimated annual savings: $78,740\nPresented figures are only estimates based on a single run and extrapolated to the whole year.\nTo know the exact savings we will need to run the processing pipeline over a year, which hasn’t been done yet.\nSummary\nWe achieved excellent outcome without even touching the processing code.\nSpeculative approach provided good results.\nThere may still be some space for optimization, but within the timeframe I was given, I treat these results as first-rate and do not find any more reasons to further\noptimize the environment and configuration of the Dataflow job.\nAlso, specified strategies do not have to lead to cost optimizations in other pipelines.\nAs every data pipeline is different, some changes which brought cost reduction in this example may result in increased processing cost in different data pipelines.\nWhat is most important in this article: how to approach cost optimization of a data pipeline, not which type of resources to choose.\nPresented figures are only estimates based on a single run (with only 3% of input data) and extrapolated to the whole year with the assumption that processing the whole dataset will result in the same relative savings as processing 3% of source data. ↩ ↩2 ↩3 ↩4 ↩5 ↩6 ↩7 ↩8 ↩9 ↩10 ↩11\nCoreMark results from CoreMark scores provided by Google Cloud itself, retrieved on 04/05/2024. ↩\nOfficial prices taken from Google Cloud site, VM instance pricing in region europe-west1, retrieved on 04/05/2024. ↩ ↩2","guid":"https://blog.allegro.tech/2024/06/cost-optimization-data-pipeline-gcp.html","categories":["tech","big data"],"isoDate":"2024-06-19T22:00:00.000Z"},{"title":"Engineering culture of Allegro \u0026 Allegro Pay: Pragmatic Engineer Score","link":"https://blog.allegro.tech/2024/06/pragmatic-engineer-score.html","pubDate":"Tue, 11 Jun 2024 00:00:00 +0200","authors":{"author":[{"name":["Jakub Dropia"],"photo":["https://blog.allegro.tech/img/authors/jakub.dropia.jpg"],"url":["https://blog.allegro.tech/authors/jakub.dropia"]}]},"content":"\u003cp\u003eOne tech blog/newsletter gained traction and popularity for a couple of years now: \u003ca href=\"https://blog.pragmaticengineer.com/\"\u003ePragmatic Engineer\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eQuoting author:\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eThe #1 technology newsletter on Substack. Highly relevant for software engineers and engineering managers, useful for those working in tech.\nWritten by engineering manager and software engineer Gergely Orosz who was previously at Uber, Skype/Microsoft, and at startups.\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eIn practice, you will find a huge amount of information and internal insights on how big tech works in many companies.\nThere are many deep dives into engineering culture, best practices, and what goes on behind the scenes.\u003c/p\u003e\n\n\u003cp\u003eThere is one particular entry in the blog that I would like to share and talk about:\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://blog.pragmaticengineer.com/pragmatic-engineer-test/\"\u003eThe Pragmatic Engineer Test\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eWhat is it?\u003c/p\u003e\n\n\u003cp\u003eIt is a checklist of 12 questions, and answering them can “measure” the company’s engineering maturity.\u003c/p\u003e\n\n\u003cp\u003eWorking in Allegro Pay for four years, I saw a lot of these practices over the years. Hell, I had the opportunity to build some of them, which\nis a valuable thing here. Everyone is open-minded and you can influence your workplace.\u003c/p\u003e\n\n\u003cp\u003eBut when I came upon this article - it was natural to try to evaluate my current workplace against it.\u003c/p\u003e\n\n\u003cp\u003eI did it, and I would like to share the results with you without further ado.\u003c/p\u003e\n\n\u003ch1 id=\"disclaimer\"\u003eDisclaimer\u003c/h1\u003e\n\n\u003cp\u003eI work at Allegro Pay, a company of Allegro Group responsible for Allegro Pay, Care, and Cash products.\nWhat I write further is heavily grounded in the Allegro Pay context, as we have different tech stacks, environments, and technical platforms.\nHowever, all practices are present both at Allegro and at Allegro Pay. The execution may differ, but engineering maturity is very similar in the end.\u003c/p\u003e\n\n\u003ch1 id=\"tldr\"\u003eTL;DR\u003c/h1\u003e\n\n\u003cp\u003eIn short - Allegro \u0026amp; Allegro Pay scored 11 points out of 12.\u003c/p\u003e\n\n\u003cp\u003eIf you want to stop here - the takeaway is:\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003ethis is a great place for software engineers\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eWe have JAVA, .NET, cloud, our own data centers, a mobile-first approach and modern web, a good microservices ecosystem,\na great internal developer platform (or even two!), data engineering and ML, and a product that makes money.\u003c/p\u003e\n\n\u003cp\u003eWould you like to hear nice, sweet, and bitter details?\u003c/p\u003e\n\n\u003cp\u003eContinue reading 🙂\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://blog.pragmaticengineer.com/pragmatic-engineer-test/\"\u003e12 Questions\u003c/a\u003e and my answers to them.\u003c/p\u003e\n\n\u003ch1 id=\"equity-or-profit-sharing\"\u003eEquity or profit sharing\u003c/h1\u003e\n\n\u003cp\u003eHalf Yes. Not all engineers.\u003c/p\u003e\n\n\u003cp\u003eAllegro Group is a \u003ca href=\"https://www.gpw.pl/company-factsheet?isin=LU2237380790\"\u003epublic trading company\u003c/a\u003e in Poland. Our engineers can gain stocks as a part of their total compensation package. How does this work?\u003c/p\u003e\n\n\u003cp\u003eWell, each senior level and above engineer gains a stocks package yearly as a part of the end-year review. The package is vested over 3 years with (25%, 25% and 50%) proportions.\nVested parts of each package are transferred to your broker account each year and can overlap. The final amount depends on company and individual results.\nIn Poland, these stocks are 19% taxed (if you decide to sell them).\u003c/p\u003e\n\n\u003cp\u003eIn addition, all employees receive a yearly bonus, which, of course, also depends on the company and individual results.\u003c/p\u003e\n\n\u003cp\u003eBoth are a significant addition to our overall compensation package.\u003c/p\u003e\n\n\u003cp\u003eCaveats?\u003c/p\u003e\n\n\u003cp\u003eStocks are still not part of the offer for newcomers, which I think could contribute to attracting more great engineers.\u003c/p\u003e\n\n\u003ch1 id=\"roadmapbacklog-that-engineers-contribute-to\"\u003eRoadmap/backlog that engineers contribute to\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eEach team usually has its backlog. The product manager assigned to that team, the engineering manager, and the team itself are responsible for building\nand maintaining this backlog around functionalities and domains that they own. The backlog is a mix of business features, some maintenance, and technical stories.\nHow it is built and tracked, if teams work in Scrum, Kanban, or some custom approach - is primarily up to the team. In the end, we have some processes that try\nto gather “bigger” deliverables and compose a roadmap and plans for the whole organization at the same root.\u003c/p\u003e\n\n\u003cp\u003eIt works great and allows teams huge flexibility and freedom in their work. As a trade-off, extra work is needed to map these backlogs into\nthe organizational level processes - which, usually, are in Google Sheets or a custom tool.\u003c/p\u003e\n\n\u003ch1 id=\"engineers-directly-working-with-other-ics-individual-contributors\"\u003eEngineers directly working with other ICs (Individual Contributors)\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eWe collaborate with each other, regardless of role and career level. Even if other ICs are in different teams, the expectation is to communicate with them directly.\nYou can just write to anyone, and can expect to get an answer. There are some protections to prevent this from turning into complete chaos, like quarterly\nplanning of dependencies between teams, help channels, and so on, but if everyone works on the same page, we are just working together without unnecessary barriers.\u003c/p\u003e\n\n\u003ch1 id=\"code-reviews-and-testing\"\u003eCode reviews and testing\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eWe have a test platform for automatic E2E tests. Manual testers are available for complex functionalities spanning multiple services.\nTo protect quality, we have code review policies for each repository. In CI/CD, the advanced build system protects us and validates many things\n(unit/integration tests, outdated / beta packages, code formats, etc.) before they go to the main branch.\u003c/p\u003e\n\n\u003cp\u003eAll of that is part of everyday workflow. Sometimes, it slows you down, but it is done smartly and, most of the time, helps. As always, everything is under your control, and in the end, it is your responsibility to use these tools properly.\u003c/p\u003e\n\n\u003ch1 id=\"ci-and-engineers-pushing-to-prod\"\u003eCI and engineers pushing to prod\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eAt Allegro Pay, every commit on the main branch triggers a pipeline that goes through the entire CI/CD process, is automatically deployed to the DEV and TEST environment,\nand stops with manual approval before releasing to PROD. Approval needs the acceptance of another engineer than the one who changes the triggered pipeline.\nEach team is responsible for its changes and deployments. We build it, we run it, and we own it.\u003c/p\u003e\n\n\u003cp\u003eOf course, that can also vary. Sometimes, additional security measurements need to be applied depending on the context and product.\nBut in the end - we have continuous delivery with dozens of deployments daily.\u003c/p\u003e\n\n\u003ch1 id=\"internal-open-source\"\u003eInternal open source\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eEach developer is welcome to issue a PR in components that do not belong to him or his team. We have common internal libraries which are developed and maintained across teams.\nOn the other hand, each repository has only one owner. It works well; people are open-minded and will always consider your contribution.\u003c/p\u003e\n\n\u003cp\u003eIn practice, this doesn’t happen that often. Most of the work is focused on components that your team owns, and sometimes differences between “services”\n(different technologies, architecture, etc.), and lack of proper documentation are barriers to quick contribution - because you need to understand the service\nand domain first before you will be able to change something that you don’t own.\u003c/p\u003e\n\n\u003cp\u003eAdditionally, we have a \u003ca href=\"https://github.com/allegro\"\u003ecatalog\u003c/a\u003e of external open-sourced repositories. You can find many great tools and libraries, some of which you may can even know, like\n\u003ca href=\"https://github.com/allegro/bigcache\"\u003ebigcache\u003c/a\u003e, \u003ca href=\"https://github.com/allegro/hermes\"\u003ehermes\u003c/a\u003e or \u003ca href=\"https://github.com/allegro/ralph\"\u003eralph\u003c/a\u003e. For Allegro Pay itself we also do have \u003ca href=\"https://github.com/topics/allegropay\"\u003esome\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhat is truly unique and I think fits into this position, is internal tourism. Anyone can request to join any team, and as a regular member work up to a couple of months (usually a quarter), contributing to other teams’ work.\u003c/p\u003e\n\n\u003ch1 id=\"healthy-on-call-as-a-priority\"\u003eHealthy on-call as a priority.\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eWe do have on-call duty. This is a part of “we own it”.\u003c/p\u003e\n\n\u003cp\u003eHow this is implemented may vary depending on the area or teams, but in the end, there are some streams of on-duty calls where people\nperform 24-hour on-duty shifts cyclically. These duties are extra paid (for being “ready”). If something happens during duty - your intervention outside working hours is\npaid according to the Polish overtime hours policy (150% or 200% hour rate depends on when this occurs), or you can exchange them for vacation at another time.\u003c/p\u003e\n\n\u003cp\u003eWe have generic alerts, but each stream also has specific rules. There is a common practice where teams improve and change them to remove noise, false positives,\nor simplify on-duty shifts. In the end - SLA must be met - and how teams will approach this - is up to them.\u003c/p\u003e\n\n\u003ch1 id=\"technical-managers\"\u003eTechnical managers.\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eMost of our engineering managers have a background in software engineering. They were seniors once and were promoted to manager, taking a step aside from pure IC.\nEven if hiring from outside, they must complete all the technical workshops. It is expected that they will still be experts in the field.\u003c/p\u003e\n\n\u003cp\u003eThey are deeply rooted in technology. They perform system designs, code reviews, consultancy, and sometimes coding. Proportion varies depending on the team and\nthe manager themselves. Besides people management, they are expected to have ownership of technical decisions and project management of the part which the team is responsible for.\u003c/p\u003e\n\n\u003ch1 id=\"career-ladder-when-above-10-engineers--parallel-ic-and-manager-tracks-when-above-30-engineers\"\u003eCareer ladder (when above 10 engineers) \u0026amp; Parallel IC and manager tracks (when above 30 engineers).\u003c/h1\u003e\n\n\u003cp\u003eYes \u0026amp; half yes\u003c/p\u003e\n\n\u003cp\u003eWe have a career level for Software Engineer Job Family, which starts from a junior position, goes through mid to senior level, and then splits into two tracks - Individual Contributor and Manager.\nThis split is fairly fresh, as there was only a Manager track before. Because of that, this one is pretty mature, with career progression starting from\nEngineering Manager, going through Senior Engineering Manager, Director, VP or CTO.\u003c/p\u003e\n\n\u003cp\u003eIf we are talking about the IC path - here we have right now the Principal Software Engineer, whose scope of the work is at least an area or even the whole organization,\nand the Senior Principal Software Engineer is one person for the whole organization.\u003c/p\u003e\n\n\u003cp\u003eAs you can see, ladders are missing in the IC track; from what I know, this is still in progress. The organization is trying to figure out what IC ladder fits its needs.\u003c/p\u003e\n\n\u003cp\u003eThere are few opportunities for Individual Contributors above the Senior level. This can be improved, and it will likely be.\u003c/p\u003e\n\n\u003ch1 id=\"feedback-culture\"\u003eFeedback culture.\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eI think it is everywhere.\u003c/p\u003e\n\n\u003cp\u003eWe have continuous feedback - 360, peer-to-peer, promotion, employee engagement surveys and during each half-year performance review. At each significant meeting, a space for Q\u0026amp;A.\nFeedback is deeply rooted in our daily work. You can see polls, surveys, requests for feedback and opinions, post-mortems, and so on everywhere.\nIt is hard to imagine what else we could do to cover this topic, one of our culture’s strongest traits.\u003c/p\u003e\n\n\u003ch1 id=\"investing-in-professional-growth\"\u003eInvesting in professional growth.\u003c/h1\u003e\n\n\u003cp\u003eYes.\u003c/p\u003e\n\n\u003cp\u003eThis is realized in multiple ways. Each team/individual has a “training budget” - this is money you can spend on external training, courses, and conferences.\nIt differs from team to team, and used to be much better in past.\nAdditionally, we have an internal learning platform with many great workshops - especially in the soft skills area. They are great! You can upskill yourself well.\u003c/p\u003e\n\n\u003cp\u003eAlso, in some areas and teams, there is a time dedicated to your self-development. You can spend it on contributions to open-source, reading a book,\nlearning from the course, or, for example, writing a PoC of new technology with your team. In Allegro Pay - it is 10%. How you spend it - is up to you,\nor the team, it just should stick to our profession.\u003c/p\u003e\n\n\u003cp\u003eThere are many internal and external communities (guilds), each with its own meeting calendar and interesting presentations and workshops taking a different kind of forms.\nThere are also many internal events, hackathons, and initiatives. Opportunities to learn are almost infinite.\u003c/p\u003e\n\n\u003cp\u003eLanding here was my biggest personal and professional progression so far.\u003c/p\u003e\n\n\u003ch1 id=\"the-bitter-or-not\"\u003eThe bitter (or not?)\u003c/h1\u003e\n\n\u003cp\u003eSounds sweet, right? Where is the bitter here? Well, I am not sure.\u003c/p\u003e\n\n\u003cp\u003eThis is a rapid-growth product and company. We are focused on delivering value to clients and maximizing profit from our products.\nEverything we do must contribute to overall success, and there is little space to “breathe”. You must often balance delivering functionalities,\npaying back technical debt, and growing scale. Taking shortcuts. Making trade-offs. Asking difficult questions. Our roadmaps often change\nbecause of the economics, law, or maybe data we gathered and told us that our actions do not convert in the way we assumed.\nIn Allegro Pay itself - the financial domain also does not help — a huge amount of our work is dedicated to legal matters. New laws pop up, and we must follow them.\u003c/p\u003e\n\n\u003cp\u003eI can imagine that it can’t be for everyone.\u003c/p\u003e\n\n\u003cp\u003eBut for me, this introduces an entirely new layer of engineering, where you need to be smart, cautious, value impact, and make the right choices.\u003c/p\u003e\n\n\u003cp\u003eWe strive to be the best in the market, which is why we succeed.\u003c/p\u003e\n\n\u003cp\u003eAnother thing can be the corporation itself. But this is very likely something that you will not notice until you become a manager.\u003c/p\u003e\n\n\u003cp\u003eAllegro is a big company that has shifted to a more centralized and structured approach over the years. Everything needs to be aligned with the process.\nTo picture this, here are a few examples:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eInstead of ordering any accessories required within the budget - the budget was removed, and you can order only specific, pre-selected accessories\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eWant to hire someone? There is budgeting once per year, and you need to come prepared to justify another full-time equivalent.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eDo you want to give someone a raise? Well, you don’t have to worry about this. Process, one per year, will do that for you.\nYou need to provide a performance review of your directs. Based on that you will get the budget, recommendations, and ability to slightly change proportions.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eWant to pursue external training? You have a budget. It would be best if you fit it in. You need to raise a request and process it through several layers of acceptance.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAs you can imagine, all of that can take time and be annoying. It is very frequent that your “request” is stuck somewhere, and you need to “push” it.\nBut on the other end of the process, there are helpful people whom you can always talk to.\u003c/p\u003e\n\n\u003cp\u003eSometimes, this leads to funny absurdities - you find an old monitor in the office that is not assigned to anyone (or a person who is not already in the company),\nand you would like to order a docking station for it. “Procedures” will not allow you to do that. The dock must have existed before; if lost,\nonly the owner can “order” a new one with a good justification. But you are not the owner. It is no man’s land - thus - no dock for it ;)\u003c/p\u003e\n\n\u003cp\u003eBut I think most big corporations work like that. The past few years were also difficult for the industry. I understand why this is happening.\nIf you are an individual contributor, most of these things will not affect you. And those which do - you need to get used to it, and if you focus on the rest - hell - this is a great place to work.\u003c/p\u003e\n\n\u003ch1 id=\"is-there-more\"\u003eIs there more?\u003c/h1\u003e\n\n\u003cp\u003eA score of 11 tells that you will find much good stuff in software engineering here.\u003c/p\u003e\n\n\u003cp\u003eBut this is not all. There are plenty of other great features of engineering culture at Allegro \u0026amp; Allegro Pay. You have great products, a big scale and\na data-driven approach which leads to many challenges; amazing, intelligent people; modern technology and approach to software engineering;\nrich off-topic communities (board games, sports, FIFA league, etc.), and many more.\u003c/p\u003e\n\n\u003cp\u003eOverall, #DobrzeTuByć (#GoodToBeHere)\u003c/p\u003e\n","contentSnippet":"One tech blog/newsletter gained traction and popularity for a couple of years now: Pragmatic Engineer.\nQuoting author:\nThe #1 technology newsletter on Substack. Highly relevant for software engineers and engineering managers, useful for those working in tech.\nWritten by engineering manager and software engineer Gergely Orosz who was previously at Uber, Skype/Microsoft, and at startups.\nIn practice, you will find a huge amount of information and internal insights on how big tech works in many companies.\nThere are many deep dives into engineering culture, best practices, and what goes on behind the scenes.\nThere is one particular entry in the blog that I would like to share and talk about:\nThe Pragmatic Engineer Test\nWhat is it?\nIt is a checklist of 12 questions, and answering them can “measure” the company’s engineering maturity.\nWorking in Allegro Pay for four years, I saw a lot of these practices over the years. Hell, I had the opportunity to build some of them, which\nis a valuable thing here. Everyone is open-minded and you can influence your workplace.\nBut when I came upon this article - it was natural to try to evaluate my current workplace against it.\nI did it, and I would like to share the results with you without further ado.\nDisclaimer\nI work at Allegro Pay, a company of Allegro Group responsible for Allegro Pay, Care, and Cash products.\nWhat I write further is heavily grounded in the Allegro Pay context, as we have different tech stacks, environments, and technical platforms.\nHowever, all practices are present both at Allegro and at Allegro Pay. The execution may differ, but engineering maturity is very similar in the end.\nTL;DR\nIn short - Allegro \u0026 Allegro Pay scored 11 points out of 12.\nIf you want to stop here - the takeaway is:\nthis is a great place for software engineers\nWe have JAVA, .NET, cloud, our own data centers, a mobile-first approach and modern web, a good microservices ecosystem,\na great internal developer platform (or even two!), data engineering and ML, and a product that makes money.\nWould you like to hear nice, sweet, and bitter details?\nContinue reading 🙂\n12 Questions and my answers to them.\nEquity or profit sharing\nHalf Yes. Not all engineers.\nAllegro Group is a public trading company in Poland. Our engineers can gain stocks as a part of their total compensation package. How does this work?\nWell, each senior level and above engineer gains a stocks package yearly as a part of the end-year review. The package is vested over 3 years with (25%, 25% and 50%) proportions.\nVested parts of each package are transferred to your broker account each year and can overlap. The final amount depends on company and individual results.\nIn Poland, these stocks are 19% taxed (if you decide to sell them).\nIn addition, all employees receive a yearly bonus, which, of course, also depends on the company and individual results.\nBoth are a significant addition to our overall compensation package.\nCaveats?\nStocks are still not part of the offer for newcomers, which I think could contribute to attracting more great engineers.\nRoadmap/backlog that engineers contribute to\nYes.\nEach team usually has its backlog. The product manager assigned to that team, the engineering manager, and the team itself are responsible for building\nand maintaining this backlog around functionalities and domains that they own. The backlog is a mix of business features, some maintenance, and technical stories.\nHow it is built and tracked, if teams work in Scrum, Kanban, or some custom approach - is primarily up to the team. In the end, we have some processes that try\nto gather “bigger” deliverables and compose a roadmap and plans for the whole organization at the same root.\nIt works great and allows teams huge flexibility and freedom in their work. As a trade-off, extra work is needed to map these backlogs into\nthe organizational level processes - which, usually, are in Google Sheets or a custom tool.\nEngineers directly working with other ICs (Individual Contributors)\nYes.\nWe collaborate with each other, regardless of role and career level. Even if other ICs are in different teams, the expectation is to communicate with them directly.\nYou can just write to anyone, and can expect to get an answer. There are some protections to prevent this from turning into complete chaos, like quarterly\nplanning of dependencies between teams, help channels, and so on, but if everyone works on the same page, we are just working together without unnecessary barriers.\nCode reviews and testing\nYes.\nWe have a test platform for automatic E2E tests. Manual testers are available for complex functionalities spanning multiple services.\nTo protect quality, we have code review policies for each repository. In CI/CD, the advanced build system protects us and validates many things\n(unit/integration tests, outdated / beta packages, code formats, etc.) before they go to the main branch.\nAll of that is part of everyday workflow. Sometimes, it slows you down, but it is done smartly and, most of the time, helps. As always, everything is under your control, and in the end, it is your responsibility to use these tools properly.\nCI and engineers pushing to prod\nYes.\nAt Allegro Pay, every commit on the main branch triggers a pipeline that goes through the entire CI/CD process, is automatically deployed to the DEV and TEST environment,\nand stops with manual approval before releasing to PROD. Approval needs the acceptance of another engineer than the one who changes the triggered pipeline.\nEach team is responsible for its changes and deployments. We build it, we run it, and we own it.\nOf course, that can also vary. Sometimes, additional security measurements need to be applied depending on the context and product.\nBut in the end - we have continuous delivery with dozens of deployments daily.\nInternal open source\nYes.\nEach developer is welcome to issue a PR in components that do not belong to him or his team. We have common internal libraries which are developed and maintained across teams.\nOn the other hand, each repository has only one owner. It works well; people are open-minded and will always consider your contribution.\nIn practice, this doesn’t happen that often. Most of the work is focused on components that your team owns, and sometimes differences between “services”\n(different technologies, architecture, etc.), and lack of proper documentation are barriers to quick contribution - because you need to understand the service\nand domain first before you will be able to change something that you don’t own.\nAdditionally, we have a catalog of external open-sourced repositories. You can find many great tools and libraries, some of which you may can even know, like\nbigcache, hermes or ralph. For Allegro Pay itself we also do have some.\nWhat is truly unique and I think fits into this position, is internal tourism. Anyone can request to join any team, and as a regular member work up to a couple of months (usually a quarter), contributing to other teams’ work.\nHealthy on-call as a priority.\nYes.\nWe do have on-call duty. This is a part of “we own it”.\nHow this is implemented may vary depending on the area or teams, but in the end, there are some streams of on-duty calls where people\nperform 24-hour on-duty shifts cyclically. These duties are extra paid (for being “ready”). If something happens during duty - your intervention outside working hours is\npaid according to the Polish overtime hours policy (150% or 200% hour rate depends on when this occurs), or you can exchange them for vacation at another time.\nWe have generic alerts, but each stream also has specific rules. There is a common practice where teams improve and change them to remove noise, false positives,\nor simplify on-duty shifts. In the end - SLA must be met - and how teams will approach this - is up to them.\nTechnical managers.\nYes.\nMost of our engineering managers have a background in software engineering. They were seniors once and were promoted to manager, taking a step aside from pure IC.\nEven if hiring from outside, they must complete all the technical workshops. It is expected that they will still be experts in the field.\nThey are deeply rooted in technology. They perform system designs, code reviews, consultancy, and sometimes coding. Proportion varies depending on the team and\nthe manager themselves. Besides people management, they are expected to have ownership of technical decisions and project management of the part which the team is responsible for.\nCareer ladder (when above 10 engineers) \u0026 Parallel IC and manager tracks (when above 30 engineers).\nYes \u0026 half yes\nWe have a career level for Software Engineer Job Family, which starts from a junior position, goes through mid to senior level, and then splits into two tracks - Individual Contributor and Manager.\nThis split is fairly fresh, as there was only a Manager track before. Because of that, this one is pretty mature, with career progression starting from\nEngineering Manager, going through Senior Engineering Manager, Director, VP or CTO.\nIf we are talking about the IC path - here we have right now the Principal Software Engineer, whose scope of the work is at least an area or even the whole organization,\nand the Senior Principal Software Engineer is one person for the whole organization.\nAs you can see, ladders are missing in the IC track; from what I know, this is still in progress. The organization is trying to figure out what IC ladder fits its needs.\nThere are few opportunities for Individual Contributors above the Senior level. This can be improved, and it will likely be.\nFeedback culture.\nYes.\nI think it is everywhere.\nWe have continuous feedback - 360, peer-to-peer, promotion, employee engagement surveys and during each half-year performance review. At each significant meeting, a space for Q\u0026A.\nFeedback is deeply rooted in our daily work. You can see polls, surveys, requests for feedback and opinions, post-mortems, and so on everywhere.\nIt is hard to imagine what else we could do to cover this topic, one of our culture’s strongest traits.\nInvesting in professional growth.\nYes.\nThis is realized in multiple ways. Each team/individual has a “training budget” - this is money you can spend on external training, courses, and conferences.\nIt differs from team to team, and used to be much better in past.\nAdditionally, we have an internal learning platform with many great workshops - especially in the soft skills area. They are great! You can upskill yourself well.\nAlso, in some areas and teams, there is a time dedicated to your self-development. You can spend it on contributions to open-source, reading a book,\nlearning from the course, or, for example, writing a PoC of new technology with your team. In Allegro Pay - it is 10%. How you spend it - is up to you,\nor the team, it just should stick to our profession.\nThere are many internal and external communities (guilds), each with its own meeting calendar and interesting presentations and workshops taking a different kind of forms.\nThere are also many internal events, hackathons, and initiatives. Opportunities to learn are almost infinite.\nLanding here was my biggest personal and professional progression so far.\nThe bitter (or not?)\nSounds sweet, right? Where is the bitter here? Well, I am not sure.\nThis is a rapid-growth product and company. We are focused on delivering value to clients and maximizing profit from our products.\nEverything we do must contribute to overall success, and there is little space to “breathe”. You must often balance delivering functionalities,\npaying back technical debt, and growing scale. Taking shortcuts. Making trade-offs. Asking difficult questions. Our roadmaps often change\nbecause of the economics, law, or maybe data we gathered and told us that our actions do not convert in the way we assumed.\nIn Allegro Pay itself - the financial domain also does not help — a huge amount of our work is dedicated to legal matters. New laws pop up, and we must follow them.\nI can imagine that it can’t be for everyone.\nBut for me, this introduces an entirely new layer of engineering, where you need to be smart, cautious, value impact, and make the right choices.\nWe strive to be the best in the market, which is why we succeed.\nAnother thing can be the corporation itself. But this is very likely something that you will not notice until you become a manager.\nAllegro is a big company that has shifted to a more centralized and structured approach over the years. Everything needs to be aligned with the process.\nTo picture this, here are a few examples:\nInstead of ordering any accessories required within the budget - the budget was removed, and you can order only specific, pre-selected accessories\nWant to hire someone? There is budgeting once per year, and you need to come prepared to justify another full-time equivalent.\nDo you want to give someone a raise? Well, you don’t have to worry about this. Process, one per year, will do that for you.\nYou need to provide a performance review of your directs. Based on that you will get the budget, recommendations, and ability to slightly change proportions.\nWant to pursue external training? You have a budget. It would be best if you fit it in. You need to raise a request and process it through several layers of acceptance.\nAs you can imagine, all of that can take time and be annoying. It is very frequent that your “request” is stuck somewhere, and you need to “push” it.\nBut on the other end of the process, there are helpful people whom you can always talk to.\nSometimes, this leads to funny absurdities - you find an old monitor in the office that is not assigned to anyone (or a person who is not already in the company),\nand you would like to order a docking station for it. “Procedures” will not allow you to do that. The dock must have existed before; if lost,\nonly the owner can “order” a new one with a good justification. But you are not the owner. It is no man’s land - thus - no dock for it ;)\nBut I think most big corporations work like that. The past few years were also difficult for the industry. I understand why this is happening.\nIf you are an individual contributor, most of these things will not affect you. And those which do - you need to get used to it, and if you focus on the rest - hell - this is a great place to work.\nIs there more?\nA score of 11 tells that you will find much good stuff in software engineering here.\nBut this is not all. There are plenty of other great features of engineering culture at Allegro \u0026 Allegro Pay. You have great products, a big scale and\na data-driven approach which leads to many challenges; amazing, intelligent people; modern technology and approach to software engineering;\nrich off-topic communities (board games, sports, FIFA league, etc.), and many more.\nOverall, #DobrzeTuByć (#GoodToBeHere)","guid":"https://blog.allegro.tech/2024/06/pragmatic-engineer-score.html","categories":["tech","engineering culture","pragmatic engineer"],"isoDate":"2024-06-10T22:00:00.000Z"},{"title":"REST service client: design, testing, monitoring","link":"https://blog.allegro.tech/2024/06/rest-service-client-design-testing-monitoring.html","pubDate":"Tue, 04 Jun 2024 00:00:00 +0200","authors":{"author":[{"name":["Piotr Klimiec"],"photo":["https://blog.allegro.tech/img/authors/piotr.klimiec.jpg"],"url":["https://blog.allegro.tech/authors/piotr.klimiec"]}]},"content":"\u003cp\u003eThe purpose of this article is to present how to design, test, and monitor a REST service client.\nThe article includes a \u003ca href=\"https://github.com/Klimiec/webclients\"\u003erepository\u003c/a\u003e with clients written in Kotlin using various technologies such as \u003ca href=\"https://docs.spring.io/spring-framework/reference/web/webflux-webclient.html\"\u003eWebClient\u003c/a\u003e,\n\u003ca href=\"https://docs.spring.io/spring-framework/reference/integration/rest-clients.html#rest-restclient\"\u003eRestClient\u003c/a\u003e,\n\u003ca href=\"https://ktor.io/docs/getting-started-ktor-client.html\"\u003eKtor Client\u003c/a\u003e,\n\u003ca href=\"https://square.github.io/retrofit/\"\u003eRetrofit\u003c/a\u003e.\nIt demonstrates how to send and retrieve data from an external service, add a cache layer, and parse the received response into domain objects.\u003c/p\u003e\n\n\u003ch2 id=\"motivation\"\u003eMotivation\u003c/h2\u003e\n\u003cp\u003eWhy do we need objects in the project that encapsulate the HTTP clients we use?\nTo begin with, we want to separate the domain from technical details.\nThe way we retrieve/send data and handle errors, which can be quite complex in the case of HTTP clients, should not clutter business logic.\nNext, testability. Even if we do not use \u003ca href=\"/2020/05/hexagonal-architecture-by-example.html\"\u003ehexagonal architecture\u003c/a\u003e in our applications,\nit’s beneficial to separate the infrastructure from the service layer, as it improves testability.\nVerifying an HTTP service client is not a simple task and requires consideration of many cases — mainly at the integration level.\nHaving a separate “building block“ that encapsulates communication with the outside world makes testing much easier.\nFinally, reusability. A service client that has been written once can be successfully used in other projects.\u003c/p\u003e\n\n\u003ch2 id=\"client-design\"\u003eClient Design\u003c/h2\u003e\n\u003cp\u003eAs a case study, I will use an example implementation that utilizes WebClient for retrieving data from the Order Management Service,\nan example service that might appear in an e-commerce site such as \u003ca href=\"https://allegro.tech/\"\u003eAllegro\u003c/a\u003e.\nThe heart of our client is the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eexecuteHttpRequest\u003c/code\u003e method, which is responsible for executing the provided HTTP request, logging, and error handling.\nIt is not part of the WebClient library.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrderManagementServiceClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eorderManagementServiceApi\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrderManagementServiceApi\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003esuspend\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eClientId\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrdersDto\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nf\"\u003eexecuteHttpRequest\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n            \u003cspan class=\"n\"\u003einitialLog\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"[$clientName] Get orders for a clientId= $clientId\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003erequest\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003eorderManagementServiceApi\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e},\u003c/span\u003e\n            \u003cspan class=\"n\"\u003esuccessLog\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"[$clientName] Returned orders for a clientId= $clientId\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n            \u003cspan class=\"n\"\u003efailureMessage\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"[$clientName] Failed to get orders for clientId= $clientId\"\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFull working example can be found \u003ca href=\"https://github.com/Klimiec/webclients/tree/main/httpclient-webclientinterface\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"client-name\"\u003eClient name\u003c/h3\u003e\n\u003cp\u003eI like to name clients using the convention: name of the service we integrate with, plus the suffix \u003cstrong\u003eClient\u003c/strong\u003e.\nIn the case of integration with the Order Management Service, such a class will be named \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eOrderManagementServiceClient\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eIf the technology we use employs an interface to describe the called REST API (RestClient, WebClient, Retrofit),\nwe can name such an interface \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eOrderManagementServiceApi\u003c/code\u003e — following the general pattern of the service name with the suffix \u003cstrong\u003eApi\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eThese names may seem intuitive and obvious, but without an established naming convention, we might end up with a project where\ndifferent integrations have the following suffixes: \u003cstrong\u003eHttpClient\u003c/strong\u003e, \u003cstrong\u003eFacade\u003c/strong\u003e, \u003cstrong\u003eWebClient\u003c/strong\u003e, \u003cstrong\u003eAdapter\u003c/strong\u003e, and \u003cstrong\u003eService\u003c/strong\u003e.\nIt’s important to have a consistent convention and adhere to it throughout the project.\u003c/p\u003e\n\n\u003ch3 id=\"api\"\u003eAPI\u003c/h3\u003e\n\u003cp\u003eMethods of our clients should have names that reflect the communicative intention behind them.\nTo capture this intention, it is necessary to use a verb in the method’s name.\nTypically, the correct name will have a structure of verb + resource name, for example, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetOrders\u003c/code\u003e  —  for methods that retrieve resources.\nIf we want to narrow down the number of returned resources using filters or return a particular resource, I recommend adding the suffix “For” before the list of parameters.\nTechnically, these parameters will be part of the query or path parameters.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003efun getOrdersFor(clientId: ClientId): OrdersDto\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFor methods responsible for creating resources, simply using the verb in the method name is enough,\nas the resource being passed as a parameter effectively conveys the intention of the method.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003efun publish(event: InvoiceCreatedEventDto)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3 id=\"logging\"\u003eLogging\u003c/h3\u003e\n\u003cp\u003eWhen communicating with external service we’d like to log the beginning of the interaction, indicating our intention to fetch or send a resource,\nas well as its outcome. The outcome can be either a success, meaning receiving a response with a 2xx status code, or a failure.\u003c/p\u003e\n\n\u003cp\u003eFailure can be signaled by status codes (3xx, 4xx, 5xx), resulting from the inability to deserialize the received response into an object,\nexceeding the response time, etc. Generally, \u003ca href=\"/2015/07/testing-server-faults-with-Wiremock.html\"\u003emany things can go wrong\u003c/a\u003e.\nDepending on the cause of failure, we may want to log the interaction result at different levels (warn/error).\nThere are critical errors that are worth distinguishing (error), and those that will occasionally occur (warn) and don’t require urgent intervention.\u003c/p\u003e\n\n\u003cp\u003eTo filter logs related to a specific service while browsing through them, I like to include the client’s name within curly braces at the beginning of the logs.\nFor logging technical aspects of the communication, such as the URL called, HTTP method used, and response code,\nwe use filters (logRequestInfo, logResponseInfo) that are plugged in at the client configuration level in the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecreateExternalServiceApi\u003c/code\u003e method.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003einline\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"k\"\u003ereified\u003c/span\u003e \u003cspan class=\"nc\"\u003eT\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"nf\"\u003ecreateExternalServiceApi\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ewebClientBuilder\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eWebClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nc\"\u003eBuilder\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eConnectionProperties\u003c/span\u003e\n\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eT\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ewebClientBuilder\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eclientConnector\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nf\"\u003ehttpClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ebaseUrl\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebaseUrl\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edefaultRequest\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eattribute\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eSERVICE_NAME\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efilter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nf\"\u003elogRequestInfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efilter\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nf\"\u003elogResponseInfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ebuild\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elet\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nc\"\u003eWebClientAdapter\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ecreate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003elet\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"nc\"\u003eHttpServiceProxyFactory\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ebuilderFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003ebuild\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ecreateClient\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eT\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003elogRequestInfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eExchangeFilterFunction\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eofRequestProcessor\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003erequest\u003c/span\u003e \u003cspan class=\"p\"\u003e-\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einfo\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"s\"\u003e\"[$clientName] method=[${request.method().name()}] url=${request.url()}}\"\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"nc\"\u003eMono\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ejust\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003elogResponseInfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eExchangeFilterFunction\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eofResponseProcessor\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e-\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einfo\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"s\"\u003e\"[$clientName] service responded with a status code= ${response.statusCode()}\"\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"nc\"\u003eMono\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ejust\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHere’s an example of logged interaction for successfully fetching a resource.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Properly logged interaction\" src=\"/img/articles/2024-06-04-rest-service-client-design-testing-monitoring/logs.png\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eTo prevent redundancy in logging code across multiple clients, it is centralized inside \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eexecuteHttpRequest\u003c/code\u003e method.\nThe only thing the developer needs to do is to provide a business-oriented description for the beginning of the interaction and its outcome (parameters: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einitialLog\u003c/code\u003e, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esuccessLog\u003c/code\u003e, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efailureMessage\u003c/code\u003e).\u003c/p\u003e\n\n\u003cp\u003eWhy do I emphasize logging so much?\nIsn’t it enough to log only errors?\nAfter all, we have metrics that inform us about the performance of our clients.\nMetrics won’t provide us with the details of the communication, but logs will.\nThese details can turn out to be crucial in the analysis of incidents, which may reveal, for example, incorrect data produced by our service.\u003c/p\u003e\n\n\u003cp\u003eLogs are like backups. We find out if we have them and how valuable they are only when they are needed,\neither because the business team requests an analysis of a particular case or when resolving an incident.\u003c/p\u003e\n\n\u003ch3 id=\"error-handling\"\u003eError handling\u003c/h3\u003e\n\u003cp\u003eWhen writing client code, we aim to highlight maximally how we send/retrieve data and hide the “noise“ that comes from error handling.\nIn the case of HTTP clients, error handling is quite extensive but generic enough that the resulting code can be written once and reused across all clients.\nIn our example, error handling mechanism is hidden inside \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eexecuteHttpRequest\u003c/code\u003e method.\nIt consists of two things: logging and throwing custom exceptions that encapsulate technical exceptions thrown by the underlying HTTP client.\u003c/p\u003e\n\n\u003cp\u003eWhat are the benefits of using custom exceptions? The very name of such a custom exception tells us exactly what went wrong.\nFor comparison, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceIncorrectResponseBodyException\u003c/code\u003e seems to be more descriptive than \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDecodingException\u003c/code\u003e.\nThey also help group various technical exceptions that lead to the same cause, for example, an incorrect response object structure.\nAdditionally, based on these exceptions, visualizations can be created to show the state of our integration.\nFor example, we can create a table that will show how many exceptions of any given type were thrown by our clients within a specified period.\nHaving custom exceptions, we are 100% certain that these exceptions were thrown only by our clients.\u003c/p\u003e\n\n\u003ch3 id=\"testing\"\u003eTesting\u003c/h3\u003e\n\u003ch4 id=\"stubs\"\u003eStubs\u003c/h4\u003e\n\u003cp\u003eTo verify different scenarios of our HTTP client, it is necessary to appropriately stub the called endpoints in tests.\nFor this purpose, we will use the \u003ca href=\"https://wiremock.org/\"\u003eWireMock\u003c/a\u003e library.\u003c/p\u003e\n\n\u003cp\u003eIt is quite important that the technical details of created stubs do not leak into the tests.\nThe test should describe the behavior being tested and encapsulate technical details.\nFor example, changing the accept/content-type header or making minor modifications to the called URL should not affect the test itself.\nTo achieve this, for each service for which we write a service client, we create an object of type \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eStubBuilder\u003c/code\u003e.\nThe \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eStubBuilder\u003c/code\u003e allows hiding the details of stubbing and verification behind a readable API.\nIt takes on the impact of changes to the called API, protecting our test from modification.\nIt fulfills a similar role to the \u003ca href=\"https://martinfowler.com/bliki/PageObject.html\"\u003ePage Object Pattern\u003c/a\u003e in end-to-end tests for web apps.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eorderManagementServiceStub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eordersPlacedBySomeCustomer\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eStubBuilders for services that return data come in two flavors - \u003ca href=\"https://github.com/Klimiec/webclients/tree/591dddd1e61ea5d922f0402534d9a96a513f59b4/httpclient-webclientinterface/src/integration/kotlin/com/dev/sandbox/httpclientwebclientinterface/order/infrastructure/ordermanagementservice/stub/internal\"\u003einternal\u003c/a\u003e and \u003ca href=\"https://github.com/Klimiec/webclients/tree/591dddd1e61ea5d922f0402534d9a96a513f59b4/httpclient-webclientinterface/src/integration/kotlin/com/dev/sandbox/httpclientwebclientinterface/order/infrastructure/ordermanagementservice/stub/external\"\u003eexternal\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"StubBuilder packages\" src=\"/img/articles/2024-06-04-rest-service-client-design-testing-monitoring/packages.png\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen testing a service client, we want to have great flexibility in simulating responses.\nTherefore, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eStubBuilders\u003c/code\u003e from the internal package will model response objects as a string. This allows us to simulate any scenario.\nIn end-to-end tests, where a given service is part of the bigger process, such flexibility is not necessary; in fact, it is not even recommended.\nTherefore, StubBuilders from the external package model responses using real objects.\nAll StubBuilders from the external packages are declared in the class \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceStubs\u003c/code\u003e, to which a reference is located in the base class for\nall integration tests, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBaseIntegrationTest\u003c/code\u003e. This allows us to have very easy access to all external service stubs in our integration tests.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003estub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eorderManagementService\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eordersPlacedBySomeCustomer\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eReading the code above, we immediately know \u003cstrong\u003ewhich\u003c/strong\u003e service is being interacted with (Order Management Service) and what will be returned from it (Orders).\nThe technical details of the stubbed endpoint have been hidden inside the StubBuilder object.\nTests should emphasize “what” and encapsulate “how.” This way, they can serve as documentation.\u003c/p\u003e\n\n\u003ch4 id=\"test-data\"\u003eTest Data\u003c/h4\u003e\n\n\u003cp\u003eThe data returned by our stubs can be prepared in three ways:\u003c/p\u003e\n\u003col type=\"a\"\u003e\n  \u003cli\u003eRead the entire response from a file/string.\u003c/li\u003e\n  \u003cli\u003ePrepare the response using real objects used in the service for deserializing responses from called services.\u003c/li\u003e\n  \u003cli\u003eCreate a set of separate objects modeling the returned response from the service for testing purposes and use them to prepare the returned data.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eWhich option to choose?\nTo answer this question, we should analyze the advantages and disadvantages of each approach.\u003c/p\u003e\n\n\u003cp\u003eOption A — read response from a file/string. Response creation is very fast and simple.\nIt allows \u003cstrong\u003everifying the contract\u003c/strong\u003e between the client and the supplier (at least at the time of writing the test).\nImagine that during refactoring, one of the fields in the response object accidentally changes.\nIn such a case, client tests using this approach will detect the defect before the code reaches production.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nd\"\u003e@Test\u003c/span\u003e\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003e`should\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eorders\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ea\u003c/span\u003e \u003cspan class=\"n\"\u003egiven\u003c/span\u003e \u003cspan class=\"nf\"\u003eclientId`\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eUnit\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003erunBlocking\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// given\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientId\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eanyClientId\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eorderManagementServiceStub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eordersPlacedBySomeCustomer\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// when\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrdersDto\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eorderManagementServiceClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// then\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldBe\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrdersDto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nf\"\u003elistOf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eOrderDto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"7952a9ab-503c-4483-beca-32d081cc2446\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eOn the other hand, keeping data in files/strings is difficult to maintain and reuse.\nProgrammers often copy entire files for new tests, introducing only minimal changes.\nThere is a problem with naming these files and refactoring them when the called service introduces an incompatible change.\u003c/p\u003e\n\n\u003cp\u003eOption B — Use real response objects.\nIt allows writing one-line, readable assertions and maximally reusing already created data, especially using \u003ca href=\"https://www.natpryce.com/articles/000714.html\"\u003etest data builders\u003c/a\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"nd\"\u003e@Test\u003c/span\u003e\n    \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003e`should\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eorders\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ea\u003c/span\u003e \u003cspan class=\"n\"\u003egiven\u003c/span\u003e \u003cspan class=\"nf\"\u003eclientId`\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eUnit\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003erunBlocking\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// given\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientId\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eanyClientId\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientOrders\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrderManagementServiceFixture\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eordersPlacedBySomeCustomer\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eorderManagementServiceStub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eclientOrders\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// when\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrdersDto\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eorderManagementServiceClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// then\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldBe\u003c/span\u003e \u003cspan class=\"n\"\u003eclientOrders\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eHowever, accidental change of field name which results in the  \u003cstrong\u003econtract violation\u003c/strong\u003e between the client and supplier won’t be caught.\nAs a result, we might have perfectly tested communication in integration tests that will not work in production.\u003c/p\u003e\n\n\u003cp\u003eOption C — create a set of separate response objects. It has all the advantages of options A and B, including maintainability, reusability, and\nverification of the contract between the client and the supplier. Unfortunately, maintaining a separate model for testing purposes comes with some overhead\nand requires discipline on the developers’ side, which can be challenging to maintain.\u003c/p\u003e\n\n\u003cp\u003eWhich option to choose? Personally, I prefer a hybrid of options A and B.\nFor the purpose of testing the “happy path“ in client tests, I return a response that is entirely stored as a string (alternatively, it can be read from a file).\nSuch a test allows not only to verify the contract but also the correctness of deserializing the received response into a response object.\nIn other tests (cache, adapter, end-to-end), I create responses returned by the stubbed endpoint using production response objects.\u003c/p\u003e\n\n\u003cp\u003eIt’s worthwhile to keep sample test data in dedicated classes, such as a Fixture class, for each integration (for example \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eOrderManagementServiceFixture\u003c/code\u003e).\nThis allows the reuse of test data and enhances the readability of the tests themselves.\u003c/p\u003e\n\n\u003ch3 id=\"test-scenarios\"\u003eTest Scenarios\u003c/h3\u003e\n\u003ch4 id=\"happy-path\"\u003eHappy Path\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eFetching a resource\u003c/strong\u003e — verification whether the client can retrieve data from the previously stubbed endpoint and deserialize it into a response object.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nd\"\u003e@Test\u003c/span\u003e\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003e`should\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eorders\u003c/span\u003e \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ea\u003c/span\u003e \u003cspan class=\"n\"\u003egiven\u003c/span\u003e \u003cspan class=\"nf\"\u003eclientId`\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eUnit\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003erunBlocking\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// given\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientId\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eanyClientId\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eorderManagementServiceStub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eordersPlacedBySomeCustomer\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// when\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eresponse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrdersDto\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eorderManagementServiceClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// then\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldBe\u003c/span\u003e \u003cspan class=\"nc\"\u003eOrdersDto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nf\"\u003elistOf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eOrderDto\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"7952a9ab-503c-4483-beca-32d081cc2446\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eAn essential part of the test for the “happy path“ is verification of the contract between the client and the supplier.\nThe \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eordersPlacedBySomeCustomer\u003c/code\u003e method returns a sample response guaranteed by the supplier (Order Management Service).\nOn the client side, in the assertion section, we check if this message has been correctly deserialized into a response object.\nInstead of comparing individual fields with the expected value, I highly recommend comparing entire objects (returned and expected).\nIt gives us confidence that all fields have been compared. In the case of regression, modern IDEs such as IntelliJ indicate exactly where the problem is.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Test regression\" src=\"/img/articles/2024-06-04-rest-service-client-design-testing-monitoring/regression.png\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eSending a resource\u003c/strong\u003e — verification whether the client sends data to the specified URL in a format acceptable by the previously stubbed endpoint.\nIn the following example, I test publishing an event to \u003ca href=\"https://hermes.allegro.tech/\"\u003eHermes\u003c/a\u003e, a message broker built on top of Kafka widely used at Allegro.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nd\"\u003e@Test\u003c/span\u003e\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003e`should\u003c/span\u003e \u003cspan class=\"n\"\u003esuccessfully\u003c/span\u003e \u003cspan class=\"n\"\u003epublish\u003c/span\u003e \u003cspan class=\"nc\"\u003eInvoiceCreatedEvent`\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eUnit\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003erunBlocking\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// given\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003einvoiceCreatedEvent\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eHermesFixture\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003einvoiceCreatedEvent\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003estub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ehermes\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillAcceptInvoiceCreatedEvent\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// when\u003c/span\u003e\n        \u003cspan class=\"n\"\u003ehermesClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epublish\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einvoiceCreatedEvent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// then\u003c/span\u003e\n        \u003cspan class=\"n\"\u003estub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ehermes\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003everifyInvoiceCreatedEventPublished\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003einvoiceCreatedEvent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eStubbed endpoints for methods accepting request bodies (e.g., POST, PUT) should not verify the values of the received request body but only its \u003cins\u003estructure\u003c/ins\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003ewillAcceptInvoiceCreatedEvent\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003estubFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"nf\"\u003einvoiceCreatedEventTopic\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithRequestBody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ematchingJsonPath\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"$.invoiceId\"\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithRequestBody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ematchingJsonPath\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"$.orderId\"\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithRequestBody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ematchingJsonPath\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"$.timestamp\"\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturn\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n                \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eaResponse\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithFixedDelay\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eresponseTime\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithStatus\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eHttpStatus\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nc\"\u003eOK\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003evalue\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWe verify the content of the request body in the assertion section.\nHere, we also want to hide the technical aspects of assertions behind a method.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003estubs\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ehermes\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003everifyInvoiceCreatedEventPublished\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003einvoiceCreatedEvent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003everifyInvoiceCreatedEventPublished\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eInvoiceCreatedEventDto\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003everify\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n        \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epostRequestedFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eurlPathEqualTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eINVOICE_CREATED_URL\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithRequestBody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ematchingJsonPath\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"$.invoiceId\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eequalTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einvoiceId\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithRequestBody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ematchingJsonPath\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"$.orderId\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eequalTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eorderId\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithRequestBody\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ematchingJsonPath\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"$.timestamp\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nc\"\u003eWireMock\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eequalTo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eevent\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etimestamp\u003c/span\u003e\u003cspan class=\"p\"\u003e)))\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eCombining stubbing and request verification in one method is not recommended.\nCreating stubs in this way makes their usage less convenient since not every test requires detailed verification of what is being sent in the request body.\nThe vast majority of tests will stub the endpoint based on the principle:\naccept a given request as long as its structure is preserved and will verify hypotheses other than the content of the request body (mainly end-to-end tests).\u003c/p\u003e\n\n\u003ch4 id=\"client-side-errors\"\u003eClient-side errors\u003c/h4\u003e\n\n\u003cp\u003eFor 4xx type errors, we want to verify the following cases:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe absence of the requested resource signaled by the response code 404 and a custom exception \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceResourceNotFoundException\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eValidation error signaled by the response code 422 and a custom exception \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceRequestValidationException\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eAny other 4xx type errors  should be cast to an \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceClientException\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nd\"\u003e@ParameterizedTest\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"s\"\u003e\"{index}) http status code: {0}\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"nd\"\u003e@MethodSource\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"clientErrors\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003e`when\u003c/span\u003e \u003cspan class=\"n\"\u003ereceive\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"n\"\u003ewith\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"n\"\u003exx\u003c/span\u003e \u003cspan class=\"n\"\u003estatus\u003c/span\u003e \u003cspan class=\"n\"\u003ecode\u003c/span\u003e \u003cspan class=\"n\"\u003ethen\u003c/span\u003e \u003cspan class=\"k\"\u003ethrow\u003c/span\u003e \u003cspan class=\"nf\"\u003eexception`\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eexceptionClass\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eClass\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eException\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003estatusCode\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eInt\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eresponseBody\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e?\u003c/span\u003e\n\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eUnit\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003erunBlocking\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// given\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientId\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eanyClientId\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eorderManagementServiceStub\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estatus\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003estatusCode\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eresponseBody\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// when\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eexception\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eshouldThrowAny\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eorderManagementServiceClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// then\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eexception\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejavaClass\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldBeSameInstanceAs\u003c/span\u003e \u003cspan class=\"n\"\u003eexceptionClass\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eexception\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldContain\u003c/span\u003e \u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoString\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eexception\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldContain\u003c/span\u003e \u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIn distributed systems, a \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\"\u003e404\u003c/a\u003e response code is quite common and may result from temporary inconsistency across the entire system.\nIts occurrence is signaled by the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceResourceNotFoundException\u003c/code\u003e and a warning-level log.\nHere, we are more interested in the scale of occurrences, which is why we use metrics, than analyzing individual cases, hence we log such cases at the warning level.\u003c/p\u003e\n\n\u003cp\u003eThe situation looks a bit different in the case of responses with a code of \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\"\u003e422\u003c/a\u003e.\nIf the request is rejected due to validation errors, either our service has a defect and produces incorrect data,\nor we receive incorrect data from external services (which is why it’s crucial to log what we receive from external services).\nAlternatively, the error may be on the recipient side in the logic validating the received request. It’s worth analyzing each such case, which is why\nerrors of this type are logged at the error level and signaled by the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceRequestValidationException\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eOther errors from the 4xx family occur less frequently.\nThey are all marked by the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceClientException\u003c/code\u003e exception and logged at the error level.\u003c/p\u003e\n\n\u003ch4 id=\"server-side-errors\"\u003eServer-side errors\u003c/h4\u003e\n\u003cp\u003eRegardless of the reason for a 5xx error, all of them are logged at the warn level because we have no control over them.\nThey are signaled by the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceServerException\u003c/code\u003e exception. Similar to 404 errors, we are more interested in aggregate information\nabout the number of such errors rather than analyzing each case individually, hence the warn log level.\u003c/p\u003e\n\n\u003cp\u003eIn tests, we consider two cases because the response from the service may or may not have a body.\nIf the response has a body, we want to log it.\u003c/p\u003e\n\n\u003ch4 id=\"read-timeout\"\u003eRead Timeout\u003c/h4\u003e\n\u003cp\u003eOur HTTP client should have a finite response timeout configured, so it’s worthwhile to write an integration test that verifies the client’s configuration.\nSimulating the delay of the stubbed endpoint can be achieved using the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ewithFixedDelay\u003c/code\u003e method from wiremock.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nd\"\u003e@Test\u003c/span\u003e\n\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003e`when\u003c/span\u003e \u003cspan class=\"n\"\u003eservice\u003c/span\u003e \u003cspan class=\"n\"\u003ereturns\u003c/span\u003e \u003cspan class=\"n\"\u003eabove\u003c/span\u003e \u003cspan class=\"n\"\u003etimeout\u003c/span\u003e \u003cspan class=\"n\"\u003ethreshold\u003c/span\u003e \u003cspan class=\"n\"\u003ethen\u003c/span\u003e \u003cspan class=\"k\"\u003ethrow\u003c/span\u003e \u003cspan class=\"nf\"\u003eexception`\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eUnit\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003erunBlocking\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// given\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eclientId\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eanyClientId\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n        \u003cspan class=\"n\"\u003eorderManagementServiceStub\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewithDelay\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereadTimeout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoInt\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewillReturnOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n                \u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"n\"\u003eresponse\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eordersPlacedBySomeCustomer\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// when\u003c/span\u003e\n        \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eexception\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldThrow\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eExternalServiceReadTimeoutException\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eorderManagementServiceClient\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetOrdersFor\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// then\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eexception\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldContain\u003c/span\u003e \u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientId\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoString\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eexception\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emessage\u003c/span\u003e \u003cspan class=\"n\"\u003eshouldContain\u003c/span\u003e \u003cspan class=\"n\"\u003eproperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eclientName\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eNo, this is not testing properties in tests.\nThis test ensures that the configuration derived from properties has indeed been applied to the given client.\nEnsuring a response within a specified time frame might be part of non-functional requirements and requires verification.\u003c/p\u003e\n\n\u003ch4 id=\"invalid-response-body\"\u003eInvalid Response Body\u003c/h4\u003e\n\u003cp\u003eConsidered cases:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eResponse body does not contain required field.\u003c/li\u003e\n  \u003cli\u003eResponse body is empty.\u003c/li\u003e\n  \u003cli\u003eResponse has an incorrect format.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eErrors of this type are signaled through \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eExternalServiceIncorrectResponseBodyException\u003c/code\u003e and logged at the error level.\u003c/p\u003e\n\n\u003ch3 id=\"metrics\"\u003eMetrics\u003c/h3\u003e\n\u003cp\u003eWhen dealing with HTTP clients, it’s essential to monitor several aspects: response times, throughput, and error rates.\nTo differentiate metrics generated by different clients easily, it’s advisable to include a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eservice.name\u003c/code\u003e tag with the respective client’s name.\u003c/p\u003e\n\n\u003cp\u003eIn HTTP clients offered by the Spring framework (WebClient, RestClient),\nmetrics are enabled out-of-the-box if we create them using predefined builders (WebClient.Builder, RestClient.Builder).\nHowever, for other technologies, third-party solutions must be employed. In Allegro, we have a set of libraries that allows us to quickly create new\nHTTP clients in the most popular technologies that provide support for our infrastructure.\nAs a result, all clients generate consistent metrics by default tailored to our dashboards.\u003c/p\u003e\n\n\u003ch4 id=\"response-time\"\u003eResponse Time\u003c/h4\u003e\n\u003cp\u003eMeasuring the response time of HTTP clients allows us to identify bottlenecks.\nAt which percentile should we set such a metric?\nGenerally, the more requests a client generates, the higher the percentile we should aim for.\nSometimes, issues become visible only at high percentiles (P99, P99.9) for a very high volume of requests.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Response Time\" src=\"/img/articles/2024-06-04-rest-service-client-design-testing-monitoring/response_time.png\" /\u003e\u003c/p\u003e\n\n\u003ch4 id=\"throughput\"\u003eThroughput\u003c/h4\u003e\n\u003cp\u003eNumber of requests that our application sends to external services per second (RPS).\nAn auxiliary metric for the response time metric, where response time is always considered in the context of the generated traffic.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Throughput\" src=\"/img/articles/2024-06-04-rest-service-client-design-testing-monitoring/rps.png\" /\u003e\u003c/p\u003e\n\n\u003ch4 id=\"error-rate\"\u003eError Rate\u003c/h4\u003e\n\u003cp\u003eCounting responses with codes 4xx/5xx.\nHere, we are interested in visualizing how many such errors occurred within a specific timeframe.\nThe number of errors we analyze depends on the overall traffic, therefore, both metrics should be expressed in the same units, usually requests per second.\nFor high traffic and a small number of errors, we can expect that the presented values will be on the order of thousandths.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg alt=\"Error Rate\" src=\"/img/articles/2024-06-04-rest-service-client-design-testing-monitoring/errors.png\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://martinfowler.com/articles/microservices.html\"\u003eMicroservices Architecture\u003c/a\u003e relies heavily on network communication.\nThe most common method of communication is REST API calls between different services.\nWriting integration code involves more than just invoking a URL and parsing a response.\nLogs, error handling, and metrics are crucial for creating a stable and fault-tolerant microservices environment.\nDevelopers should have tools that take care of these aspects, enabling fast and reliable development of such integrations.\nHowever, tools alone are insufficient. We also need established rules and guidelines that allow us to write readable and maintainable code,\nboth in production and tests.\u003c/p\u003e\n\n\u003ch2 id=\"code-examples\"\u003eCode examples\u003c/h2\u003e\n\u003cp\u003eTo explore comprehensive examples, including the usage of WebClient and other HTTP clients, check out the GitHub \u003ca href=\"https://github.com/Klimiec/webclients\"\u003erepository\u003c/a\u003e.\u003c/p\u003e\n","contentSnippet":"The purpose of this article is to present how to design, test, and monitor a REST service client.\nThe article includes a repository with clients written in Kotlin using various technologies such as WebClient,\nRestClient,\nKtor Client,\nRetrofit.\nIt demonstrates how to send and retrieve data from an external service, add a cache layer, and parse the received response into domain objects.\nMotivation\nWhy do we need objects in the project that encapsulate the HTTP clients we use?\nTo begin with, we want to separate the domain from technical details.\nThe way we retrieve/send data and handle errors, which can be quite complex in the case of HTTP clients, should not clutter business logic.\nNext, testability. Even if we do not use hexagonal architecture in our applications,\nit’s beneficial to separate the infrastructure from the service layer, as it improves testability.\nVerifying an HTTP service client is not a simple task and requires consideration of many cases — mainly at the integration level.\nHaving a separate “building block“ that encapsulates communication with the outside world makes testing much easier.\nFinally, reusability. A service client that has been written once can be successfully used in other projects.\nClient Design\nAs a case study, I will use an example implementation that utilizes WebClient for retrieving data from the Order Management Service,\nan example service that might appear in an e-commerce site such as Allegro.\nThe heart of our client is the executeHttpRequest method, which is responsible for executing the provided HTTP request, logging, and error handling.\nIt is not part of the WebClient library.\n\nclass OrderManagementServiceClient(\n    private val orderManagementServiceApi: OrderManagementServiceApi,\n    private val clientName: String\n) {\n    suspend fun getOrdersFor(clientId: ClientId): OrdersDto {\n        return executeHttpRequest(\n            initialLog = \"[$clientName] Get orders for a clientId= $clientId\",\n            request = { orderManagementServiceApi.getOrdersFor(clientId) },\n            successLog = \"[$clientName] Returned orders for a clientId= $clientId\",\n            failureMessage = \"[$clientName] Failed to get orders for clientId= $clientId\"\n        )\n    }\n}\n\n\n\nFull working example can be found here.\nClient name\nI like to name clients using the convention: name of the service we integrate with, plus the suffix Client.\nIn the case of integration with the Order Management Service, such a class will be named OrderManagementServiceClient.\nIf the technology we use employs an interface to describe the called REST API (RestClient, WebClient, Retrofit),\nwe can name such an interface OrderManagementServiceApi — following the general pattern of the service name with the suffix Api.\nThese names may seem intuitive and obvious, but without an established naming convention, we might end up with a project where\ndifferent integrations have the following suffixes: HttpClient, Facade, WebClient, Adapter, and Service.\nIt’s important to have a consistent convention and adhere to it throughout the project.\nAPI\nMethods of our clients should have names that reflect the communicative intention behind them.\nTo capture this intention, it is necessary to use a verb in the method’s name.\nTypically, the correct name will have a structure of verb + resource name, for example, getOrders  —  for methods that retrieve resources.\nIf we want to narrow down the number of returned resources using filters or return a particular resource, I recommend adding the suffix “For” before the list of parameters.\nTechnically, these parameters will be part of the query or path parameters.\n\nfun getOrdersFor(clientId: ClientId): OrdersDto\n\n\nFor methods responsible for creating resources, simply using the verb in the method name is enough,\nas the resource being passed as a parameter effectively conveys the intention of the method.\n\nfun publish(event: InvoiceCreatedEventDto)\n\n\nLogging\nWhen communicating with external service we’d like to log the beginning of the interaction, indicating our intention to fetch or send a resource,\nas well as its outcome. The outcome can be either a success, meaning receiving a response with a 2xx status code, or a failure.\nFailure can be signaled by status codes (3xx, 4xx, 5xx), resulting from the inability to deserialize the received response into an object,\nexceeding the response time, etc. Generally, many things can go wrong.\nDepending on the cause of failure, we may want to log the interaction result at different levels (warn/error).\nThere are critical errors that are worth distinguishing (error), and those that will occasionally occur (warn) and don’t require urgent intervention.\nTo filter logs related to a specific service while browsing through them, I like to include the client’s name within curly braces at the beginning of the logs.\nFor logging technical aspects of the communication, such as the URL called, HTTP method used, and response code,\nwe use filters (logRequestInfo, logResponseInfo) that are plugged in at the client configuration level in the createExternalServiceApi method.\n\ninline fun \u003creified T\u003e createExternalServiceApi(\n    webClientBuilder: WebClient.Builder,\n    properties: ConnectionProperties\n): T =\n    webClientBuilder\n        .clientConnector(httpClient(properties))\n        .baseUrl(properties.baseUrl)\n        .defaultRequest { it.attribute(SERVICE_NAME, properties.clientName) }\n        .filter(logRequestInfo(properties.clientName))\n        .filter(logResponseInfo(properties.clientName))\n        .build()\n        .let { WebClientAdapter.create(it) }\n        .let { HttpServiceProxyFactory.builderFor(it).build() }\n        .createClient(T::class.java)\n\nfun logRequestInfo(clientName: String) = ExchangeFilterFunction.ofRequestProcessor { request -\u003e\n    logger.info {\n        \"[$clientName] method=[${request.method().name()}] url=${request.url()}}\"\n    }\n    Mono.just(request)\n}\n\nfun logResponseInfo(clientName: String) = ExchangeFilterFunction.ofResponseProcessor { response -\u003e\n    logger.info { \"[$clientName] service responded with a status code= ${response.statusCode()}\" }\n    Mono.just(response)\n}\n\n\nHere’s an example of logged interaction for successfully fetching a resource.\n\nTo prevent redundancy in logging code across multiple clients, it is centralized inside executeHttpRequest method.\nThe only thing the developer needs to do is to provide a business-oriented description for the beginning of the interaction and its outcome (parameters: initialLog, successLog, failureMessage).\nWhy do I emphasize logging so much?\nIsn’t it enough to log only errors?\nAfter all, we have metrics that inform us about the performance of our clients.\nMetrics won’t provide us with the details of the communication, but logs will.\nThese details can turn out to be crucial in the analysis of incidents, which may reveal, for example, incorrect data produced by our service.\nLogs are like backups. We find out if we have them and how valuable they are only when they are needed,\neither because the business team requests an analysis of a particular case or when resolving an incident.\nError handling\nWhen writing client code, we aim to highlight maximally how we send/retrieve data and hide the “noise“ that comes from error handling.\nIn the case of HTTP clients, error handling is quite extensive but generic enough that the resulting code can be written once and reused across all clients.\nIn our example, error handling mechanism is hidden inside executeHttpRequest method.\nIt consists of two things: logging and throwing custom exceptions that encapsulate technical exceptions thrown by the underlying HTTP client.\nWhat are the benefits of using custom exceptions? The very name of such a custom exception tells us exactly what went wrong.\nFor comparison, ExternalServiceIncorrectResponseBodyException seems to be more descriptive than DecodingException.\nThey also help group various technical exceptions that lead to the same cause, for example, an incorrect response object structure.\nAdditionally, based on these exceptions, visualizations can be created to show the state of our integration.\nFor example, we can create a table that will show how many exceptions of any given type were thrown by our clients within a specified period.\nHaving custom exceptions, we are 100% certain that these exceptions were thrown only by our clients.\nTesting\nStubs\nTo verify different scenarios of our HTTP client, it is necessary to appropriately stub the called endpoints in tests.\nFor this purpose, we will use the WireMock library.\nIt is quite important that the technical details of created stubs do not leak into the tests.\nThe test should describe the behavior being tested and encapsulate technical details.\nFor example, changing the accept/content-type header or making minor modifications to the called URL should not affect the test itself.\nTo achieve this, for each service for which we write a service client, we create an object of type StubBuilder.\nThe StubBuilder allows hiding the details of stubbing and verification behind a readable API.\nIt takes on the impact of changes to the called API, protecting our test from modification.\nIt fulfills a similar role to the Page Object Pattern in end-to-end tests for web apps.\n\norderManagementServiceStub.willReturnOrdersFor(clientId, response = ordersPlacedBySomeCustomer())\n\n\nStubBuilders for services that return data come in two flavors - internal and external.\n\nWhen testing a service client, we want to have great flexibility in simulating responses.\nTherefore, StubBuilders from the internal package will model response objects as a string. This allows us to simulate any scenario.\nIn end-to-end tests, where a given service is part of the bigger process, such flexibility is not necessary; in fact, it is not even recommended.\nTherefore, StubBuilders from the external package model responses using real objects.\nAll StubBuilders from the external packages are declared in the class ExternalServiceStubs, to which a reference is located in the base class for\nall integration tests, BaseIntegrationTest. This allows us to have very easy access to all external service stubs in our integration tests.\n\nstub.orderManagementService().willReturnOrdersFor(clientId, response = ordersPlacedBySomeCustomer())\n\n\nReading the code above, we immediately know which service is being interacted with (Order Management Service) and what will be returned from it (Orders).\nThe technical details of the stubbed endpoint have been hidden inside the StubBuilder object.\nTests should emphasize “what” and encapsulate “how.” This way, they can serve as documentation.\nTest Data\nThe data returned by our stubs can be prepared in three ways:\nRead the entire response from a file/string.\nPrepare the response using real objects used in the service for deserializing responses from called services.\nCreate a set of separate objects modeling the returned response from the service for testing purposes and use them to prepare the returned data.\nWhich option to choose?\nTo answer this question, we should analyze the advantages and disadvantages of each approach.\nOption A — read response from a file/string. Response creation is very fast and simple.\nIt allows verifying the contract between the client and the supplier (at least at the time of writing the test).\nImagine that during refactoring, one of the fields in the response object accidentally changes.\nIn such a case, client tests using this approach will detect the defect before the code reaches production.\n\n@Test\nfun `should return orders for a given clientId`(): Unit = runBlocking {\n        // given\n        val clientId = anyClientId()\n        orderManagementServiceStub.willReturnOrdersFor(clientId, response = ordersPlacedBySomeCustomer())\n\n        // when\n        val response: OrdersDto = orderManagementServiceClient.getOrdersFor(clientId)\n\n        // then\n        response shouldBe OrdersDto(listOf(OrderDto(\"7952a9ab-503c-4483-beca-32d081cc2446\")))\n}\n\n\nOn the other hand, keeping data in files/strings is difficult to maintain and reuse.\nProgrammers often copy entire files for new tests, introducing only minimal changes.\nThere is a problem with naming these files and refactoring them when the called service introduces an incompatible change.\nOption B — Use real response objects.\nIt allows writing one-line, readable assertions and maximally reusing already created data, especially using test data builders.\n\n    @Test\n    fun `should return orders for a given clientId`(): Unit = runBlocking {\n        // given\n        val clientId = anyClientId()\n        val clientOrders = OrderManagementServiceFixture.ordersPlacedBySomeCustomer()\n        orderManagementServiceStub.willReturnOrdersFor(clientId, response = clientOrders)\n\n        // when\n        val response: OrdersDto = orderManagementServiceClient.getOrdersFor(clientId)\n\n        // then\n        response shouldBe clientOrders\n    }\n\n\nHowever, accidental change of field name which results in the  contract violation between the client and supplier won’t be caught.\nAs a result, we might have perfectly tested communication in integration tests that will not work in production.\nOption C — create a set of separate response objects. It has all the advantages of options A and B, including maintainability, reusability, and\nverification of the contract between the client and the supplier. Unfortunately, maintaining a separate model for testing purposes comes with some overhead\nand requires discipline on the developers’ side, which can be challenging to maintain.\nWhich option to choose? Personally, I prefer a hybrid of options A and B.\nFor the purpose of testing the “happy path“ in client tests, I return a response that is entirely stored as a string (alternatively, it can be read from a file).\nSuch a test allows not only to verify the contract but also the correctness of deserializing the received response into a response object.\nIn other tests (cache, adapter, end-to-end), I create responses returned by the stubbed endpoint using production response objects.\nIt’s worthwhile to keep sample test data in dedicated classes, such as a Fixture class, for each integration (for example OrderManagementServiceFixture).\nThis allows the reuse of test data and enhances the readability of the tests themselves.\nTest Scenarios\nHappy Path\nFetching a resource — verification whether the client can retrieve data from the previously stubbed endpoint and deserialize it into a response object.\n\n@Test\nfun `should return orders for a given clientId`(): Unit = runBlocking {\n        // given\n        val clientId = anyClientId()\n        orderManagementServiceStub.willReturnOrdersFor(clientId, response = ordersPlacedBySomeCustomer())\n\n        // when\n        val response: OrdersDto = orderManagementServiceClient.getOrdersFor(clientId)\n\n        // then\n        response shouldBe OrdersDto(listOf(OrderDto(\"7952a9ab-503c-4483-beca-32d081cc2446\")))\n}\n\n\nAn essential part of the test for the “happy path“ is verification of the contract between the client and the supplier.\nThe ordersPlacedBySomeCustomer method returns a sample response guaranteed by the supplier (Order Management Service).\nOn the client side, in the assertion section, we check if this message has been correctly deserialized into a response object.\nInstead of comparing individual fields with the expected value, I highly recommend comparing entire objects (returned and expected).\nIt gives us confidence that all fields have been compared. In the case of regression, modern IDEs such as IntelliJ indicate exactly where the problem is.\n\nSending a resource — verification whether the client sends data to the specified URL in a format acceptable by the previously stubbed endpoint.\nIn the following example, I test publishing an event to Hermes, a message broker built on top of Kafka widely used at Allegro.\n\n@Test\nfun `should successfully publish InvoiceCreatedEvent`(): Unit = runBlocking {\n        // given\n        val invoiceCreatedEvent = HermesFixture.invoiceCreatedEvent()\n        stub.hermes().willAcceptInvoiceCreatedEvent()\n\n        // when\n        hermesClient.publish(invoiceCreatedEvent)\n\n        // then\n        stub.hermes().verifyInvoiceCreatedEventPublished(event = invoiceCreatedEvent)\n}\n\n\nStubbed endpoints for methods accepting request bodies (e.g., POST, PUT) should not verify the values of the received request body but only its structure.\n\nfun willAcceptInvoiceCreatedEvent() {\n    WireMock.stubFor(\n        invoiceCreatedEventTopic()\n            .withRequestBody(WireMock.matchingJsonPath(\"$.invoiceId\"))\n            .withRequestBody(WireMock.matchingJsonPath(\"$.orderId\"))\n            .withRequestBody(WireMock.matchingJsonPath(\"$.timestamp\"))\n            .willReturn(\n                WireMock.aResponse()\n                    .withFixedDelay(responseTime)\n                    .withStatus(HttpStatus.OK.value())\n            )\n    )\n}\n\n\nWe verify the content of the request body in the assertion section.\nHere, we also want to hide the technical aspects of assertions behind a method.\n\nstubs.hermes().verifyInvoiceCreatedEventPublished(event = invoiceCreatedEvent)\n\n\nfun verifyInvoiceCreatedEventPublished(event: InvoiceCreatedEventDto) {\n    WireMock.verify(\n        1,\n        WireMock.postRequestedFor(WireMock.urlPathEqualTo(INVOICE_CREATED_URL))\n            .withRequestBody(WireMock.matchingJsonPath(\"$.invoiceId\", WireMock.equalTo(event.invoiceId)))\n            .withRequestBody(WireMock.matchingJsonPath(\"$.orderId\", WireMock.equalTo(event.orderId)))\n            .withRequestBody(WireMock.matchingJsonPath(\"$.timestamp\", WireMock.equalTo(event.timestamp)))\n    )\n}\n\n\nCombining stubbing and request verification in one method is not recommended.\nCreating stubs in this way makes their usage less convenient since not every test requires detailed verification of what is being sent in the request body.\nThe vast majority of tests will stub the endpoint based on the principle:\naccept a given request as long as its structure is preserved and will verify hypotheses other than the content of the request body (mainly end-to-end tests).\nClient-side errors\nFor 4xx type errors, we want to verify the following cases:\nThe absence of the requested resource signaled by the response code 404 and a custom exception ExternalServiceResourceNotFoundException\nValidation error signaled by the response code 422 and a custom exception ExternalServiceRequestValidationException\nAny other 4xx type errors  should be cast to an ExternalServiceClientException\n\n@ParameterizedTest(name = \"{index}) http status code: {0}\")\n@MethodSource(\"clientErrors\")\nfun `when receive response with 4xx status code then throw exception`(\n    exceptionClass: Class\u003cException\u003e,\n    statusCode: Int,\n    responseBody: String?\n): Unit = runBlocking {\n        // given\n        val clientId = anyClientId()\n        orderManagementServiceStub.willReturnOrdersFor(clientId, status = statusCode, response = responseBody)\n\n        // when\n        val exception = shouldThrowAny {\n            orderManagementServiceClient.getOrdersFor(clientId)\n        }\n\n        // then\n        exception.javaClass shouldBeSameInstanceAs exceptionClass\n        exception.message shouldContain clientId.clientId.toString()\n        exception.message shouldContain properties.clientName\n}\n\n\nIn distributed systems, a 404 response code is quite common and may result from temporary inconsistency across the entire system.\nIts occurrence is signaled by the ExternalServiceResourceNotFoundException and a warning-level log.\nHere, we are more interested in the scale of occurrences, which is why we use metrics, than analyzing individual cases, hence we log such cases at the warning level.\nThe situation looks a bit different in the case of responses with a code of 422.\nIf the request is rejected due to validation errors, either our service has a defect and produces incorrect data,\nor we receive incorrect data from external services (which is why it’s crucial to log what we receive from external services).\nAlternatively, the error may be on the recipient side in the logic validating the received request. It’s worth analyzing each such case, which is why\nerrors of this type are logged at the error level and signaled by the ExternalServiceRequestValidationException.\nOther errors from the 4xx family occur less frequently.\nThey are all marked by the ExternalServiceClientException exception and logged at the error level.\nServer-side errors\nRegardless of the reason for a 5xx error, all of them are logged at the warn level because we have no control over them.\nThey are signaled by the ExternalServiceServerException exception. Similar to 404 errors, we are more interested in aggregate information\nabout the number of such errors rather than analyzing each case individually, hence the warn log level.\nIn tests, we consider two cases because the response from the service may or may not have a body.\nIf the response has a body, we want to log it.\nRead Timeout\nOur HTTP client should have a finite response timeout configured, so it’s worthwhile to write an integration test that verifies the client’s configuration.\nSimulating the delay of the stubbed endpoint can be achieved using the withFixedDelay method from wiremock.\n\n@Test\nfun `when service returns above timeout threshold then throw exception`(): Unit = runBlocking {\n        // given\n        val clientId = anyClientId()\n\n        orderManagementServiceStub\n            .withDelay(properties.readTimeout.toInt())\n            .willReturnOrdersFor(\n                clientId,\n                response = ordersPlacedBySomeCustomer()\n            )\n\n        // when\n        val exception = shouldThrow\u003cExternalServiceReadTimeoutException\u003e {\n            orderManagementServiceClient.getOrdersFor(clientId)\n        }\n        // then\n        exception.message shouldContain clientId.clientId.toString()\n        exception.message shouldContain properties.clientName\n}\n\n\nNo, this is not testing properties in tests.\nThis test ensures that the configuration derived from properties has indeed been applied to the given client.\nEnsuring a response within a specified time frame might be part of non-functional requirements and requires verification.\nInvalid Response Body\nConsidered cases:\nResponse body does not contain required field.\nResponse body is empty.\nResponse has an incorrect format.\nErrors of this type are signaled through ExternalServiceIncorrectResponseBodyException and logged at the error level.\nMetrics\nWhen dealing with HTTP clients, it’s essential to monitor several aspects: response times, throughput, and error rates.\nTo differentiate metrics generated by different clients easily, it’s advisable to include a service.name tag with the respective client’s name.\nIn HTTP clients offered by the Spring framework (WebClient, RestClient),\nmetrics are enabled out-of-the-box if we create them using predefined builders (WebClient.Builder, RestClient.Builder).\nHowever, for other technologies, third-party solutions must be employed. In Allegro, we have a set of libraries that allows us to quickly create new\nHTTP clients in the most popular technologies that provide support for our infrastructure.\nAs a result, all clients generate consistent metrics by default tailored to our dashboards.\nResponse Time\nMeasuring the response time of HTTP clients allows us to identify bottlenecks.\nAt which percentile should we set such a metric?\nGenerally, the more requests a client generates, the higher the percentile we should aim for.\nSometimes, issues become visible only at high percentiles (P99, P99.9) for a very high volume of requests.\n\nThroughput\nNumber of requests that our application sends to external services per second (RPS).\nAn auxiliary metric for the response time metric, where response time is always considered in the context of the generated traffic.\n\nError Rate\nCounting responses with codes 4xx/5xx.\nHere, we are interested in visualizing how many such errors occurred within a specific timeframe.\nThe number of errors we analyze depends on the overall traffic, therefore, both metrics should be expressed in the same units, usually requests per second.\nFor high traffic and a small number of errors, we can expect that the presented values will be on the order of thousandths.\n\nSummary\nMicroservices Architecture relies heavily on network communication.\nThe most common method of communication is REST API calls between different services.\nWriting integration code involves more than just invoking a URL and parsing a response.\nLogs, error handling, and metrics are crucial for creating a stable and fault-tolerant microservices environment.\nDevelopers should have tools that take care of these aspects, enabling fast and reliable development of such integrations.\nHowever, tools alone are insufficient. We also need established rules and guidelines that allow us to write readable and maintainable code,\nboth in production and tests.\nCode examples\nTo explore comprehensive examples, including the usage of WebClient and other HTTP clients, check out the GitHub repository.","guid":"https://blog.allegro.tech/2024/06/rest-service-client-design-testing-monitoring.html","categories":["kotlin","testing","integration tests","rest","wiremock"],"isoDate":"2024-06-03T22:00:00.000Z"},{"title":"Unveiling bottlenecks of couchbase sub-documents operations","link":"https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html","pubDate":"Thu, 16 May 2024 00:00:00 +0200","authors":{"author":[{"name":["Tomasz Ziółkowski"],"photo":["https://blog.allegro.tech/img/authors/tomasz.ziolkowski.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.ziolkowski"]}]},"content":"\u003cp\u003eThis story shows our journey in addressing a platform stability issue related to autoscaling, which, paradoxically, added some additional overhead instead\nof reducing the load. A pivotal part of this narrative is how we used \u003ca href=\"https://www.couchbase.com/\"\u003eCouchbase\u003c/a\u003e — a distributed NoSQL database. If you find\nyourself intrigued by another enigmatic story involving Couchbase, don’t miss my\n\u003ca href=\"/2024/02/couchbase-expired-docs-tuning.html\"\u003eblog post on tuning expired doc settings\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThis post unfolds our quest to discover the root cause of the bottleneck. Initially, I will outline the symptoms of the issue. Subsequently, you will be\nintroduced to how Couchbase is utilized by the aforementioned service. Equipped with this knowledge, I will recount our attempts to diagnose the problem and\nindicate which observations raised our suspicions. The following section is dedicated to conducting benchmarks to verify our predictions using\na custom benchmarking tool. Ultimately, we will explore the source code of Couchbase to uncover how the problematic operations are executed. This section\naims to provide a deep understanding of Couchbase’s inner workings. I firmly believe that the knowledge shared in that part is its most valuable asset and may\nenable you to swiftly identify and address some of the potential performance issues when using Couchbase.\u003c/p\u003e\n\n\u003ch2 id=\"set-the-scene\"\u003eSet the scene\u003c/h2\u003e\n\n\u003cp\u003eThe service at the heart of the stability issues handles external HTTP traffic; for the purpose of this discussion, we’ll refer to it as\n“the gateway service”. The traffic routed to the gateway service reflects a pattern similar to organic traffic on \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e,\ncharacterized by significant fluctuations in throughput between day and night hours. To efficiently utilize resources, the gateway service employs an autoscaler\nto dynamically adjust the number of instances based on current demands. It’s also important to note that spawning a new instance involves a warm-up phase,\nduring which the instance retrieves some data from Couchbase to populate its in-memory cache. The gateway service relies on a Couchbase cluster\ncomprised of \u003cstrong\u003ethree\u003c/strong\u003e nodes.\u003c/p\u003e\n\n\u003ch2 id=\"observations\"\u003eObservations\u003c/h2\u003e\n\n\u003cp\u003eThe team managing the service encountered a series of errors in communication with Couchbase. These errors indicated that 3-second timeouts occurred while\nfetching data from Couchbase:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ecom. couchbase.client.core.error.UnambiguousTimeoutException: SubdocGetRequest, Reason: TIMEOUT {\n    \"cancelled\":true,\n    \"completed\":true,\n    ... IRRELEVANT METADATA ...\n    \"timeoutMs\":3000,\n    \"timings\":{\"totalMicros\":3004052}\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eInterestingly, during these incidents, the Couchbase cluster did not exhibit high CPU or RAM usage. Furthermore, the traffic to Couchbase, measured in\noperations per second, was not exceptionally high. I mean that other Couchbase clients (different microservices) were generating an order of magnitude more\noperations per second without encountering stability issues.\u003c/p\u003e\n\n\u003cp\u003eAdditional key observations related to the issue include:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe instability primarily occurred during the service scaling-up process, initially triggered by the autoscaler.\u003c/li\u003e\n  \u003cli\u003eNewly spawned instances were predominantly affected.\u003c/li\u003e\n  \u003cli\u003eThe issues were reported solely for operations directed to a specific node within the cluster.\u003c/li\u003e\n  \u003cli\u003eA temporary mitigation of the problems involved repeatedly restarting the failing application instances.\u003c/li\u003e\n  \u003cli\u003eThere was a noticeable pattern on the driver side that preceded the widespread errors, including timeouts and the inability to send requests due to\na non-writable channel.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"temporary-solution\"\u003eTemporary solution\u003c/h3\u003e\n\n\u003cp\u003eAs a temporary measure, the team overseeing the gateway service implemented the following workarounds:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eDisabled certain types of requests to reduce the overall traffic volume directed to Couchbase.\u003c/li\u003e\n  \u003cli\u003eDeactivated the autoscaler, and manually scaled up the application to manage peak traffic loads.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese actions successfully halted the problems, but they also had repercussions, including business impacts and decreased efficiency in resource utilization.\u003c/p\u003e\n\n\u003ch2 id=\"raising-suspicions\"\u003eRaising suspicions\u003c/h2\u003e\n\n\u003cp\u003eA pivotal aspect of this issue was the use of the \u003ca href=\"https://docs.couchbase.com/go-sdk/2.4/concept-docs/subdocument-operations.html\"\u003eCouchbase sub-document API\u003c/a\u003e\nwithin the gateway service, an approach not widely adopted across our internal microservice landscape, yet notable for its efficiency. According to\nthe documentation, this API significantly reduces traffic by allowing the fetching or mutating only specific parts of a Couchbase document.\nEssentially, it acts as a substitute for the concept of \u003ca href=\"https://en.wikipedia.org/wiki/Projection_(relational_algebra)\"\u003eprojection\u003c/a\u003e, familiar to SQL users.\u003c/p\u003e\n\n\u003cp\u003eIn our investigation we closely examined the data collected on the Couchbase node, the operational dynamics of the gateway service’s cache, and insights\nfrom scrutinizing both the Couchbase driver and server code. We hypothesized that the crux of the problem might be linked to the cache warm-up process for\nnewly launched instances.\u003c/p\u003e\n\n\u003cp\u003eOur investigation uncovered several indicators pointing toward the core of the issue:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eA disproportionately large number of requests targeted a single document, inevitably directing traffic to a specific node.\u003c/li\u003e\n  \u003cli\u003eThe node hosting this heavily queried document corresponded with the one mentioned in timeout-related logs.\u003c/li\u003e\n  \u003cli\u003eInstances that had been running for an extended period reported virtually no errors.\u003c/li\u003e\n  \u003cli\u003eThe volume of requests to Couchbase from the affected instances was extraordinarily high, not aligning with the number of requests registered on\nthe Couchbase side. This discrepancy suggested that if the cache warming process was at fault, the sheer magnitude of attempted requests was overwhelming\neven the local network buffers.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eHowever, these observations were merely pieces of a larger puzzle. We noticed a “snowball effect” where the system’s inability to process an initial set\nof requests for newly initiated instances triggered a cascade of failures. But the question remained: Why? What made these instances different,\nand why didn’t other clients on the same cluster experience similar issues? This was the crucial moment to take a closer examination of the sub-document\noperations to determine their efficiency and optimization.\u003c/p\u003e\n\n\u003ch2 id=\"lets-benchmark-it\"\u003eLet’s benchmark it\u003c/h2\u003e\n\n\u003cp\u003eDespite an extensive search, we were unable to locate any tools capable of reliably testing our hypothesis—that sub-document operations executed during\nthe warm-up phase could significantly challenge Couchbase’s handling capabilities. As a result, we developed a simple tool and made it\n\u003ca href=\"https://github.com/ziollek/cb-perf-tester\"\u003eavailable in on \u003cem\u003eGitHub\u003c/em\u003e\u003c/a\u003e.\nThis tool is designed to create a sample document and then execute parallel sub-document fetch operations concurrently.\nThe sample document is structured as follows:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e{\n    \"key\": \"test-subdoc\",\n    \"data\": {\n        \"subkey-000000\": \"value-000000\",\n        \"subkey-000001\": \"value-000001\",\n        . . .\n        \"subkey-0….N\": \"value-0…..N\",\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe tool allows manipulating several knobs, which includes:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eParallelism\u003c/strong\u003e: Determines the number of parallel \u003ca href=\"https://gobyexample.com/goroutines\"\u003egoroutines\u003c/a\u003e that will attempt to fetch the same sub-documents concurrently.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eDocument Size\u003c/strong\u003e: Defined by the number of sub-keys, this directly affects the document’s binary size.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eLevel of Search Difficulty\u003c/strong\u003e: This essentially refers to how deep or how far into the main document the target sub-document is located.\nThe concept is illustrated in the diagram below:\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-sub-difficulty.png\" alt=\"Difficulty of sub-document search\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"caveats\"\u003eCaveats\u003c/h3\u003e\n\n\u003cp\u003eThe primary objective of this exercise was to identify potential bottlenecks, not to conduct a highly accurate performance assessment of Couchbase clusters.\nTherefore, we opted to run our experiments using a local Docker container (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecouchbase:community-6.6.0\u003c/code\u003e), rather than on a dedicated, well-isolated cluster.\nWe acknowledge that hosting both the server and the benchmarking tool on the same machine may compromise the reliability and accuracy of the results.\nConsequently, we advise against using the findings from these tests for comprehensive assessments or comparisons with other technologies.\u003c/p\u003e\n\n\u003ch3 id=\"benchmark-steps\"\u003eBenchmark steps\u003c/h3\u003e\n\n\u003cp\u003eThe procedure for each experiment follows a similar framework, outlined in the steps below:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eDocument Preparation\u003c/strong\u003e: Initiate the document with the desired number of sub-documents, as dictated by one of the experimental variables.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eDocument Storage\u003c/strong\u003e: Save this document under a predetermined key.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eGoroutine Initialization\u003c/strong\u003e: Launch a specified number of goroutines, the quantity of which is determined by another experimental variable.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eFetch Operations\u003c/strong\u003e: Each goroutine executes a series of fetch operations, which can be either regular (retrieving the entire sample document) or\nsub-document (accessing a set of sub-documents). It’s important to note that these requests are executed in a blocking manner; a new fetch operation is\nperformed only after the completion of the preceding one. In sub-document mode, the difficulty of the fetch operation is controlled through\nan experiment variable.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCompletion Wait\u003c/strong\u003e: Await the termination of all goroutines.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eResults Reporting\u003c/strong\u003e: Calculate and display the estimated RPS (requests per second).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"estimate-baseline\"\u003eEstimate baseline\u003c/h3\u003e\n\n\u003cp\u003ePrior to delving into sub-document operations, we sought to establish the maximum number of regular get operations that our local Couchbase Server instance\ncould handle. Through testing at various levels of concurrency, we determined the maximum throughput for our specific setup.\nIt was approximately 6,000 to 7,000 RPS, regardless of whether the requests were for small documents (less than 200 bytes)\nor for non-existent documents. These findings were further validated by the statistics available through the Couchbase UI.\u003c/p\u003e\n\n\u003cp\u003eBenchmark Command: Attempting to fetch a non-existent document yielded a rate of \u003cem\u003e6388 RPS\u003c/em\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5 --search-non-existent\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=true, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: not-exists\n\nregular report: successes: 0, errors: 200000, duration: 31.306684977s, rps: 6388.411937, success rps: 0.000000\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-baseline-non-exitstent.png\" alt=\"Baseline - fetch a not-existent document\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBenchmark Command: Fetching an existing small (195 bytes) document yielded a rate of \u003cem\u003e6341 rps\u003c/em\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=false, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: test-regular\n\nregular report: successes: 200000, errors: 0, duration: 31.536538682s, rps: 6341.850068, success rps: 6341.850068\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-baseline-hits.png\" alt=\"Baseline - fetch an existing document\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"testing-scenarios\"\u003eTesting scenarios\u003c/h3\u003e\n\n\u003cp\u003eNow that we have a baseline for comparison, we’re set to evaluate it against the outcomes of various scenarios. To ensure the tests are comparable,\nwe’ll maintain constant parallelism across all tests, specifically using 200 goroutines. The variables that will differ across scenarios include:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eTotal Number of Sub-Documents\u003c/strong\u003e: This determines the overall size of the sample document, as the document’s size is directly related to the number\nof sub-documents it contains.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eNumber of Searched Sub-Documents\u003c/strong\u003e: This refers to how many sub-paths within the sample document will be targeted in a single fetch operation.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eSearch Difficulty\u003c/strong\u003e: This aspect dictates the difficulty of locating the searched sub-paths within the document.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIt’s important to highlight that in each scenario, we will manipulate only one variable at a time while keeping the other parameters constant.\u003c/p\u003e\n\n\u003ch4 id=\"scenario-a-the-impact-of-document-size-on-performance\"\u003eScenario A: The impact of document size on performance\u003c/h4\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-doc-size-vs-performance-all.png\" alt=\"Document size vs performance - aggregated\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eTo better visualize the impact, let’s look at the diagram for HARD scenario:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-doc-size-vs-performance-hard.png\" alt=\"Document size vs performance - hard\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIt is clearly visible that there is a strict correlation between document size and performance.\u003c/p\u003e\n\n\u003ch4 id=\"scenario-b-the-impact-of-the-number-of-searched-sub-documents-on-performance\"\u003eScenario B: The impact of the number of searched sub-documents on performance\u003c/h4\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-subdocs-num-vs-performance-all.png\" alt=\"Number of searched sub-documents vs performance - aggregated\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eTo better visualize the impact, let’s look at the diagram for HARD scenario:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-subdocs-num-vs-performance-hard.png\" alt=\"Number of searched sub-documents vs performance - hard\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAs observed in the previous scenario, there is a clear correlation between the number of sub-documents accessed and the system’s performance.\u003c/p\u003e\n\n\u003ch2 id=\"further-analysis\"\u003eFurther analysis\u003c/h2\u003e\n\n\u003cp\u003eGiven the evident correlation between the document size/number of queried sub-paths and performance degradation, we delve into the mechanics to understand\nthe root cause of these test results. A notable observation during the tests relates to CPU utilization within the Docker environment, where, despite having\nsix cores available, only a single core was actively utilized. Intriguingly, this usage was monopolized by a single thread (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emc:worker_X\u003c/code\u003e).\nThis phenomenon directly stems from Couchbase’s handling of Key-Value (KV) connections. By default, the Couchbase driver initiates only a single connection to\neach cluster node for KV operations. However, this configuration can be adjusted in certain Software Development Kits (SDKs)—the Java SDK,\nfor instance, allows modification through \u003ca href=\"https://docs.couchbase.com/java-sdk/current/ref/client-settings.html#io-options\"\u003eIoConfig.numKvConnections\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhen a connection is established, Couchbase assigns it to\na \u003ca href=\"https://github.com/couchbase/kv_engine/blob/master/docs/in-depth/C10k.md#current-approach-why-not-both\"\u003especific worker thread\u003c/a\u003e) using\na \u003ca href=\"https://github.com/couchbase/kv_engine/blob/master/daemon/front_end_thread.h#L84\"\u003eRound-Robin (RR)\u003c/a\u003e) algorithm. As a result, the Couchbase Server does not\nfully utilize available CPU power for a single connection, even when a lot of resources are free. This behavior can be seen as beneficial, serving to mitigate\nthe \u003ca href=\"https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors\"\u003e“noisy neighbor” effect\u003c/a\u003e, provided there are sufficient\nspare cores available to manage new connections. This mechanism ensures balanced resource use across connections, as illustrated in the diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-dispatch-ok.png\" alt=\"Handling connections - free cores scenario\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eConversely, one may encounter fluctuating performance due to instances of misfortune, where if other clients significantly burden certain worker threads,\nand your connection is allocated to one of these overloaded threads, performance inconsistencies arise. This scenario, where a client experiences higher than\nusual response times due to an imbalanced distribution of workload across worker threads, is depicted in the diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-dispatch-bad.png\" alt=\"Handling connections - overloaded core scenario\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThis behavior explains the apparent paradox observed during the stability issues: the Couchbase node showed no clear signs of being overloaded, yet certain\nanomalous symptoms were present, such as a metric indicating the minimum \u003ca href=\"https://en.wikipedia.org/wiki/Idle_(CPU)\"\u003eidle\u003c/a\u003e percentage across all cores plummeting\nto 0% during the disturbances. This leaves no doubt that operations on sub-documents have the potential to overburden worker threads within\nthe Couchbase Server. With this understanding we are now ready to delve deeper into the root cause of such behavior.\u003c/p\u003e\n\n\u003ch3 id=\"what-documentation-says\"\u003eWhat documentation says\u003c/h3\u003e\n\n\u003cp\u003eThe documentation for Couchbase, housed alongside the server’s source code, is notably comprehensive, including a dedicated section on\n\u003ca href=\"https://github.com/couchbase/kv_engine/blob/master/docs/SubDocument.md\"\u003esub-documents\u003c/a\u003e. However, it falls short of providing detailed insights into\nthe internal workings of these operations. Additionally, there is a lack of discussion on the performance implications of sub-document operations,\nwith only a few remarks on data typing and float numbers that do not apply to the cases we tested. Attempts to find answers on the Couchbase forum were also\nunfruitful, yielding no substantial information on the performance issues we encountered. Despite this, there is confirmation from others in the community who\nhave observed\n\u003ca href=\"https://www.couchbase.com/forums/t/frequent-timeouts-and-requests-over-threshold-for-subdocgetrequests-via-reactive-java-sdk/30211\"\u003esimilar problems\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"what-source-code-says\"\u003eWhat source code says\u003c/h3\u003e\n\n\u003cp\u003eA thorough analysis of the codebase reveals a definitive cause for the performance degradation observed. It’s important to note that Couchbase requires\n\u003ca href=\"https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/compression.html\"\u003edecompression\u003c/a\u003e of a document for any lookup or manipulation\noperation, whether the document is retrieved from RAM or disk. Let’s start from a point where Couchbase starts doing\n\u003ca href=\"https://github.com/couchbase/kv_engine/blob/cf020888d2e09b132a02c90b99e160044ddabb11/daemon/subdocument.cc#L568\"\u003elookups\u003c/a\u003e on a decompressed object:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e// 2. Perform each of the operations on document.\nfor (auto\u0026amp; op : operations) {\n    switch (op.traits.scope) {\n    case CommandScope::SubJSON:\n        if (cb::mcbp::datatype::is_json(doc_datatype)) {\n            // Got JSON, perform the operation.\n            op.status = subdoc_operate_one_path(context, op, current.view);\n        }\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eA critical observation from our analysis is that each operation (lookup) is executed sequentially through the invocation of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esubdoc_operate_one_path\u003c/code\u003e.\nTo understand the performance implications, let’s examine\n\u003ca href=\"https://github.com/couchbase/kv_engine/blob/cf020888d2e09b132a02c90b99e160044ddabb11/daemon/subdocument.cc#L413C5-L414C76\"\u003ethe implementation\u003c/a\u003e of this lookup\nprocess:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e// ... and execute it.\nconst auto subdoc_res = op.op_exec(spec.path.data(), spec.path.size());\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe Investigation reveals that the lookup functionality is powered by a specialized library,\n\u003ca href=\"https://github.com/couchbase/subjson/blob/4b93d966f791209209a0825e46f7049df0673e8f/subdoc/operations.cc#L757\"\u003elibrary \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esubjson\u003c/code\u003e\u003c/a\u003e, which in turn\n\u003ca href=\"https://github.com/couchbase/subjson/blob/4b93d966f791209209a0825e46f7049df0673e8f/subdoc/match.cc#L371\"\u003euses\u003c/a\u003e\nthe \u003ca href=\"https://github.com/mnunberg/jsonsl\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ejsonsl\u003c/code\u003e library\u003c/a\u003e for parsing JSON in a streaming manner. An enlightening piece of information about performance can be found in\nthe README of the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esubjson\u003c/code\u003e library, which is integral to Couchbase’s solution. The direct quote from the README is as follows:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eBecause the library does not actually build a JSON tree, the memory usage and CPU consumption is constant, regardless of the size of the actual JSON object\nbeing operated upon, and thus the only variable performance factor is the amount of actual time the library can seek to the location in the document to\nbe modified.\u003c/p\u003e\n\n  \u003cp\u003eOn a single Xeon E5520 core, this library can process about 150MB/s-300MB/s of JSON. This processing includes the search logic as well as any\nreplacement logic.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis analysis clearly demonstrates that lookups targeting paths situated towards the end of a document are markedly slower compared to those aimed\nat the beginning. Moreover, each sequential lookup \u003cstrong\u003eneeds to repeat document parsing\u003c/strong\u003e. The impact of this implementation on performance is significant.\nFor a more intuitive understanding of these effects, please refer to the diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-processing-sub-documents.png\" alt=\"Processing sub-documents in details\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe performance characteristics we’ve outlined align with the outcomes observed in our experiments. To illustrate, consider a detailed examination of\na single \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eHARD\u003c/code\u003e test scenario—specifically, a case where the sub-documents targeted for search were positioned towards the end of the JSON document:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./cb-perf-tester subdoc  --parallel 200 --repeat 50 --search-keys 10 --difficulty hard\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=10000, level=Hard, search-keys=10, repeats=50, parallel=200\nGenerated doc with subkeys: 10000, byte size is: 310043\n\nsearch for subkeys [level=Hard]: [data.subkey-009999 data.subkey-009998 data.subkey-009997 data.subkey-009996 data.subkey-009995 data.subkey-009994 data.subkey-009993 data.subkey-009992 data.subkey-009991 data.subkey-009990]\n\nsubdoc report: successes: 10000, errors: 0, duration: 1m19.784865193s, rps: 125.337055, success rps: 125.337055\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eBy multiplying the size of the document by the number of sub-documents queried, we can determine the total stream size that the library must process, which,\nin this case, approximates to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e~3MB\u003c/code\u003e. Further multiplying this figure by the RPS gives us an insight into the overall throughput of\nthe stream processing:\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e3MB x 125 RPS ~= 375 MBps\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eThe calculated throughput slightly exceeds the benchmarks outlined in the README. Moreover, the estimated throughput remains nearly constant across\nvarious tests. For a comprehensive view of these findings, please refer to the diagram below, which displays the estimated throughput for tests conducted under\nthe HARD level with a document size of approximately \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e300KB\u003c/code\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-estimated-subdocs-throughput.png\" alt=\"Estimated throughput\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eThe Couchbase sub-document API, while designed to optimize network throughput, introduces significant performance trade-offs. Notably, even\n\u003cem\u003eunder optimal conditions\u003c/em\u003e, operations on sub-documents are noticeably slower compared to regular get operations fetching\nsmall documents—evidenced by a comparison of the baseline performance; approximately 4-5k RPS for sub-document operations vs. 6-7k RPS for\nregular get operations.\u003c/p\u003e\n\n\u003cp\u003eThe method Couchbase employs for executing lookups directly influences performance, manifesting declines as either the document size increases or the number of\nlookups per request raises. This slowdown affects all requests over the same connection due to the high CPU demand of sub-document lookup operations.\nParticularly in environments utilizing reactive/asynchronous clients, this can overload the Couchbase worker thread, leading to a halt in request servicing.\nImportantly, an overloaded worker may manage connections from multiple clients, potentially exacerbating the “noisy neighbor” effect.\u003c/p\u003e\n\n\u003cp\u003eWhile there are strategies to mitigate these issues from the perspective of a client, such as the gateway service, these considerations warrant\na separate discussion, which I plan to address in a future blog post.\u003c/p\u003e\n","contentSnippet":"This story shows our journey in addressing a platform stability issue related to autoscaling, which, paradoxically, added some additional overhead instead\nof reducing the load. A pivotal part of this narrative is how we used Couchbase — a distributed NoSQL database. If you find\nyourself intrigued by another enigmatic story involving Couchbase, don’t miss my\nblog post on tuning expired doc settings.\nThis post unfolds our quest to discover the root cause of the bottleneck. Initially, I will outline the symptoms of the issue. Subsequently, you will be\nintroduced to how Couchbase is utilized by the aforementioned service. Equipped with this knowledge, I will recount our attempts to diagnose the problem and\nindicate which observations raised our suspicions. The following section is dedicated to conducting benchmarks to verify our predictions using\na custom benchmarking tool. Ultimately, we will explore the source code of Couchbase to uncover how the problematic operations are executed. This section\naims to provide a deep understanding of Couchbase’s inner workings. I firmly believe that the knowledge shared in that part is its most valuable asset and may\nenable you to swiftly identify and address some of the potential performance issues when using Couchbase.\nSet the scene\nThe service at the heart of the stability issues handles external HTTP traffic; for the purpose of this discussion, we’ll refer to it as\n“the gateway service”. The traffic routed to the gateway service reflects a pattern similar to organic traffic on Allegro,\ncharacterized by significant fluctuations in throughput between day and night hours. To efficiently utilize resources, the gateway service employs an autoscaler\nto dynamically adjust the number of instances based on current demands. It’s also important to note that spawning a new instance involves a warm-up phase,\nduring which the instance retrieves some data from Couchbase to populate its in-memory cache. The gateway service relies on a Couchbase cluster\ncomprised of three nodes.\nObservations\nThe team managing the service encountered a series of errors in communication with Couchbase. These errors indicated that 3-second timeouts occurred while\nfetching data from Couchbase:\n\ncom. couchbase.client.core.error.UnambiguousTimeoutException: SubdocGetRequest, Reason: TIMEOUT {\n    \"cancelled\":true,\n    \"completed\":true,\n    ... IRRELEVANT METADATA ...\n    \"timeoutMs\":3000,\n    \"timings\":{\"totalMicros\":3004052}\n}\n\n\nInterestingly, during these incidents, the Couchbase cluster did not exhibit high CPU or RAM usage. Furthermore, the traffic to Couchbase, measured in\noperations per second, was not exceptionally high. I mean that other Couchbase clients (different microservices) were generating an order of magnitude more\noperations per second without encountering stability issues.\nAdditional key observations related to the issue include:\nThe instability primarily occurred during the service scaling-up process, initially triggered by the autoscaler.\nNewly spawned instances were predominantly affected.\nThe issues were reported solely for operations directed to a specific node within the cluster.\nA temporary mitigation of the problems involved repeatedly restarting the failing application instances.\nThere was a noticeable pattern on the driver side that preceded the widespread errors, including timeouts and the inability to send requests due to\na non-writable channel.\nTemporary solution\nAs a temporary measure, the team overseeing the gateway service implemented the following workarounds:\nDisabled certain types of requests to reduce the overall traffic volume directed to Couchbase.\nDeactivated the autoscaler, and manually scaled up the application to manage peak traffic loads.\nThese actions successfully halted the problems, but they also had repercussions, including business impacts and decreased efficiency in resource utilization.\nRaising suspicions\nA pivotal aspect of this issue was the use of the Couchbase sub-document API\nwithin the gateway service, an approach not widely adopted across our internal microservice landscape, yet notable for its efficiency. According to\nthe documentation, this API significantly reduces traffic by allowing the fetching or mutating only specific parts of a Couchbase document.\nEssentially, it acts as a substitute for the concept of projection, familiar to SQL users.\nIn our investigation we closely examined the data collected on the Couchbase node, the operational dynamics of the gateway service’s cache, and insights\nfrom scrutinizing both the Couchbase driver and server code. We hypothesized that the crux of the problem might be linked to the cache warm-up process for\nnewly launched instances.\nOur investigation uncovered several indicators pointing toward the core of the issue:\nA disproportionately large number of requests targeted a single document, inevitably directing traffic to a specific node.\nThe node hosting this heavily queried document corresponded with the one mentioned in timeout-related logs.\nInstances that had been running for an extended period reported virtually no errors.\nThe volume of requests to Couchbase from the affected instances was extraordinarily high, not aligning with the number of requests registered on\nthe Couchbase side. This discrepancy suggested that if the cache warming process was at fault, the sheer magnitude of attempted requests was overwhelming\neven the local network buffers.\nHowever, these observations were merely pieces of a larger puzzle. We noticed a “snowball effect” where the system’s inability to process an initial set\nof requests for newly initiated instances triggered a cascade of failures. But the question remained: Why? What made these instances different,\nand why didn’t other clients on the same cluster experience similar issues? This was the crucial moment to take a closer examination of the sub-document\noperations to determine their efficiency and optimization.\nLet’s benchmark it\nDespite an extensive search, we were unable to locate any tools capable of reliably testing our hypothesis—that sub-document operations executed during\nthe warm-up phase could significantly challenge Couchbase’s handling capabilities. As a result, we developed a simple tool and made it\navailable in on GitHub.\nThis tool is designed to create a sample document and then execute parallel sub-document fetch operations concurrently.\nThe sample document is structured as follows:\n\n{\n    \"key\": \"test-subdoc\",\n    \"data\": {\n        \"subkey-000000\": \"value-000000\",\n        \"subkey-000001\": \"value-000001\",\n        . . .\n        \"subkey-0….N\": \"value-0…..N\",\n    }\n}\n\n\nThe tool allows manipulating several knobs, which includes:\nParallelism: Determines the number of parallel goroutines that will attempt to fetch the same sub-documents concurrently.\nDocument Size: Defined by the number of sub-keys, this directly affects the document’s binary size.\nLevel of Search Difficulty: This essentially refers to how deep or how far into the main document the target sub-document is located.\nThe concept is illustrated in the diagram below:\n\nCaveats\nThe primary objective of this exercise was to identify potential bottlenecks, not to conduct a highly accurate performance assessment of Couchbase clusters.\nTherefore, we opted to run our experiments using a local Docker container (couchbase:community-6.6.0), rather than on a dedicated, well-isolated cluster.\nWe acknowledge that hosting both the server and the benchmarking tool on the same machine may compromise the reliability and accuracy of the results.\nConsequently, we advise against using the findings from these tests for comprehensive assessments or comparisons with other technologies.\nBenchmark steps\nThe procedure for each experiment follows a similar framework, outlined in the steps below:\nDocument Preparation: Initiate the document with the desired number of sub-documents, as dictated by one of the experimental variables.\nDocument Storage: Save this document under a predetermined key.\nGoroutine Initialization: Launch a specified number of goroutines, the quantity of which is determined by another experimental variable.\nFetch Operations: Each goroutine executes a series of fetch operations, which can be either regular (retrieving the entire sample document) or\nsub-document (accessing a set of sub-documents). It’s important to note that these requests are executed in a blocking manner; a new fetch operation is\nperformed only after the completion of the preceding one. In sub-document mode, the difficulty of the fetch operation is controlled through\nan experiment variable.\nCompletion Wait: Await the termination of all goroutines.\nResults Reporting: Calculate and display the estimated RPS (requests per second).\nEstimate baseline\nPrior to delving into sub-document operations, we sought to establish the maximum number of regular get operations that our local Couchbase Server instance\ncould handle. Through testing at various levels of concurrency, we determined the maximum throughput for our specific setup.\nIt was approximately 6,000 to 7,000 RPS, regardless of whether the requests were for small documents (less than 200 bytes)\nor for non-existent documents. These findings were further validated by the statistics available through the Couchbase UI.\nBenchmark Command: Attempting to fetch a non-existent document yielded a rate of 6388 RPS.\n\n./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5 --search-non-existent\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=true, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: not-exists\n\nregular report: successes: 0, errors: 200000, duration: 31.306684977s, rps: 6388.411937, success rps: 0.000000\n\n\n\nBenchmark Command: Fetching an existing small (195 bytes) document yielded a rate of 6341 rps.\n\n./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=false, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: test-regular\n\nregular report: successes: 200000, errors: 0, duration: 31.536538682s, rps: 6341.850068, success rps: 6341.850068\n\n\n\nTesting scenarios\nNow that we have a baseline for comparison, we’re set to evaluate it against the outcomes of various scenarios. To ensure the tests are comparable,\nwe’ll maintain constant parallelism across all tests, specifically using 200 goroutines. The variables that will differ across scenarios include:\nTotal Number of Sub-Documents: This determines the overall size of the sample document, as the document’s size is directly related to the number\nof sub-documents it contains.\nNumber of Searched Sub-Documents: This refers to how many sub-paths within the sample document will be targeted in a single fetch operation.\nSearch Difficulty: This aspect dictates the difficulty of locating the searched sub-paths within the document.\nIt’s important to highlight that in each scenario, we will manipulate only one variable at a time while keeping the other parameters constant.\nScenario A: The impact of document size on performance\n\nTo better visualize the impact, let’s look at the diagram for HARD scenario:\n\nIt is clearly visible that there is a strict correlation between document size and performance.\nScenario B: The impact of the number of searched sub-documents on performance\n\nTo better visualize the impact, let’s look at the diagram for HARD scenario:\n\nAs observed in the previous scenario, there is a clear correlation between the number of sub-documents accessed and the system’s performance.\nFurther analysis\nGiven the evident correlation between the document size/number of queried sub-paths and performance degradation, we delve into the mechanics to understand\nthe root cause of these test results. A notable observation during the tests relates to CPU utilization within the Docker environment, where, despite having\nsix cores available, only a single core was actively utilized. Intriguingly, this usage was monopolized by a single thread (mc:worker_X).\nThis phenomenon directly stems from Couchbase’s handling of Key-Value (KV) connections. By default, the Couchbase driver initiates only a single connection to\neach cluster node for KV operations. However, this configuration can be adjusted in certain Software Development Kits (SDKs)—the Java SDK,\nfor instance, allows modification through IoConfig.numKvConnections.\nWhen a connection is established, Couchbase assigns it to\na specific worker thread) using\na Round-Robin (RR)) algorithm. As a result, the Couchbase Server does not\nfully utilize available CPU power for a single connection, even when a lot of resources are free. This behavior can be seen as beneficial, serving to mitigate\nthe “noisy neighbor” effect, provided there are sufficient\nspare cores available to manage new connections. This mechanism ensures balanced resource use across connections, as illustrated in the diagram below:\n\nConversely, one may encounter fluctuating performance due to instances of misfortune, where if other clients significantly burden certain worker threads,\nand your connection is allocated to one of these overloaded threads, performance inconsistencies arise. This scenario, where a client experiences higher than\nusual response times due to an imbalanced distribution of workload across worker threads, is depicted in the diagram below:\n\nThis behavior explains the apparent paradox observed during the stability issues: the Couchbase node showed no clear signs of being overloaded, yet certain\nanomalous symptoms were present, such as a metric indicating the minimum idle percentage across all cores plummeting\nto 0% during the disturbances. This leaves no doubt that operations on sub-documents have the potential to overburden worker threads within\nthe Couchbase Server. With this understanding we are now ready to delve deeper into the root cause of such behavior.\nWhat documentation says\nThe documentation for Couchbase, housed alongside the server’s source code, is notably comprehensive, including a dedicated section on\nsub-documents. However, it falls short of providing detailed insights into\nthe internal workings of these operations. Additionally, there is a lack of discussion on the performance implications of sub-document operations,\nwith only a few remarks on data typing and float numbers that do not apply to the cases we tested. Attempts to find answers on the Couchbase forum were also\nunfruitful, yielding no substantial information on the performance issues we encountered. Despite this, there is confirmation from others in the community who\nhave observed\nsimilar problems.\nWhat source code says\nA thorough analysis of the codebase reveals a definitive cause for the performance degradation observed. It’s important to note that Couchbase requires\ndecompression of a document for any lookup or manipulation\noperation, whether the document is retrieved from RAM or disk. Let’s start from a point where Couchbase starts doing\nlookups on a decompressed object:\n\n// 2. Perform each of the operations on document.\nfor (auto\u0026 op : operations) {\n    switch (op.traits.scope) {\n    case CommandScope::SubJSON:\n        if (cb::mcbp::datatype::is_json(doc_datatype)) {\n            // Got JSON, perform the operation.\n            op.status = subdoc_operate_one_path(context, op, current.view);\n        }\n\n\nA critical observation from our analysis is that each operation (lookup) is executed sequentially through the invocation of subdoc_operate_one_path.\nTo understand the performance implications, let’s examine\nthe implementation of this lookup\nprocess:\n\n// ... and execute it.\nconst auto subdoc_res = op.op_exec(spec.path.data(), spec.path.size());\n\n\nThe Investigation reveals that the lookup functionality is powered by a specialized library,\nlibrary subjson, which in turn\nuses\nthe jsonsl library for parsing JSON in a streaming manner. An enlightening piece of information about performance can be found in\nthe README of the subjson library, which is integral to Couchbase’s solution. The direct quote from the README is as follows:\nBecause the library does not actually build a JSON tree, the memory usage and CPU consumption is constant, regardless of the size of the actual JSON object\nbeing operated upon, and thus the only variable performance factor is the amount of actual time the library can seek to the location in the document to\nbe modified.\nOn a single Xeon E5520 core, this library can process about 150MB/s-300MB/s of JSON. This processing includes the search logic as well as any\nreplacement logic.\nThis analysis clearly demonstrates that lookups targeting paths situated towards the end of a document are markedly slower compared to those aimed\nat the beginning. Moreover, each sequential lookup needs to repeat document parsing. The impact of this implementation on performance is significant.\nFor a more intuitive understanding of these effects, please refer to the diagram below:\n\nThe performance characteristics we’ve outlined align with the outcomes observed in our experiments. To illustrate, consider a detailed examination of\na single HARD test scenario—specifically, a case where the sub-documents targeted for search were positioned towards the end of the JSON document:\n\n./cb-perf-tester subdoc  --parallel 200 --repeat 50 --search-keys 10 --difficulty hard\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=10000, level=Hard, search-keys=10, repeats=50, parallel=200\nGenerated doc with subkeys: 10000, byte size is: 310043\n\nsearch for subkeys [level=Hard]: [data.subkey-009999 data.subkey-009998 data.subkey-009997 data.subkey-009996 data.subkey-009995 data.subkey-009994 data.subkey-009993 data.subkey-009992 data.subkey-009991 data.subkey-009990]\n\nsubdoc report: successes: 10000, errors: 0, duration: 1m19.784865193s, rps: 125.337055, success rps: 125.337055\n\n\nBy multiplying the size of the document by the number of sub-documents queried, we can determine the total stream size that the library must process, which,\nin this case, approximates to ~3MB. Further multiplying this figure by the RPS gives us an insight into the overall throughput of\nthe stream processing:\n3MB x 125 RPS ~= 375 MBps\nThe calculated throughput slightly exceeds the benchmarks outlined in the README. Moreover, the estimated throughput remains nearly constant across\nvarious tests. For a comprehensive view of these findings, please refer to the diagram below, which displays the estimated throughput for tests conducted under\nthe HARD level with a document size of approximately 300KB:\n\nConclusions\nThe Couchbase sub-document API, while designed to optimize network throughput, introduces significant performance trade-offs. Notably, even\nunder optimal conditions, operations on sub-documents are noticeably slower compared to regular get operations fetching\nsmall documents—evidenced by a comparison of the baseline performance; approximately 4-5k RPS for sub-document operations vs. 6-7k RPS for\nregular get operations.\nThe method Couchbase employs for executing lookups directly influences performance, manifesting declines as either the document size increases or the number of\nlookups per request raises. This slowdown affects all requests over the same connection due to the high CPU demand of sub-document lookup operations.\nParticularly in environments utilizing reactive/asynchronous clients, this can overload the Couchbase worker thread, leading to a halt in request servicing.\nImportantly, an overloaded worker may manage connections from multiple clients, potentially exacerbating the “noisy neighbor” effect.\nWhile there are strategies to mitigate these issues from the perspective of a client, such as the gateway service, these considerations warrant\na separate discussion, which I plan to address in a future blog post.","guid":"https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html","categories":["tech","couchbase","sub-documents","performance","bottlenecks"],"isoDate":"2024-05-15T22:00:00.000Z"}],"jobs":[{"id":"743999994499625","name":"Software Engineer (.NET) - Product Team - Allegro Pay","uuid":"a427e05d-f51e-44a3-9e18-27f5c2fa9c80","jobAdId":"57040109-b330-495b-aa0d-247de3ea2066","defaultJobAd":false,"refNumber":"REF4748E","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-06-17T07:58:16.666Z","location":{"city":"Warsaw","region":"Masovian Voivodeship","country":"pl","remote":false,"latitude":"52.2296756","longitude":"21.0122287"},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"b3da614a-1ddb-441b-a557-5acfdb6fcb90","valueLabel":"NEW Technology CL 1-6"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999994499625","creator":{"name":"Martyna Stafa"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999994263391","name":"Senior Salesforce Software Engineer","uuid":"2742b4d4-eabb-48dd-9b0c-399cdb0afd6b","jobAdId":"274a39ab-ebda-4c9c-a9bd-5cd21a08a275","defaultJobAd":false,"refNumber":"REF4747J","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-06-14T13:42:56.357Z","location":{"city":"Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"26b58095-3c5f-4596-937f-27547fb80b07","valueLabel":"5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999994263391","creator":{"name":"Agnieszka Adamus"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999994260676","name":"Senior Salesforce Software Engineer","uuid":"d487ff86-8cfc-4076-818f-505afe848fd1","jobAdId":"d0b08f39-f02c-4e32-ad36-475f900ddfc1","defaultJobAd":true,"refNumber":"REF4747J","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-06-14T13:42:31.056Z","location":{"city":"Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"26b58095-3c5f-4596-937f-27547fb80b07","valueLabel":"5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999994260676","creator":{"name":"Agnieszka Adamus"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999994187830","name":"Contractor Software Engineer (Java/Kotlin)","uuid":"a4a96698-9b67-415f-a1c8-dd917f0e0980","jobAdId":"eba60e1f-3a45-4af9-a9e7-2b36917ce3eb","defaultJobAd":true,"refNumber":"REF4956T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-06-14T08:16:05.358Z","location":{"city":"Warsaw, Poznań, Cracow","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"contract","label":"Contract"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"e543c79f-6df9-499f-8ba8-237c3c331cc1","valueLabel":"NEW B2B Technology CL 1-6"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"f7af19b5-5d6a-43a0-9a2b-1e99277515c7","valueLabel":"Opennet.pl Sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999994187830","creator":{"name":"Magdalena Szetelnicka"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999994187205","name":"Software Engineer (Java) - Platform Team","uuid":"7bcc877a-15ce-456b-983b-143acd0d62e6","jobAdId":"e667c7ae-00a8-4611-b5aa-85fd155a5c54","defaultJobAd":true,"refNumber":"REF5039L","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-06-14T08:06:22.659Z","location":{"city":"Warsaw","region":"Masovian Voivodeship","country":"pl","remote":false,"latitude":"52.2296756","longitude":"21.0122287"},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3976147c-fe25-42a8-8c97-78273250960b","valueLabel":"4"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"b3da614a-1ddb-441b-a557-5acfdb6fcb90","valueLabel":"NEW Technology CL 1-6"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999994187205","creator":{"name":"Magdalena Szetelnicka"},"language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1715716710000,"duration":7200000,"id":"301022703","name":"Allegro Tech Talks #43 - Wszystko o programie e-Xperience","date_in_series_pattern":false,"status":"past","time":1716393600000,"local_date":"2024-05-22","local_time":"18:00","updated":1716406648000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":14,"venue":{"id":27570147,"name":"Allegro Office - Poznań (Nowy Rynek)","lat":52.40021514892578,"lon":16.92083168029785,"repinned":false,"address_1":"Wierzbięcice 1B - budynek D","city":"Poznań","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/301022703/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-43/](https://app.evenea.pl/event/allegro-tech-talk-43/) Zapraszamy Was na #43 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej kawie☕, napojach🥤…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Nowy Rynek. Szukaj budynku D i kieruj się do wejścia od ul. Wierzbięcice. Komunikacja miejska: najbliższy przystanek to Wierzbięcice i kursują tu linie tramwajowe numer 2, 5, 6, 10, 12, 18 Spacerem - z dworca Poznań Główny przejście zajmie Ci około 5 minut.","visibility":"public","member_pay_fee":false},{"created":1702979844000,"duration":187200000,"id":"298027809","name":"UX Research Confetti - IV edycja","date_in_series_pattern":false,"status":"past","time":1716202800000,"local_date":"2024-05-20","local_time":"13:00","updated":1716392955000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":82,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/298027809/","description":"**🎉 Przedstawiamy 4. edycję UX Research Confetti - bezpłatną, polską konferencję poświęconą badaniom UX, organizowaną przez zespół badaczy z Allegro.** ✨ Konferencja odbędzie się w…","visibility":"public","member_pay_fee":false},{"created":1712583756000,"duration":14400000,"id":"300288303","name":"DDD \u0026 EventStorming na luzie - unconference na 2 lata gildii w Allegro","date_in_series_pattern":false,"status":"past","time":1714129200000,"local_date":"2024-04-26","local_time":"13:00","updated":1714146607000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":103,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/300288303/","description":"**➡ Rejestracja:** **[https://app.evenea.pl/event/allegro-tech-talk-ddd/](https://app.evenea.pl/event/allegro-tech-talk-ddd/)** Dobrze Was widzieć! Allegro Tech to miejsce, w którym dzielimy się wiedzą, dobrymi praktykami i case study z różnych projektów prowadzonych w…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Fabryki Norblina. Szukaj wejścia Plater 3, od ul. Żelaznej. \n\nKomunikacja miejska: najbliższe przystanki to Norblin 05 i 06 z liniami: 109, 178, 157. Dojedziecie do nas również tramwajami numer 10 i 11 oraz metrem (Rondo ONZ lub Rondo Daszyńskiego).","visibility":"public","member_pay_fee":false},{"created":1712756447000,"duration":7200000,"id":"300327359","name":"Allegro Tech Talks #42 - Kariera Product Managera","date_in_series_pattern":false,"status":"past","time":1713888000000,"local_date":"2024-04-23","local_time":"18:00","updated":1713900030000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":37,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/300327359/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-42/ ](https://app.evenea.pl/event/allegro-tech-talk-42/) Zapraszamy Was na #42 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Fabryki Norblina. Szukaj wejścia Plater 3, od ul. Żelaznej. \n\nKomunikacja miejska: najbliższe przystanki to Norblin 05 i 06 z liniami: 109, 178, 157. Dojedziesz do nas również tramwajami numer 10 i 11 oraz metrem (Rondo ONZ lub Rondo Daszyńskiego).","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali","link":"https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/","pubDate":"Thu, 29 Feb 2024 00:00:00 GMT","content":"Na czym polega praca analityków w obszarze technologii w Allegro? Jakich narzędzi i technologii na co dzień używają osoby pracujące na tych stanowiskach? Jak efekty pracy analityków wpływają na naszą platformę, produkty i funkcjonalności? Czym zajmuje się Data Product Manager w Allegro Pay? Dlaczego monety są ważnym elementem ekosystemu Allegro? Posłuchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziałem Adrianny Napiórkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.","contentSnippet":"Na czym polega praca analityków w obszarze technologii w Allegro? Jakich narzędzi i technologii na co dzień używają osoby pracujące na tych stanowiskach? Jak efekty pracy analityków wpływają na naszą platformę, produkty i funkcjonalności? Czym zajmuje się Data Product Manager w Allegro Pay? Dlaczego monety są ważnym elementem ekosystemu Allegro? Posłuchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziałem Adrianny Napiórkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.","guid":"https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/","isoDate":"2024-02-29T00:00:00.000Z"},{"title":"Programowanie - co liczy się w nim najbardziej?","link":"https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/","pubDate":"Thu, 01 Feb 2024 00:00:00 GMT","content":"Jaką ścieżkę trzeba przejść, aby dobrze programować? Gdzie zdobywać wiedzę, doświadczenie i szlifować swoje umiejętności? Ile czasu potrzeba aby nabrać doświadczenia i jak zadbać o swój dalszy rozwój? Na czym w praktyce polegają role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza się w naszych zespołach? Posłuchajcie nowego odcinka Allegro Tech Podcast z udziałem Rafała Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.","contentSnippet":"Jaką ścieżkę trzeba przejść, aby dobrze programować? Gdzie zdobywać wiedzę, doświadczenie i szlifować swoje umiejętności? Ile czasu potrzeba aby nabrać doświadczenia i jak zadbać o swój dalszy rozwój? Na czym w praktyce polegają role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza się w naszych zespołach? Posłuchajcie nowego odcinka Allegro Tech Podcast z udziałem Rafała Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.","guid":"https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/","isoDate":"2024-02-01T00:00:00.000Z"},{"title":"MBox: server-driven UI dla aplikacji mobilnych","link":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","pubDate":"Thu, 16 Nov 2023 00:00:00 GMT","content":"Czym jest i jak powstał MBox: wewnętrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? Skąd wziął się pomysł na to rozwiązanie i na jakie bolączki odpowiada? Dlaczego zdecydowaliśmy się na budowanie tego typu rozwiązania in-house i z jakimi wyzwaniami mierzyliśmy się w procesie tworzenia? Co wyróżnia zespoły pracujące nad tym narzędziem i jak pracuje im się bez Product Ownera? Posłuchajcie siódmego odcinka Allegro Tech Podcast z udziałem Pauliny Sadowskiej i Tomasza Gębarowskiego - Managerów w obszarze Technical Platform Services w Allegro.","contentSnippet":"Czym jest i jak powstał MBox: wewnętrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? Skąd wziął się pomysł na to rozwiązanie i na jakie bolączki odpowiada? Dlaczego zdecydowaliśmy się na budowanie tego typu rozwiązania in-house i z jakimi wyzwaniami mierzyliśmy się w procesie tworzenia? Co wyróżnia zespoły pracujące nad tym narzędziem i jak pracuje im się bez Product Ownera? Posłuchajcie siódmego odcinka Allegro Tech Podcast z udziałem Pauliny Sadowskiej i Tomasza Gębarowskiego - Managerów w obszarze Technical Platform Services w Allegro.","guid":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","isoDate":"2023-11-16T00:00:00.000Z"},{"title":"O chatbotach i ich wpływie na Allegro","link":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","pubDate":"Wed, 11 Oct 2023 00:00:00 GMT","content":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieć w kontekście obszaru Customer Experience? W czym pomagają nam chatboty, jak je rozwijamy i dbamy o ich jakość? Kim są Allina oraz Albert i co mają wspólnego z automatyzacją? Za jakie rozwiązania otrzymaliśmy nagrodę hiperautomatyzacji? O tym wszystkim posłuchacie w odcinku z udziałem Rafała Gajewskiego - Managera w obszarze IT Services w Allegro.","contentSnippet":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieć w kontekście obszaru Customer Experience? W czym pomagają nam chatboty, jak je rozwijamy i dbamy o ich jakość? Kim są Allina oraz Albert i co mają wspólnego z automatyzacją? Za jakie rozwiązania otrzymaliśmy nagrodę hiperautomatyzacji? O tym wszystkim posłuchacie w odcinku z udziałem Rafała Gajewskiego - Managera w obszarze IT Services w Allegro.","guid":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","isoDate":"2023-10-11T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"cJo7-lX_UosNxZ78IKqnF","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>