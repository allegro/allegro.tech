<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w ktÃ³rym nasi inÅ¼ynierowie dzielÄ… siÄ™ wiedzÄ… oraz case study z wybranych projektÃ³w w firmie - w formie artykuÅ‚Ã³w, podcastÃ³w oraz eventÃ³w."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><link rel="author" href="humans.txt"/><script defer="" data-domain="allegro.tech" src="https://plausible.io/js/script.js"></script><script defer="" src="/clear-cookies.js"></script><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/c4277531f90028a4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c4277531f90028a4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/79db8b1e27b0a093.css" as="style"/><link rel="stylesheet" href="/_next/static/css/79db8b1e27b0a093.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d73513da824d371.js" defer=""></script><script src="/_next/static/chunks/206-3a56e5ded293e83e.js" defer=""></script><script src="/_next/static/chunks/pages/index-7fe746bd88df1759.js" defer=""></script><script src="/_next/static/Aoyoj7t6lTS_pXksuxS9r/_buildManifest.js" defer=""></script><script src="/_next/static/Aoyoj7t6lTS_pXksuxS9r/_ssgManifest.js" defer=""></script><script src="/_next/static/Aoyoj7t6lTS_pXksuxS9r/_middlewareManifest.js" defer=""></script></head><body class="m-color-bg_desk"><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://jobs.allegro.eu">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="OtwÃ³rz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">About us</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro is one of the most technologically advanced companies in our part of Europe. Allegro is also over 1700Â IT specialists of various specializations, developing our website. The unique scale and complexity of the problems that we solve on a daily basis give us the opportunity to develop on a wide variety of projects. Allegro Tech is a place where our engineers share knowledge and case studies from selected projects in the companyÂ â€“ in the form of articles, podcasts and events.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html" title="Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF"><img width="388" src="images/blogpost.png" alt="Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html" title="Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">dzieÅ„ temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kafka">#<!-- -->kafka</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ebpf">#<!-- -->ebpf</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bcc">#<!-- -->bcc</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/linux">#<!-- -->linux</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kernel">#<!-- -->kernel</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ext4">#<!-- -->ext4</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/xfs">#<!-- -->xfs</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tuning">#<!-- -->tuning</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/file system">#<!-- -->file system</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">At Allegro, we use Kafka as a backbone for asynchronous communication between microservices. With up to
300k messages published and 1M messages consumed every second, itâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:2"><img alt="Maciej MoÅ›cicki" src="https://blog.allegro.tech/img/authors/maciej.moscicki.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Piotr RÅ¼ysko" src="https://blog.allegro.tech/img/authors/piotr.rzysko.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/maciej.moscicki">Maciej MoÅ›cickiâ€¦</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/02/rpa.html" title="Tired of repetitive tasks?! Go for RPA!"><img width="388" src="images/blogpost.png" alt="Tired of repetitive tasks?! Go for RPA!" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/02/rpa.html" title="Tired of repetitive tasks?! Go for RPA!" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Tired of repetitive tasks?! Go for RPA!</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">16 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/rpa">#<!-- -->rpa</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Have you ever thought about ways of reducing repetitive, monotonous tasks? Maybe you would like to try to automate your own tasks? I will showâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Dominika PleÅ›niak" src="https://blog.allegro.tech/img/authors/dominika.plesniak.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/dominika.plesniak">Dominika PleÅ›niak</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/02/rpa.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/02/rpa.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/02/couchbase-expired-docs-tuning.html" title="Donâ€™t bother: it is only a little expired"><img width="388" src="images/blogpost.png" alt="Donâ€™t bother: it is only a little expired" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/02/couchbase-expired-docs-tuning.html" title="Donâ€™t bother: it is only a little expired" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Donâ€™t bother: it is only a little expired</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">24 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/couchbase">#<!-- -->couchbase</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/replication">#<!-- -->replication</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance bottleneck">#<!-- -->performance bottleneck</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/open source">#<!-- -->open source</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ttl">#<!-- -->ttl</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/metrics">#<!-- -->metrics</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">This story shows how we strive to fix issues reported by our customers regarding inconsistent listing views on our e-commerce platform.
We will use a top-downâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Tomasz ZiÃ³Å‚kowski" src="https://blog.allegro.tech/img/authors/tomasz.ziolkowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/tomasz.ziolkowski">Tomasz ZiÃ³Å‚kowski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/02/couchbase-expired-docs-tuning.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/02/couchbase-expired-docs-tuning.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/01/wcag-2-2.html" title="WCAG 2.2 is here! And what about 3.0?"><img width="388" src="images/blogpost.png" alt="WCAG 2.2 is here! And what about 3.0?" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/01/wcag-2-2.html" title="WCAG 2.2 is here! And what about 3.0?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">WCAG 2.2 is here! And what about 3.0?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o miesiÄ…c temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/accessibility">#<!-- -->accessibility</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/a11y">#<!-- -->a11y</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/wcag">#<!-- -->wcag</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Ready to turn web accessibility from a headache into a breeze? Join us as we demystify WCAG, explore its latest 2.2 version, and gaze intoâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Barbara Szott" src="https://blog.allegro.tech/img/authors/barbara.szott.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/barbara.szott">Barbara Szott</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/01/wcag-2-2.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/01/wcag-2-2.html">przejdÅº do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz wiÄ™cej wpisÃ³w</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/" title="O pracy analitykÃ³w w obszarze technologii i przetwarzaniu danych w duÅ¼ej skali"><img src="images/podcast.png" alt="O pracy analitykÃ³w w obszarze technologii i przetwarzaniu danych w duÅ¼ej skali" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/" title="O pracy analitykÃ³w w obszarze technologii i przetwarzaniu danych w duÅ¼ej skali" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O pracy analitykÃ³w w obszarze technologii i przetwarzaniu danych w duÅ¼ej skali</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">7 dni temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Na czym polega praca analitykÃ³w w obszarze technologii w Allegro? Jakich narzÄ™dzi i technologii na co dzieÅ„ uÅ¼ywajÄ… osoby pracujÄ…ce na tych stanowiskach? Jak efekty pracy analitykÃ³w wpÅ‚ywajÄ… na naszÄ… platformÄ™, produkty i funkcjonalnoÅ›ci? Czym zajmuje siÄ™ Data Product Manager w Allegro Pay? Dlaczego monety sÄ… waÅ¼nym elementem ekosystemu Allegro? PosÅ‚uchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziaÅ‚em Adrianny NapiÃ³rkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/" title="Programowanie - co liczy siÄ™ w nim najbardziej?"><img src="images/podcast.png" alt="Programowanie - co liczy siÄ™ w nim najbardziej?" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/" title="Programowanie - co liczy siÄ™ w nim najbardziej?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Programowanie - co liczy siÄ™ w nim najbardziej?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o miesiÄ…c temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">JakÄ… Å›cieÅ¼kÄ™ trzeba przejÅ›Ä‡, aby dobrze programowaÄ‡? Gdzie zdobywaÄ‡ wiedzÄ™, doÅ›wiadczenie i szlifowaÄ‡ swoje umiejÄ™tnoÅ›ci? Ile czasu potrzeba aby nabraÄ‡ doÅ›wiadczenia i jak zadbaÄ‡ o swÃ³j dalszy rozwÃ³j? Na czym w praktyce polegajÄ… role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza siÄ™ w naszych zespoÅ‚ach? PosÅ‚uchajcie nowego odcinka Allegro Tech Podcast z udziaÅ‚em RafaÅ‚a Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/" title="MBox: server-driven UI dla aplikacji mobilnych"><img src="images/podcast.png" alt="MBox: server-driven UI dla aplikacji mobilnych" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/" title="MBox: server-driven UI dla aplikacji mobilnych" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">MBox: server-driven UI dla aplikacji mobilnych</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 miesiÄ…ce temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym jest i jak powstaÅ‚ MBox: wewnÄ™trzna platforma server-driven UI dla aplikacji mobilnych w Allegro? SkÄ…d wziÄ…Å‚ siÄ™ pomysÅ‚ na to rozwiÄ…zanie i na jakie bolÄ…czki odpowiada? Dlaczego zdecydowaliÅ›my siÄ™ na budowanie tego typu rozwiÄ…zania in-house i z jakimi wyzwaniami mierzyliÅ›my siÄ™ w procesie tworzenia? Co wyrÃ³Å¼nia zespoÅ‚y pracujÄ…ce nad tym narzÄ™dziem i jak pracuje im siÄ™ bez Product Ownera? PosÅ‚uchajcie siÃ³dmego odcinka Allegro Tech Podcast z udziaÅ‚em Pauliny Sadowskiej i Tomasza GÄ™barowskiego - ManagerÃ³w w obszarze Technical Platform Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/" title="O chatbotach i ich wpÅ‚ywie na Allegro"><img src="images/podcast.png" alt="O chatbotach i ich wpÅ‚ywie na Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/" title="O chatbotach i ich wpÅ‚ywie na Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O chatbotach i ich wpÅ‚ywie na Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">5 miesiÄ™cy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieÄ‡ w kontekÅ›cie obszaru Customer Experience? W czym pomagajÄ… nam chatboty, jak je rozwijamy i dbamy o ich jakoÅ›Ä‡? Kim sÄ… Allina oraz Albert i co majÄ… wspÃ³lnego z automatyzacjÄ…? Za jakie rozwiÄ…zania otrzymaliÅ›my nagrodÄ™ hiperautomatyzacji? O tym wszystkim posÅ‚uchacie w odcinku z udziaÅ‚em RafaÅ‚a Gajewskiego - Managera w obszarze IT Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/">PosÅ‚uchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz wiÄ™cej podcastÃ³w</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/298027809/" title="UX Research Confetti - IV edycja" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - IV edycja"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/298027809/" title="UX Research Confetti - IV edycja" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - IV edycja</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">za 2 miesiÄ…ce<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**ğŸ‰ Przedstawiamy 4. edycjÄ™ UX Research Confetti - bezpÅ‚atnÄ…, polskÄ… konferencjÄ™ poÅ›wiÄ™conÄ… badaniom UX, organizowanÄ… przez zespÃ³Å‚ badaczy z Allegro.** âœ¨ Konferencja odbÄ™dzie siÄ™ wâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/298027809/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/297614064/" title="Allegro Tech Talks #40 - Testy: dynamiczne dashboardy &amp; optymalizacja pracy" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #40 - Testy: dynamiczne dashboardy &amp; optymalizacja pracy"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/297614064/" title="Allegro Tech Talks #40 - Testy: dynamiczne dashboardy &amp; optymalizacja pracy" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #40 - Testy: dynamiczne dashboardy &amp; optymalizacja pracy</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiÄ…ce temu<!-- -->, Allegro KrakÃ³w Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-40/](https://app.evenea.pl/event/allegro-tech-talk-40/) Jeszcze przed Å›wiÄ™tami zapraszamy Was na #40 wydarzenie z serii Allegro Tech Talk, podczas ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemyâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/297614064/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/297480100/" title="Allegro Tech Talks #39 - Big Data: o podejÅ›ciu do pracy z danymi" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #39 - Big Data: o podejÅ›ciu do pracy z danymi"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/297480100/" title="Allegro Tech Talks #39 - Big Data: o podejÅ›ciu do pracy z danymi" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #39 - Big Data: o podejÅ›ciu do pracy z danymi</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiÄ…ce temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**Rejestracja: [https://app.evenea.pl/event/allegro-tech-talk-39/](https://app.evenea.pl/event/allegro-tech-talk-39/)** BÄ…dÅºcie z nami podczas #39 wydarzenia z serii **Allegro Tech Talk**, podczas ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³w przyâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/297480100/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #38 - Mobile: o iOS bez spinki"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #38 - Mobile: o iOS bez spinki</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">9 miesiÄ™cy temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwÄ… wakacyjnÄ…, stacjonarne spotkanie z cyklu Allegro Tech Talks, na ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³wâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293929321/">SzczegÃ³Å‚y</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz wiÄ™cej wydarzeÅ„</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (.NET) - Opennet</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw, PoznaÅ„</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999972114613-software-engineer-net-opennet?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Salesforce Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw, PoznaÅ„</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999971068323-senior-salesforce-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">(.NET) Software Engineer - OpenNet</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999970171023-net-software-engineer-opennet?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Software Engineer (Java/Kotlin) - Technology Consumer Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999967550443-senior-software-engineer-javakotlin-technology-consumer-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (Java/Kotlin) - Delivery Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, PoznaÅ„</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999967549418-software-engineer-javakotlin-delivery-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://jobs.allegro.eu">Zobacz wiÄ™cej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Unlocking Kafka's Potential: Tackling Tail Latency with eBPF","link":"https://blog.allegro.tech/2024/03/kafka-performance-analysis.html","pubDate":"Wed, 06 Mar 2024 00:00:00 +0100","authors":{"author":[{"name":["Maciej MoÅ›cicki"],"photo":["https://blog.allegro.tech/img/authors/maciej.moscicki.jpg"],"url":["https://blog.allegro.tech/authors/maciej.moscicki"]},{"name":["Piotr RÅ¼ysko"],"photo":["https://blog.allegro.tech/img/authors/piotr.rzysko.jpg"],"url":["https://blog.allegro.tech/authors/piotr.rzysko"]}]},"content":"\u003cp\u003eAt \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e, we use \u003ca href=\"https://kafka.apache.org/\"\u003eKafka\u003c/a\u003e as a backbone for asynchronous communication between microservices. With up to\n300k messages published and 1M messages consumed every second, it is a key part of our infrastructure. A few months ago, in our main Kafka cluster, we noticed\nthe following discrepancy: while median response times for \u003ca href=\"https://developer.confluent.io/courses/architecture/broker/#inside-the-apache-kafka-broker:~:text=Client%20requests%20fall%20into%20two%20categories%3A%20produce%20requests%20and%20fetch%20requests.%20A%20produce%20request%20is%20requesting%20that%20a%20batch%20of%20data%20be%20written%20to%20a%20specified%20topic.%20A%20fetch%20request%20is%20requesting%20data%20from%20Kafka%20topics.\"\u003eproduce requests\u003c/a\u003e\nwere in single-digit milliseconds, the tail latency was much worse. Namely, the\np99 latency was up to 1 second, and the p999 latency was up to 3 seconds. This was unacceptable for a new project that we were about to start, so we\ndecided to look into this issue. In this blog post, we would like to describe our journey â€” how we used Kafka protocol sniffing and eBPF to identify and remove\nthe performance bottleneck.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/kafka-performance-analysis.png\" alt=\"Kafka Produce Latency\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"the-need-for-tracing\"\u003eThe Need for Tracing\u003c/h2\u003e\n\u003cp\u003eKafka brokers \u003ca href=\"https://docs.confluent.io/platform/current/kafka/monitoring.html#localtimems\"\u003eexpose various metrics\u003c/a\u003e. From them, we were able to tell that\nproduce requests were slow for high percentiles, but we couldnâ€™t identify the cause. System metrics were also not showing anything alarming.\u003c/p\u003e\n\n\u003cp\u003eTo pinpoint the underlying problem, we decided to trace individual requests. By analyzing components of Kafka involved in handling produce requests,\nwe aimed to uncover the source of the latency spikes. One way of doing that would be to fork Kafka, implement instrumentation, and deploy our custom version\nto the cluster. However, this would be very time-consuming and invasive. We decided to try an alternative approach.\u003c/p\u003e\n\n\u003cp\u003eThe first thing we did was finding \u003cem\u003earrival\u003c/em\u003e and \u003cem\u003eend\u003c/em\u003e times for every Kafka produce request.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/request_timeline1.png\" alt=\"Timeline of Kafka produce request\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eTimeline of a produce request. Arrival and end times define the boundaries of the request. The components of Kafka involved in handling the request and their latencies are unknown.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eKafka uses a binary protocol over TCP to send requests from producers (and consumers) to brokers. We started by capturing the network traffic on a selected\nbroker using \u003ca href=\"https://www.tcpdump.org/\"\u003etcpdump\u003c/a\u003e. Then we wrote a tool for analyzing the captured packets, which enabled us to list all the request and response\ntimes. In the output, we saw a confirmation of what we already knew â€” there were many slow produce requests taking over a second to complete. Whatâ€™s more we\nwere able to see request metadata â€” \u003cem\u003etopic name\u003c/em\u003e, \u003cem\u003epartition ID\u003c/em\u003e and \u003cem\u003emessage ID\u003c/em\u003e (our internal identifier included in Kafka headers):\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eARRIVAL TIME  END TIME      LATENCY(ms)  MESSAGE_ID  TOPIC   PARTITION\n12:11:36.521  12:11:37.060  538          371409548   topicA  2\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.555  12:11:37.061  505          371409578   topicC  7\n12:11:36.587  12:11:37.061  473          375783728   topicD  16\n12:11:36.690  12:11:37.061  370          375783907   topicB  18\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWith that extra knowledge in hand, we were ready to dig deeper.\u003c/p\u003e\n\n\u003ch2 id=\"dynamic-tracing\"\u003eDynamic Tracing\u003c/h2\u003e\n\n\u003cp\u003eThanks to network traffic analysis we had arrival time, end time and metadata for each request. We then wanted to gain insights into\nwhich Kafka components were the source of latency. Since produce requests are mostly concerned with saving data,\nwe decided to instrument writes to the underlying storage.\u003c/p\u003e\n\n\u003cp\u003eOn Linux, Kafka uses regular files for storing data. Writes are done using ordinary \u003ca href=\"https://man7.org/linux/man-pages/man2/write.2.html\"\u003ewrite system calls\u003c/a\u003e â€” data is first stored in the page cache\nand then asynchronously flushed to disk. How can we trace individual file writes without modifying the source code? We can make use of \u003cem\u003edynamic tracing\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhat is \u003cem\u003edynamic tracing\u003c/em\u003e? In Brendan Greggâ€™s \u003cem\u003eSystem Performance\u003c/em\u003e, he uses the following analogy that we really like:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eConsider an operating system kernel: analyzing kernel internals can be like venturing into a dark room, with candles [â€¦] placed where the kernel engineers\nthought they were needed. Dynamic instrumentation is like having a flashlight that you can point anywhere.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis basically means that it is possible to instrument arbitrary kernel code without the need to modify a user space application or the kernel itself. For\nexample, we can use dynamic tracing to instrument file system calls to check whether they are the source of latency. To do that we can make use of a technology\ncalled BPF.\u003c/p\u003e\n\n\u003cp\u003eBPF (or eBPF) which stands for \u003cem\u003e(extended) Berkeley Packet Filter\u003c/em\u003e is a technology with a rich history, but today it is a generic in-kernel execution\nenvironment [\u003cem\u003eGregg Brendan (2020). Systems Performance: Enterprise and the Cloud, 2nd Edition\u003c/em\u003e]. It has a wide range of applications, including networking,\nsecurity and tracing tools. eBPF programs are compiled to bytecode which is then interpreted by the Linux Kernel.\u003c/p\u003e\n\n\u003cp\u003eThere are a couple of well-established front-ends for eBPF, including \u003ca href=\"https://github.com/iovisor/bcc/tree/master\"\u003eBCC\u003c/a\u003e,\n\u003ca href=\"https://github.com/bpftrace/bpftrace\"\u003ebpftrace\u003c/a\u003e and \u003ca href=\"https://github.com/libbpf/libbpf\"\u003elibbpf\u003c/a\u003e. They can be used to write custom tracing programs, but they\nalso ship with many useful tools already implemented. One such tool is \u003ca href=\"https://github.com/iovisor/bcc/blob/master/tools/ext4slower.py\"\u003eext4slower\u003c/a\u003e.\nIt allows tracing file system operations in the ext4 file system, which is the default file system for Linux.\u003c/p\u003e\n\n\u003ch2 id=\"tracing-kafka\"\u003eTracing Kafka\u003c/h2\u003e\n\u003cp\u003eIn Kafka, every partition has its own directory, named according to the pattern: \u003cem\u003etopicName\u003c/em\u003e-\u003cem\u003epartitionID\u003c/em\u003e. Within each of these directories, there are segment\nfiles where messages are stored. In the figure below, we can see an example of this structure. In this scenario, the broker hosts two partitions (0 and 7)\nfor \u003cem\u003etopicA\u003c/em\u003e and one partition (1) for \u003cem\u003etopicB\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/kafka_directories.png\" alt=\"Kafka Partition Directories\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBy slightly altering the ext4slower program to include parent directories, we were able to trace Kafka file system writes. For every write with a duration\nexceeding a specified threshold, we observed the following:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eStart time and end time\u003c/li\u003e\n  \u003cli\u003eDuration\u003c/li\u003e\n  \u003cli\u003eThread ID (TID)\u003c/li\u003e\n  \u003cli\u003eNumber of bytes written\u003c/li\u003e\n  \u003cli\u003eFile offset\u003c/li\u003e\n  \u003cli\u003eTopic name\u003c/li\u003e\n  \u003cli\u003ePartition ID\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBelow is an example output from the program:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n15:37:00.627  15:37:00.785  158 ms   4478  2009   88847331   topicA-0/00000000002938697123.log\n15:37:00.629  15:37:00.785  156 ms   4492  531    289315894  topicB-7/00000000001119733846.log\n15:37:00.629  15:37:00.785  156 ms   4495  815    167398027  topicC-7/00000000015588371822.log\n15:37:00.631  15:37:00.785  154 ms   4488  778    502626221  topicD-7/00000000004472160265.log\n15:37:00.644  15:37:00.785  141 ms   4486  341    340818418  topicE-7/00000000002661443174.log\n15:37:00.650  15:37:00.785  135 ms   4470  374    230883174  topicF-7/00000000006102922534.log\n15:37:00.653  15:37:00.785  132 ms   4461  374    375758631  topicF-19/00000000001555977358.log\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThis was already very helpful since we could, based on timestamp, topic and partition, correlate produce requests from the tcpdump output with writes to\nthe file system:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA  0\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB  7\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC  7\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD  7\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE  7\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF  7\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF  19\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eTo gain extra confidence, we wrote a tool that parses a Kafka log file, reads the records written to it (using file offset and number of bytes written),\nparses them, and returns their \u003cem\u003emessage IDs\u003c/em\u003e. With that, we were able to perfectly correlate incoming requests with their respective writes:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  MESSAGE_ID  FILE                                TOPIC   PARTITION  BYTES  OFF_KB\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA-0/00000000002938697123.log   topicA  0          2009   88847331\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB-7/00000000001119733846.log   topicB  7          531    289315894\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC-7/00000000015588371822.log   topicC  7          815    167398027\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD-7/00000000004472160265.log   topicD  7          778    502626221\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE-7/00000000002661443174.log   topicE  7          341    340818418\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF-7/00000000006102922534.log   topicF  7          374    230883174\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF-19/00000000001555977358.log  topicF  19         374    375758631\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFrom the analysis, we were able to tell that \u003cstrong\u003ethere were many slow produce requests that spent all of their time waiting for the file system write to\ncomplete.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/timeline_slow_write.png\" alt=\"Request Timeline with Slow Write\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThere were however requests that didnâ€™t have corresponding slow writes.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/timeline_fast_write.png\" alt=\"Request Timeline with Fast Write\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"kafka-lock-contention\"\u003eKafka Lock Contention\u003c/h2\u003e\n\u003cp\u003eSlow produce requests without corresponding slow writes were always occurring around the time of some other slow write. We started wondering whether those\nrequests were perhaps queuing and waiting for something to finish. By analyzing Kafka source code, we identified a couple of places that use \u003cem\u003esynchronized\u003c/em\u003e\nblocks, including those guarding log file writes.\u003c/p\u003e\n\n\u003cp\u003eWe set out to measure how much time Kafkaâ€™s threads, processing produce requests, spend on the aforementioned locks. Our goal was to correlate periods when\nthey were waiting on locks with writes to the file system. We considered two approaches to do that.\u003c/p\u003e\n\n\u003cp\u003eThe first one was to use tracing again, and perhaps combine its results with the tool we already had for tracing the ext4 file system.\nLooking at the JDK source code we were not able to identify a connection between \u003cem\u003esynchronized\u003c/em\u003e blocks and traceable kernel routines. Instead, we learned that\nJVM ships with predefined DTrace tracepoints (DTrace can be thought of as a predecessor of eBPF). These tracepoints include \u003cem\u003ehotspot:monitor__contended__enter\u003c/em\u003e\nand \u003cem\u003ehotspot:monitor__contended__entered\u003c/em\u003e, which monitor when a thread begins waiting on a contended lock and when it finally enters it. By running Kafka\nwith the \u003cem\u003e-XX:+DTraceMonitorProbes\u003c/em\u003e VM option and attaching to these tracepoints we were able to see monitor wait times for a given thread.\u003c/p\u003e\n\n\u003cp\u003eAnother approach we came up with was to capture states of Kafkaâ€™s threads by running \u003ca href=\"https://github.com/async-profiler/async-profiler\"\u003easync-profiler\u003c/a\u003e\nalongside the ext4 tracing script. We would then analyze results from both tools and correlate their outputs.\u003c/p\u003e\n\n\u003cp\u003eAfter experimenting with both ideas, we ultimately chose to stick with async-profiler. It provided a clean visualization of thread states and offered more\ninsights into JVM-specific properties of threads.\u003c/p\u003e\n\n\u003cp\u003eNow, letâ€™s delve into how we analyzed a situation when a latency spike occurred, based on an example async-profiler recording, eBPF traces, and parsed\ntcpdump output. For brevity, weâ€™ll focus on one Kafka topic.\u003c/p\u003e\n\n\u003cp\u003eBy capturing network traffic on a broker, we were able to see that there were four slow produce requests to the selected topic:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n17:58:00.644  17:58:00.770  126 ms   75567596    topicF  6\n17:58:00.651  17:58:00.770  119 ms   33561917    topicF  6\n17:58:00.655  17:58:00.775  119 ms   20422312    topicF  6\n17:58:00.661  17:58:00.776  114 ms   18658935    topicF  6\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHowever, there was only one slow file system write for that topic:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.643  17:58:00.769  126 ms   4462  498    167428091  topicF-6/00000000000966764382.log\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAll other writes to that topic were fast at that time:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.770  17:58:00.770  0 ms     4484  798    167451825  topicF-6/00000000000966764382.log\n17:58:00.775  17:58:00.775  0 ms     4499  14410  167437415  topicF-6/00000000000966764382.log\n17:58:00.776  17:58:00.776  0 ms     4467  1138   167436277  topicF-6/00000000000966764382.log\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWe knew that one of the fast writes was performed from a thread with ID 4484. From a thread dump, we extracted thread names and Native IDs (NIDs).\nKnowing that NIDs translate directly to Linux TIDs (thread IDs), we found a thread with NID 0x1184 (decimal: 4484). We determined that the name of\nthis thread was \u003cem\u003edata-plane-kafka-request-handler-24\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eWe searched for this threadâ€™s activity in the async-profiler output:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/locks.png\" alt=\"Async profiler output visualized in Java Mission Control\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eAsync profiler output visualized in Java Mission Control. Thread with TID 4484 is blocked on a monitor.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eIn the output, we saw what we suspected â€” a thread was waiting on a lock for approximately the same duration as the slow write occurring on another thread.\nThis confirmed our initial hypothesis.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/timeline_lock.png\" alt=\"For a slow request with fast file system writes, waiting to obtain a lock turned out to be the source of latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eFor a slow request with fast file system writes, waiting to acquire a lock turned out to be the source of latency.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eApplying this technique, we analyzed numerous cases, and the results were consistent: \u003cstrong\u003efor a slow produce request there was either a matching slow write or a\nthread was waiting to acquire a lock guarding access to a log file\u003c/strong\u003e. We confirmed that file system writes were the root cause of slow produce requests.\u003c/p\u003e\n\n\u003ch2 id=\"tracing-the-file-system\"\u003eTracing the File System\u003c/h2\u003e\n\u003cp\u003eOur original eBPF script traced only calls to the \u003ca href=\"https://elixir.bootlin.com/linux/v5.15.91/source/fs/ext4/file.c#L673\"\u003eext4_file_write_iter\u003c/a\u003e function.\nWhile this was sufficient to roughly determine that slow writes to the file system were causing the latency spikes, it was not enough to pinpoint which\nparameters of the file system needed tuning. To address this, we captured both \u003ca href=\"https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html\"\u003eon-CPU\u003c/a\u003e\nand \u003ca href=\"https://www.brendangregg.com/offcpuanalysis.html\"\u003eoff-CPU\u003c/a\u003e profiles of \u003cem\u003eext4_file_write_iter\u003c/em\u003e, using\n\u003ca href=\"https://github.com/iovisor/bcc/blob/master/tools/profile.py\"\u003eprofile\u003c/a\u003e and \u003ca href=\"https://github.com/iovisor/bcc/blob/master/tools/offcputime.py\"\u003eoffcputime\u003c/a\u003e,\nrespectively. Our goal was to identify the activated paths in the kernel and then measure the latency of functions associated with them.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/on_cpu.png\" alt=\"on-CPU profile of ext4_file_write_iter\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eon-CPU profile of ext4_file_write_iter\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/off_cpu.png\" alt=\"off-CPU profile of ext4_file_write_iter\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eoff-CPU profile of ext4_file_write_iter\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eWe noticed that the function \u003ca href=\"https://elixir.bootlin.com/linux/v5.15.91/source/fs/ext4/inode.c#L5971\"\u003eext4_dirty_inode\u003c/a\u003e [1] was present in both flamegraphs.\nIn the Linux kernel, the \u003cem\u003eext4_dirty_inode\u003c/em\u003e function is responsible for marking an inode (file or directory data structure) as being in a dirty state. A \u003cem\u003edirty\u003c/em\u003e\ninode indicates that the corresponding fileâ€™s data or metadata has been modified and needs to be synchronized with the underlying storage device, typically a\ndisk, to ensure data consistency.\u003c/p\u003e\n\n\u003cp\u003eWhat caught our attention in the off-CPU profile was the \u003ca href=\"https://elixir.bootlin.com/linux/v5.15.91/source/fs/jbd2/transaction.c#L490\"\u003ejbd2__journal_start\u003c/a\u003e\n[2] function which is part of a journaling mechanism employed in ext4 that ensures data integrity and reliability. Journaling in ext4 involves maintaining a\ndetailed log that records the changes before they are committed to the file system. This log, often referred to as the \u003cem\u003ejournal\u003c/em\u003e, serves as a safety net in the\nevent of an unexpected system crash or power failure. When a file system operation occurs, such as creating, modifying, or deleting a file, ext4 first records\nthis change in the journal. Subsequently, the actual file system structures are updated. The process of updating the file system is known as \u003cem\u003ecommitting\u003c/em\u003e the\njournal. During a commit, the changes recorded in the journal are applied to the file system structures in a controlled and atomic manner. In the event of an\ninterruption, the file system can recover quickly by replaying the journal, ensuring that it reflects the consistent state of the file system.\u003c/p\u003e\n\n\u003cp\u003eAs seen in the figure with the off-CPU profile, \u003ca href=\"https://elixir.bootlin.com/linux/v5.15/source/fs/jbd2/transaction.c#L169\"\u003ewait_transaction_locked\u003c/a\u003e [3] is the\nfunction executed before voluntarily yielding the processor, allowing the scheduler to select and switch to a different process or thread ready to run\n(\u003ca href=\"https://elixir.bootlin.com/linux/v5.15/source/kernel/sched/core.c#L6359\"\u003eschedule()\u003c/a\u003e). Guided by the comment above the \u003cem\u003ewait_transaction_locked\u003c/em\u003e function:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eWait until running transaction passes to T_FLUSH state and new transaction can thus be started. Also starts the commit if needed. The function expects running\ntransaction to exist and releases j_state_lock.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe searched the kernel code to identify what sets the \u003cem\u003eT_FLUSH\u003c/em\u003e flag. The only place that we discovered was within the\n\u003ca href=\"https://elixir.bootlin.com/linux/v5.15/source/fs/jbd2/commit.c#L381\"\u003ejbd2_journal_commit_transaction\u003c/a\u003e function executed periodically by a kernel journal\nthread. Consequently, we decided to trace this function to explore any correlation between its latency and the latency of \u003cem\u003eext4_dirty_inode\u003c/em\u003e. The obtained\nresults aligned precisely with our expectations â€“ namely, \u003cstrong\u003ea high latency in  \u003cem\u003ejbd2_journal_commit_transaction\u003c/em\u003e translates to a high latency in\n\u003cem\u003eext4_dirty_inode\u003c/em\u003e.\u003c/strong\u003e The details of our findings are presented below:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  FUNCTION\n19:35:24.503  19:35:24.680  176 ms   jbd2_journal_commit_transaction\n19:35:24.507  19:35:24.648  141 ms   ext4_dirty_inode\n19:35:24.508  19:35:24.648  139 ms   ext4_dirty_inode\n19:35:24.514  19:35:24.648  134 ms   ext4_dirty_inode\n...\n19:38:14.508  19:38:14.929  420 ms   jbd2_journal_commit_transaction\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.512  19:38:14.868  356 ms   ext4_dirty_inode\n...\n19:48:39.475  19:48:40.808  1332 ms  jbd2_journal_commit_transaction\n19:48:39.477  19:48:40.757  1280 ms  ext4_dirty_inode\n19:48:39.487  19:48:40.757  1270 ms  ext4_dirty_inode\n19:48:39.543  19:48:40.757  1213 ms  ext4_dirty_inode\n...\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2 id=\"ext4-improvements-monitoring\"\u003eext4 Improvements Monitoring\u003c/h2\u003e\n\u003cp\u003eHaving identified journal commits as the cause of slow writes, we started thinking how to alleviate the problem. We had a few ideas, but we were wondering how\nwe would be able to observe improvements.  Up until that point, we relied on command-line tools and analyzing their output for short time ranges. We wanted\nto be able to observe the impact of our optimizations over longer periods.\u003c/p\u003e\n\n\u003cp\u003eTo report traced functions latency over long periods, we used \u003ca href=\"https://github.com/cloudflare/ebpf_exporter\"\u003eebpf_exporter\u003c/a\u003e, a tool that exposes eBPF-based\nmetrics in Prometheus format. We were then able to visualize traces in Grafana. For example, maximum ext4 write latency for a given broker:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_max_write_iter.png\" alt=\"Base ext4 Latency\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWith that, we were able to run brokers with different configurations and observe their write latency over time.\u003c/p\u003e\n\n\u003ch2 id=\"ext4-improvements\"\u003eext4 Improvements\u003c/h2\u003e\n\u003cp\u003eLetâ€™s go back to ext4. We knew that journal commits were the source of latency. By studying ext4 documentation, we identified a few possible solutions for\nimproving the performance:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eDisabling journaling\u003c/li\u003e\n  \u003cli\u003eDecreasing the commit interval\u003c/li\u003e\n  \u003cli\u003eChanging the journaling mode from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=ordered\u003c/code\u003e to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eEnabling fast commits\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eLetâ€™s discuss each of them.\u003c/p\u003e\n\n\u003ch3 id=\"disabling-journaling\"\u003eDisabling Journaling\u003c/h3\u003e\n\u003cp\u003eIf journaling is the source of high latency, why not disable it completely? Well, it turns out that journaling is there for a reason. Without journaling, we\nwould risk long recovery in case of a crash. Thus, we quickly ruled out this solution.\u003c/p\u003e\n\n\u003ch3 id=\"decreasing-the-commit-interval\"\u003eDecreasing the Commit Interval\u003c/h3\u003e\n\u003cp\u003eext4 has the \u003cem\u003ecommit\u003c/em\u003e mount parameter which tells how often to perform commits. It has the default value of 5 seconds. According to the ext4 documentation:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThis default value (or any low value) will hurt performance, but itâ€™s good for data-safety. [â€¦] Setting it to very large values will improve performance.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eHowever, instead of increasing the value we decided to decrease it. Why? Our intuition was that by performing commits more frequently we would make them\nâ€œlighterâ€ which would make them faster. We would trade throughput for lower latency. We experimented with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecommit=1\u003c/code\u003e, and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecommit=3\u003c/code\u003e but observed no\nsignificant differences.\u003c/p\u003e\n\n\u003ch3 id=\"changing-the-journaling-mode-from-dataordered-to-datawriteback\"\u003eChanging the Journaling Mode from data=ordered to data=writeback\u003c/h3\u003e\n\u003cp\u003eext4 offers three journaling modes: \u003cem\u003ejournal\u003c/em\u003e, \u003cem\u003eordered\u003c/em\u003e and \u003cem\u003ewriteback\u003c/em\u003e. The default mode is \u003cem\u003eordered\u003c/em\u003e and compared to the most performant mode, \u003cem\u003ewriteback\u003c/em\u003e,\nit guarantees that the data is written to the main file system prior to the metadata being committed to the journal. As mentioned in\n\u003ca href=\"https://kafka.apache.org/documentation/#ext4\"\u003edocs\u003c/a\u003e, Kafka does not rely on this property, so switching the mode to \u003cem\u003ewriteback\u003c/em\u003e should reduce latency.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eWe switched the journaling mode on one of the brokers, and indeed, we observed latency improvements:\u003c/strong\u003e\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_p999_2.png\" alt=\"Base Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/writeback_p999_2.png\" alt=\"Writeback Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eWith data=writeback, p999 decreased from 3 seconds to 800 milliseconds.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3 id=\"enabling-fast-commit\"\u003eEnabling Fast Commit\u003c/h3\u003e\n\u003cp\u003eWhen reading about ext4 journaling, we stumbled upon an \u003ca href=\"https://lwn.net/Articles/842385/\"\u003earticle\u003c/a\u003e describing a new feature introduced in Linux 5.10 called\n\u003cem\u003efast commits\u003c/em\u003e. As explained in the article, \u003cem\u003efast commit\u003c/em\u003e is a lighter-weight journaling method that could result in performance boost for certain workloads.\u003c/p\u003e\n\n\u003cp\u003eWe enabled \u003cem\u003efast commit\u003c/em\u003e on one of the brokers. \u003cstrong\u003eWe noticed that max write latency decreased significantly.\u003c/strong\u003e Diving deeper we found out that on a broker with\n\u003cem\u003efast commit\u003c/em\u003e enabled:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe latency of \u003cem\u003ejdb2_journal_commit_transaction\u003c/em\u003e decreased by an order of magnitude. This meant that periodic journal commits were indeed much faster\nthanks to enabling \u003cem\u003efast commits\u003c/em\u003e.\u003c/li\u003e\n  \u003cli\u003eSlow ext4 writes occurred at the same time when there was a spike in latency of \u003cem\u003ejbd2_fc_begin_commit\u003c/em\u003e. This method is part of the \u003cem\u003efast commit\u003c/em\u003e flow. It\nbecame the new source of latency but its maximum latency was lower than that of \u003cem\u003ejdb2_journal_commit_transaction\u003c/em\u003e without fast commits.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/write_iter_heatmap.png\" alt=\"Comparison of maximum latency [s] of ext4 writes for brokers without and with fast commit.\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eComparison of maximum latency [s] of ext4 writes for brokers without and with fast commit.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eLower file system write latency, in turn, resulted in reduced produce latency:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003eÂ \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_p999_2.png\" alt=\"Base Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/fc_p999_2.png\" alt=\"Fast Commit Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eWith fast commit enabled, produce P999 latency went down from 3 seconds to 500 milliseconds\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\u003cp\u003eTo summarize, weâ€™ve tested the following ext4 optimizations:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eDecreasing the commit interval\u003c/li\u003e\n  \u003cli\u003eChanging the journaling mode to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eEnabling \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commit\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe observed that both \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commit\u003c/code\u003e significantly reduced latency, with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commit\u003c/code\u003e having slightly lower latency. The results were\npromising, but we had higher hopes. Thankfully, we had one more idea left.\u003c/p\u003e\n\n\u003ch2 id=\"xfs\"\u003eXFS\u003c/h2\u003e\n\u003cp\u003eWhile researching the topic of journaling in ext4, we stumbled upon a few sources suggesting that the XFS file system, with its more advanced journaling,\nis well-suited for handling large files and high-throughput workloads, often outperforming ext4 in such scenarios. Kafka documentation also mentions that XFS\nhas a lot of tuning already in place and should be a better fit than the default ext4.\u003c/p\u003e\n\n\u003cp\u003eWe migrated one of the brokers to the XFS file system. The results were impressive. The thing that was very distinctive compared to the aforementioned ext4\noptimizations was the consistency of XFS performance. While other broker configurations experienced p999 latency spikes throughout the day, XFS â€“ with its default configuration â€“ had only a\nfew hiccups.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_p999_2.png\" alt=\"Base Produce Latency\" /\u003e\n\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/xfs_p999_2.png\" alt=\"Produce Latency XFS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAfter a couple of weeks of testing, we were confident that XFS was the best choice. Consequently, we migrated all our brokers from ext4 to XFS.\u003c/p\u003e\n\n\u003ch2 id=\"summary-1\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eUsing a combination of packet sniffing, eBPF, and async-profiler we managed to identify the root cause of slow produce requests in our Kafka cluster. We\nthen tested a couple of solutions to the problem: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e journaling mode, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commits\u003c/code\u003e, and changing the file system to XFS. The results of these\noptimizations are visualized in the heatmap below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/heatmap_p999.png\" alt=\"Produce Latency Heatmap\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eUltimately, we found XFS to be the most performant and rolled it out to all of our brokers. \u003cstrong\u003eWith XFS, the number of produce requests exceeding 65ms (our SLO)\nwas lowered by 82%.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eHere are some of the lessons we learned along the way:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eeBPF was extremely useful during the analysis. It was straightforward to utilize one of the pre-existing tools from bcc or bpftrace. We were also able to\neasily modify them for our custom use cases.\u003c/li\u003e\n  \u003cli\u003eebpf_exporter is a great tool for observing trace results over longer periods of time. It allows to expose Prometheus metrics based on libbpf programs.\u003c/li\u003e\n  \u003cli\u003ep99 and p999 analysis is sometimes not enough. In our case, the p999 latency of file system writes was less than 1ms. It turned out that a single slow write\ncould cause lock contention and a cascade of slow requests. Without tracing individual requests, the root cause would have been very hard to catch.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe hope that you found this blog post useful, and we wish you good luck in your future performance analysis endeavors!\u003c/p\u003e\n\n\u003cstyle\u003e\n  .post-content table, .post-content td, .post-content th {\n    border: none;\n    background-color: transparent;\n}\n\n.post-content th {\n    display: none;\n}\n\n.post-content td {\n    padding: 0;\n}\n\n\u003c/style\u003e\n\n","contentSnippet":"At Allegro, we use Kafka as a backbone for asynchronous communication between microservices. With up to\n300k messages published and 1M messages consumed every second, it is a key part of our infrastructure. A few months ago, in our main Kafka cluster, we noticed\nthe following discrepancy: while median response times for produce requests\nwere in single-digit milliseconds, the tail latency was much worse. Namely, the\np99 latency was up to 1 second, and the p999 latency was up to 3 seconds. This was unacceptable for a new project that we were about to start, so we\ndecided to look into this issue. In this blog post, we would like to describe our journey â€” how we used Kafka protocol sniffing and eBPF to identify and remove\nthe performance bottleneck.\n\nThe Need for Tracing\nKafka brokers expose various metrics. From them, we were able to tell that\nproduce requests were slow for high percentiles, but we couldnâ€™t identify the cause. System metrics were also not showing anything alarming.\nTo pinpoint the underlying problem, we decided to trace individual requests. By analyzing components of Kafka involved in handling produce requests,\nwe aimed to uncover the source of the latency spikes. One way of doing that would be to fork Kafka, implement instrumentation, and deploy our custom version\nto the cluster. However, this would be very time-consuming and invasive. We decided to try an alternative approach.\nThe first thing we did was finding arrival and end times for every Kafka produce request.\nÂ \n    \n\n    \nTimeline of a produce request. Arrival and end times define the boundaries of the request. The components of Kafka involved in handling the request and their latencies are unknown.\n    \nKafka uses a binary protocol over TCP to send requests from producers (and consumers) to brokers. We started by capturing the network traffic on a selected\nbroker using tcpdump. Then we wrote a tool for analyzing the captured packets, which enabled us to list all the request and response\ntimes. In the output, we saw a confirmation of what we already knew â€” there were many slow produce requests taking over a second to complete. Whatâ€™s more we\nwere able to see request metadata â€” topic name, partition ID and message ID (our internal identifier included in Kafka headers):\n\nARRIVAL TIME  END TIME      LATENCY(ms)  MESSAGE_ID  TOPIC   PARTITION\n12:11:36.521  12:11:37.060  538          371409548   topicA  2\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.555  12:11:37.061  505          371409578   topicC  7\n12:11:36.587  12:11:37.061  473          375783728   topicD  16\n12:11:36.690  12:11:37.061  370          375783907   topicB  18\n\n\nWith that extra knowledge in hand, we were ready to dig deeper.\nDynamic Tracing\nThanks to network traffic analysis we had arrival time, end time and metadata for each request. We then wanted to gain insights into\nwhich Kafka components were the source of latency. Since produce requests are mostly concerned with saving data,\nwe decided to instrument writes to the underlying storage.\nOn Linux, Kafka uses regular files for storing data. Writes are done using ordinary write system calls â€” data is first stored in the page cache\nand then asynchronously flushed to disk. How can we trace individual file writes without modifying the source code? We can make use of dynamic tracing.\nWhat is dynamic tracing? In Brendan Greggâ€™s System Performance, he uses the following analogy that we really like:\nConsider an operating system kernel: analyzing kernel internals can be like venturing into a dark room, with candles [â€¦] placed where the kernel engineers\nthought they were needed. Dynamic instrumentation is like having a flashlight that you can point anywhere.\nThis basically means that it is possible to instrument arbitrary kernel code without the need to modify a user space application or the kernel itself. For\nexample, we can use dynamic tracing to instrument file system calls to check whether they are the source of latency. To do that we can make use of a technology\ncalled BPF.\nBPF (or eBPF) which stands for (extended) Berkeley Packet Filter is a technology with a rich history, but today it is a generic in-kernel execution\nenvironment [Gregg Brendan (2020). Systems Performance: Enterprise and the Cloud, 2nd Edition]. It has a wide range of applications, including networking,\nsecurity and tracing tools. eBPF programs are compiled to bytecode which is then interpreted by the Linux Kernel.\nThere are a couple of well-established front-ends for eBPF, including BCC,\nbpftrace and libbpf. They can be used to write custom tracing programs, but they\nalso ship with many useful tools already implemented. One such tool is ext4slower.\nIt allows tracing file system operations in the ext4 file system, which is the default file system for Linux.\nTracing Kafka\nIn Kafka, every partition has its own directory, named according to the pattern: topicName-partitionID. Within each of these directories, there are segment\nfiles where messages are stored. In the figure below, we can see an example of this structure. In this scenario, the broker hosts two partitions (0 and 7)\nfor topicA and one partition (1) for topicB.\n\nBy slightly altering the ext4slower program to include parent directories, we were able to trace Kafka file system writes. For every write with a duration\nexceeding a specified threshold, we observed the following:\nStart time and end time\nDuration\nThread ID (TID)\nNumber of bytes written\nFile offset\nTopic name\nPartition ID\nBelow is an example output from the program:\n\nSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n15:37:00.627  15:37:00.785  158 ms   4478  2009   88847331   topicA-0/00000000002938697123.log\n15:37:00.629  15:37:00.785  156 ms   4492  531    289315894  topicB-7/00000000001119733846.log\n15:37:00.629  15:37:00.785  156 ms   4495  815    167398027  topicC-7/00000000015588371822.log\n15:37:00.631  15:37:00.785  154 ms   4488  778    502626221  topicD-7/00000000004472160265.log\n15:37:00.644  15:37:00.785  141 ms   4486  341    340818418  topicE-7/00000000002661443174.log\n15:37:00.650  15:37:00.785  135 ms   4470  374    230883174  topicF-7/00000000006102922534.log\n15:37:00.653  15:37:00.785  132 ms   4461  374    375758631  topicF-19/00000000001555977358.log\n\n\nThis was already very helpful since we could, based on timestamp, topic and partition, correlate produce requests from the tcpdump output with writes to\nthe file system:\n\nARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA  0\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB  7\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC  7\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD  7\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE  7\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF  7\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF  19\n\n\nTo gain extra confidence, we wrote a tool that parses a Kafka log file, reads the records written to it (using file offset and number of bytes written),\nparses them, and returns their message IDs. With that, we were able to perfectly correlate incoming requests with their respective writes:\n\nSTART TIME    END TIME      LATENCY  MESSAGE_ID  FILE                                TOPIC   PARTITION  BYTES  OFF_KB\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA-0/00000000002938697123.log   topicA  0          2009   88847331\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB-7/00000000001119733846.log   topicB  7          531    289315894\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC-7/00000000015588371822.log   topicC  7          815    167398027\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD-7/00000000004472160265.log   topicD  7          778    502626221\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE-7/00000000002661443174.log   topicE  7          341    340818418\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF-7/00000000006102922534.log   topicF  7          374    230883174\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF-19/00000000001555977358.log  topicF  19         374    375758631\n\n\nFrom the analysis, we were able to tell that there were many slow produce requests that spent all of their time waiting for the file system write to\ncomplete.\n\nThere were however requests that didnâ€™t have corresponding slow writes.\n\nKafka Lock Contention\nSlow produce requests without corresponding slow writes were always occurring around the time of some other slow write. We started wondering whether those\nrequests were perhaps queuing and waiting for something to finish. By analyzing Kafka source code, we identified a couple of places that use synchronized\nblocks, including those guarding log file writes.\nWe set out to measure how much time Kafkaâ€™s threads, processing produce requests, spend on the aforementioned locks. Our goal was to correlate periods when\nthey were waiting on locks with writes to the file system. We considered two approaches to do that.\nThe first one was to use tracing again, and perhaps combine its results with the tool we already had for tracing the ext4 file system.\nLooking at the JDK source code we were not able to identify a connection between synchronized blocks and traceable kernel routines. Instead, we learned that\nJVM ships with predefined DTrace tracepoints (DTrace can be thought of as a predecessor of eBPF). These tracepoints include hotspot:monitor__contended__enter\nand hotspot:monitor__contended__entered, which monitor when a thread begins waiting on a contended lock and when it finally enters it. By running Kafka\nwith the -XX:+DTraceMonitorProbes VM option and attaching to these tracepoints we were able to see monitor wait times for a given thread.\nAnother approach we came up with was to capture states of Kafkaâ€™s threads by running async-profiler\nalongside the ext4 tracing script. We would then analyze results from both tools and correlate their outputs.\nAfter experimenting with both ideas, we ultimately chose to stick with async-profiler. It provided a clean visualization of thread states and offered more\ninsights into JVM-specific properties of threads.\nNow, letâ€™s delve into how we analyzed a situation when a latency spike occurred, based on an example async-profiler recording, eBPF traces, and parsed\ntcpdump output. For brevity, weâ€™ll focus on one Kafka topic.\nBy capturing network traffic on a broker, we were able to see that there were four slow produce requests to the selected topic:\n\nARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n17:58:00.644  17:58:00.770  126 ms   75567596    topicF  6\n17:58:00.651  17:58:00.770  119 ms   33561917    topicF  6\n17:58:00.655  17:58:00.775  119 ms   20422312    topicF  6\n17:58:00.661  17:58:00.776  114 ms   18658935    topicF  6\n\n\nHowever, there was only one slow file system write for that topic:\n\nSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.643  17:58:00.769  126 ms   4462  498    167428091  topicF-6/00000000000966764382.log\n\n\nAll other writes to that topic were fast at that time:\n\nSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.770  17:58:00.770  0 ms     4484  798    167451825  topicF-6/00000000000966764382.log\n17:58:00.775  17:58:00.775  0 ms     4499  14410  167437415  topicF-6/00000000000966764382.log\n17:58:00.776  17:58:00.776  0 ms     4467  1138   167436277  topicF-6/00000000000966764382.log\n\n\nWe knew that one of the fast writes was performed from a thread with ID 4484. From a thread dump, we extracted thread names and Native IDs (NIDs).\nKnowing that NIDs translate directly to Linux TIDs (thread IDs), we found a thread with NID 0x1184 (decimal: 4484). We determined that the name of\nthis thread was data-plane-kafka-request-handler-24.\nWe searched for this threadâ€™s activity in the async-profiler output:\nÂ \n    \n\n    \nAsync profiler output visualized in Java Mission Control. Thread with TID 4484 is blocked on a monitor.\n    \nIn the output, we saw what we suspected â€” a thread was waiting on a lock for approximately the same duration as the slow write occurring on another thread.\nThis confirmed our initial hypothesis.\nÂ \n    \n\n    \nFor a slow request with fast file system writes, waiting to acquire a lock turned out to be the source of latency.\n    \nApplying this technique, we analyzed numerous cases, and the results were consistent: for a slow produce request there was either a matching slow write or a\nthread was waiting to acquire a lock guarding access to a log file. We confirmed that file system writes were the root cause of slow produce requests.\nTracing the File System\nOur original eBPF script traced only calls to the ext4_file_write_iter function.\nWhile this was sufficient to roughly determine that slow writes to the file system were causing the latency spikes, it was not enough to pinpoint which\nparameters of the file system needed tuning. To address this, we captured both on-CPU\nand off-CPU profiles of ext4_file_write_iter, using\nprofile and offcputime,\nrespectively. Our goal was to identify the activated paths in the kernel and then measure the latency of functions associated with them.\nÂ \n    \n\n    \non-CPU profile of ext4_file_write_iter\n    \nÂ \n    \n\n    \noff-CPU profile of ext4_file_write_iter\n    \nWe noticed that the function ext4_dirty_inode [1] was present in both flamegraphs.\nIn the Linux kernel, the ext4_dirty_inode function is responsible for marking an inode (file or directory data structure) as being in a dirty state. A dirty\ninode indicates that the corresponding fileâ€™s data or metadata has been modified and needs to be synchronized with the underlying storage device, typically a\ndisk, to ensure data consistency.\nWhat caught our attention in the off-CPU profile was the jbd2__journal_start\n[2] function which is part of a journaling mechanism employed in ext4 that ensures data integrity and reliability. Journaling in ext4 involves maintaining a\ndetailed log that records the changes before they are committed to the file system. This log, often referred to as the journal, serves as a safety net in the\nevent of an unexpected system crash or power failure. When a file system operation occurs, such as creating, modifying, or deleting a file, ext4 first records\nthis change in the journal. Subsequently, the actual file system structures are updated. The process of updating the file system is known as committing the\njournal. During a commit, the changes recorded in the journal are applied to the file system structures in a controlled and atomic manner. In the event of an\ninterruption, the file system can recover quickly by replaying the journal, ensuring that it reflects the consistent state of the file system.\nAs seen in the figure with the off-CPU profile, wait_transaction_locked [3] is the\nfunction executed before voluntarily yielding the processor, allowing the scheduler to select and switch to a different process or thread ready to run\n(schedule()). Guided by the comment above the wait_transaction_locked function:\nWait until running transaction passes to T_FLUSH state and new transaction can thus be started. Also starts the commit if needed. The function expects running\ntransaction to exist and releases j_state_lock.\nWe searched the kernel code to identify what sets the T_FLUSH flag. The only place that we discovered was within the\njbd2_journal_commit_transaction function executed periodically by a kernel journal\nthread. Consequently, we decided to trace this function to explore any correlation between its latency and the latency of ext4_dirty_inode. The obtained\nresults aligned precisely with our expectations â€“ namely, a high latency in  jbd2_journal_commit_transaction translates to a high latency in\next4_dirty_inode. The details of our findings are presented below:\n\nSTART TIME    END TIME      LATENCY  FUNCTION\n19:35:24.503  19:35:24.680  176 ms   jbd2_journal_commit_transaction\n19:35:24.507  19:35:24.648  141 ms   ext4_dirty_inode\n19:35:24.508  19:35:24.648  139 ms   ext4_dirty_inode\n19:35:24.514  19:35:24.648  134 ms   ext4_dirty_inode\n...\n19:38:14.508  19:38:14.929  420 ms   jbd2_journal_commit_transaction\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.512  19:38:14.868  356 ms   ext4_dirty_inode\n...\n19:48:39.475  19:48:40.808  1332 ms  jbd2_journal_commit_transaction\n19:48:39.477  19:48:40.757  1280 ms  ext4_dirty_inode\n19:48:39.487  19:48:40.757  1270 ms  ext4_dirty_inode\n19:48:39.543  19:48:40.757  1213 ms  ext4_dirty_inode\n...\n\n\next4 Improvements Monitoring\nHaving identified journal commits as the cause of slow writes, we started thinking how to alleviate the problem. We had a few ideas, but we were wondering how\nwe would be able to observe improvements.  Up until that point, we relied on command-line tools and analyzing their output for short time ranges. We wanted\nto be able to observe the impact of our optimizations over longer periods.\nTo report traced functions latency over long periods, we used ebpf_exporter, a tool that exposes eBPF-based\nmetrics in Prometheus format. We were then able to visualize traces in Grafana. For example, maximum ext4 write latency for a given broker:\n\nWith that, we were able to run brokers with different configurations and observe their write latency over time.\next4 Improvements\nLetâ€™s go back to ext4. We knew that journal commits were the source of latency. By studying ext4 documentation, we identified a few possible solutions for\nimproving the performance:\nDisabling journaling\nDecreasing the commit interval\nChanging the journaling mode from data=ordered to data=writeback\nEnabling fast commits\nLetâ€™s discuss each of them.\nDisabling Journaling\nIf journaling is the source of high latency, why not disable it completely? Well, it turns out that journaling is there for a reason. Without journaling, we\nwould risk long recovery in case of a crash. Thus, we quickly ruled out this solution.\nDecreasing the Commit Interval\next4 has the commit mount parameter which tells how often to perform commits. It has the default value of 5 seconds. According to the ext4 documentation:\nThis default value (or any low value) will hurt performance, but itâ€™s good for data-safety. [â€¦] Setting it to very large values will improve performance.\nHowever, instead of increasing the value we decided to decrease it. Why? Our intuition was that by performing commits more frequently we would make them\nâ€œlighterâ€ which would make them faster. We would trade throughput for lower latency. We experimented with commit=1, and commit=3 but observed no\nsignificant differences.\nChanging the Journaling Mode from data=ordered to data=writeback\next4 offers three journaling modes: journal, ordered and writeback. The default mode is ordered and compared to the most performant mode, writeback,\nit guarantees that the data is written to the main file system prior to the metadata being committed to the journal. As mentioned in\ndocs, Kafka does not rely on this property, so switching the mode to writeback should reduce latency.\nWe switched the journaling mode on one of the brokers, and indeed, we observed latency improvements:\nÂ \n    \n\n    \n\n    \nWith data=writeback, p999 decreased from 3 seconds to 800 milliseconds.\n    \nEnabling Fast Commit\nWhen reading about ext4 journaling, we stumbled upon an article describing a new feature introduced in Linux 5.10 called\nfast commits. As explained in the article, fast commit is a lighter-weight journaling method that could result in performance boost for certain workloads.\nWe enabled fast commit on one of the brokers. We noticed that max write latency decreased significantly. Diving deeper we found out that on a broker with\nfast commit enabled:\nThe latency of jdb2_journal_commit_transaction decreased by an order of magnitude. This meant that periodic journal commits were indeed much faster\nthanks to enabling fast commits.\nSlow ext4 writes occurred at the same time when there was a spike in latency of jbd2_fc_begin_commit. This method is part of the fast commit flow. It\nbecame the new source of latency but its maximum latency was lower than that of jdb2_journal_commit_transaction without fast commits.\nÂ \n    \n\n    \nComparison of maximum latency [s] of ext4 writes for brokers without and with fast commit.\n    \nLower file system write latency, in turn, resulted in reduced produce latency:\nÂ \n    \n\n    \n\n    \nWith fast commit enabled, produce P999 latency went down from 3 seconds to 500 milliseconds\n    \nSummary\nTo summarize, weâ€™ve tested the following ext4 optimizations:\nDecreasing the commit interval\nChanging the journaling mode to data=writeback\nEnabling fast commit\nWe observed that both data=writeback and fast commit significantly reduced latency, with fast commit having slightly lower latency. The results were\npromising, but we had higher hopes. Thankfully, we had one more idea left.\nXFS\nWhile researching the topic of journaling in ext4, we stumbled upon a few sources suggesting that the XFS file system, with its more advanced journaling,\nis well-suited for handling large files and high-throughput workloads, often outperforming ext4 in such scenarios. Kafka documentation also mentions that XFS\nhas a lot of tuning already in place and should be a better fit than the default ext4.\nWe migrated one of the brokers to the XFS file system. The results were impressive. The thing that was very distinctive compared to the aforementioned ext4\noptimizations was the consistency of XFS performance. While other broker configurations experienced p999 latency spikes throughout the day, XFS â€“ with its default configuration â€“ had only a\nfew hiccups.\n\n\nAfter a couple of weeks of testing, we were confident that XFS was the best choice. Consequently, we migrated all our brokers from ext4 to XFS.\nSummary\nUsing a combination of packet sniffing, eBPF, and async-profiler we managed to identify the root cause of slow produce requests in our Kafka cluster. We\nthen tested a couple of solutions to the problem: data=writeback journaling mode, fast commits, and changing the file system to XFS. The results of these\noptimizations are visualized in the heatmap below:\n\nUltimately, we found XFS to be the most performant and rolled it out to all of our brokers. With XFS, the number of produce requests exceeding 65ms (our SLO)\nwas lowered by 82%.\nHere are some of the lessons we learned along the way:\neBPF was extremely useful during the analysis. It was straightforward to utilize one of the pre-existing tools from bcc or bpftrace. We were also able to\neasily modify them for our custom use cases.\nebpf_exporter is a great tool for observing trace results over longer periods of time. It allows to expose Prometheus metrics based on libbpf programs.\np99 and p999 analysis is sometimes not enough. In our case, the p999 latency of file system writes was less than 1ms. It turned out that a single slow write\ncould cause lock contention and a cascade of slow requests. Without tracing individual requests, the root cause would have been very hard to catch.\nWe hope that you found this blog post useful, and we wish you good luck in your future performance analysis endeavors!\n\n\n  .post-content table, .post-content td, .post-content th {\n    border: none;\n    background-color: transparent;\n}\n\n.post-content th {\n    display: none;\n}\n\n.post-content td {\n    padding: 0;\n}","guid":"https://blog.allegro.tech/2024/03/kafka-performance-analysis.html","categories":["tech","kafka","ebpf","bcc","linux","kernel","ext4","xfs","performance","tuning","file system"],"isoDate":"2024-03-05T23:00:00.000Z"},{"title":"Tired of repetitive tasks?! Go for RPA!","link":"https://blog.allegro.tech/2024/02/rpa.html","pubDate":"Tue, 20 Feb 2024 00:00:00 +0100","authors":{"author":[{"name":["Dominika PleÅ›niak"],"photo":["https://blog.allegro.tech/img/authors/dominika.plesniak.jpg"],"url":["https://blog.allegro.tech/authors/dominika.plesniak"]}]},"content":"\u003cp\u003eHave you ever thought about ways of reducing repetitive, monotonous tasks? Maybe you would like to try to automate your own tasks? I will show you what\ntechnology we use at Allegro, what processes we have automated, and how to do it on your own.\u003c/p\u003e\n\n\u003ch2 id=\"what-is-rpa-and-how-do-we-use-it-at-allegro\"\u003eWhat is RPA and how do we use it at Allegro?\u003c/h2\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003eâ€œRobotic process automation (RPA) is a software technology that makes it easy to build, deploy, and manage software robots that emulate humans actions\ninteracting with digital systems and software. Just like people, software robots can do things like understand whatâ€™s on a screen, complete the right \nkeystrokes, navigate systems, identify and extract data, and perform a wide range of defined actions.â€\u003c/em\u003e \n\u003cbr /\u003e \nSource: \u003ca href=\"https://www.uipath.com/rpa/robotic-process-automation\"\u003eUiPath Robotic Process Automation\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAt Allegro, our Process Automation Team primarily relies on UiPath as our key RPA tool. Processes suitable for automation through RPA are standardized, repetitive, manual, with high volume, stable and has data in digital form. If possible, we try to combine UiPath with different integrations such as scripts, databases, chatbots.\u003c/p\u003e\n\n\u003cp\u003eUiPath provides the ability to automate all kinds of applications (web, desktop, java, etc.). Automations can be created through the user interface of an \napplication, meaning that the created robot imitates an employeeâ€™s clicks. Furthermore, when an applicationâ€™s API is available, it is easy to integrate \nUiPath with API, and in that case, the robotâ€™s steps are performed in the backend-side. UiPath also allows the use of\nOCR (optical character recognition) and machine learning modules.\u003c/p\u003e\n\n\u003cp\u003eThanks to the various roles within Process Automation Team, such as analysts and developers, we are able to approach processes holistically. When we receive an idea for \nautomatization, we first perform an assessment to establish if the process is suitable for robotization, and we calculate the Return On Investment (ROI) and the potential of\ntime savings from automation in terms of \u003ca href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\"\u003eFull-Time Equivalent (FTE)\u003c/a\u003e. Once the assessment is done and costs of investment return are approved, we analyze and optimize the process. \nAs a result, the analyst prepares a Process Definition Document (PDD) which serves as an instruction/description of the process.\nIn the next phase, based on PDD, a developer takes over the process and designs a solution. After that, the development part begins while the robot is created.\nLast but not least, there is the testing phase, where we check the results of the robotization together with the analyst and the business process owner.\nIf the solution is successful and performs as intended, we run the robot in production. Then we enter the hypercare period, during which we monitor and make necessary adjustments\nin tandem with the business process owner. After about two weeks of this phase, if both sides are satisfied with the results, we â€œgo liveâ€.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-02-20-rpa/robotic_process_automation_workflow.png\" alt=\"Robotic Process Automation Workflow\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"what-kind-of-processes-do-we-automate\"\u003eWhat kind of processes do we automate?\u003c/h2\u003e\n\n\u003ch3 id=\"jira-automations\"\u003eJira automations\u003c/h3\u003e\n\n\u003cp\u003eIn our organization we have a lot of processes based on Jira â€œticketsâ€. Many employees had to manage and operate Jiraâ€™s queues. Our team implemented several \nrobots to relieve administrators from repetitive tasks. Jira has an API available which we used in combination with the UiPath platform. \nFor example, when an employee is leaving Allegro, several Jira tasks are automatically created to retract authorization in various systems. Previously, these tasks were \nperformed manually by administrators. Now, the process is fully automated. The robot manages tickets via API and checks accounts in systems by GUI.\u003c/p\u003e\n\n\u003ch3 id=\"sap-enterprise-resource-planning-system-processes\"\u003eSAP (Enterprise Resource Planning system) processes\u003c/h3\u003e\n\n\u003cp\u003eAll repetitive, rule-based processes in SAP can be automated. For instance, letâ€™s consider the processes in the Finance team. They handle massive amounts of \ninvoices. For some suppliers, with the largest quantity of invoices and unchangeable invoice layout, we were able to automate the accounting process in SAP. \nThe robot accesses an appropriate transaction in SAP and lists invoices. Based on business rules, the bot selects a specific invoice, opens it, and \nreads selected values. Then, it compares these values with business conditions which were established in the Process Definition Document. Depending on the \nsituation, the robot fills or corrects adequate fields and either accepts or rejects the invoice.\u003c/p\u003e\n\n\u003ch3 id=\"automation-by-api\"\u003eAutomation by API\u003c/h3\u003e\n\n\u003cp\u003eOne of the excellent examples of API automations is the process of changing product categories on the Allegro platform. Allegro hosts a vast number of products. Initially, \nnot all of them are assigned to the proper category. We were able to create a robot that uses Allegroâ€™s REST API to move these products to the target category.\nBefore automation, this task was time-consuming and monotonous. Recently, the robot completed a huge task, moving almost 3 million products in two days!\u003c/p\u003e\n\n\u003ch3 id=\"processes-across-multiple-applications-and-integrations\"\u003eProcesses across multiple applications and integrations\u003c/h3\u003e\n\n\u003cp\u003eIt is possible to combine tasks from different applications into one robot. This approach allows us to automate more complex processes. \nThe most interesting ones include:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe process of userâ€™s data change in Allegro platform is carried out at the request of users. To perform all actions, the bot uses Salesforce and \nInternal Admin tools. The robot downloads a report with requests, then checks the pre-set business rules. Based on the results, the bot changes userâ€™s data or\n rejects the request.\nThe robot works 24 hours a day, handling 80% of applications. The number of tasks it performs can be compared to the work of four full-time employees.\u003c/li\u003e\n  \u003cli\u003eThe anti-fraud process. The robot verifies hundreds of thousands of messages and blocks suspected accounts. Using suspicious messages from the Spoof \napplication (Message Center), the robot determines if a message is spam or not. To make a proper decision, it checks various business conditions to decide \nif an account should be blocked. After making the decision, the bot blocks the accountâ€™s message sending capabilities.\u003c/li\u003e\n  \u003cli\u003eThe process for the HR team where the robot works on two applications. The robot interacts with the interface of an application and also uses its API. \nIn the recruitment process, specialists from different fields participate and help recruiters to find the best candidates. These specialists are known as \nthe Hiring Squad. A significant number of people are involved in this process, and the robot is responsible for keeping the Hiring Squad updated. Based \non a report with job offers the robot checks if a candidate has active status for specific skills required for the interview process. If the status is active, \nthe bot selects a particular job offer from the platform and assigns the interviewer from Hiring Squad.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"human-in-the-loop\"\u003eâ€œHuman in the Loopâ€\u003c/h3\u003e\n\n\u003cp\u003eThose processes which are rule-based, repetitive, but require human decision-making or the robot does not know what to do on the basis of the collected data, \nare referred to as â€œhuman in the loopâ€. A great example could be the process of damage complaints regarding packages that we have automated. The robot gathers a report \nfrom Salesforce and filters all jobs referring to damaged packages. Then, in the Internal Admin tool, the bot checks and collects various pieces of information based \non specific business rules. If necessary, it also checks the status of packages on carrier websites. Finally, the robot creates a form with all the gathered \ndetails, information, and attachments. This form is sent to a human for verification. With all this collected information, an employee can quickly decide \nwhether the complaint should be accepted. Then, the decision is sent back to the robot, which is able to close the case. It sends appropriate emails and \nrecords the results in the system.\u003c/p\u003e\n\n\u003ch2 id=\"robotics-workshops-for-employees\"\u003eRobotics workshops for employees\u003c/h2\u003e\n\n\u003cp\u003eAdditionally, twice a year, we organize an educational program for employees called â€œAllegro Robot Schoolâ€. Employees get a chance to learn how to build \nbasic robots in UiPath and build one to support their daily tasks. To sign up for the program there is no need to have coding experience. It is enough if \nan employee can think analytically and has motivation to learn new things.\nThe program is intensive, consisting of five days of workshops. After the workshops, there is a three-weeks period where, with our support, employees choose \ntheir own processes and build robots.\nFor each edition we have around ten participants. The robots created during one edition account for about 3 FTEs! We have many examples of graduate \nemployees who created more robots to support their daily work in a team. Moreover, we created a Slack community for graduates to stay in touch, share \nknowledge, and support the development of new robots.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eToday, the number of processes and different applications used in companies is enormous. Moreover, it can sometimes be challenging to integrate one application\nwith another, and employees are burdened with many manual, repetitive tasks. It is important to know that there is a solution to automate these processes. \nThe RPA technology can quickly help with that, freeing up employees for more creative tasks. The above examples visualize the possibilities of using UiPath.\nThe most important thing to remember is that the process to automate has to be manual, rule-based, repetitive, with data in digital form. What is more, \nit is possible to learn for those who donâ€™t have coding experience. Thanks to that, the automation of processes can be expanded across the company beyond the Process Automation Team.\u003c/p\u003e\n","contentSnippet":"Have you ever thought about ways of reducing repetitive, monotonous tasks? Maybe you would like to try to automate your own tasks? I will show you what\ntechnology we use at Allegro, what processes we have automated, and how to do it on your own.\nWhat is RPA and how do we use it at Allegro?\nâ€œRobotic process automation (RPA) is a software technology that makes it easy to build, deploy, and manage software robots that emulate humans actions\ninteracting with digital systems and software. Just like people, software robots can do things like understand whatâ€™s on a screen, complete the right \nkeystrokes, navigate systems, identify and extract data, and perform a wide range of defined actions.â€ \n \nSource: UiPath Robotic Process Automation\nAt Allegro, our Process Automation Team primarily relies on UiPath as our key RPA tool. Processes suitable for automation through RPA are standardized, repetitive, manual, with high volume, stable and has data in digital form. If possible, we try to combine UiPath with different integrations such as scripts, databases, chatbots.\nUiPath provides the ability to automate all kinds of applications (web, desktop, java, etc.). Automations can be created through the user interface of an \napplication, meaning that the created robot imitates an employeeâ€™s clicks. Furthermore, when an applicationâ€™s API is available, it is easy to integrate \nUiPath with API, and in that case, the robotâ€™s steps are performed in the backend-side. UiPath also allows the use of\nOCR (optical character recognition) and machine learning modules.\nThanks to the various roles within Process Automation Team, such as analysts and developers, we are able to approach processes holistically. When we receive an idea for \nautomatization, we first perform an assessment to establish if the process is suitable for robotization, and we calculate the Return On Investment (ROI) and the potential of\ntime savings from automation in terms of Full-Time Equivalent (FTE). Once the assessment is done and costs of investment return are approved, we analyze and optimize the process. \nAs a result, the analyst prepares a Process Definition Document (PDD) which serves as an instruction/description of the process.\nIn the next phase, based on PDD, a developer takes over the process and designs a solution. After that, the development part begins while the robot is created.\nLast but not least, there is the testing phase, where we check the results of the robotization together with the analyst and the business process owner.\nIf the solution is successful and performs as intended, we run the robot in production. Then we enter the hypercare period, during which we monitor and make necessary adjustments\nin tandem with the business process owner. After about two weeks of this phase, if both sides are satisfied with the results, we â€œgo liveâ€.\n\nWhat kind of processes do we automate?\nJira automations\nIn our organization we have a lot of processes based on Jira â€œticketsâ€. Many employees had to manage and operate Jiraâ€™s queues. Our team implemented several \nrobots to relieve administrators from repetitive tasks. Jira has an API available which we used in combination with the UiPath platform. \nFor example, when an employee is leaving Allegro, several Jira tasks are automatically created to retract authorization in various systems. Previously, these tasks were \nperformed manually by administrators. Now, the process is fully automated. The robot manages tickets via API and checks accounts in systems by GUI.\nSAP (Enterprise Resource Planning system) processes\nAll repetitive, rule-based processes in SAP can be automated. For instance, letâ€™s consider the processes in the Finance team. They handle massive amounts of \ninvoices. For some suppliers, with the largest quantity of invoices and unchangeable invoice layout, we were able to automate the accounting process in SAP. \nThe robot accesses an appropriate transaction in SAP and lists invoices. Based on business rules, the bot selects a specific invoice, opens it, and \nreads selected values. Then, it compares these values with business conditions which were established in the Process Definition Document. Depending on the \nsituation, the robot fills or corrects adequate fields and either accepts or rejects the invoice.\nAutomation by API\nOne of the excellent examples of API automations is the process of changing product categories on the Allegro platform. Allegro hosts a vast number of products. Initially, \nnot all of them are assigned to the proper category. We were able to create a robot that uses Allegroâ€™s REST API to move these products to the target category.\nBefore automation, this task was time-consuming and monotonous. Recently, the robot completed a huge task, moving almost 3 million products in two days!\nProcesses across multiple applications and integrations\nIt is possible to combine tasks from different applications into one robot. This approach allows us to automate more complex processes. \nThe most interesting ones include:\nThe process of userâ€™s data change in Allegro platform is carried out at the request of users. To perform all actions, the bot uses Salesforce and \nInternal Admin tools. The robot downloads a report with requests, then checks the pre-set business rules. Based on the results, the bot changes userâ€™s data or\n rejects the request.\nThe robot works 24 hours a day, handling 80% of applications. The number of tasks it performs can be compared to the work of four full-time employees.\nThe anti-fraud process. The robot verifies hundreds of thousands of messages and blocks suspected accounts. Using suspicious messages from the Spoof \napplication (Message Center), the robot determines if a message is spam or not. To make a proper decision, it checks various business conditions to decide \nif an account should be blocked. After making the decision, the bot blocks the accountâ€™s message sending capabilities.\nThe process for the HR team where the robot works on two applications. The robot interacts with the interface of an application and also uses its API. \nIn the recruitment process, specialists from different fields participate and help recruiters to find the best candidates. These specialists are known as \nthe Hiring Squad. A significant number of people are involved in this process, and the robot is responsible for keeping the Hiring Squad updated. Based \non a report with job offers the robot checks if a candidate has active status for specific skills required for the interview process. If the status is active, \nthe bot selects a particular job offer from the platform and assigns the interviewer from Hiring Squad.\nâ€œHuman in the Loopâ€\nThose processes which are rule-based, repetitive, but require human decision-making or the robot does not know what to do on the basis of the collected data, \nare referred to as â€œhuman in the loopâ€. A great example could be the process of damage complaints regarding packages that we have automated. The robot gathers a report \nfrom Salesforce and filters all jobs referring to damaged packages. Then, in the Internal Admin tool, the bot checks and collects various pieces of information based \non specific business rules. If necessary, it also checks the status of packages on carrier websites. Finally, the robot creates a form with all the gathered \ndetails, information, and attachments. This form is sent to a human for verification. With all this collected information, an employee can quickly decide \nwhether the complaint should be accepted. Then, the decision is sent back to the robot, which is able to close the case. It sends appropriate emails and \nrecords the results in the system.\nRobotics workshops for employees\nAdditionally, twice a year, we organize an educational program for employees called â€œAllegro Robot Schoolâ€. Employees get a chance to learn how to build \nbasic robots in UiPath and build one to support their daily tasks. To sign up for the program there is no need to have coding experience. It is enough if \nan employee can think analytically and has motivation to learn new things.\nThe program is intensive, consisting of five days of workshops. After the workshops, there is a three-weeks period where, with our support, employees choose \ntheir own processes and build robots.\nFor each edition we have around ten participants. The robots created during one edition account for about 3 FTEs! We have many examples of graduate \nemployees who created more robots to support their daily work in a team. Moreover, we created a Slack community for graduates to stay in touch, share \nknowledge, and support the development of new robots.\nSummary\nToday, the number of processes and different applications used in companies is enormous. Moreover, it can sometimes be challenging to integrate one application\nwith another, and employees are burdened with many manual, repetitive tasks. It is important to know that there is a solution to automate these processes. \nThe RPA technology can quickly help with that, freeing up employees for more creative tasks. The above examples visualize the possibilities of using UiPath.\nThe most important thing to remember is that the process to automate has to be manual, rule-based, repetitive, with data in digital form. What is more, \nit is possible to learn for those who donâ€™t have coding experience. Thanks to that, the automation of processes can be expanded across the company beyond the Process Automation Team.","guid":"https://blog.allegro.tech/2024/02/rpa.html","categories":["tech","rpa"],"isoDate":"2024-02-19T23:00:00.000Z"},{"title":"Donâ€™t bother: it is only a little expired","link":"https://blog.allegro.tech/2024/02/couchbase-expired-docs-tuning.html","pubDate":"Mon, 12 Feb 2024 00:00:00 +0100","authors":{"author":[{"name":["Tomasz ZiÃ³Å‚kowski"],"photo":["https://blog.allegro.tech/img/authors/tomasz.ziolkowski.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.ziolkowski"]}]},"content":"\u003cp\u003eThis story shows how we strive to fix issues reported by our customers regarding inconsistent listing views on our e-commerce platform.\nWe will use a top-down manner to guide you through our story. At the beginning, we highlight the challenges faced by our customers, followed by presenting\nbasic information on how views are personalized on our web application. We then delve deeper into our internal architecture, aiming to clarify how\nit supports High Availability (HA) by using two data centers. Finally, we advertise a little \u003cem\u003e\u003ca href=\"https://www.couchbase.com/\"\u003eCouchbase\u003c/a\u003e\u003c/em\u003e,\ndistributed NoSQL database, and explain why it is an excellent storage solution for such an architecture.\u003c/p\u003e\n\n\u003cp\u003eLater, we explain how the absence of adequate tools hindered us from uncovering the root cause of the problem and detail the adjustments we made in Couchbase\nto overcome these challenges. What can you glean from our experience? Firstly, you might be inspired to consider Couchbase as a storage solution in\na multi-regional, active-active architecture. Secondly, youâ€™ll discover a tool that aids in monitoring Couchbase behavior in a multi-region setting.\nThirdly, we share some tips on manipulating settings in Couchbase.\nLastly, youâ€™ll be able to decipher the mysterious title of our story and understand a few technological abbreviations.\u003c/p\u003e\n\n\u003ch2 id=\"what-you-ask-is-not-what-you-get\"\u003eWhat you ask is NOT what you get\u003c/h2\u003e\n\n\u003cp\u003eAn ongoing challenge in the development of the \u003ca href=\"https://allegro.tech/\"\u003eAllegro platform\u003c/a\u003e has been the product catalog.\nOriginating from a C2C platform where offers lacked references to pre-existing products, our shift towards the B2C model brought forth the need to merge\noffers representing the same product. This was essential for enhancing the selection experience for our buyers. The journey to construct such a catalog involved\nvarious approaches, and after numerous iterations, it now functions seamlessly.\nFor the context of this article, a crucial detail is that our platform must support at least two ways of selecting offers:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eOffer listing: Each presented entity is a unique offer listed by a particular merchant.\n  \u003cimg src=\"/img/articles/2024-02-12-couchbase-expired-docs-tuning/offers-view.png\" alt=\"offers-listing\" /\u003e\u003c/li\u003e\n  \u003cli\u003eProduct listing: Each presented entity represents a unique product connected to a set of offers where you can make a purchase.\n  \u003cimg src=\"/img/articles/2024-02-12-couchbase-expired-docs-tuning/products-view.png\" alt=\"products-listing\" /\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eGiven the diverse factors influencing whether a customer prefers selecting offers or products, weâ€™ve deliberately avoided limiting the selection experience.\nIn many cases, customers are free to choose either method, and their preferred choice is remembered to ensure a consistent and personalized experience.\nBoth offer and product listing views feature a switch that allows users to change their personal preference. With each click,\nthe personalized preference is updated, and the current view is refreshed accordingly.\u003c/p\u003e\n\n\u003cp\u003eHowever, our journey took an unexpected turn when we began receiving reports that customers, despite changing their preference, continued to see the same view.\nFor instance, if a customer switched from the offer listing to the product listing, they would still receive the offer listing. The frustration escalated\nas this invalid view persisted, even after manual page refreshes, lasting several times before finally aligning with the customerâ€™s preference after a delay,\nsometimes up to a minute. This discrepancy became the starting point of our investigation.\u003c/p\u003e\n\n\u003ch2 id=\"remedy-for-rendering-views-in-soa---opbox\"\u003eRemedy for rendering views in SOA - Opbox\u003c/h2\u003e\n\n\u003cp\u003eDedicated readers of this blog may already be aware that the Allegro platform embraces a microservices architecture, a powerful strategy for\ndividing domains and responsibilities. However, this approach presents challenges when it comes to offering a unified graphical user interface (GUI) for\nour customers. To bridge this gap, we successfully implemented our internal Content Management System (CMS) platform named Opbox.\nWhile delving into the intricate details of Opbox is beyond the scope of this narrative, those interested in our frontend management can explore\nour \u003ca href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\"\u003eblogpost\u003c/a\u003e or,\nif inclined, listen to our podcast in Polish \u003ca href=\"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eFor our story, whatâ€™s crucial to note is that Opbox plays a pivotal role in fetching information from microservices, particularly about customer preferences.\nIt collaborates with the microservice responsible for storing such data, ensuring the preparation of a personalized view for each user.\u003c/p\u003e\n\n\u003cp\u003eNo detective skills were needed to deduce that the observed issues originated from the microservice responsible for storing customer preferences,\nwhich was serving outdated information.\nThe real question at hand was how to mitigate this challenge.\u003c/p\u003e\n\n\u003cp\u003eCompleting our narrative puzzle involves highlighting the interface familiar to our web application customers,\nwhich operates across two domains with distinct responsibilities:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eallegro.pl\u003c/em\u003e: This domain is responsible for providing the GUI (HTML views), rendered via Opbox.\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eedge.allegro.pl\u003c/em\u003e: This domain takes charge of direct interactions via \u003cem\u003eAJAX\u003c/em\u003e (Asynchronous JavaScript) with specific microservices.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn simpler terms, the listing view is rendered through the \u003cem\u003eallegro.pl\u003c/em\u003e domain, while the task of switching personal preferences is handled via the\n\u003cem\u003eedge.allegro.pl\u003c/em\u003e domain.\u003c/p\u003e\n\n\u003ch2 id=\"you-cant-handle-this\"\u003eYou canâ€™t HAndle this\u003c/h2\u003e\n\n\u003cp\u003eAs a member of the Technical Platform department, my perspective is likely biased towards High Availability (HA) and everything that enhances\nAllegroâ€™s resilience to the outage of individual components or services. The fundamental principle guiding our HA strategy involves dispersing each\nmicroservice to two different locations, typically different data centers. This approach serves as a robust contingency plan, enabling us to overcome not only\nminor outages but also significant disasters, such as the outage of an entire data center.\u003c/p\u003e\n\n\u003cp\u003eItâ€™s essential to note that our HA strategy operates on a multi-region active-active approach. In simpler terms, all our data centers or clouds are actively\nhandling traffic simultaneously. While this approach ensures that everything remains operational in each location, it also introduces its own set of challenges.\nBalancing the benefits of simultaneous activity with the complexities it brings is a constant consideration in our pursuit of a resilient\nand fault-tolerant system.\u003c/p\u003e\n\n\u003ch3 id=\"navigating-the-multi-region-challenges\"\u003eNavigating the multi-region challenges\u003c/h3\u003e\n\n\u003cp\u003eHandling traffic in such a manner can undoubtedly impact performance. Each HTTP request from our customers typically involves a set of microservices.\nTo mitigate the challenges of cross-datacenter traffic between these services, we introduced the principle of locality. In simple terms, if an instance of\nmicroservice A needs to communicate with microservice B, we prioritize instances running in the same location or data center.\u003c/p\u003e\n\n\u003cp\u003eHowever, itâ€™s crucial to note that the locality principle faces limitations, especially when it comes to certain storage solutions. For instance,\nmost Relational Database Management Systems (RDBMS) and MongoDB databases only allow writes through a specific node. This means that even if the traffic\nis handled by an instance in \u003cem\u003eDATA CENTER A\u003c/em\u003e, it may still be necessary to query a database node in \u003cem\u003eDATA CENTER B\u003c/em\u003e to write some data.\nThe challenge lies in finding storage solutions that permit simultaneous writing to nodes in the same location. One such example is\nCouchbase clusters with cross-data center replication, offering a solution to the intricacies of our multi-region, active-active architecture.\u003c/p\u003e\n\n\u003ch3 id=\"roots-of-inconsistency\"\u003eRoots of inconsistency\u003c/h3\u003e\n\n\u003cp\u003eAs mentioned earlier, we employ two domains to provide an interface for our customers. In the context of High Availability (HA), this setup implies that\nrendering can be handled by \u003cem\u003eDATA CENTER A\u003c/em\u003e, while \u003cem\u003eAJAX\u003c/em\u003e communication simultaneously takes place in \u003cem\u003eDATA CENTER B\u003c/em\u003e. This dual-domain approach necessitates\na replication mechanism that applies changes made in one data center to the other.\u003c/p\u003e\n\n\u003cp\u003eHowever, a critical challenge arises when the replication mechanism lags behind the Round-Trip Time (RTT) of client requests, as illustrated in\nthe diagram below. The red rectangle in the diagram symbolizes the replication process of a single write operation. If this process takes longer than\nthe back-and-forth exchange of HTTP response and request, the client may receive an invalid view. Itâ€™s crucial to note that the second request is directed\nstraight to data center B and is not proxied by \u003cem\u003eDATA CENTER A\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eMitigating this issue, short of radical architectural changes, becomes a significant concern. The intricacies of replication timing are central to ensuring\na seamless and accurate user experience in our multi-data center, active-active architecture.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-02-12-couchbase-expired-docs-tuning/replication.png\" alt=\"replication-lag\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe replication lag can be influenced by various factors depending on the storage solutions in use. One undeniable factor is that it cannot be faster than\nthe light distance between data centers. Fortunately, in our case, this distance is minimal, and for the purpose of this story, it can be considered negligible.\u003c/p\u003e\n\n\u003ch2 id=\"fortunate-storage\"\u003eFortunate storage\u003c/h2\u003e\n\n\u003cp\u003eAs I mentioned earlier, Couchbase lends itself to adhering to our locality principle in the communication between microservices and databases within\nour architecture. Fortunately, it serves as the storage solution for the microservice responsible for maintaining customersâ€™ preferences. Whatâ€™s even more\nfortunate is the swift cross-cluster replication mechanism,\n\u003cem\u003e\u003ca href=\"https://docs.couchbase.com/server/current/learn/clusters-and-availability/xdcr-overview.html\"\u003eXDCR\u003c/a\u003e\u003c/em\u003e, employed by Couchbase.\nIn fact, the changes are applied on a cluster in the second data center faster than one Round-Trip Time (RTT) between data centers â€“ pretty cool, isnâ€™t it?\nItâ€™s noteworthy that \u003cem\u003eXDCR\u003c/em\u003e can be configured in either a unidirectional or bidirectional manner. In our case, given active-active writes in all locations,\nwe use a bidirectional configuration to replicate changes, irrespective of the location in which they were applied.\u003c/p\u003e\n\n\u003ch2 id=\"deeper-analysis-unraveling-the-replication-enigma\"\u003eDeeper analysis: unraveling the replication enigma\u003c/h2\u003e\n\n\u003cp\u003eCouchbase offers an abundance of highly detailed metrics regarding the internal state of the cluster. However, without a deep understanding,\nit becomes challenging to decipher whether irregularities or spikes in these metrics may indicate potential problems for customers. This challenge is\nparticularly pronounced when dealing with cross-cluster replication, where overseeing the state of two clusters simultaneously is a complex task.\u003c/p\u003e\n\n\u003cp\u003eAt Allegro, our commitment to meticulous data collection extends to being detailed in gathering performance metrics from a microservice perspective.\nDespite Couchbase providing meaningful information, the story would be incomplete if it solely relied on easily accessible metrics. In reality, these metrics\nfailed to reveal relevant information; read/write times remained relatively stable even during moments when customers reported issues.\u003c/p\u003e\n\n\u003cp\u003eTo gain a deeper understanding of the situation, our approach was clearâ€”gather more data, with a specific focus on replication performance. This strategic\nemphasis on targeted data collection allowed us to delve into the intricacies of the replication process and uncover the underlying factors contributing to\nthe challenges faced by our customers.\u003c/p\u003e\n\n\u003ch3 id=\"measuring-replication-performance-the-birth-of-cb-tracker\"\u003eMeasuring replication performance: the birth of cb-tracker\u003c/h3\u003e\n\n\u003cp\u003eDue to the lack of readily available tools and our inability to find suitable options, we took matters into our own hands and developed our open-source\ncommand-line tool, \u003cem\u003e\u003ca href=\"https://github.com/ziollek/couchbase-replication-ping\"\u003ecb-tracker\u003c/a\u003e\u003c/em\u003e. This tool serves the purpose of continuously measuring\nreplication lag. Its functionality revolves around connecting to two Couchbase clusters, designated as A and B, connected via \u003cem\u003eXDCR\u003c/em\u003e replication.\u003c/p\u003e\n\n\u003cp\u003eThe primary objective of our tool is to measure the Replication Round-Trip Time (RTT). In simpler terms, it gauges how quickly a document written to cluster A\nbecomes accessible on cluster B, and vice versa. The logic behind this measurement is inspired by the widely used network diagnostic tool \u003cem\u003eping\u003c/em\u003e.\nTo provide a clearer understanding of how this tool performs measurements, refer to the diagram below.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-02-12-couchbase-expired-docs-tuning/cb-tracker.png\" alt=\"cb-tracker-flow\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWith the deployment of such a tool, we initiated continuous replication monitoring in the Couchbase bucket used by the microservice responsible for managing\ncustomer preferences. This monitoring effort provided us with valuable insights into the cyclic spikes in replication time. As depicted in the diagram below,\nwe observed two spikes recurring every hour, about 20 minutes apart. This observation prompted us to investigate potential periodic tasks\nwithin Couchbase that might be contributing to this cyclic behavior.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-02-12-couchbase-expired-docs-tuning/replication-spikes.png\" alt=\"replication-spikes\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"there-are-my-knobs-unraveling-couchbase-quirks\"\u003eThere are my knobs: unraveling couchbase quirks\u003c/h3\u003e\n\n\u003cp\u003eWhile weâ€™ve extolled the virtues of Couchbase, every solution has its quirks, and as the saying goes, the devil is in the detailsâ€¦ and defaults :).\nIn our case, the intricacy arose from the documents stored in the scrutinized bucket, each having a set Time-To-Live (TTL).\nThe implementation of \u003cem\u003eTTL\u003c/em\u003e on the Couchbase side is quite intriguing â€” expired documents are not deleted immediately; instead, they are skipped by\nthe fetching logic. This situation could potentially last indefinitely, leading to storage consumption concerns.\u003c/p\u003e\n\n\u003cp\u003eTo handle expired documents, Couchbase triggers a dedicated process every 60 minutes; the interval is controlled by \u003ca href=\"https://docs.couchbase.com/server/current/cli/cbepctl/set-flush_param.html#options\"\u003eexp_pager_stime parameter\u003c/a\u003e.\nEach run flushes out these expired documents from storage. However, an additional default setting â€” specifically,\nthe flag \u003cem\u003e\u003ca href=\"https://docs.couchbase.com/server/current/rest-api/rest-xdcr-adv-settings.html#get-settings-for-all-replications\"\u003efilterExpiration\u003c/a\u003e\u003c/em\u003e indicating that\neach flush should be replicated via \u003cem\u003eXDCR\u003c/em\u003e â€” created an unintended consequence. This default behavior caused a significant influx of events every hour,\noverwhelming \u003cem\u003eXDCR\u003c/em\u003e. Consequently, other events, such as changes made by the microservice, had to be queued.\u003c/p\u003e\n\n\u003cp\u003eUnderstanding that this mechanism operates on each cluster and that every expired document would eventually be deleted,\nwe recognized the need to address this overload of the replication mechanism. To rectify this, we adjusted the mentioned flag and increased the frequency\nof cleaning expired documents. Following this optimization, we observed a notable improvement, with no further instances of replication spikes.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eWe grappled with a shortage of effective tools to monitor replication from a client perspective. As weâ€™ve illustrated, pinpointing the genuine root cause of our\nproblem was crucial. I hope that the tool weâ€™ve introduced can also assist you in the ongoing quest for those valuable milliseconds.\u003c/p\u003e\n\n\u003cp\u003eCouchbase offers a comprehensive set of configuration parameters with default settings that might not be optimal for handling high-volume traffic.\nAs demonstrated, there are subtle threats that can undermine the experience of an otherwise speedy replication mechanism like \u003cem\u003eXDCR\u003c/em\u003e.\nItâ€™s worth noting that our narrative is based on the community edition of Couchbase (v6), and itâ€™s unfortunate that \u003cem\u003eXDCR\u003c/em\u003e is\n\u003ca href=\"https://www.couchbase.com/blog/couchbase-modifies-license-free-community-edition-package/\"\u003ewithdrawn\u003c/a\u003e from the open-source\nversion of Couchbase in the next release. I trust that our story can offer insights to help fine-tune your configuration and navigate potential challenges.\u003c/p\u003e\n\n\u003ch3 id=\"links\"\u003eLinks\u003c/h3\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.couchbase.com/server/current/rest-api/rest-xdcr-adv-settings.html#get-settings-for-all-replications\"\u003eCouchbase: advanced replication settings\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.couchbase.com/server/current/cli/cbepctl/set-flush_param.html#options\"\u003eCouchbase: flush parameters\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/ziollek/couchbase-replication-ping\"\u003ecb-tracker repository\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n","contentSnippet":"This story shows how we strive to fix issues reported by our customers regarding inconsistent listing views on our e-commerce platform.\nWe will use a top-down manner to guide you through our story. At the beginning, we highlight the challenges faced by our customers, followed by presenting\nbasic information on how views are personalized on our web application. We then delve deeper into our internal architecture, aiming to clarify how\nit supports High Availability (HA) by using two data centers. Finally, we advertise a little Couchbase,\ndistributed NoSQL database, and explain why it is an excellent storage solution for such an architecture.\nLater, we explain how the absence of adequate tools hindered us from uncovering the root cause of the problem and detail the adjustments we made in Couchbase\nto overcome these challenges. What can you glean from our experience? Firstly, you might be inspired to consider Couchbase as a storage solution in\na multi-regional, active-active architecture. Secondly, youâ€™ll discover a tool that aids in monitoring Couchbase behavior in a multi-region setting.\nThirdly, we share some tips on manipulating settings in Couchbase.\nLastly, youâ€™ll be able to decipher the mysterious title of our story and understand a few technological abbreviations.\nWhat you ask is NOT what you get\nAn ongoing challenge in the development of the Allegro platform has been the product catalog.\nOriginating from a C2C platform where offers lacked references to pre-existing products, our shift towards the B2C model brought forth the need to merge\noffers representing the same product. This was essential for enhancing the selection experience for our buyers. The journey to construct such a catalog involved\nvarious approaches, and after numerous iterations, it now functions seamlessly.\nFor the context of this article, a crucial detail is that our platform must support at least two ways of selecting offers:\nOffer listing: Each presented entity is a unique offer listed by a particular merchant.\n  \nProduct listing: Each presented entity represents a unique product connected to a set of offers where you can make a purchase.\n  \nGiven the diverse factors influencing whether a customer prefers selecting offers or products, weâ€™ve deliberately avoided limiting the selection experience.\nIn many cases, customers are free to choose either method, and their preferred choice is remembered to ensure a consistent and personalized experience.\nBoth offer and product listing views feature a switch that allows users to change their personal preference. With each click,\nthe personalized preference is updated, and the current view is refreshed accordingly.\nHowever, our journey took an unexpected turn when we began receiving reports that customers, despite changing their preference, continued to see the same view.\nFor instance, if a customer switched from the offer listing to the product listing, they would still receive the offer listing. The frustration escalated\nas this invalid view persisted, even after manual page refreshes, lasting several times before finally aligning with the customerâ€™s preference after a delay,\nsometimes up to a minute. This discrepancy became the starting point of our investigation.\nRemedy for rendering views in SOA - Opbox\nDedicated readers of this blog may already be aware that the Allegro platform embraces a microservices architecture, a powerful strategy for\ndividing domains and responsibilities. However, this approach presents challenges when it comes to offering a unified graphical user interface (GUI) for\nour customers. To bridge this gap, we successfully implemented our internal Content Management System (CMS) platform named Opbox.\nWhile delving into the intricate details of Opbox is beyond the scope of this narrative, those interested in our frontend management can explore\nour blogpost or,\nif inclined, listen to our podcast in Polish here.\nFor our story, whatâ€™s crucial to note is that Opbox plays a pivotal role in fetching information from microservices, particularly about customer preferences.\nIt collaborates with the microservice responsible for storing such data, ensuring the preparation of a personalized view for each user.\nNo detective skills were needed to deduce that the observed issues originated from the microservice responsible for storing customer preferences,\nwhich was serving outdated information.\nThe real question at hand was how to mitigate this challenge.\nCompleting our narrative puzzle involves highlighting the interface familiar to our web application customers,\nwhich operates across two domains with distinct responsibilities:\nallegro.pl: This domain is responsible for providing the GUI (HTML views), rendered via Opbox.\nedge.allegro.pl: This domain takes charge of direct interactions via AJAX (Asynchronous JavaScript) with specific microservices.\nIn simpler terms, the listing view is rendered through the allegro.pl domain, while the task of switching personal preferences is handled via the\nedge.allegro.pl domain.\nYou canâ€™t HAndle this\nAs a member of the Technical Platform department, my perspective is likely biased towards High Availability (HA) and everything that enhances\nAllegroâ€™s resilience to the outage of individual components or services. The fundamental principle guiding our HA strategy involves dispersing each\nmicroservice to two different locations, typically different data centers. This approach serves as a robust contingency plan, enabling us to overcome not only\nminor outages but also significant disasters, such as the outage of an entire data center.\nItâ€™s essential to note that our HA strategy operates on a multi-region active-active approach. In simpler terms, all our data centers or clouds are actively\nhandling traffic simultaneously. While this approach ensures that everything remains operational in each location, it also introduces its own set of challenges.\nBalancing the benefits of simultaneous activity with the complexities it brings is a constant consideration in our pursuit of a resilient\nand fault-tolerant system.\nNavigating the multi-region challenges\nHandling traffic in such a manner can undoubtedly impact performance. Each HTTP request from our customers typically involves a set of microservices.\nTo mitigate the challenges of cross-datacenter traffic between these services, we introduced the principle of locality. In simple terms, if an instance of\nmicroservice A needs to communicate with microservice B, we prioritize instances running in the same location or data center.\nHowever, itâ€™s crucial to note that the locality principle faces limitations, especially when it comes to certain storage solutions. For instance,\nmost Relational Database Management Systems (RDBMS) and MongoDB databases only allow writes through a specific node. This means that even if the traffic\nis handled by an instance in DATA CENTER A, it may still be necessary to query a database node in DATA CENTER B to write some data.\nThe challenge lies in finding storage solutions that permit simultaneous writing to nodes in the same location. One such example is\nCouchbase clusters with cross-data center replication, offering a solution to the intricacies of our multi-region, active-active architecture.\nRoots of inconsistency\nAs mentioned earlier, we employ two domains to provide an interface for our customers. In the context of High Availability (HA), this setup implies that\nrendering can be handled by DATA CENTER A, while AJAX communication simultaneously takes place in DATA CENTER B. This dual-domain approach necessitates\na replication mechanism that applies changes made in one data center to the other.\nHowever, a critical challenge arises when the replication mechanism lags behind the Round-Trip Time (RTT) of client requests, as illustrated in\nthe diagram below. The red rectangle in the diagram symbolizes the replication process of a single write operation. If this process takes longer than\nthe back-and-forth exchange of HTTP response and request, the client may receive an invalid view. Itâ€™s crucial to note that the second request is directed\nstraight to data center B and is not proxied by DATA CENTER A.\nMitigating this issue, short of radical architectural changes, becomes a significant concern. The intricacies of replication timing are central to ensuring\na seamless and accurate user experience in our multi-data center, active-active architecture.\n\nThe replication lag can be influenced by various factors depending on the storage solutions in use. One undeniable factor is that it cannot be faster than\nthe light distance between data centers. Fortunately, in our case, this distance is minimal, and for the purpose of this story, it can be considered negligible.\nFortunate storage\nAs I mentioned earlier, Couchbase lends itself to adhering to our locality principle in the communication between microservices and databases within\nour architecture. Fortunately, it serves as the storage solution for the microservice responsible for maintaining customersâ€™ preferences. Whatâ€™s even more\nfortunate is the swift cross-cluster replication mechanism,\nXDCR, employed by Couchbase.\nIn fact, the changes are applied on a cluster in the second data center faster than one Round-Trip Time (RTT) between data centers â€“ pretty cool, isnâ€™t it?\nItâ€™s noteworthy that XDCR can be configured in either a unidirectional or bidirectional manner. In our case, given active-active writes in all locations,\nwe use a bidirectional configuration to replicate changes, irrespective of the location in which they were applied.\nDeeper analysis: unraveling the replication enigma\nCouchbase offers an abundance of highly detailed metrics regarding the internal state of the cluster. However, without a deep understanding,\nit becomes challenging to decipher whether irregularities or spikes in these metrics may indicate potential problems for customers. This challenge is\nparticularly pronounced when dealing with cross-cluster replication, where overseeing the state of two clusters simultaneously is a complex task.\nAt Allegro, our commitment to meticulous data collection extends to being detailed in gathering performance metrics from a microservice perspective.\nDespite Couchbase providing meaningful information, the story would be incomplete if it solely relied on easily accessible metrics. In reality, these metrics\nfailed to reveal relevant information; read/write times remained relatively stable even during moments when customers reported issues.\nTo gain a deeper understanding of the situation, our approach was clearâ€”gather more data, with a specific focus on replication performance. This strategic\nemphasis on targeted data collection allowed us to delve into the intricacies of the replication process and uncover the underlying factors contributing to\nthe challenges faced by our customers.\nMeasuring replication performance: the birth of cb-tracker\nDue to the lack of readily available tools and our inability to find suitable options, we took matters into our own hands and developed our open-source\ncommand-line tool, cb-tracker. This tool serves the purpose of continuously measuring\nreplication lag. Its functionality revolves around connecting to two Couchbase clusters, designated as A and B, connected via XDCR replication.\nThe primary objective of our tool is to measure the Replication Round-Trip Time (RTT). In simpler terms, it gauges how quickly a document written to cluster A\nbecomes accessible on cluster B, and vice versa. The logic behind this measurement is inspired by the widely used network diagnostic tool ping.\nTo provide a clearer understanding of how this tool performs measurements, refer to the diagram below.\n\nWith the deployment of such a tool, we initiated continuous replication monitoring in the Couchbase bucket used by the microservice responsible for managing\ncustomer preferences. This monitoring effort provided us with valuable insights into the cyclic spikes in replication time. As depicted in the diagram below,\nwe observed two spikes recurring every hour, about 20 minutes apart. This observation prompted us to investigate potential periodic tasks\nwithin Couchbase that might be contributing to this cyclic behavior.\n\nThere are my knobs: unraveling couchbase quirks\nWhile weâ€™ve extolled the virtues of Couchbase, every solution has its quirks, and as the saying goes, the devil is in the detailsâ€¦ and defaults :).\nIn our case, the intricacy arose from the documents stored in the scrutinized bucket, each having a set Time-To-Live (TTL).\nThe implementation of TTL on the Couchbase side is quite intriguing â€” expired documents are not deleted immediately; instead, they are skipped by\nthe fetching logic. This situation could potentially last indefinitely, leading to storage consumption concerns.\nTo handle expired documents, Couchbase triggers a dedicated process every 60 minutes; the interval is controlled by exp_pager_stime parameter.\nEach run flushes out these expired documents from storage. However, an additional default setting â€” specifically,\nthe flag filterExpiration indicating that\neach flush should be replicated via XDCR â€” created an unintended consequence. This default behavior caused a significant influx of events every hour,\noverwhelming XDCR. Consequently, other events, such as changes made by the microservice, had to be queued.\nUnderstanding that this mechanism operates on each cluster and that every expired document would eventually be deleted,\nwe recognized the need to address this overload of the replication mechanism. To rectify this, we adjusted the mentioned flag and increased the frequency\nof cleaning expired documents. Following this optimization, we observed a notable improvement, with no further instances of replication spikes.\nSummary\nWe grappled with a shortage of effective tools to monitor replication from a client perspective. As weâ€™ve illustrated, pinpointing the genuine root cause of our\nproblem was crucial. I hope that the tool weâ€™ve introduced can also assist you in the ongoing quest for those valuable milliseconds.\nCouchbase offers a comprehensive set of configuration parameters with default settings that might not be optimal for handling high-volume traffic.\nAs demonstrated, there are subtle threats that can undermine the experience of an otherwise speedy replication mechanism like XDCR.\nItâ€™s worth noting that our narrative is based on the community edition of Couchbase (v6), and itâ€™s unfortunate that XDCR is\nwithdrawn from the open-source\nversion of Couchbase in the next release. I trust that our story can offer insights to help fine-tune your configuration and navigate potential challenges.\nLinks\nCouchbase: advanced replication settings\nCouchbase: flush parameters\ncb-tracker repository","guid":"https://blog.allegro.tech/2024/02/couchbase-expired-docs-tuning.html","categories":["tech","couchbase","replication","performance bottleneck","open source","ttl","metrics"],"isoDate":"2024-02-11T23:00:00.000Z"},{"title":"WCAG 2.2 is here! And what about 3.0?","link":"https://blog.allegro.tech/2024/01/wcag-2-2.html","pubDate":"Wed, 24 Jan 2024 00:00:00 +0100","authors":{"author":[{"name":["Barbara Szott"],"photo":["https://blog.allegro.tech/img/authors/barbara.szott.jpg"],"url":["https://blog.allegro.tech/authors/barbara.szott"]}]},"content":"\u003cp\u003eReady to turn web accessibility from a headache into a breeze? Join us as we demystify WCAG, explore its latest 2.2 version, and gaze into the future of digital\ninclusivity. Get ready for a journey thatâ€™s as enlightening as it is entertaining!\u003c/p\u003e\n\n\u003ch2 id=\"what-exactly-is-wcag\"\u003eWhat exactly is WCAG?\u003c/h2\u003e\n\n\u003cp\u003eImagine you are opening a bar and want to invite all your friends there, and you want them to have a great time hanging out at your place. But some of your\nfriends have different disabilities, such as using wheelchairs or having trouble seeing or hearing. You want everyone to have fun, right? Well, WCAG which stands for\nWeb Contents Accessibility Guidelines, is set of rules to make sure Internet is like a bar that everyone can enjoy.\u003c/p\u003e\n\n\u003ch2 id=\"four-principles-of-accessibility\"\u003eFour principles of accessibility\u003c/h2\u003e\n\n\u003cp\u003eThe guidelines and Success Criteria are based on four key principles that provide the fundamental basis for anyone to access and use web content. Let me explain\nthem as simply as possible:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003ePerceivable (big text and clear pictures)\u003c/strong\u003e - everything on a website should be easy to see and understand. Imagine reading a book. Some people need big\nletters, and some might not see well. So, WCAG says websites should have big letters and clear pictures, like a book with large print and colorful\nillustrations,\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eOperable (easy to click and move)\u003c/strong\u003e - the user interface and navigation of a website are easy to operate. It means that people should be able to interact\nwith the content using various methods, such as a keyboard or assistive technologies like screen readers. It also encourages predictable and consistent\nnavigation and input methods,\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eUnderstandable (no confusing stuff)\u003c/strong\u003e - ever been on a website and got lost because it was too complicated? WCAG says websites should be like a\nstraightforward treasure hunt, not a confusing maze. They should have clear instructions and be easy to understand, just like a fun treasure hunt game with\nsimple clues,\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eRobust (works everywhere)\u003c/strong\u003e - robustness means that websites should be built to last and work well with different technologies. This ensures that websites\nremain accessible as technology evolves. Itâ€™s like building a house with strong foundations so that it stands the test of time.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIn essence, WCAG is like a set of rules and recommendations to make the Internet a welcoming and usable place for everyone, regardless of their abilities or\ndisabilities. Itâ€™s about creating a digital world that is inclusive and user-friendly for all.\u003c/p\u003e\n\n\u003cp\u003eWCAG guidelines are organized into three levels of conformance:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eA - addressing the most critical issues to ensure a minimal level of accessibility\u003c/li\u003e\n  \u003cli\u003eAA - covers a broader range of requirements; the website is more accessible and user-friendly\u003c/li\u003e\n  \u003cli\u003eAAA - ensures the highest level of accessibility\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"timeline\"\u003eTimeline\u003c/h2\u003e\n\n\u003cp\u003eI donâ€™t want to bore you with details, but I think it is good to understand that WCAG is not something new. Itâ€™s been around for quite a while!\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e1997 - W3C started working on WCAG 1.0\u003c/li\u003e\n  \u003cli\u003e1999 - WCAG 1.0 shipped\u003c/li\u003e\n  \u003cli\u003e2001-2007 - WCAG 2.0\u003c/li\u003e\n  \u003cli\u003e2017-2018 - WCAG 2.1\u003c/li\u003e\n  \u003cli\u003e2020 - W3C started working on WCAG 2.2\u003c/li\u003e\n  \u003cli\u003e2023 - WCAG 2.2\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"wcag-22\"\u003eWCAG 2.2\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://www.w3.org/TR/WCAG22/\"\u003eThe new version\u003c/a\u003e introduces 9 new success criteria (SC) - two at level A, four at level AA, and three at AAA. Letâ€™s give them a closer look.\u003c/p\u003e\n\n\u003ch3 id=\"2411-a--2412-aa---stop-hiding-my-focus\"\u003e2.4.11 (A) \u0026amp; 2.4.12 (AA) - â€œStop hiding my focus!â€\u003c/h3\u003e\n\n\u003cp\u003eI think we can all agree here - it is just annoying if you are browsing the web and all of a sudden something appears - a modal, cookie banner, chat\nwidget, and covers what we were just reading.\u003c/p\u003e\n\n\u003cp\u003eSo make sure the focus is not obscured. It also applies to submenus and sticky headers.\u003c/p\u003e\n\n\u003cp\u003eFor A and AA levels focused user interface component should be at least partially visible; for AAA it must be fully visible.\u003c/p\u003e\n\n\u003ch3 id=\"257-aa---aaargh-i-cant-do-this-drag--drop-thing-i-need-another-way-to-do-this\"\u003e2.5.7 (AA) - â€œAaargh! I canâ€™t do this drag \u0026amp; drop thing! I need another way to do this..!â€\u003c/h3\u003e\n\n\u003cp\u003eIf a feature on a website or app can be used by moving your cursor or finger in a dragging motion (like when you click and drag a slider), it should also be\npossible to use that feature without dragging.\u003c/p\u003e\n\n\u003cp\u003eThis helps people with mobility difficulties, especially when it might be difficult for them to press and hold down and precisely control their movement.\u003c/p\u003e\n\n\u003ch3 id=\"258-aa---i-need-these-buttons-to-be-big-enough-that-im-sure-i-will-press-the-right-one\"\u003e2.5.8 (AA) - â€œI need these buttons to be big enough that Iâ€™m sure I will press the right oneâ€\u003c/h3\u003e\n\n\u003cp\u003eThe size of the target for pointer inputs should be at least 24 by 24 CSS pixels.\u003c/p\u003e\n\n\u003cp\u003eThis comes along with criterion 2.5.5 introduced in WCAG 2.1 (\u003ca href=\"https://www.w3.org/WAI/WCAG21/Understanding/target-size.html\"\u003eminimum target size\u003c/a\u003e) - this is still valid, but itâ€™s AAA level.\u003c/p\u003e\n\n\u003cp\u003eButtons that are too small or too close to each other are a common problem for people with mobility issues. Itâ€™s difficult to interact with the right one. This\nrule is not the same as the AAA success rule, which says that things you click on need to be big enough to easily click on. This rule is about making sure that\nthings you click on are either naturally big enough or spaced far enough apart from other things, so you donâ€™t accidentally click on the wrong thing. The worst\nthing here that can happen is that you accidentally submit something you didnâ€™t want to!\u003c/p\u003e\n\n\u003cp\u003eHow about links that are stacked on top of each other or small images adjacent to each other? How can we make them fulfill SC 2.5.8?\u003c/p\u003e\n\n\u003cp\u003eLetâ€™s use an imaginary 24px diameter circle. If you draw this circle in the center of your link - if that imaginary circle doesnâ€™t intersect another target or\nanother imaginary circle from a small target, then the link is sufficiently spaced.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-01-24-wcag-2-2/imaginary-circles.png\" alt=\"\" class=\"small-image\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eJust as many other success criteria, there are exceptions, and the most important for us are:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003elinks in text,\u003c/li\u003e\n  \u003cli\u003euser-agent controls - as we do not always control what the browser provides. Common examples are select elements (checkboxes, date pickers, etc.); note, that\nthis rule does apply to these structures if you are building them by yourself,\u003c/li\u003e\n  \u003cli\u003eessential elements - when things just have to be close to each other; an example of this is a map.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"2413-aaa---wh-where-is-my-keyboard-focus-\"\u003e2.4.13 (AAA) - â€œWhâ€¦ Where is my keyboard focus..? :(â€\u003c/h2\u003e\n\n\u003cp\u003eAlthough it is AAA level, it was hoped and tried to have the success criterion at AA.\u003c/p\u003e\n\n\u003cp\u003eFocus helps people who rely on keyboard navigation - not only people who are using some assistive technologies but also power users (curb-cut effect again!). The\nfocus indicator must be visible, with good contrast and size:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003econtrast ratio must be at least 3:1 between the same pixels in the focused and unfocused states,\u003c/li\u003e\n  \u003cli\u003esize needs to be at least the area of a 2px thick perimeter on the unfocused components.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe following 4 criteria are meant to improve \u003cstrong\u003ecognitive accessibility\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"326-a---i-need-help-but-i-have-no-idea-how-to-find-it\"\u003e3.2.6 (A) - â€œI need help but I have no idea how to find it!â€\u003c/h2\u003e\n\n\u003cp\u003eThis criterion does not require that help be provided. But if it is - it should be easy to find and its placement should be consistent among multiple screens\n(for example: always in the same place in the footer). The help section location may change as the viewport size changes, but it must be consistent across pages at the\nsame viewport size.\u003c/p\u003e\n\n\u003ch2 id=\"337-a---ive-just-told-you-that-why-do-i-have-to-repeat-myself\"\u003e3.3.7 (A) - â€œIâ€™ve just told you that, why do I have to repeat myself?â€\u003c/h2\u003e\n\n\u003cp\u003eDo not require people to enter the same information more than once during a process. It makes it easier for people with cognitive disabilities to avoid errors\nand finish a multi-step task by not making them repeat information theyâ€™ve already entered in one step when they move on to the next step.\u003c/p\u003e\n\n\u003cp\u003eLetâ€™s think of a very common example - filling shipping address and billing address. In many cases they are exactly the same - so why should the user provide\nthe same information twice? Good practice here is to provide a simple checkbox â€œsame as shipping addressâ€.\u003c/p\u003e\n\n\u003cp\u003eAnother example is to give the possibility to select previously provided information - for example, you can save in your account settings a few addresses, and\nafter purchase, you can just select an option instead of reentering all the data.\u003c/p\u003e\n\n\u003cp\u003eThere are a few exceptions:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ereentering information is \u003ca href=\"https://www.w3.org/WAI/WCAG22/Understanding/redundant-entry#dfn-essential\"\u003eessential\u003c/a\u003e,\u003c/li\u003e\n  \u003cli\u003esecurity reasons (when you have to reenter your password),\u003c/li\u003e\n  \u003cli\u003epreviously provided information may no longer be valid (for example some government sites may force you to fill form with crucial data once in a while)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"338-aa--339-aaa---accessible-authentication\"\u003e3.3.8 (AA) \u0026amp; 3.3.9 (AAA) - Accessible authentication\u003c/h2\u003e\n\n\u003cp\u003eAccessible authentication requires that there is a path to authenticate that doesnâ€™t rely on cognitive function tests such as memorization, transcription, or\nsolving puzzles. This requirement will help people with cognitive disabilities including memory, dyslexia, dyscalculiaâ€¦ but not only (check these \u003ca href=\"https://www.boredpanda.com/captcha-struggles-fails/\"\u003ecaptcha struggles fails\u003c/a\u003e).\u003c/p\u003e\n\n\u003cp\u003eAA allows CAPTCHAs that use recognizing common objects such as â€œselect all the pictures that include dogsâ€ but the AAA success criterion does not allow for that.\u003c/p\u003e\n\n\u003ch2 id=\"the-future-of-wcag\"\u003eThe future of WCAG\u003c/h2\u003e\n\u003cp\u003eCurrently, a task force is working on WCAG 2.x maintenance, addressing issues (there are over 600 of them opened on \u003ca href=\"https://github.com/w3c/wcag/issues\"\u003eGitHub\u003c/a\u003e).\nWCAG 2.2 will be around for awhile, and WCAG 3.0 is still years away from becoming a recommendation (â€¦and that will be a very long transition).\u003c/p\u003e\n\n\u003cp\u003eItâ€™s 2023, and WCAG 2.0 was shipped in 2001 - the world changed a lot during these years and technology evolved rapidly. WCAG 3.0 (â€œSilverâ€) aims to fill gaps,\nreflect real-world accessibility, and prioritize issues more effectively. Research began in 2016, and in 2021 the first draft was shown - but it is still very\nraw (check \u003ca href=\"https://www.w3.org/WAI/GL/task-forces/silver/wiki/Major_Milestones_for_Silver\"\u003emajor milestones for Silver\u003c/a\u003e).\u003c/p\u003e\n\n\u003cp\u003eIn conclusion, WCAG remains a vital framework for ensuring digital accessibility and empowering individuals with disabilities to fully engage with online\ncontent. The introduction of new criteria in WCAG 2.2 signifies a commitment to continually improving and expanding accessibility guidelines to meet the diverse\nneeds of users. As we look to the future of WCAG, with the development of WCAG 3.0 on the horizon, it is clear that accessibility is not just an evolving field\nbut a transformative one. The journey towards a more inclusive digital landscape may be ongoing, but it is driven by a shared dedication to ensuring that the\nweb is a place where everyone can participate, regardless of their abilities. Embracing these evolving standards and fostering a culture of accessibility will\nnot only benefit individuals with disabilities but will ultimately create a more equitable and accessible online world for all.\u003c/p\u003e\n","contentSnippet":"Ready to turn web accessibility from a headache into a breeze? Join us as we demystify WCAG, explore its latest 2.2 version, and gaze into the future of digital\ninclusivity. Get ready for a journey thatâ€™s as enlightening as it is entertaining!\nWhat exactly is WCAG?\nImagine you are opening a bar and want to invite all your friends there, and you want them to have a great time hanging out at your place. But some of your\nfriends have different disabilities, such as using wheelchairs or having trouble seeing or hearing. You want everyone to have fun, right? Well, WCAG which stands for\nWeb Contents Accessibility Guidelines, is set of rules to make sure Internet is like a bar that everyone can enjoy.\nFour principles of accessibility\nThe guidelines and Success Criteria are based on four key principles that provide the fundamental basis for anyone to access and use web content. Let me explain\nthem as simply as possible:\nPerceivable (big text and clear pictures) - everything on a website should be easy to see and understand. Imagine reading a book. Some people need big\nletters, and some might not see well. So, WCAG says websites should have big letters and clear pictures, like a book with large print and colorful\nillustrations,\nOperable (easy to click and move) - the user interface and navigation of a website are easy to operate. It means that people should be able to interact\nwith the content using various methods, such as a keyboard or assistive technologies like screen readers. It also encourages predictable and consistent\nnavigation and input methods,\nUnderstandable (no confusing stuff) - ever been on a website and got lost because it was too complicated? WCAG says websites should be like a\nstraightforward treasure hunt, not a confusing maze. They should have clear instructions and be easy to understand, just like a fun treasure hunt game with\nsimple clues,\nRobust (works everywhere) - robustness means that websites should be built to last and work well with different technologies. This ensures that websites\nremain accessible as technology evolves. Itâ€™s like building a house with strong foundations so that it stands the test of time.\nIn essence, WCAG is like a set of rules and recommendations to make the Internet a welcoming and usable place for everyone, regardless of their abilities or\ndisabilities. Itâ€™s about creating a digital world that is inclusive and user-friendly for all.\nWCAG guidelines are organized into three levels of conformance:\nA - addressing the most critical issues to ensure a minimal level of accessibility\nAA - covers a broader range of requirements; the website is more accessible and user-friendly\nAAA - ensures the highest level of accessibility\nTimeline\nI donâ€™t want to bore you with details, but I think it is good to understand that WCAG is not something new. Itâ€™s been around for quite a while!\n1997 - W3C started working on WCAG 1.0\n1999 - WCAG 1.0 shipped\n2001-2007 - WCAG 2.0\n2017-2018 - WCAG 2.1\n2020 - W3C started working on WCAG 2.2\n2023 - WCAG 2.2\nWCAG 2.2\nThe new version introduces 9 new success criteria (SC) - two at level A, four at level AA, and three at AAA. Letâ€™s give them a closer look.\n2.4.11 (A) \u0026 2.4.12 (AA) - â€œStop hiding my focus!â€\nI think we can all agree here - it is just annoying if you are browsing the web and all of a sudden something appears - a modal, cookie banner, chat\nwidget, and covers what we were just reading.\nSo make sure the focus is not obscured. It also applies to submenus and sticky headers.\nFor A and AA levels focused user interface component should be at least partially visible; for AAA it must be fully visible.\n2.5.7 (AA) - â€œAaargh! I canâ€™t do this drag \u0026 drop thing! I need another way to do this..!â€\nIf a feature on a website or app can be used by moving your cursor or finger in a dragging motion (like when you click and drag a slider), it should also be\npossible to use that feature without dragging.\nThis helps people with mobility difficulties, especially when it might be difficult for them to press and hold down and precisely control their movement.\n2.5.8 (AA) - â€œI need these buttons to be big enough that Iâ€™m sure I will press the right oneâ€\nThe size of the target for pointer inputs should be at least 24 by 24 CSS pixels.\nThis comes along with criterion 2.5.5 introduced in WCAG 2.1 (minimum target size) - this is still valid, but itâ€™s AAA level.\nButtons that are too small or too close to each other are a common problem for people with mobility issues. Itâ€™s difficult to interact with the right one. This\nrule is not the same as the AAA success rule, which says that things you click on need to be big enough to easily click on. This rule is about making sure that\nthings you click on are either naturally big enough or spaced far enough apart from other things, so you donâ€™t accidentally click on the wrong thing. The worst\nthing here that can happen is that you accidentally submit something you didnâ€™t want to!\nHow about links that are stacked on top of each other or small images adjacent to each other? How can we make them fulfill SC 2.5.8?\nLetâ€™s use an imaginary 24px diameter circle. If you draw this circle in the center of your link - if that imaginary circle doesnâ€™t intersect another target or\nanother imaginary circle from a small target, then the link is sufficiently spaced.\n\nJust as many other success criteria, there are exceptions, and the most important for us are:\nlinks in text,\nuser-agent controls - as we do not always control what the browser provides. Common examples are select elements (checkboxes, date pickers, etc.); note, that\nthis rule does apply to these structures if you are building them by yourself,\nessential elements - when things just have to be close to each other; an example of this is a map.\n2.4.13 (AAA) - â€œWhâ€¦ Where is my keyboard focus..? :(â€\nAlthough it is AAA level, it was hoped and tried to have the success criterion at AA.\nFocus helps people who rely on keyboard navigation - not only people who are using some assistive technologies but also power users (curb-cut effect again!). The\nfocus indicator must be visible, with good contrast and size:\ncontrast ratio must be at least 3:1 between the same pixels in the focused and unfocused states,\nsize needs to be at least the area of a 2px thick perimeter on the unfocused components.\nThe following 4 criteria are meant to improve cognitive accessibility.\n3.2.6 (A) - â€œI need help but I have no idea how to find it!â€\nThis criterion does not require that help be provided. But if it is - it should be easy to find and its placement should be consistent among multiple screens\n(for example: always in the same place in the footer). The help section location may change as the viewport size changes, but it must be consistent across pages at the\nsame viewport size.\n3.3.7 (A) - â€œIâ€™ve just told you that, why do I have to repeat myself?â€\nDo not require people to enter the same information more than once during a process. It makes it easier for people with cognitive disabilities to avoid errors\nand finish a multi-step task by not making them repeat information theyâ€™ve already entered in one step when they move on to the next step.\nLetâ€™s think of a very common example - filling shipping address and billing address. In many cases they are exactly the same - so why should the user provide\nthe same information twice? Good practice here is to provide a simple checkbox â€œsame as shipping addressâ€.\nAnother example is to give the possibility to select previously provided information - for example, you can save in your account settings a few addresses, and\nafter purchase, you can just select an option instead of reentering all the data.\nThere are a few exceptions:\nreentering information is essential,\nsecurity reasons (when you have to reenter your password),\npreviously provided information may no longer be valid (for example some government sites may force you to fill form with crucial data once in a while)\n3.3.8 (AA) \u0026 3.3.9 (AAA) - Accessible authentication\nAccessible authentication requires that there is a path to authenticate that doesnâ€™t rely on cognitive function tests such as memorization, transcription, or\nsolving puzzles. This requirement will help people with cognitive disabilities including memory, dyslexia, dyscalculiaâ€¦ but not only (check these captcha struggles fails).\nAA allows CAPTCHAs that use recognizing common objects such as â€œselect all the pictures that include dogsâ€ but the AAA success criterion does not allow for that.\nThe future of WCAG\nCurrently, a task force is working on WCAG 2.x maintenance, addressing issues (there are over 600 of them opened on GitHub).\nWCAG 2.2 will be around for awhile, and WCAG 3.0 is still years away from becoming a recommendation (â€¦and that will be a very long transition).\nItâ€™s 2023, and WCAG 2.0 was shipped in 2001 - the world changed a lot during these years and technology evolved rapidly. WCAG 3.0 (â€œSilverâ€) aims to fill gaps,\nreflect real-world accessibility, and prioritize issues more effectively. Research began in 2016, and in 2021 the first draft was shown - but it is still very\nraw (check major milestones for Silver).\nIn conclusion, WCAG remains a vital framework for ensuring digital accessibility and empowering individuals with disabilities to fully engage with online\ncontent. The introduction of new criteria in WCAG 2.2 signifies a commitment to continually improving and expanding accessibility guidelines to meet the diverse\nneeds of users. As we look to the future of WCAG, with the development of WCAG 3.0 on the horizon, it is clear that accessibility is not just an evolving field\nbut a transformative one. The journey towards a more inclusive digital landscape may be ongoing, but it is driven by a shared dedication to ensuring that the\nweb is a place where everyone can participate, regardless of their abilities. Embracing these evolving standards and fostering a culture of accessibility will\nnot only benefit individuals with disabilities but will ultimately create a more equitable and accessible online world for all.","guid":"https://blog.allegro.tech/2024/01/wcag-2-2.html","categories":["tech","accessibility","a11y","wcag"],"isoDate":"2024-01-23T23:00:00.000Z"}],"jobs":[{"id":"743999972114613","name":"Software Engineer (.NET) - Opennet","uuid":"fd4f6064-4a97-4bb0-94d4-64080aa93b37","jobAdId":"fd2f00d1-0366-439f-bad2-248f91570721","defaultJobAd":true,"refNumber":"REF4222P","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-03-06T11:03:03.858Z","location":{"city":"Warsaw, PoznaÅ„","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"f7af19b5-5d6a-43a0-9a2b-1e99277515c7","valueLabel":"Opennet.pl Sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999972114613","language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999971068323","name":"Senior Salesforce Software Engineer","uuid":"db041a48-6655-4e84-a522-0f460631195d","jobAdId":"d0b08f39-f02c-4e32-ad36-475f900ddfc1","defaultJobAd":true,"refNumber":"REF4747J","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-03-01T12:30:56.335Z","location":{"city":"Warsaw, PoznaÅ„","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"26b58095-3c5f-4596-937f-27547fb80b07","valueLabel":"5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999971068323","creator":{"name":"Agnieszka Adamus"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999970171023","name":"(.NET) Software Engineer - OpenNet","uuid":"299e03b3-8d23-41c5-aac9-007801ebf1aa","jobAdId":"0f49adbf-1c3e-46f3-80df-a3408163f289","defaultJobAd":true,"refNumber":"REF4800C","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-02-27T13:11:13.332Z","location":{"city":"Warsaw","region":"Masovian Voivodeship","country":"pl","remote":false,"latitude":"52.2296756","longitude":"21.0122287"},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"f7af19b5-5d6a-43a0-9a2b-1e99277515c7","valueLabel":"Opennet.pl Sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999970171023","creator":{"name":"Wiktoria Mitruk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999967550443","name":"Senior Software Engineer (Java/Kotlin) - Technology Consumer Experience","uuid":"c0630d2d-eaba-42c6-bd2c-48dc4689acb9","jobAdId":"501ba2ce-9d33-4ca7-90b1-e0871f8b6687","defaultJobAd":false,"refNumber":"REF4072X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-02-14T13:42:20.139Z","location":{"city":"PoznaÅ„, Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999967550443","language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999967549418","name":"Software Engineer (Java/Kotlin) - Delivery Experience","uuid":"2010b497-d27d-4623-992d-f6fde7093f4a","jobAdId":"ee69cca3-645d-4080-af3b-1d9a32601d7b","defaultJobAd":false,"refNumber":"REF4072X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-02-14T13:40:57.032Z","location":{"city":"Warszawa, PoznaÅ„","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999967549418","language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1702979844000,"duration":187200000,"id":"298027809","name":"UX Research Confetti - IV edycja","date_in_series_pattern":false,"status":"upcoming","time":1716202800000,"local_date":"2024-05-20","local_time":"13:00","updated":1702985612000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":28,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Å»elazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/298027809/","description":"**ğŸ‰ Przedstawiamy 4. edycjÄ™ UX Research Confetti - bezpÅ‚atnÄ…, polskÄ… konferencjÄ™ poÅ›wiÄ™conÄ… badaniom UX, organizowanÄ… przez zespÃ³Å‚ badaczy z Allegro.** âœ¨ Konferencja odbÄ™dzie siÄ™ wâ€¦","visibility":"public","member_pay_fee":false},{"created":1701092071000,"duration":7200000,"id":"297614064","name":"Allegro Tech Talks #40 - Testy: dynamiczne dashboardy \u0026 optymalizacja pracy","date_in_series_pattern":false,"status":"past","time":1701968400000,"local_date":"2023-12-07","local_time":"18:00","updated":1701978668000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":13,"venue":{"id":27528185,"name":"Allegro KrakÃ³w Office","lat":50.06517028808594,"lon":19.951927185058594,"repinned":true,"address_1":"Lubicz Park A (5 piÄ™tro)","address_2":"ul. Lubicz 23","city":"KrakÃ³w","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/297614064/","description":"**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-40/](https://app.evenea.pl/event/allegro-tech-talk-40/) Jeszcze przed Å›wiÄ™tami zapraszamy Was na #40 wydarzenie z serii Allegro Tech Talk, podczas ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemyâ€¦","how_to_find_us":"Biuro Allegro znajduje siÄ™ w Centrum Biurowym Lubicz. \n\nObok budynku znajduje siÄ™ przystanek Lubicz. Przy przystanku zatrzymujÄ… siÄ™ tramwaje 2, 4, 10, 14, 20, 52, 62, 64 oraz autobusy: 124, 152, 424, 601, 611, 662, 664.\n\n","visibility":"public","member_pay_fee":false},{"created":1700495058000,"duration":7200000,"id":"297480100","name":"Allegro Tech Talks #39 - Big Data: o podejÅ›ciu do pracy z danymi","date_in_series_pattern":false,"status":"past","time":1701363600000,"local_date":"2023-11-30","local_time":"18:00","updated":1701377876000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":50,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Å»elazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/297480100/","description":"**Rejestracja: [https://app.evenea.pl/event/allegro-tech-talk-39/](https://app.evenea.pl/event/allegro-tech-talk-39/)** BÄ…dÅºcie z nami podczas #39 wydarzenia z serii **Allegro Tech Talk**, podczas ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³w przyâ€¦","how_to_find_us":"Biuro Allegro znajduje siÄ™ w kompleksie Fabryki Norblina (wejÅ›cie Plater 3 od ul. Å»elaznej). W niedalekiej odlegÅ‚oÅ›ci znajdujÄ… siÄ™ dwie stacje metra linii M2, Rondo DaszyÅ„skiego i Rondo ONZ. Autobusy, tramwaje i inne Å›rodki transportu sprawdzisz teÅ¼ na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false},{"created":1685697967000,"duration":7200000,"id":"293929321","name":"Allegro Tech Talks #38 - Mobile: o iOS bez spinki","date_in_series_pattern":false,"status":"past","time":1686760200000,"local_date":"2023-06-14","local_time":"18:30","updated":1686773845000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":17,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":0,"lon":0,"repinned":true,"address_1":"ul. Å»elazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293929321/","description":"**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwÄ… wakacyjnÄ…, stacjonarne spotkanie z cyklu Allegro Tech Talks, na ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³wâ€¦","how_to_find_us":"Biuro Allegro znajduje siÄ™ w kompleksie Fabryki Norblina (wejÅ›cie Plater 3 od ul. Å»elaznej). W niedalekiej odlegÅ‚oÅ›ci znajdujÄ… siÄ™ dwie stacje metra linii M2, Rondo DaszyÅ„skiego i Rondo ONZ. Autobusy, tramwaje i inne Å›rodki transportu sprawdzisz teÅ¼ na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"O pracy analitykÃ³w w obszarze technologii i przetwarzaniu danych w duÅ¼ej skali","link":"https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/","pubDate":"Thu, 29 Feb 2024 00:00:00 GMT","content":"Na czym polega praca analitykÃ³w w obszarze technologii w Allegro? Jakich narzÄ™dzi i technologii na co dzieÅ„ uÅ¼ywajÄ… osoby pracujÄ…ce na tych stanowiskach? Jak efekty pracy analitykÃ³w wpÅ‚ywajÄ… na naszÄ… platformÄ™, produkty i funkcjonalnoÅ›ci? Czym zajmuje siÄ™ Data Product Manager w Allegro Pay? Dlaczego monety sÄ… waÅ¼nym elementem ekosystemu Allegro? PosÅ‚uchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziaÅ‚em Adrianny NapiÃ³rkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.","contentSnippet":"Na czym polega praca analitykÃ³w w obszarze technologii w Allegro? Jakich narzÄ™dzi i technologii na co dzieÅ„ uÅ¼ywajÄ… osoby pracujÄ…ce na tych stanowiskach? Jak efekty pracy analitykÃ³w wpÅ‚ywajÄ… na naszÄ… platformÄ™, produkty i funkcjonalnoÅ›ci? Czym zajmuje siÄ™ Data Product Manager w Allegro Pay? Dlaczego monety sÄ… waÅ¼nym elementem ekosystemu Allegro? PosÅ‚uchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziaÅ‚em Adrianny NapiÃ³rkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.","guid":"https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/","isoDate":"2024-02-29T00:00:00.000Z"},{"title":"Programowanie - co liczy siÄ™ w nim najbardziej?","link":"https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/","pubDate":"Thu, 01 Feb 2024 00:00:00 GMT","content":"JakÄ… Å›cieÅ¼kÄ™ trzeba przejÅ›Ä‡, aby dobrze programowaÄ‡? Gdzie zdobywaÄ‡ wiedzÄ™, doÅ›wiadczenie i szlifowaÄ‡ swoje umiejÄ™tnoÅ›ci? Ile czasu potrzeba aby nabraÄ‡ doÅ›wiadczenia i jak zadbaÄ‡ o swÃ³j dalszy rozwÃ³j? Na czym w praktyce polegajÄ… role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza siÄ™ w naszych zespoÅ‚ach? PosÅ‚uchajcie nowego odcinka Allegro Tech Podcast z udziaÅ‚em RafaÅ‚a Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.","contentSnippet":"JakÄ… Å›cieÅ¼kÄ™ trzeba przejÅ›Ä‡, aby dobrze programowaÄ‡? Gdzie zdobywaÄ‡ wiedzÄ™, doÅ›wiadczenie i szlifowaÄ‡ swoje umiejÄ™tnoÅ›ci? Ile czasu potrzeba aby nabraÄ‡ doÅ›wiadczenia i jak zadbaÄ‡ o swÃ³j dalszy rozwÃ³j? Na czym w praktyce polegajÄ… role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza siÄ™ w naszych zespoÅ‚ach? PosÅ‚uchajcie nowego odcinka Allegro Tech Podcast z udziaÅ‚em RafaÅ‚a Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.","guid":"https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/","isoDate":"2024-02-01T00:00:00.000Z"},{"title":"MBox: server-driven UI dla aplikacji mobilnych","link":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","pubDate":"Thu, 16 Nov 2023 00:00:00 GMT","content":"Czym jest i jak powstaÅ‚ MBox: wewnÄ™trzna platforma server-driven UI dla aplikacji mobilnych w Allegro? SkÄ…d wziÄ…Å‚ siÄ™ pomysÅ‚ na to rozwiÄ…zanie i na jakie bolÄ…czki odpowiada? Dlaczego zdecydowaliÅ›my siÄ™ na budowanie tego typu rozwiÄ…zania in-house i z jakimi wyzwaniami mierzyliÅ›my siÄ™ w procesie tworzenia? Co wyrÃ³Å¼nia zespoÅ‚y pracujÄ…ce nad tym narzÄ™dziem i jak pracuje im siÄ™ bez Product Ownera? PosÅ‚uchajcie siÃ³dmego odcinka Allegro Tech Podcast z udziaÅ‚em Pauliny Sadowskiej i Tomasza GÄ™barowskiego - ManagerÃ³w w obszarze Technical Platform Services w Allegro.","contentSnippet":"Czym jest i jak powstaÅ‚ MBox: wewnÄ™trzna platforma server-driven UI dla aplikacji mobilnych w Allegro? SkÄ…d wziÄ…Å‚ siÄ™ pomysÅ‚ na to rozwiÄ…zanie i na jakie bolÄ…czki odpowiada? Dlaczego zdecydowaliÅ›my siÄ™ na budowanie tego typu rozwiÄ…zania in-house i z jakimi wyzwaniami mierzyliÅ›my siÄ™ w procesie tworzenia? Co wyrÃ³Å¼nia zespoÅ‚y pracujÄ…ce nad tym narzÄ™dziem i jak pracuje im siÄ™ bez Product Ownera? PosÅ‚uchajcie siÃ³dmego odcinka Allegro Tech Podcast z udziaÅ‚em Pauliny Sadowskiej i Tomasza GÄ™barowskiego - ManagerÃ³w w obszarze Technical Platform Services w Allegro.","guid":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","isoDate":"2023-11-16T00:00:00.000Z"},{"title":"O chatbotach i ich wpÅ‚ywie na Allegro","link":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","pubDate":"Wed, 11 Oct 2023 00:00:00 GMT","content":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieÄ‡ w kontekÅ›cie obszaru Customer Experience? W czym pomagajÄ… nam chatboty, jak je rozwijamy i dbamy o ich jakoÅ›Ä‡? Kim sÄ… Allina oraz Albert i co majÄ… wspÃ³lnego z automatyzacjÄ…? Za jakie rozwiÄ…zania otrzymaliÅ›my nagrodÄ™ hiperautomatyzacji? O tym wszystkim posÅ‚uchacie w odcinku z udziaÅ‚em RafaÅ‚a Gajewskiego - Managera w obszarze IT Services w Allegro.","contentSnippet":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieÄ‡ w kontekÅ›cie obszaru Customer Experience? W czym pomagajÄ… nam chatboty, jak je rozwijamy i dbamy o ich jakoÅ›Ä‡? Kim sÄ… Allina oraz Albert i co majÄ… wspÃ³lnego z automatyzacjÄ…? Za jakie rozwiÄ…zania otrzymaliÅ›my nagrodÄ™ hiperautomatyzacji? O tym wszystkim posÅ‚uchacie w odcinku z udziaÅ‚em RafaÅ‚a Gajewskiego - Managera w obszarze IT Services w Allegro.","guid":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","isoDate":"2023-10-11T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"Aoyoj7t6lTS_pXksuxS9r","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>