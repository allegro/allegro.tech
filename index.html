<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><link rel="author" href="humans.txt"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/c4277531f90028a4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c4277531f90028a4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/79db8b1e27b0a093.css" as="style"/><link rel="stylesheet" href="/_next/static/css/79db8b1e27b0a093.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-179adf437ae674f2.js" defer=""></script><script src="/_next/static/chunks/206-3a56e5ded293e83e.js" defer=""></script><script src="/_next/static/chunks/pages/index-9e9857288e1eab25.js" defer=""></script><script src="/_next/static/QeoYl8Gr6fKEkOuhx_7FZ/_buildManifest.js" defer=""></script><script src="/_next/static/QeoYl8Gr6fKEkOuhx_7FZ/_ssgManifest.js" defer=""></script><script src="/_next/static/QeoYl8Gr6fKEkOuhx_7FZ/_middlewareManifest.js" defer=""></script></head><body class="m-color-bg_desk"><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">About us</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro is one of the most technologically advanced companies in our part of Europe. Allegro is also over 1700 IT specialists of various specializations, developing our website. The unique scale and complexity of the problems that we solve on a daily basis give us the opportunity to develop on a wide variety of projects. Allegro Tech is a place where our engineers share knowledge and case studies from selected projects in the company – in the form of articles, podcasts and events.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/05/debugging-hangs.html" title="Debugging hangs - piecing together why nothing happens"><img width="388" src="images/post-headers/java.png" alt="Debugging hangs - piecing together why nothing happens" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/05/debugging-hangs.html" title="Debugging hangs - piecing together why nothing happens" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Debugging hangs - piecing together why nothing happens</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">15 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/java">#<!-- -->java</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/jvm">#<!-- -->jvm</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/debugging">#<!-- -->debugging</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/dependency hell">#<!-- -->dependency hell</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">As a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.
The task…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Łukasz Rokita" src="https://blog.allegro.tech/img/authors/lukasz.rokita.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/lukasz.rokita">Łukasz Rokita</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/05/debugging-hangs.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/05/debugging-hangs.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html" title="Trust no one, not even your training data! Machine learning from noisy data"><img width="388" src="images/post-headers/default.jpg" alt="Trust no one, not even your training data! Machine learning from noisy data" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html" title="Trust no one, not even your training data! Machine learning from noisy data" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Trust no one, not even your training data! Machine learning from noisy data</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mlr">#<!-- -->mlr</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/robustness">#<!-- -->robustness</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/research">#<!-- -->research</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ml">#<!-- -->ml</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/machine-learning">#<!-- -->machine-learning</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ai">#<!-- -->ai</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Label noise is ever-present in machine learning practice.
Allegro datasets are no exception.
We compared 7 methods for training classifiers robust to label noise.
All of them improved…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:5"><img alt="Łukasz Rączkowski" src="https://blog.allegro.tech/img/authors/lukasz.raczkowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar MuiAvatar-colorDefault" style="z-index:0">+<!-- -->4</div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/lukasz.raczkowski">Łukasz Rączkowski…</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/04/dynamic-workload-balancing-in-hermes.html" title="Dynamic Workload Balancing in Hermes"><img width="388" src="images/post-headers/default.jpg" alt="Dynamic Workload Balancing in Hermes" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/04/dynamic-workload-balancing-in-hermes.html" title="Dynamic Workload Balancing in Hermes" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Dynamic Workload Balancing in Hermes</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/architecture">#<!-- -->architecture</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/hermes">#<!-- -->hermes</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kafka">#<!-- -->kafka</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/algorithms">#<!-- -->algorithms</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/pub/sub">#<!-- -->pub/sub</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/publish-subscribe">#<!-- -->publish-subscribe</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/load balancing">#<!-- -->load balancing</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/open source">#<!-- -->open source</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Hermes is a distributed publish-subscribe
message broker that we use at Allegro to facilitate asynchronous communication between our
microservices. As our usage of Hermes has grown over…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Piotr Rżysko" src="https://blog.allegro.tech/img/authors/piotr.rzysko.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/piotr.rzysko">Piotr Rżysko</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/04/dynamic-workload-balancing-in-hermes.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/04/dynamic-workload-balancing-in-hermes.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/03/neuroscience-for-software-engineers-motivation.html" title="How neuroscience can help you as a software engineer - motivation"><img width="388" src="images/post-headers/default.jpg" alt="How neuroscience can help you as a software engineer - motivation" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/03/neuroscience-for-software-engineers-motivation.html" title="How neuroscience can help you as a software engineer - motivation" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">How neuroscience can help you as a software engineer - motivation</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/soft skills">#<!-- -->soft skills</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/neuroscience">#<!-- -->neuroscience</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/productivity">#<!-- -->productivity</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Many of us, software engineers, have experienced those days when nothing really sparks joy in coding, debugging,
preparing spikes or refining tasks for the next sprints.…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Paulina Szwed" src="https://blog.allegro.tech/img/authors/paulina.szwed.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/paulina.szwed">Paulina Szwed</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/03/neuroscience-for-software-engineers-motivation.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/03/neuroscience-for-software-engineers-motivation.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-developer-experience-czyli-jak-pracuje-sie-programistkom-w-allegro/" title="O Developer Experience, czyli jak pracuje się programist(k)om w Allegro"><img src="images/podcast.png" alt="O Developer Experience, czyli jak pracuje się programist(k)om w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-developer-experience-czyli-jak-pracuje-sie-programistkom-w-allegro/" title="O Developer Experience, czyli jak pracuje się programist(k)om w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O Developer Experience, czyli jak pracuje się programist(k)om w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 9 godzin temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym jest Developer Experience (DX) w Allegro i kto za nie odpowiada? Co robimy, jako firma, aby nasi developerzy(-ki) mieli(-ły) jak największy komfort pracy? Czym jest Allegro Developer Platform i jak dużą swobodę w wyborze technologii pozostawia developer(k)om? Jak mierzymy zadowolenie użytkowników i efektywność naszych narzędzi? Skąd wiemy co naprawdę się sprawdza na różnych płaszczyznach?</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-developer-experience-czyli-jak-pracuje-sie-programistkom-w-allegro/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-pracy-liderek-i-liderow-w-allegro/" title="O pracy liderek i liderów w Allegro"><img src="images/podcast.png" alt="O pracy liderek i liderów w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-pracy-liderek-i-liderow-w-allegro/" title="O pracy liderek i liderów w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O pracy liderek i liderów w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jakimi umiejętnościami powinny wyróżniać się osoby na stanowiskach liderskich? Czy wykształcenie techniczne to “must have”, aby dołączyć do zespołów Tech &amp; Data w Allegro? Czym charakteryzuje się praca liderek i liderów w Allegro oraz jak wspieramy ich rozwój? Jaki wpływ na produkt oraz organizację mają liderki i liderzy w Allegro? Jak zacząć budowanie swojej ścieżki kariery w roli liderskiej? Jakich wyzwań się spodziewać i jak sobie z nimi poradzić? Posłuchajcie rozmowy z udziałem Aliny Magowskiej - Dyrektorki obszaru User Experience i Agnieszki Jagusiak - Senior Managerki w zespole Group IT Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-pracy-liderek-i-liderow-w-allegro/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-pracy-i-rozwoju-w-zespole-it-support/" title="O pracy i rozwoju w zespole IT Support"><img src="images/podcast.png" alt="O pracy i rozwoju w zespole IT Support" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-pracy-i-rozwoju-w-zespole-it-support/" title="O pracy i rozwoju w zespole IT Support" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O pracy i rozwoju w zespole IT Support</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">5 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak wygląda praca w zespole, który zawsze udziela odpowiedzi na zadane pytania? Co można zautomatyzować w obszarze wsparcia IT i jaki to może mieć cel? Czy praca w zespole IT Support jest bramą do kariery w IT, może dawać możliwości rozwoju i przynosić satysfakcję? Jakie wyzwania przed tym zespołem w Allegro postawiła pandemia koronawirusa? O umożliwianiu pracownikom Grupy Allegro sprawnej pracy na narzędziach i usługach IT dostarczanych przez zespół Business Services &amp; Automation opowiada Bartosz Kaczyński - IT Service Operations Manager w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-pracy-i-rozwoju-w-zespole-it-support/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/" title="O tym jak przygotowujemy rozwiązania dla klientów w oparciu o badania"><img src="images/podcast.png" alt="O tym jak przygotowujemy rozwiązania dla klientów w oparciu o badania" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/" title="O tym jak przygotowujemy rozwiązania dla klientów w oparciu o badania" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O tym jak przygotowujemy rozwiązania dla klientów w oparciu o badania</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">5 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">W jaki sposób przygotowujemy rozwiązania dla klientów Allegro w oparciu o badania? Jak wygląda ścieżka projektu od eksploracji do wdrożenia i późniejszego monitorowania? Jaką korzyść dają badania usability? Dlaczego warto, aby badanie było prowadzone przez dwoje badaczy? O współpracy między badaczami i projektantami UX rozmawialiśmy z Zofią Śmierzchalską - Design Managerką i Jakubem Dodotem - Senior UX Research Managerem w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #38 - Mobile: o iOS bez spinki"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #38 - Mobile: o iOS bez spinki</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około 17 godzin temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwą wakacyjną, stacjonarne spotkanie z cyklu Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293929321/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/292278882/" title="UX Research Confetti - III edycja " class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - III edycja "/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/292278882/" title="UX Research Confetti - III edycja " class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - III edycja </h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">22 dni temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**Rejestracja na wydarzenie ➡ [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **🎉 Przedstawiamy 3. edycję UX Research Confetti organizowaną przez Allegro - bezpłatną, polską konferencję poświęconą badaniom…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/292278882/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293341234/" title="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293341234/" title="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">28 dni temu<!-- -->, Allegro Office - Poznań (Nowy Rynek)</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-37/](https://app.evenea.pl/event/allegro-tech-talk-37/) Ciąg dalszy naszych stacjonarnych spotkań Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów w kuluarach. 📌…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293341234/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293215214/" title="AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293215214/" title="AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">30 dni temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-kwanty/](https://app.evenea.pl/event/allegro-tech-kwanty/) Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293215214/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Front-End Software Engineer - Merchant Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Poznań, Wrocław</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999912833698-front-end-software-engineer-merchant-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Front-End Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Poznań, Wrocław</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999912830523-front-end-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Front-End Software Engineer - Delivery Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999912829875-front-end-software-engineer-delivery-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Mid/Senior Software Engineer (Java/Kotlin)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Kraków, Poznań, Wrocław, Gdańsk</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999910309044-midsenior-software-engineer-javakotlin?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Engineering Manager (Java/Kotlin) - Merchant Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Poznań, Wrocław, Kraków, Gdańsk</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999910043146-engineering-manager-javakotlin-merchant-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=7855008445406.371;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Debugging hangs - piecing together why nothing happens","link":"https://blog.allegro.tech/2023/05/debugging-hangs.html","pubDate":"Wed, 31 May 2023 00:00:00 +0200","authors":{"author":[{"name":["Łukasz Rokita"],"photo":["https://blog.allegro.tech/img/authors/lukasz.rokita.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.rokita"]}]},"content":"\u003cp\u003eAs a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.\nThe task is daunting and filled with challenges,\nhowever overall progress is steady and thanks to the modular nature of our code it should end in finite time.\nEveryone who has performed such an upgrade knows that you need to expect the unexpected and at the end of the day prepare for lots of debugging.\nNo amount of migration guide would prepare you for what’s coming in the field.\nIn the words of Donald Rumsfeld there are unknown unknowns and we need to be equipped with the tools to uncover these unknowns and patch them up.\nIn this blog post I’d like to walk you through a process that should show where the application hangs,\nalthough there seems to be nothing wrong with it. I will also show that you don’t always know what code you have – problem known as dependecy hell,\nplace we got quite cosy in during this upgrade.\u003c/p\u003e\n\n\u003ch2 id=\"the-change\"\u003eThe change\u003c/h2\u003e\n\u003cp\u003eNote that we keep versions as separate key–value pairs in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebuild.gradle\u003c/code\u003e files and reference them in dependencies by key.\nUpdating often means a single line change. The upgrade is trivial and git diff looks like this.\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eext.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n]\n\next.versions = [\n+        spring         : '6.0.5',\n+        spock          : '2.4-M1-groovy-4.0',\n+        groovy         : '4.0.9',\n]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eNothing much happens. We upgrade Spring and since there are some problems with Spock not working well with the newest Spring\nwe need to upgrade it as well, along with Groovy. This is the easy part.\nNow we run the tests and expect to be either elated with the sight of a successful build or greeted with descriptive error messages\nthat help us quickly patch the issue. Nobody expects anything and in this case this is an unknown unknown.\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e97% EXECUTING [15m 55s]\n\u0026gt; :platform-libraries-webclient:integrationTest \u0026gt; 1 test completed, 1 failed\n\u0026gt; :platform-libraries-webclient:integrationTest \u0026gt; Executing test pl.allegro....WebClientContextContainerInterceptorSpec\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eAfter 15 minutes we expect the process to end. A quick cross-check with the master branch confirms that tests run and execute in less than a minute.\nSomething is wrong and it’s on us. However, no error is presented. Adding logging does not help, nothing streams to standard output.\nSomething hangs and refuses to budge. When that happenes there is only one way to inspect what is going on and\nthat is to pop the hood open and look into JVM to see what the threads are doing or where they are slacking.\u003c/p\u003e\n\n\u003ch2 id=\"thread-theory\"\u003eThread theory\u003c/h2\u003e\n\n\u003cp\u003eLet’s interrupt this story with a short summary of threading in JVM. You can skip this chapter if you are familiar with the topic.\nAs the priceless book Java Concurrency in Practice by Brian Goetz et al. teaches us:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e“Threads may block, or pause, for several reasons: waiting for I/O completion, waiting to acquire a lock,\nwaiting to wake up from Thread.sleep, or waiting for the result of a computation in another thread.\nWhen a thread blocks, it is usually suspended and placed in one of the blocked thread states\n(BLOCKED, WAITING, or TIMED_WAITING). (…) blocked thread must wait for an event beyond its control before it can proceed”.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis sounds exactly like the situation we are in. So there is hope. Let’s educate ourselves further.\nAnother excerpt that would prove insightful reads as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e“(…) tasks can block for exteded periods of time, even if deadlock is not a possibility.\n(…) One technique that can mitigate the ill effects of long–running tasks is for tasks to use timed resource waits instead of\nunbound waits.”\nThis seems like an answer to our woes. However, two mysteries remain.\nWhere to put the timeout? What the thread is waiting for? To answer these questions we need to inspect the threads in the JVM itself.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"the-investigation\"\u003eThe investigation\u003c/h2\u003e\n\u003cp\u003eAt this point we did two things. First we pushed our code to a branch.\nAfter all at any moment our laptops could burst into flames and all the work would go to waste.\nThe remote CI confirmed our suspicion since it also hung. The problem was real and not only confined to the local machine.\nThe second thing is to scout for the offending thread. This is easy with the help of some JDK binaries:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ejps -lv | grep platform-libraries\n38983 worker.org.gradle.process.internal.worker.GradleWorkerMain -Dorg.gradle.internal.worker.tmpdir=/path/to/code/platform-libraries/platform-libraries-webclient/build/tmp/integrationTest/work -Dorg.gradle.native=false -Xmx512m -Dfile.encoding=UTF-8\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eSo we have the a lvmid – local JVM identifier, which will help us locate the offending thread in jconsole.\nIn the screen below we can see that the thread waits on \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMono.block()\u003c/code\u003e which is left unbounded in a happy path scenario.\nWell, we are in the worst case so first of all we add a simple timeout \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMono.block(Duration.ofSeconds(10))\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-05-31-debugging-hangs/jconsole.png\" alt=\"jconsole\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThis fails our tests and for the first time the error appears:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\t08:13:39.556 [Test worker] WARN reactor.core.Exceptions - throwIfFatal detected a jvm fatal exception, which is thrown and logged below:\njava.lang.NoSuchMethodError: 'reactor.core.publisher.Mono reactor.core.publisher.Mono.subscriberContext(reactor.util.context.Context)'\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\tat reactor.netty.internal.shaded.reactor.pool.AbstractPool$Borrower.request(AbstractPool.java:427)\n\tat reactor.netty.resources.PooledConnectionProvider$DisposableAcquire.onSubscribe(PooledConnectionProvider.java:533)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool$QueueBorrowerMono.subscribe(SimpleDequePool.java:676)\n\tat reactor.netty.resources.PooledConnectionProvider.disposableAcquire(PooledConnectionProvider.java:219)\n\tat reactor.netty.resources.PooledConnectionProvider.lambda$acquire$3(PooledConnectionProvider.java:183)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:326)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.core.publisher.FluxRetryWhen.subscribe(FluxRetryWhen.java:77)\n\tat reactor.core.publisher.MonoRetryWhen.subscribeOrReturn(MonoRetryWhen.java:46)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:329)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)\n\tat reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2545)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.request(MonoFlatMap.java:194)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.request(FluxOnAssembly.java:649)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2341)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:2215)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onSubscribe(FluxOnAssembly.java:633)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onSubscribe(MonoFlatMap.java:117)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)\n\tat reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoDeferContextual.subscribe(MonoDeferContextual.java:55)\n\tat reactor.core.publisher.Mono.subscribe(Mono.java:4485)\n\tat reactor.core.publisher.Mono.block(Mono.java:1733)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$Trait$Helper.makeRequest(WebClientContextContainerAdapterConfiguration.groovy:22)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$makeRequest.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)\n\tat pl.allegro....AdapterConfiguration$Trait$Helper.makeRequest(AdapterConfiguration.groovy:11)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....SharedInterceptorSpec$makeRequest.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:203)\n\tat pl.allegro....SharedInterceptorSpec.$spock_feature_0_0(SharedInterceptorSpec.groovy:44)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.spockframework.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:196)\n\tat org.spockframework.runtime.model.MethodInfo.lambda$new$0(MethodInfo.java:49)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeatureMethod(PlatformSpecRunner.java:324)\n\tat org.spockframework.runtime.IterationNode.execute(IterationNode.java:50)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:58)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.IterationNode.lambda$around$0(IterationNode.java:67)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunIteration$5(PlatformSpecRunner.java:236)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.junit4.AbstractRuleInterceptor$1.evaluate(AbstractRuleInterceptor.java:46)\n\tat com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:79)\n\tat org.spockframework.junit4.MethodRuleInterceptor.intercept(MethodRuleInterceptor.java:40)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runIteration(PlatformSpecRunner.java:218)\n\tat org.spockframework.runtime.IterationNode.around(IterationNode.java:67)\n\tat org.spockframework.runtime.SimpleFeatureNode.lambda$around$0(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)\n\tat org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:63)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:11)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eFor the first time we force the entire reactive code to finally execute itself and present us with the result,\neven if it is an error this moves us in the right direction.\u003c/p\u003e\n\n\u003ch2 id=\"result\"\u003eResult\u003c/h2\u003e\n\n\u003cp\u003eLike in any good crime story uncovering one mystery presents another.\nA quick \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egrep\u003c/code\u003e shows that there are no calls to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereactor.core.publisher.Mono.subscriberContext\u003c/code\u003e.\nWhere could this call be hiding, if it’s not present in our code?\u003c/p\u003e\n\n\u003cp\u003eThe answer is simple but I assure you that it took us some time to come up with it.\nIf it isn’t in our code and it runs inside our JVM then this must be dependency code.\nThe observant reader is able to spot it from afar. The stack trace confirms where the error lies:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    at reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWe need to patch \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereactor–netty\u003c/code\u003e which in this version still used deprecated code. Referring back to our diff:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eext.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n-        reactorNetty   : '0.9.25.RELEASE',\n]\n\next.versions = [\n+        spring        : '6.0.5',\n+        spock         : '2.4-M1-groovy-4.0',\n+        groovy        : '4.0.9',\n+        reactorNetty  : '1.1.3',\n]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWe escape the dependency hell and are delighted to see the green letters \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBUILD SUCCESSFUL in 24s\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWell this was quite a thrilling journey one doesn’t often embark on.\nThe odd peculiarity of the problem combined with peculiarity of the task provided us with a great challange and satisfaction.\nDependency hell is no joke, but armed with the JDK tools and thinking the problem through, there is no obstacle that could not be overcome.\nNext time your code hangs with no apparent reason this is a perfect opportunity to dust off the swiss army knife of JDK binaries and dig in.\u003c/p\u003e\n","contentSnippet":"As a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.\nThe task is daunting and filled with challenges,\nhowever overall progress is steady and thanks to the modular nature of our code it should end in finite time.\nEveryone who has performed such an upgrade knows that you need to expect the unexpected and at the end of the day prepare for lots of debugging.\nNo amount of migration guide would prepare you for what’s coming in the field.\nIn the words of Donald Rumsfeld there are unknown unknowns and we need to be equipped with the tools to uncover these unknowns and patch them up.\nIn this blog post I’d like to walk you through a process that should show where the application hangs,\nalthough there seems to be nothing wrong with it. I will also show that you don’t always know what code you have – problem known as dependecy hell,\nplace we got quite cosy in during this upgrade.\nThe change\nNote that we keep versions as separate key–value pairs in build.gradle files and reference them in dependencies by key.\nUpdating often means a single line change. The upgrade is trivial and git diff looks like this.\n\next.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n]\n\next.versions = [\n+        spring         : '6.0.5',\n+        spock          : '2.4-M1-groovy-4.0',\n+        groovy         : '4.0.9',\n]\n\n\nNothing much happens. We upgrade Spring and since there are some problems with Spock not working well with the newest Spring\nwe need to upgrade it as well, along with Groovy. This is the easy part.\nNow we run the tests and expect to be either elated with the sight of a successful build or greeted with descriptive error messages\nthat help us quickly patch the issue. Nobody expects anything and in this case this is an unknown unknown.\n\n97% EXECUTING [15m 55s]\n\u003e :platform-libraries-webclient:integrationTest \u003e 1 test completed, 1 failed\n\u003e :platform-libraries-webclient:integrationTest \u003e Executing test pl.allegro....WebClientContextContainerInterceptorSpec\n\n\nAfter 15 minutes we expect the process to end. A quick cross-check with the master branch confirms that tests run and execute in less than a minute.\nSomething is wrong and it’s on us. However, no error is presented. Adding logging does not help, nothing streams to standard output.\nSomething hangs and refuses to budge. When that happenes there is only one way to inspect what is going on and\nthat is to pop the hood open and look into JVM to see what the threads are doing or where they are slacking.\nThread theory\nLet’s interrupt this story with a short summary of threading in JVM. You can skip this chapter if you are familiar with the topic.\nAs the priceless book Java Concurrency in Practice by Brian Goetz et al. teaches us:\n“Threads may block, or pause, for several reasons: waiting for I/O completion, waiting to acquire a lock,\nwaiting to wake up from Thread.sleep, or waiting for the result of a computation in another thread.\nWhen a thread blocks, it is usually suspended and placed in one of the blocked thread states\n(BLOCKED, WAITING, or TIMED_WAITING). (…) blocked thread must wait for an event beyond its control before it can proceed”.\nThis sounds exactly like the situation we are in. So there is hope. Let’s educate ourselves further.\nAnother excerpt that would prove insightful reads as follows:\n“(…) tasks can block for exteded periods of time, even if deadlock is not a possibility.\n(…) One technique that can mitigate the ill effects of long–running tasks is for tasks to use timed resource waits instead of\nunbound waits.”\nThis seems like an answer to our woes. However, two mysteries remain.\nWhere to put the timeout? What the thread is waiting for? To answer these questions we need to inspect the threads in the JVM itself.\nThe investigation\nAt this point we did two things. First we pushed our code to a branch.\nAfter all at any moment our laptops could burst into flames and all the work would go to waste.\nThe remote CI confirmed our suspicion since it also hung. The problem was real and not only confined to the local machine.\nThe second thing is to scout for the offending thread. This is easy with the help of some JDK binaries:\n\njps -lv | grep platform-libraries\n38983 worker.org.gradle.process.internal.worker.GradleWorkerMain -Dorg.gradle.internal.worker.tmpdir=/path/to/code/platform-libraries/platform-libraries-webclient/build/tmp/integrationTest/work -Dorg.gradle.native=false -Xmx512m -Dfile.encoding=UTF-8\n\n\nSo we have the a lvmid – local JVM identifier, which will help us locate the offending thread in jconsole.\nIn the screen below we can see that the thread waits on Mono.block() which is left unbounded in a happy path scenario.\nWell, we are in the worst case so first of all we add a simple timeout Mono.block(Duration.ofSeconds(10)).\n\nThis fails our tests and for the first time the error appears:\n\n\t08:13:39.556 [Test worker] WARN reactor.core.Exceptions - throwIfFatal detected a jvm fatal exception, which is thrown and logged below:\njava.lang.NoSuchMethodError: 'reactor.core.publisher.Mono reactor.core.publisher.Mono.subscriberContext(reactor.util.context.Context)'\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\tat reactor.netty.internal.shaded.reactor.pool.AbstractPool$Borrower.request(AbstractPool.java:427)\n\tat reactor.netty.resources.PooledConnectionProvider$DisposableAcquire.onSubscribe(PooledConnectionProvider.java:533)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool$QueueBorrowerMono.subscribe(SimpleDequePool.java:676)\n\tat reactor.netty.resources.PooledConnectionProvider.disposableAcquire(PooledConnectionProvider.java:219)\n\tat reactor.netty.resources.PooledConnectionProvider.lambda$acquire$3(PooledConnectionProvider.java:183)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:326)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.core.publisher.FluxRetryWhen.subscribe(FluxRetryWhen.java:77)\n\tat reactor.core.publisher.MonoRetryWhen.subscribeOrReturn(MonoRetryWhen.java:46)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:329)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)\n\tat reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2545)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.request(MonoFlatMap.java:194)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.request(FluxOnAssembly.java:649)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2341)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:2215)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onSubscribe(FluxOnAssembly.java:633)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onSubscribe(MonoFlatMap.java:117)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)\n\tat reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoDeferContextual.subscribe(MonoDeferContextual.java:55)\n\tat reactor.core.publisher.Mono.subscribe(Mono.java:4485)\n\tat reactor.core.publisher.Mono.block(Mono.java:1733)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$Trait$Helper.makeRequest(WebClientContextContainerAdapterConfiguration.groovy:22)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$makeRequest.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)\n\tat pl.allegro....AdapterConfiguration$Trait$Helper.makeRequest(AdapterConfiguration.groovy:11)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....SharedInterceptorSpec$makeRequest.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:203)\n\tat pl.allegro....SharedInterceptorSpec.$spock_feature_0_0(SharedInterceptorSpec.groovy:44)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.spockframework.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:196)\n\tat org.spockframework.runtime.model.MethodInfo.lambda$new$0(MethodInfo.java:49)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeatureMethod(PlatformSpecRunner.java:324)\n\tat org.spockframework.runtime.IterationNode.execute(IterationNode.java:50)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:58)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.IterationNode.lambda$around$0(IterationNode.java:67)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunIteration$5(PlatformSpecRunner.java:236)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.junit4.AbstractRuleInterceptor$1.evaluate(AbstractRuleInterceptor.java:46)\n\tat com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:79)\n\tat org.spockframework.junit4.MethodRuleInterceptor.intercept(MethodRuleInterceptor.java:40)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runIteration(PlatformSpecRunner.java:218)\n\tat org.spockframework.runtime.IterationNode.around(IterationNode.java:67)\n\tat org.spockframework.runtime.SimpleFeatureNode.lambda$around$0(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)\n\tat org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:63)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:11)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\n\nFor the first time we force the entire reactive code to finally execute itself and present us with the result,\neven if it is an error this moves us in the right direction.\nResult\nLike in any good crime story uncovering one mystery presents another.\nA quick grep shows that there are no calls to reactor.core.publisher.Mono.subscriberContext.\nWhere could this call be hiding, if it’s not present in our code?\nThe answer is simple but I assure you that it took us some time to come up with it.\nIf it isn’t in our code and it runs inside our JVM then this must be dependency code.\nThe observant reader is able to spot it from afar. The stack trace confirms where the error lies:\n\n    at reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\n\nWe need to patch reactor–netty which in this version still used deprecated code. Referring back to our diff:\n\next.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n-        reactorNetty   : '0.9.25.RELEASE',\n]\n\next.versions = [\n+        spring        : '6.0.5',\n+        spock         : '2.4-M1-groovy-4.0',\n+        groovy        : '4.0.9',\n+        reactorNetty  : '1.1.3',\n]\n\n\nWe escape the dependency hell and are delighted to see the green letters BUILD SUCCESSFUL in 24s.\nSummary\nWell this was quite a thrilling journey one doesn’t often embark on.\nThe odd peculiarity of the problem combined with peculiarity of the task provided us with a great challange and satisfaction.\nDependency hell is no joke, but armed with the JDK tools and thinking the problem through, there is no obstacle that could not be overcome.\nNext time your code hangs with no apparent reason this is a perfect opportunity to dust off the swiss army knife of JDK binaries and dig in.","guid":"https://blog.allegro.tech/2023/05/debugging-hangs.html","categories":["tech","java","jvm","debugging","dependency hell"],"isoDate":"2023-05-30T22:00:00.000Z","thumbnail":"images/post-headers/java.png"},{"title":"Trust no one, not even your training data! Machine learning from noisy data","link":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","pubDate":"Tue, 18 Apr 2023 00:00:00 +0200","authors":{"author":[{"name":["Łukasz Rączkowski"],"photo":["https://blog.allegro.tech/img/authors/lukasz.raczkowski.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.raczkowski"]},{"name":["Aleksandra Osowska-Kurczab"],"photo":["https://blog.allegro.tech/img/authors/aleksandra.osowska-kurczab.jpg"],"url":["https://blog.allegro.tech/authors/aleksandra.osowska-kurczab"]},{"name":["Jacek Szczerbiński"],"photo":["https://blog.allegro.tech/img/authors/jacek.szczerbinski.jpg"],"url":["https://blog.allegro.tech/authors/jacek.szczerbinski"]},{"name":["Klaudia Nazarko"],"photo":["https://blog.allegro.tech/img/authors/klaudia.nazarko.jpg"],"url":["https://blog.allegro.tech/authors/klaudia.nazarko"]},{"name":["Kalina Kobus"],"photo":["https://blog.allegro.tech/img/authors/kalina.kobus.jpg"],"url":["https://blog.allegro.tech/authors/kalina.kobus"]}]},"content":"\u003cul\u003e\n  \u003cli\u003eLabel noise is ever-present in machine learning practice.\u003c/li\u003e\n  \u003cli\u003eAllegro datasets are no exception.\u003c/li\u003e\n  \u003cli\u003eWe compared 7 methods for training classifiers robust to label noise.\u003c/li\u003e\n  \u003cli\u003eAll of them improved the model’s performance on noisy datasets.\u003c/li\u003e\n  \u003cli\u003eSome of the methods decreased the model’s performance in the absence of label noise.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"what-is-label-noise-and-why-does-it-matter\"\u003eWhat is label noise and why does it matter?\u003c/h2\u003e\n\n\u003cp\u003eIn the scope of supervised machine learning, specifically in classification tasks, the problem of label noise\nis of critical importance. It involves cases of incorrectly labelled training data. For example, let’s say that\nwe want to train a classification model to distinguish cats from dogs. For that purpose, we compose a training\ndataset with images labelled as either cat or dog. The labelling process is usually performed by human annotators,\nwho almost certainly produce some labelling errors. Unfortunately, human annotators can be confused by poor image\nquality, ambiguous image contents, or simply click the wrong item. As such, we inevitably end up with a dataset\nwhere some percentage of cats are labelled as dogs and vice versa (\u003ca href=\"#figure1\"\u003e\u003cstrong\u003eFigure 1\u003c/strong\u003e\u003c/a\u003e).\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure1\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Cats and dogs are equally nice.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure1-label-noise-example.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003eFigure 1. An example of label noise in a binary classification dataset.\u003c/b\u003e Some images in both categories were mislabelled by human annotators, which introduces noise to the training dataset. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eConsequently, the model trained with such data learns partially wrong associations, which then can lead to incorrect\npredictions for new images. The more label noise we have, the more we confuse the model during training. We can\nmeasure this by evaluating the classification error on a held-out test dataset (\u003ca href=\"#figure2\"\u003e\u003cstrong\u003eFigure 2\u003c/strong\u003e\u003c/a\u003e). It is clear\nthat for high noise levels, it is very hard to recover the true training signal from the corrupted training data.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure2\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Oh no, please, not the noise!\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure2-test-accuracy.png\" style=\"width:70%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:70%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 2. Test accuracy as a function of label noise percentage. \u003c/b\u003e The X axis indicates the ratio of mislabelled to correctly labelled examples. The dataset used here was ImageNet, corrupted with synthetic label noise. Image source: \u003csup id=\"fnref:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHow can this problem be mitigated? One approach is to simply put more effort into the labelling process — we can let\nmultiple annotators label each data point and then evaluate the cross-annotator agreement. With enough time and effort,\nwe hope to obtain a dataset free of label noise. However, in practice this approach is rarely feasible due to large\nvolumes of training data and the need for efficient turnaround of machine learning projects. Consequently, we need\na different approach for handling corrupted training data, \u003cem\u003ei.e.\u003c/em\u003e ML models robust to label noise.\u003c/p\u003e\n\n\u003cp\u003eIn the context of this blog post, we define robustness as the model’s ability to efficiently learn in the presence\nof corrupted training data. In other words, a robust model can recover the correct training signal and ignore\nthe noise, so that it does not overfit to the corrupted traning set and can generalise during prediction. A major\nchallenge in this regard is the difficulty to estimate the proportion of label noise in real-world data. As such,\nrobust models are expected to handle varying amounts of label noise.\u003c/p\u003e\n\n\u003ch2 id=\"how-to-train-a-robust-classifier\"\u003eHow to train a robust classifier?\u003c/h2\u003e\n\n\u003cp\u003eWe can improve the robustness of deep neural networks (DNNs) with a few tips and tricks presented in the recent\nliterature on \u003cem\u003eLearning from Noisy Data\u003c/em\u003e. In general, there are three approaches to boosting the model’s resistance\nto noisy labels (\u003ca href=\"#figure3\"\u003e\u003cstrong\u003eFigure 3\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eRobust loss function\u003c/strong\u003e boosting the training dynamics in the presence of noise.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eImplicit regularisation\u003c/strong\u003e of the network aiming at decreasing the impact of noisy labels.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eFiltration of noisy data samples\u003c/strong\u003e during the training or at the pre-training stage.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure3\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Flat-topped pyramids are better than sharp-topped ones.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure3-robustness-strategies.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 3. Strategies for robustness. \u003c/b\u003e In this blog post, we focused on two main approaches improving model robustness: utilisation of a robust loss function and implicit regularisation.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the scope of this blog post, we present seven different methods that are strong baselines for improving\nthe generalisation of classifiers in the presence of label noise.\u003c/p\u003e\n\n\u003ch3 id=\"robust-loss-function\"\u003eRobust loss function\u003c/h3\u003e\n\n\u003ch4 id=\"self-paced-learning-spl\"\u003eSelf-Paced Learning (SPL)\u003c/h4\u003e\n\u003cp\u003eThe authors of \u003cstrong\u003eSelf-Paced Learning\u003c/strong\u003e\u003csup id=\"fnref:2\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:2\" class=\"footnote\" rel=\"footnote\"\u003e2\u003c/a\u003e\u003c/sup\u003e noticed that large per-sample loss might be an indication of label\ncorruption, especially in the latter stages of training. Clean labels should be easy to learn, while corrupted labels\nwould appear as difficult, resulting in a high per-sample loss.\u003c/p\u003e\n\n\u003cp\u003eSPL proposes to exclude some predefined ratio of examples from the batch depending on their per-sample loss values\n(\u003ca href=\"#figure4\"\u003e\u003cstrong\u003eFigure 4a\u003c/strong\u003e\u003c/a\u003e). Usually, the ratio is set as the estimated noise level in the dataset.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure4\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"PRL makes everything equal.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure4-loss-filtration.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 4. Comparison of loss filtration methods (SPL, PRL and CCE, see below). \u003c/b\u003e While SPL and PRL exclude samples from loss calculation, CCE decreases the impact of potentially corrupted labels by clipping the per-sample loss values. Orange colour indicates candidate noisy samples. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch4 id=\"provably-robust-learning-prl\"\u003eProvably Robust Learning (PRL)\u003c/h4\u003e\n\n\u003cp\u003e\u003cstrong\u003eProvably Robust Learning\u003c/strong\u003e\u003csup id=\"fnref:3\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:3\" class=\"footnote\" rel=\"footnote\"\u003e3\u003c/a\u003e\u003c/sup\u003e derives from the ideas presented in the SPL paper, but the authors state that\ncorrupted labels should be detected depending on the gradient norm, instead of per-sample loss (\u003ca href=\"#figure4\"\u003e\u003cstrong\u003eFigure 4b\u003c/strong\u003e\u003c/a\u003e).\nThe underlying intuition is that corrupted samples provoke the optimiser to make inadequately large steps\nin the optimisation space. The rest of the logic is the same as in SPL.\u003c/p\u003e\n\n\u003ch4 id=\"clipped-cross-entropy-cce\"\u003eClipped Cross-Entropy (CCE)\u003c/h4\u003e\n\n\u003cp\u003eRejection of samples might not be optimal from the training’s point of view, because DNNs need vast amounts of data\nto be able to generalise properly. Therefore, \u003cstrong\u003eClipped Cross-Entropy\u003c/strong\u003e doesn’t exclude the most contributing samples\nfrom the batch, but rather alleviates their impact by clipping the per-sample loss to a predefined value (\u003ca href=\"#figure4\"\u003e\u003cstrong\u003eFigure 4c\u003c/strong\u003e\u003c/a\u003e).\u003c/p\u003e\n\n\u003ch4 id=\"early-learning-regularisation-elr\"\u003eEarly Learning Regularisation (ELR)\u003c/h4\u003e\n\n\u003cp\u003eIt has been recently observed that DNNs first fit clean samples, and then start memorising the noisy ones. This\nphenomenon reduces the generalisation properties of the model, distracting it from learning true patterns present\nin the data. \u003cstrong\u003eEarly Learning Regularisation\u003c/strong\u003e\u003csup id=\"fnref:4\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:4\" class=\"footnote\" rel=\"footnote\"\u003e4\u003c/a\u003e\u003c/sup\u003e mitigates memorisation with two tricks:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eTemporal ensembling\u003c/em\u003e of targets: during the training step \\([k]\\), the original targets \\(\\pmb{\\text{t}}\\) are mixed\nwith the model’s predictions \\(\\pmb{\\text{p}}\\) from previous training steps. This prevents the gradient from diverging\nhugely between subsequent steps. This trick is well-known in semi-supervised learning\u003csup id=\"fnref:5\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:5\" class=\"footnote\" rel=\"footnote\"\u003e5\u003c/a\u003e\u003c/sup\u003e:\u003c/li\u003e\n\u003c/ul\u003e\n\n\\[\\pmb{\\text{t}}^{[k]} = \\left(\\beta\\ \\pmb{\\text{t}}^{[k-1]} + (1-\\beta)\\ \\pmb{\\text{p}}^{[k-1]}\\right)\\]\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eExplicit regularisation\u003c/em\u003e: an extra term is added to the default cross-entropy loss \\(\\mathcal{L}_{CE}(\\Theta)\\) that\nallows refinement of the early-learnt concepts, but penalises drastically contradicting predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\n\\[\\mathcal{L}_{ELR}(\\Theta)=\\mathcal{L}_{CE}(\\Theta) + \\frac{\\lambda}{n} \\sum\\text{log}(1-\\langle \\pmb{\\text{p}}, \\pmb{\\text{t}} \\rangle)\\]\n\n\u003cp\u003eThus, the gradient gets a boost for the clean samples, while the impact of noisy samples is neutralised\nby temporal ensembling.\u003c/p\u003e\n\n\u003ch4 id=\"jensen-shannon-divergence-loss-jsd\"\u003eJensen-Shannon Divergence Loss (JSD)\u003c/h4\u003e\n\n\u003cp\u003eThe authors of \u003cstrong\u003eJensen-Shannon Divergence Loss\u003c/strong\u003e \u003csup id=\"fnref:6\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:6\" class=\"footnote\" rel=\"footnote\"\u003e6\u003c/a\u003e\u003c/sup\u003e take yet another approach to loss construction,\nwhich is inspired by an empirical comparison between Cross-Entropy (CE) and Mean Absolute Error (MAE) loss. CE is known\nfor its fast convergence and brilliant training dynamics, while MAE provides spectacular robustness at the price\nof slow convergence.\u003c/p\u003e\n\n\u003cp\u003eEnglesson et al. came up with the idea to use Jensen-Shannon Divergence, which is a proven generalisation of CE\nand MAE loss (\u003ca href=\"#figure5\"\u003e\u003cstrong\u003eFigure 5\u003c/strong\u003e\u003c/a\u003e). JSD uses Kullback-Leibler Divergence \\(\\text{D}_{\\text{KL}}\\) between the target\nlabels \\(\\pmb{y}\\) and predictions of the model \\(f(\\pmb{x})\\) vs. their averaged distribution \\(\\pmb{m}\\). Summing up, one\ncan think of JSD as a CE with a robustness boost, or MAE with improved convergence.\u003c/p\u003e\n\n\\[\\mathcal{L}_{\\text{JS}}(\\pmb{x}, \\pmb{y}) = \\frac{1}{Z} \\left( \\pi_1 \\text{D}_{\\text{KL}}(\\pmb{y}||\\pmb{m}) + (1-\\pi_1) \\text{D}_{\\text{KL}}(f(\\pmb{x})||\\pmb{m}) \\right)\\]\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure5\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Big proportion of pie makes your weight high.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure5-jsd.png\" style=\"width:70%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:70%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 5. JSD as a generalisation of CE and MAE loss. \u003c/b\u003e Depending on the parameter \\(\\pi_1\\), JSD resembles CE or MAE. Image source: \u003csup id=\"fnref:6:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:6\" class=\"footnote\" rel=\"footnote\"\u003e6\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"implicit-regularisation\"\u003eImplicit regularisation\u003c/h3\u003e\n\n\u003ch4 id=\"co-teaching-ct\"\u003eCo-teaching (CT)\u003c/h4\u003e\n\n\u003cp\u003eIn \u003cstrong\u003eco-teaching\u003c/strong\u003e \u003csup id=\"fnref:7\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:7\" class=\"footnote\" rel=\"footnote\"\u003e7\u003c/a\u003e\u003c/sup\u003e, we simultaneously train two independent DNNs (\u003ca href=\"#figure6\"\u003e\u003cstrong\u003eFigure 6\u003c/strong\u003e\u003c/a\u003e), and let them\nexchange examples during the training. The \u003cem\u003etraining feed\u003c/em\u003e (learning samples) provided by the peer network should\nideally consist only of clean samples. In CT, each network predicts which samples are clean and provides them to its\ncounterpart. Deciding whether a sample is clean relies on the trick known from SPL: the sample’s label is probably\nclean if its per-sample loss is low.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure6\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Co-operation is key to success, especially when you want to reduce noise in your garage band.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure6-co-teaching.png\" style=\"width:50%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:50%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 6. Exchange of training feed in co-teaching. \u003c/b\u003e Two peer networks exchange samples that are expected\nto be clean from noise. Image source: \u003csup id=\"fnref:7:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:7\" class=\"footnote\" rel=\"footnote\"\u003e7\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eCo-teaching is one of the most popular and universal baselines in the domain of learning from noisy data. It has\nwell-established empirical results, offers good performance even in extreme noise scenarios and can be simply\nintegrated into almost any architecture or downstream task. Unfortunately, it also has a few downsides. Firstly, there\nis no theoretical guarantee that such a training setup will eventually converge. Secondly, we may end up with\na consensus between the two networks, causing them to produce identical training feeds, and making the CT redundant.\u003c/p\u003e\n\n\u003ch4 id=\"mixup\"\u003eMixup\u003c/h4\u003e\n\n\u003cp\u003e\u003cstrong\u003eMixup\u003c/strong\u003e\u003csup id=\"fnref:8\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:8\" class=\"footnote\" rel=\"footnote\"\u003e8\u003c/a\u003e\u003c/sup\u003e is a simple augmentation scheme that enforces linear behaviour of the model for in-between\ntraining samples (\u003ca href=\"#figure7\"\u003e\u003cstrong\u003eFigure 7\u003c/strong\u003e\u003c/a\u003e). It linearly combines two training samples \\((\\pmb{x}_i, \\pmb{y}_i)\\)\nand \\((\\pmb{x}_j, \\pmb{y}_j)\\) with weight \\(\\lambda\\) sampled from the \u003cem\u003eBeta\u003c/em\u003e distribution. It results in a new augmented sample with mixed input features \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\):\u003c/p\u003e\n\n\\[\\pmb{x}_{aug} = \\lambda \\pmb{x}_i + (1 - \\lambda)\\pmb{x}_j \\\\\n\\pmb{y}_{aug} = \\lambda \\pmb{y}_i + (1 - \\lambda)\\pmb{y}_j \\\\\\]\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure7\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"When you can’t decide between cats and dogs, why don’t have both?\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure7-mixup.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 7. Augmentation through mixup. \u003c/b\u003e Two samples \\(i\\) and \\(j\\) are linearly combined into a synthetic image \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\). This new augmented input encourages the model to linearly interpolate the predictions between the original samples. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe method is a simple, universal, yet very effective approach. It yields good empirical results while adding\nno severe computational overhead.\u003c/p\u003e\n\n\u003ch2 id=\"cleaning-up-allegro\"\u003eCleaning up Allegro\u003c/h2\u003e\n\n\u003cp\u003eEvery offer has its right place at \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e, belonging to one out of over 23,000 categories. The category structure\nis a tree consisting of:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ethe root (Allegro),\u003c/li\u003e\n  \u003cli\u003eup to 7 levels of intermediate nodes (departments, metacategories, \u003cem\u003eetc.\u003c/em\u003e) — over 2,600 nodes in total,\u003c/li\u003e\n  \u003cli\u003eover 23,000 leaves.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOffers located in wrong categories are hard to find and hard to buy. As such, we need a way to properly assign offers\nto correct category leaves. To this end, our Machine Learning Research team has developed a category classifier\nfor Allegro offers.\u003c/p\u003e\n\n\u003cp\u003eThe model in question is a large language model pre-trained on the Allegro catalogue (see more\nin \u003ca href=\"https://www.youtube.com/watch?v=6T-R4kgIbBs\u0026amp;list=PLzveSKBX_3N7yPb4ErB5HJ83eB6XvH37C\u0026amp;index=20\"\u003e\u003ci\u003eDo you speak Allegro?\u003c/i\u003e\u003c/a\u003e) and fine-tuned for offer classification. Specifically, the downstream task here is extreme text classification: each offer is represented by text (title) and is classified into over 23,000 categories — hence the word \u003ci\u003eextreme\u003c/i\u003e.\u003c/p\u003e\n\n\u003cp\u003eClassification is particularly challenging for offers listed in ambiguous categories such as \u003cem\u003eOther, Accessories, etc.\u003c/em\u003e\nThese categories are broad and hard to navigate, as they contain a wide variety of products. Most of those products\nactually belong to some well-defined categories, but the merchant couldn’t find the right place for those offers\nat the time of their listing, because of the very rich taxonomy of the category tree. Consequently, we decided\nto clean up the offers in ambiguous categories.\u003c/p\u003e\n\n\u003cp\u003eHere’s the setup (\u003ca href=\"#figure8\"\u003e\u003cstrong\u003eFigure 8\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eWe train the category classifier on offers in well-defined categories: the model learns what lies where at Allegro.\u003c/li\u003e\n  \u003cli\u003eNext, we run inference on offers in ambiguous categories: the model moves the offers to their right destination.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eNote that this task is subject to domain shift: the assortment listed in these ambiguous categories may be harder\nto categorise than the regular assortment in other categories.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure8\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Always trust your friendly neighbourhood language model.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure8-category-classifier.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 8. Category classifier: training \u0026amp; inference. \u003c/b\u003e The model is trained on offers listed in well-defined categories. Then, it is used to move offers from ambiguous categories (\u003ci\u003eOther, Accessories, etc.\u003c/i\u003e) to the well-defined categories. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"real-world-label-noise-at-allegro\"\u003eReal-world label noise at Allegro\u003c/h3\u003e\n\u003cp\u003eThe training set (offers in well-defined categories) is not 100% correct, for several reasons (\u003ca href=\"#figure9\"\u003e\u003cstrong\u003eFigure 9\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ethe merchant may have put the offer in the wrong category,\u003c/li\u003e\n  \u003cli\u003ethere are several similar categories in the catalogue,\u003c/li\u003e\n  \u003cli\u003ethere is no appropriate category for a given offer,\u003c/li\u003e\n  \u003cli\u003ethe taxonomy of the Allegro category tree changes over time.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure9\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"AHHH, FRESH MEAT.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure9-mislabelled-offers.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 9. Examples of mislabelled offers. \u003c/b\u003e With over 23,000 categories at Allegro, listing each offer in its best-matching category can be challenging for merchants. Hence, label noise is an inherent feature of our training dataset. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe ML model is prone to memorisation of the wrong labels in the training set, \u003cem\u003ei.e.\u003c/em\u003e overfitting. These errors will\nlikely be reproduced at prediction time. Our goal is to train a robust classifier that will learn the true patterns\nand ignore the mislabelled training instances.\u003c/p\u003e\n\n\u003cp\u003eThe training methods described in the previous section were developed and evaluated on computer vision tasks,\n\u003cem\u003ee.g.\u003c/em\u003e image classification, into a relatively small number of categories. Here, we face the problem of extreme text\nclassification. Thus, we need to adapt those methods for textual input and find out which concepts transfer well between\nthe two domains.\u003c/p\u003e\n\n\u003ch3 id=\"synthetic-label-noise\"\u003eSynthetic label noise\u003c/h3\u003e\n\n\u003cp\u003eTo evaluate the model’s robustness experimentally, we need to know \u003cem\u003ea priori\u003c/em\u003e which training instances were\nmislabelled. For that, we use a generator of controllable noise. The experimental setup consists of five steps\n(\u003ca href=\"#figure10\"\u003e\u003cstrong\u003eFigure 10\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003edumping a clean dataset from a curated pool of offers that are \u003cem\u003ecertainly\u003c/em\u003e in the right place,\u003c/li\u003e\n  \u003cli\u003esplitting it into training, validation and test sets,\u003c/li\u003e\n  \u003cli\u003eapplication of synthetic noise to 20% of instances in the training and validation sets (changing the offer’s category\nto a wrong one),\u003c/li\u003e\n  \u003cli\u003etraining the model on the noisy dataset,\u003c/li\u003e\n  \u003cli\u003etesting the model on a held-out fraction of the clean dataset.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure10\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Staying clean has many benefits. Stay clean kids!\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure10-datasets.png\" style=\"width:70%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:70%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 10. Testing the model’s robustness. \u003c/b\u003e The full dataset of clean instances (offers with true category labels) is split into training, validation and test sets. Next, label noise is introduced to the training and validation sets and the model is trained. The model is tested on a held-out fraction of the clean dataset. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThis setup lets us answer the following question:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eHow much does the noise in the training set hurt the model’s performance on the clean test set?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis way, we can evaluate different methods of training classifiers under label noise and choose the most robust\nclassifier, according to accuracy on the test set.\u003c/p\u003e\n\n\u003ch2 id=\"and-it-works\"\u003eAnd… it works!\u003c/h2\u003e\n\n\u003cp\u003eBelow we present the results of experiments for 1.3M offers listed in the \u003cem\u003eConstruction Work \u0026amp; Equipment\u003c/em\u003e category.\nSymmetric noise was applied to 20% of the training set. This means that the category labels of that percentage\nof offers were changed to different randomly chosen labels. We evaluated the 7 training methods outlined above\nand compared them to the baseline: classification with cross-entropy loss.\u003c/p\u003e\n\n\u003ch3 id=\"baseline-memorising-doesnt-pay-off\"\u003eBaseline: Memorising doesn’t pay off\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eHow does the presence of noise impact the baseline model?\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThe validation curves for non-corrupted samples clearly show the severe impact of noisy labels on the model’s\nperformance (\u003ca href=\"#figure11\"\u003e\u003cstrong\u003eFigure 11\u003c/strong\u003e\u003c/a\u003e). In the early stage of training, the performance of the model trained\non noisy data is on par with the metrics of the model trained on clean data. Yet, starting from the 4th epoch,\nthe wrong labels in the noisy dataset appear to prevent the model from discovering the true patterns in the training\ndata, resulting in a 5 p.p. drop in accuracy at the end of the training. We attribute this drop to the \u003cem\u003ememorisation\u003c/em\u003e\nof the wrong labels: instead of refining the originally learnt concepts, the network starts to overfit to the noisy\nlabels. The labels memorised for particular offers don’t help with classifying previously unseen offers at test time.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure11\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Absolute noise corrupts absolutely.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure11-baseline-degradation.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003eFigure 11. Degradation of the baseline model in the presence of noise.\u003c/b\u003e The 20% synthetic noise degrades the model throughout the training. In the end, the model trained on the corrupted dataset exhibits 5 p.p. lower accuracy in comparison to its clean counterpart \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"towards-robust-classification\"\u003eTowards robust classification\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eDoes robustness imply underfitting?\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eTo verify if the evaluated methods have any effect on the model’s performance when there is no noise in the training\ndata, we tested all of them on a clean dataset without any synthetic noise.\u003c/p\u003e\n\n\u003cp\u003eIn the absence of corrupted data, three of the tested methods (SPL, PRL and CT) are effectively reduced to the baseline\nCross-Entropy. Therefore, the accuracy for those methods was exactly the same as for the baseline (\u003ca href=\"#table1\"\u003e\u003cstrong\u003eTable 1\u003c/strong\u003e\u003c/a\u003e).\nFor mixup, the difference from the baseline was within the standard deviation range, so it was marked as no improvement\nas well.\u003c/p\u003e\n\n\u003cp\u003eFor CCE and JSD the performance degraded, but only slightly — by 0.04 p.p. for the former and 0.34 p.p. for the latter.\nThis drop is an acceptable compromise considering the robustness to noise that these methods enable (see below).\u003c/p\u003e\n\n\u003cp\u003eELR was the only method that improved upon the baseline, by 0.07 p.p. As ELR relies on temporal ensembling, which\ndiminishes the impact of corrupted samples during training, we hypothesise that our clean dataset contained a small\nnumber of mislabelled examples. Such paradoxes are a frequent case in machine learning practice, even for renowned\nbenchmark datasets like CIFAR-100\u003csup id=\"fnref:9\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:9\" class=\"footnote\" rel=\"footnote\"\u003e9\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003ca id=\"table1\"\u003e\u003c/a\u003e\u003cstrong\u003eTable 1.\u003c/strong\u003e Test accuracy scores of the models trained on the clean and corrupted\n(20% synthetic noise) datasets for the 8 training methods. Light red highlight indicates deterioration in comparison\nto the baseline, while light blue denotes improvement. \u003cem\u003eNotation\u003c/em\u003e: (mean \\(\\pm\\) std)% from 5 independently seeded runs.\u003c/p\u003e\n\u003ctable\u003e\n    \u003cthead\u003e\n        \u003ctr\u003e\n            \u003cth rowspan=\"2\" colspan=\"2\" style=\"text-align:center\"\u003eMethod\u003c/th\u003e\n            \u003cth style=\"text-align:center\" colspan=\"2\"\u003eTest accuracy [%]\u003c/th\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003cth style=\"text-align:center\"\u003eclean dataset\u003c/th\u003e\n            \u003cth style=\"text-align:center\"\u003enoisy dataset\u003c/th\u003e\n        \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n        \u003ctr\u003e\n            \u003ctd colspan=\"2\" style=\"border-bottom-width: thick;text-align:center;font-weight:bold\"\u003eBaseline\u003c/td\u003e\n            \u003ctd style=\"border-bottom-width: thick;\"\u003e90.26 ± 0.03\u003c/td\u003e\n            \u003ctd style=\"border-bottom-width: thick;\"\u003e85.31 ± 0.08\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd rowspan=\"5\" style=\"font-weight:bold;transform: rotate(180deg);writing-mode: vertical-rl;text-align: center;vertical-align: middle;width: 3em\"\u003e\n                Robust loss\u003cbr /\u003efunction\n            \u003c/td\u003e\n            \u003ctd\u003eSelf-Paced Learning (SPL)\u003c/td\u003e\n            \u003ctd\u003e90.26 ± 0.03\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e88.51 ± 0.02\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eProvably Robust Learning (PRL)\u003c/td\u003e\n            \u003ctd\u003e90.26 ± 0.03\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e88.31 ± 0.02\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eClipped Cross-Entropy (CCE)\u003c/td\u003e\n            \u003ctd style=\"background:#ffdecb;color:black\"\u003e90.22 ± 0.03\u003c/td\u003e\n            \u003ctd style=\"font-weight:bold;background:#E1F4F4;color:black\"\u003e89.51 ± 0.01\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eEarly Learning Regularisation (ELR)\u003c/td\u003e\n            \u003ctd style=\"font-weight:bold;background:#E1F4F4;color:black\"\u003e90.33 ± 0.01\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e89.29 ± 0.03\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eJensen-Shannon Divergence (JSD) \u003c/td\u003e\n            \u003ctd style=\"background:#ffdecb;color:black\"\u003e89.92 ± 0.02\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e89.24 ± 0.01\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd rowspan=\"2\" style=\"font-weight:bold;transform: rotate(180deg);writing-mode: vertical-rl;white-space: wrap;text-align: center;vertical-align: middle;width: 3em;height: 8em\"\u003e\n                Implicit regularisation\n            \u003c/td\u003e\n            \u003ctd\u003eCo-teaching (CT)\u003c/td\u003e\n            \u003ctd\u003e90.26 ± 0.03\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e88.72 ± 0.03\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eMixup\u003c/td\u003e\n            \u003ctd\u003e90.27 ± 0.02\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e86.02 ± 0.06\u003c/td\u003e\n        \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\u003cstrong\u003eRobust classification results\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eAll methods discussed in this study improved the model’s performance on the noisy dataset when compared to the baseline\n(\u003ca href=\"#table1\"\u003e\u003cstrong\u003eTable 1\u003c/strong\u003e\u003c/a\u003e). The best results were obtained with CCE (+4.2 p.p.), ELR (+3.98 p.p.) and JSD (+3.93 p.p.).\nCT, SPL, PRL performed a bit worse, but still proved to be quite robust, improving upon the baseline by 3.41 p.p.,\n3.2 p.p. and 3.0 p.p., respectively.\u003c/p\u003e\n\n\u003cp\u003eMixup is a clear outlier — while it does improve upon the baseline by 0.71 p.p., this increase is noticeably smaller\nthan for the other evaluated methods. Our interpretation is that the linear augmentation at the heart of this method\nregularises the DNN, but does not address label noise \u003cem\u003eper se\u003c/em\u003e. Mixup treats all samples equally, even if their labels\nare corrupted. The marginal improvement upon the baseline is evident in the validation accuracy training curve\n(\u003ca href=\"#figure12\"\u003e\u003cstrong\u003eFigure 12\u003c/strong\u003e\u003c/a\u003e). Mixup starts to overfit around the 5th epoch, similarly to the baseline, and unlike all\nthe other methods.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure12\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Mixing it up doesn’t always work as intended.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure12-validation-accuracy.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003eFigure 12. Validation accuracy during training.\u003c/b\u003e Validation accuracy for all methods was measured during training. It is evident that the best methods are CCE, ELR and JSD, with CT, PRL and SPL trailing slightly behind. Mixup behaves similarly to the baseline. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eThe problem of label noise is unavoidable in machine learning practice, and Allegro datasets are no exception.\nFortunately, there exist numerous methods that diminish the impact of label noise on prediction performance\nby increasing the robustness of machine learning models. In our experiments we implemented 7 of those methods\nand showed that they increase prediction accuracy in the presence of 20% synthetic noise when compared to the baseline\n(Cross-Entropy loss), most of them by a significant margin. The simple Clipped Cross-Entropy proved to be the best,\nwith an accuracy score of 89.51% (increase of 4.2 p.p. vs the baseline trained with noisy labels). This result is very\nclose to the baseline trained with clean labels (90.26%). Thus, we showed that for the case of 20% synthetic label\nnoise, it is possible to increase robustness so that the impact of label noise is negligible.\u003c/p\u003e\n\n\u003cp\u003eThese experiments are only a first step in making classifiers at Allegro robust to label noise. The case of synthetic\nnoise presented here is not very realistic: real-world label noise tends to be instance-dependent,\ni.e. it is influenced by individual sample features. As such, we plan to further evaluate the methods for increasing\nmodel robustness with a real-world dataset perturbed by instance-dependent noise.\u003c/p\u003e\n\n\u003cp\u003eIf you’d like to know more about label noise and model robustness, please refer to the papers listed below.\u003c/p\u003e\n\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n  \u003col\u003e\n    \u003cli id=\"fn:1\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1705.10694\"\u003e\u003cem\u003eDeep Learning is Robust to Massive Label Noise\u003c/em\u003e, Rolnick et al., 2018\u003c/a\u003e \u003ca href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:2\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://papers.nips.cc/paper/2010/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html\"\u003e\u003cem\u003eSelf-Paced Learning for Latent Variable Models\u003c/em\u003e, Kumar et al., 2010\u003c/a\u003e \u003ca href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:3\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2102.06735\"\u003e\u003cem\u003eLearning Deep Neural Networks under Agnostic Corrupted Supervision\u003c/em\u003e, Liu et al., 2021\u003c/a\u003e \u003ca href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:4\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2007.00151\"\u003e\u003cem\u003eEarly-Learning Regularization Prevents Memorization of Noisy Labels\u003c/em\u003e, Liu et al., 2020\u003c/a\u003e \u003ca href=\"#fnref:4\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:5\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1610.02242\"\u003e\u003cem\u003eTemporal Ensembling for Semi-Supervised Learning\u003c/em\u003e, Laine et al., 2017\u003c/a\u003e \u003ca href=\"#fnref:5\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:6\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2105.04522\"\u003e\u003cem\u003eGeneralized Jensen-Shannon Divergence Loss for Learning with Noisy Labels\u003c/em\u003e, Englesson et al., 2021\u003c/a\u003e \u003ca href=\"#fnref:6\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e \u003ca href=\"#fnref:6:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e2\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:7\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1804.06872\"\u003e\u003cem\u003eCo-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\u003c/em\u003e, Han et al., 2018\u003c/a\u003e \u003ca href=\"#fnref:7\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e \u003ca href=\"#fnref:7:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e2\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:8\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1710.09412\"\u003e\u003cem\u003emixup: Beyond Empirical Risk Minimization\u003c/em\u003e, Zhang et al., 2018\u003c/a\u003e \u003ca href=\"#fnref:8\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:9\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2103.14749\"\u003e\u003cem\u003ePervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks\u003c/em\u003e, Northcutt et al., 2021\u003c/a\u003e \u003ca href=\"#fnref:9\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n  \u003c/ol\u003e\n\u003c/div\u003e\n","contentSnippet":"Label noise is ever-present in machine learning practice.\nAllegro datasets are no exception.\nWe compared 7 methods for training classifiers robust to label noise.\nAll of them improved the model’s performance on noisy datasets.\nSome of the methods decreased the model’s performance in the absence of label noise.\nWhat is label noise and why does it matter?\nIn the scope of supervised machine learning, specifically in classification tasks, the problem of label noise\nis of critical importance. It involves cases of incorrectly labelled training data. For example, let’s say that\nwe want to train a classification model to distinguish cats from dogs. For that purpose, we compose a training\ndataset with images labelled as either cat or dog. The labelling process is usually performed by human annotators,\nwho almost certainly produce some labelling errors. Unfortunately, human annotators can be confused by poor image\nquality, ambiguous image contents, or simply click the wrong item. As such, we inevitably end up with a dataset\nwhere some percentage of cats are labelled as dogs and vice versa (Figure 1).\n\n    \n    \n Figure 1. An example of label noise in a binary classification dataset. Some images in both categories were mislabelled by human annotators, which introduces noise to the training dataset. \nConsequently, the model trained with such data learns partially wrong associations, which then can lead to incorrect\npredictions for new images. The more label noise we have, the more we confuse the model during training. We can\nmeasure this by evaluating the classification error on a held-out test dataset (Figure 2). It is clear\nthat for high noise levels, it is very hard to recover the true training signal from the corrupted training data.\n\n    \n    \n Figure 2. Test accuracy as a function of label noise percentage.  The X axis indicates the ratio of mislabelled to correctly labelled examples. The dataset used here was ImageNet, corrupted with synthetic label noise. Image source: 1.\nHow can this problem be mitigated? One approach is to simply put more effort into the labelling process — we can let\nmultiple annotators label each data point and then evaluate the cross-annotator agreement. With enough time and effort,\nwe hope to obtain a dataset free of label noise. However, in practice this approach is rarely feasible due to large\nvolumes of training data and the need for efficient turnaround of machine learning projects. Consequently, we need\na different approach for handling corrupted training data, i.e. ML models robust to label noise.\nIn the context of this blog post, we define robustness as the model’s ability to efficiently learn in the presence\nof corrupted training data. In other words, a robust model can recover the correct training signal and ignore\nthe noise, so that it does not overfit to the corrupted traning set and can generalise during prediction. A major\nchallenge in this regard is the difficulty to estimate the proportion of label noise in real-world data. As such,\nrobust models are expected to handle varying amounts of label noise.\nHow to train a robust classifier?\nWe can improve the robustness of deep neural networks (DNNs) with a few tips and tricks presented in the recent\nliterature on Learning from Noisy Data. In general, there are three approaches to boosting the model’s resistance\nto noisy labels (Figure 3):\nRobust loss function boosting the training dynamics in the presence of noise.\nImplicit regularisation of the network aiming at decreasing the impact of noisy labels.\nFiltration of noisy data samples during the training or at the pre-training stage.\n\n    \n    \n Figure 3. Strategies for robustness.  In this blog post, we focused on two main approaches improving model robustness: utilisation of a robust loss function and implicit regularisation.\nIn the scope of this blog post, we present seven different methods that are strong baselines for improving\nthe generalisation of classifiers in the presence of label noise.\nRobust loss function\nSelf-Paced Learning (SPL)\nThe authors of Self-Paced Learning2 noticed that large per-sample loss might be an indication of label\ncorruption, especially in the latter stages of training. Clean labels should be easy to learn, while corrupted labels\nwould appear as difficult, resulting in a high per-sample loss.\nSPL proposes to exclude some predefined ratio of examples from the batch depending on their per-sample loss values\n(Figure 4a). Usually, the ratio is set as the estimated noise level in the dataset.\n\n    \n    \n  Figure 4. Comparison of loss filtration methods (SPL, PRL and CCE, see below).  While SPL and PRL exclude samples from loss calculation, CCE decreases the impact of potentially corrupted labels by clipping the per-sample loss values. Orange colour indicates candidate noisy samples. \nProvably Robust Learning (PRL)\nProvably Robust Learning3 derives from the ideas presented in the SPL paper, but the authors state that\ncorrupted labels should be detected depending on the gradient norm, instead of per-sample loss (Figure 4b).\nThe underlying intuition is that corrupted samples provoke the optimiser to make inadequately large steps\nin the optimisation space. The rest of the logic is the same as in SPL.\nClipped Cross-Entropy (CCE)\nRejection of samples might not be optimal from the training’s point of view, because DNNs need vast amounts of data\nto be able to generalise properly. Therefore, Clipped Cross-Entropy doesn’t exclude the most contributing samples\nfrom the batch, but rather alleviates their impact by clipping the per-sample loss to a predefined value (Figure 4c).\nEarly Learning Regularisation (ELR)\nIt has been recently observed that DNNs first fit clean samples, and then start memorising the noisy ones. This\nphenomenon reduces the generalisation properties of the model, distracting it from learning true patterns present\nin the data. Early Learning Regularisation4 mitigates memorisation with two tricks:\nTemporal ensembling of targets: during the training step \\([k]\\), the original targets \\(\\pmb{\\text{t}}\\) are mixed\nwith the model’s predictions \\(\\pmb{\\text{p}}\\) from previous training steps. This prevents the gradient from diverging\nhugely between subsequent steps. This trick is well-known in semi-supervised learning5:\nExplicit regularisation: an extra term is added to the default cross-entropy loss \\(\\mathcal{L}_{CE}(\\Theta)\\) that\nallows refinement of the early-learnt concepts, but penalises drastically contradicting predictions.\nThus, the gradient gets a boost for the clean samples, while the impact of noisy samples is neutralised\nby temporal ensembling.\nJensen-Shannon Divergence Loss (JSD)\nThe authors of Jensen-Shannon Divergence Loss 6 take yet another approach to loss construction,\nwhich is inspired by an empirical comparison between Cross-Entropy (CE) and Mean Absolute Error (MAE) loss. CE is known\nfor its fast convergence and brilliant training dynamics, while MAE provides spectacular robustness at the price\nof slow convergence.\nEnglesson et al. came up with the idea to use Jensen-Shannon Divergence, which is a proven generalisation of CE\nand MAE loss (Figure 5). JSD uses Kullback-Leibler Divergence \\(\\text{D}_{\\text{KL}}\\) between the target\nlabels \\(\\pmb{y}\\) and predictions of the model \\(f(\\pmb{x})\\) vs. their averaged distribution \\(\\pmb{m}\\). Summing up, one\ncan think of JSD as a CE with a robustness boost, or MAE with improved convergence.\n\n    \n    \n Figure 5. JSD as a generalisation of CE and MAE loss.  Depending on the parameter \\(\\pi_1\\), JSD resembles CE or MAE. Image source: 6.\nImplicit regularisation\nCo-teaching (CT)\nIn co-teaching 7, we simultaneously train two independent DNNs (Figure 6), and let them\nexchange examples during the training. The training feed (learning samples) provided by the peer network should\nideally consist only of clean samples. In CT, each network predicts which samples are clean and provides them to its\ncounterpart. Deciding whether a sample is clean relies on the trick known from SPL: the sample’s label is probably\nclean if its per-sample loss is low.\n\n    \n    \n Figure 6. Exchange of training feed in co-teaching.  Two peer networks exchange samples that are expected\nto be clean from noise. Image source: 7.\nCo-teaching is one of the most popular and universal baselines in the domain of learning from noisy data. It has\nwell-established empirical results, offers good performance even in extreme noise scenarios and can be simply\nintegrated into almost any architecture or downstream task. Unfortunately, it also has a few downsides. Firstly, there\nis no theoretical guarantee that such a training setup will eventually converge. Secondly, we may end up with\na consensus between the two networks, causing them to produce identical training feeds, and making the CT redundant.\nMixup\nMixup8 is a simple augmentation scheme that enforces linear behaviour of the model for in-between\ntraining samples (Figure 7). It linearly combines two training samples \\((\\pmb{x}_i, \\pmb{y}_i)\\)\nand \\((\\pmb{x}_j, \\pmb{y}_j)\\) with weight \\(\\lambda\\) sampled from the Beta distribution. It results in a new augmented sample with mixed input features \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\):\n\n    \n    \n  Figure 7. Augmentation through mixup.  Two samples \\(i\\) and \\(j\\) are linearly combined into a synthetic image \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\). This new augmented input encourages the model to linearly interpolate the predictions between the original samples. \nThe method is a simple, universal, yet very effective approach. It yields good empirical results while adding\nno severe computational overhead.\nCleaning up Allegro\nEvery offer has its right place at Allegro, belonging to one out of over 23,000 categories. The category structure\nis a tree consisting of:\nthe root (Allegro),\nup to 7 levels of intermediate nodes (departments, metacategories, etc.) — over 2,600 nodes in total,\nover 23,000 leaves.\nOffers located in wrong categories are hard to find and hard to buy. As such, we need a way to properly assign offers\nto correct category leaves. To this end, our Machine Learning Research team has developed a category classifier\nfor Allegro offers.\nThe model in question is a large language model pre-trained on the Allegro catalogue (see more\nin Do you speak Allegro?) and fine-tuned for offer classification. Specifically, the downstream task here is extreme text classification: each offer is represented by text (title) and is classified into over 23,000 categories — hence the word extreme.\nClassification is particularly challenging for offers listed in ambiguous categories such as Other, Accessories, etc.\nThese categories are broad and hard to navigate, as they contain a wide variety of products. Most of those products\nactually belong to some well-defined categories, but the merchant couldn’t find the right place for those offers\nat the time of their listing, because of the very rich taxonomy of the category tree. Consequently, we decided\nto clean up the offers in ambiguous categories.\nHere’s the setup (Figure 8):\nWe train the category classifier on offers in well-defined categories: the model learns what lies where at Allegro.\nNext, we run inference on offers in ambiguous categories: the model moves the offers to their right destination.\nNote that this task is subject to domain shift: the assortment listed in these ambiguous categories may be harder\nto categorise than the regular assortment in other categories.\n\n    \n    \n  Figure 8. Category classifier: training \u0026 inference.  The model is trained on offers listed in well-defined categories. Then, it is used to move offers from ambiguous categories (Other, Accessories, etc.) to the well-defined categories. \nReal-world label noise at Allegro\nThe training set (offers in well-defined categories) is not 100% correct, for several reasons (Figure 9):\nthe merchant may have put the offer in the wrong category,\nthere are several similar categories in the catalogue,\nthere is no appropriate category for a given offer,\nthe taxonomy of the Allegro category tree changes over time.\n\n    \n    \n  Figure 9. Examples of mislabelled offers.  With over 23,000 categories at Allegro, listing each offer in its best-matching category can be challenging for merchants. Hence, label noise is an inherent feature of our training dataset. \nThe ML model is prone to memorisation of the wrong labels in the training set, i.e. overfitting. These errors will\nlikely be reproduced at prediction time. Our goal is to train a robust classifier that will learn the true patterns\nand ignore the mislabelled training instances.\nThe training methods described in the previous section were developed and evaluated on computer vision tasks,\ne.g. image classification, into a relatively small number of categories. Here, we face the problem of extreme text\nclassification. Thus, we need to adapt those methods for textual input and find out which concepts transfer well between\nthe two domains.\nSynthetic label noise\nTo evaluate the model’s robustness experimentally, we need to know a priori which training instances were\nmislabelled. For that, we use a generator of controllable noise. The experimental setup consists of five steps\n(Figure 10):\ndumping a clean dataset from a curated pool of offers that are certainly in the right place,\nsplitting it into training, validation and test sets,\napplication of synthetic noise to 20% of instances in the training and validation sets (changing the offer’s category\nto a wrong one),\ntraining the model on the noisy dataset,\ntesting the model on a held-out fraction of the clean dataset.\n\n    \n    \n  Figure 10. Testing the model’s robustness.  The full dataset of clean instances (offers with true category labels) is split into training, validation and test sets. Next, label noise is introduced to the training and validation sets and the model is trained. The model is tested on a held-out fraction of the clean dataset. \nThis setup lets us answer the following question:\nHow much does the noise in the training set hurt the model’s performance on the clean test set?\nThis way, we can evaluate different methods of training classifiers under label noise and choose the most robust\nclassifier, according to accuracy on the test set.\nAnd… it works!\nBelow we present the results of experiments for 1.3M offers listed in the Construction Work \u0026 Equipment category.\nSymmetric noise was applied to 20% of the training set. This means that the category labels of that percentage\nof offers were changed to different randomly chosen labels. We evaluated the 7 training methods outlined above\nand compared them to the baseline: classification with cross-entropy loss.\nBaseline: Memorising doesn’t pay off\nHow does the presence of noise impact the baseline model?\nThe validation curves for non-corrupted samples clearly show the severe impact of noisy labels on the model’s\nperformance (Figure 11). In the early stage of training, the performance of the model trained\non noisy data is on par with the metrics of the model trained on clean data. Yet, starting from the 4th epoch,\nthe wrong labels in the noisy dataset appear to prevent the model from discovering the true patterns in the training\ndata, resulting in a 5 p.p. drop in accuracy at the end of the training. We attribute this drop to the memorisation\nof the wrong labels: instead of refining the originally learnt concepts, the network starts to overfit to the noisy\nlabels. The labels memorised for particular offers don’t help with classifying previously unseen offers at test time.\n\n    \n    \n Figure 11. Degradation of the baseline model in the presence of noise. The 20% synthetic noise degrades the model throughout the training. In the end, the model trained on the corrupted dataset exhibits 5 p.p. lower accuracy in comparison to its clean counterpart \nTowards robust classification\nDoes robustness imply underfitting?\nTo verify if the evaluated methods have any effect on the model’s performance when there is no noise in the training\ndata, we tested all of them on a clean dataset without any synthetic noise.\nIn the absence of corrupted data, three of the tested methods (SPL, PRL and CT) are effectively reduced to the baseline\nCross-Entropy. Therefore, the accuracy for those methods was exactly the same as for the baseline (Table 1).\nFor mixup, the difference from the baseline was within the standard deviation range, so it was marked as no improvement\nas well.\nFor CCE and JSD the performance degraded, but only slightly — by 0.04 p.p. for the former and 0.34 p.p. for the latter.\nThis drop is an acceptable compromise considering the robustness to noise that these methods enable (see below).\nELR was the only method that improved upon the baseline, by 0.07 p.p. As ELR relies on temporal ensembling, which\ndiminishes the impact of corrupted samples during training, we hypothesise that our clean dataset contained a small\nnumber of mislabelled examples. Such paradoxes are a frequent case in machine learning practice, even for renowned\nbenchmark datasets like CIFAR-1009.\nTable 1. Test accuracy scores of the models trained on the clean and corrupted\n(20% synthetic noise) datasets for the 8 training methods. Light red highlight indicates deterioration in comparison\nto the baseline, while light blue denotes improvement. Notation: (mean \\(\\pm\\) std)% from 5 independently seeded runs.\nMethod\n            Test accuracy [%]\n        \nclean dataset\n            noisy dataset\n        \nBaseline\n            90.26 ± 0.03\n            85.31 ± 0.08\n        \nfunction\n            \n            Self-Paced Learning (SPL)\n            90.26 ± 0.03\n            88.51 ± 0.02\n        \nProvably Robust Learning (PRL)\n            90.26 ± 0.03\n            88.31 ± 0.02\n        \nClipped Cross-Entropy (CCE)\n            90.22 ± 0.03\n            89.51 ± 0.01\n        \nEarly Learning Regularisation (ELR)\n            90.33 ± 0.01\n            89.29 ± 0.03\n        \nJensen-Shannon Divergence (JSD) \n            89.92 ± 0.02\n            89.24 ± 0.01\n        \nCo-teaching (CT)\n            90.26 ± 0.03\n            88.72 ± 0.03\n        \nMixup\n            90.27 ± 0.02\n            86.02 ± 0.06\n        \nRobust classification results\nAll methods discussed in this study improved the model’s performance on the noisy dataset when compared to the baseline\n(Table 1). The best results were obtained with CCE (+4.2 p.p.), ELR (+3.98 p.p.) and JSD (+3.93 p.p.).\nCT, SPL, PRL performed a bit worse, but still proved to be quite robust, improving upon the baseline by 3.41 p.p.,\n3.2 p.p. and 3.0 p.p., respectively.\nMixup is a clear outlier — while it does improve upon the baseline by 0.71 p.p., this increase is noticeably smaller\nthan for the other evaluated methods. Our interpretation is that the linear augmentation at the heart of this method\nregularises the DNN, but does not address label noise per se. Mixup treats all samples equally, even if their labels\nare corrupted. The marginal improvement upon the baseline is evident in the validation accuracy training curve\n(Figure 12). Mixup starts to overfit around the 5th epoch, similarly to the baseline, and unlike all\nthe other methods.\n\n    \n    \n Figure 12. Validation accuracy during training. Validation accuracy for all methods was measured during training. It is evident that the best methods are CCE, ELR and JSD, with CT, PRL and SPL trailing slightly behind. Mixup behaves similarly to the baseline. \nConclusions\nThe problem of label noise is unavoidable in machine learning practice, and Allegro datasets are no exception.\nFortunately, there exist numerous methods that diminish the impact of label noise on prediction performance\nby increasing the robustness of machine learning models. In our experiments we implemented 7 of those methods\nand showed that they increase prediction accuracy in the presence of 20% synthetic noise when compared to the baseline\n(Cross-Entropy loss), most of them by a significant margin. The simple Clipped Cross-Entropy proved to be the best,\nwith an accuracy score of 89.51% (increase of 4.2 p.p. vs the baseline trained with noisy labels). This result is very\nclose to the baseline trained with clean labels (90.26%). Thus, we showed that for the case of 20% synthetic label\nnoise, it is possible to increase robustness so that the impact of label noise is negligible.\nThese experiments are only a first step in making classifiers at Allegro robust to label noise. The case of synthetic\nnoise presented here is not very realistic: real-world label noise tends to be instance-dependent,\ni.e. it is influenced by individual sample features. As such, we plan to further evaluate the methods for increasing\nmodel robustness with a real-world dataset perturbed by instance-dependent noise.\nIf you’d like to know more about label noise and model robustness, please refer to the papers listed below.\nDeep Learning is Robust to Massive Label Noise, Rolnick et al., 2018 ↩\nSelf-Paced Learning for Latent Variable Models, Kumar et al., 2010 ↩\nLearning Deep Neural Networks under Agnostic Corrupted Supervision, Liu et al., 2021 ↩\nEarly-Learning Regularization Prevents Memorization of Noisy Labels, Liu et al., 2020 ↩\nTemporal Ensembling for Semi-Supervised Learning, Laine et al., 2017 ↩\nGeneralized Jensen-Shannon Divergence Loss for Learning with Noisy Labels, Englesson et al., 2021 ↩ ↩2\nCo-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels, Han et al., 2018 ↩ ↩2\nmixup: Beyond Empirical Risk Minimization, Zhang et al., 2018 ↩\nPervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks, Northcutt et al., 2021 ↩","guid":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","categories":["tech","mlr","robustness","research","ml","machine-learning","ai"],"isoDate":"2023-04-17T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Dynamic Workload Balancing in Hermes","link":"https://blog.allegro.tech/2023/04/dynamic-workload-balancing-in-hermes.html","pubDate":"Wed, 05 Apr 2023 00:00:00 +0200","authors":{"author":[{"name":["Piotr Rżysko"],"photo":["https://blog.allegro.tech/img/authors/piotr.rzysko.jpg"],"url":["https://blog.allegro.tech/authors/piotr.rzysko"]}]},"content":"\u003cp\u003e\u003ca href=\"https://github.com/allegro/hermes\"\u003eHermes\u003c/a\u003e is a distributed \u003ca href=\"https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern\"\u003epublish-subscribe\u003c/a\u003e\nmessage broker that we use at \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e to facilitate asynchronous communication between our\nmicroservices. As our usage of Hermes has grown over time, we faced a challenge in effectively distributing the\nload it handles to optimize resource utilization. In this blog post, we will present the implementation of a dynamic\nworkload balancing algorithm that we developed to address this challenge. We will describe the approach we took, the\nlessons we learned along the way, and the results we achieved.\u003c/p\u003e\n\n\u003ch2 id=\"hermes-architecture\"\u003eHermes Architecture\u003c/h2\u003e\n\n\u003cp\u003eBefore we delve deeper into the article’s topic, let’s first briefly introduce the architecture of Hermes, depicted in\nthe diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/hermes_architecture.png\" alt=\"Hermes Architecture\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAs we can see, Hermes is composed of two main modules:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003eHermes Frontend\u003c/strong\u003e acts as a gateway, receiving messages from publishers via its REST interface, applying necessary\n  preprocessing, and eventually storing them in \u003ca href=\"https://kafka.apache.org/\"\u003eApache Kafka\u003c/a\u003e.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003eHermes Consumers\u003c/strong\u003e is a component that constitutes the delivery part of the system. Its role is to fetch messages\n  from Kafka and push them to predefined subscribers while providing reliability mechanisms such as retries, backpressure,\n  and rate limiting. For the sake of brevity, in the latter parts of the article, we’ll refer to a single instance of this\n  module as a \u003cem\u003econsumer\u003c/em\u003e.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSince the rest of this post discusses topics that mainly pertain to the delivery side of the system, let’s turn our\nattention to that now.\u003c/p\u003e\n\n\u003cp\u003eApache Kafka organizes messages, also known as events, into topics. To facilitate parallelism, a topic usually has\nmultiple partitions. Each event is stored in only one partition. When someone wants to receive messages from a given\ntopic via Hermes, they create a new subscription with the defined HTTP endpoint to which messages will be delivered. Under the\nhood, Hermes assigns a group of \u003cem\u003econsumers\u003c/em\u003e to that subscription, with each \u003cem\u003econsumer\u003c/em\u003e handling at least one of the\ntopic partitions. By default, a fixed number of \u003cem\u003econsumers\u003c/em\u003e are assigned to a subscription, but the administrator can\nmanually override this number on a per-subscription basis. It’s also worth noting that a single \u003cem\u003econsumer\u003c/em\u003e can handle\nmultiple subscriptions. The following diagram illustrates this:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/partitions.png\" alt=\"Hermes Consumers\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"workload-balancer\"\u003eWorkload Balancer\u003c/h2\u003e\n\n\u003cp\u003eThe Hermes Consumers module is designed to operate in highly dynamic environments, e.g. in the cloud, where new\ninstances can be added, restarted, or removed at almost any time. This means that the module can handle these situations\nseamlessly and without disrupting the flow of messages. Additionally, it is horizontally scalable, meaning that\nwhen there is an increase in the number of subscriptions or an increase in outgoing traffic, we can easily scale out\nthe cluster by adding new \u003cem\u003econsumers\u003c/em\u003e. This adaptability to changing circumstances is achieved by a mechanism called the\n“workload balancer.” It acts as an arbiter, monitoring the state of the cluster, and if necessary, proposing\nappropriate adjustments in the distribution of subscriptions to the rest of the nodes.\u003c/p\u003e\n\n\u003ch2 id=\"motivations-for-improving-workload-balancer\"\u003eMotivations for Improving Workload Balancer\u003c/h2\u003e\n\n\u003cp\u003eOur first implementation of the workload balancer aimed to always assign the same number of subscriptions to each\n\u003cem\u003econsumer\u003c/em\u003e. This strategy is easy to understand and performs optimally when subscriptions are equal with respect to\ntheir load. However, this is not always the case. For example, imagine that we have two subscriptions. The first\nprocesses 1,000 messages per second, and the second only 10 messages per second. It is highly likely that they will\nnot consume the same number of CPU cores, network bandwidth, etc. Thus, if we want to spread the load evenly,\nwe should not assume that they are equal.\u003c/p\u003e\n\n\u003cp\u003eUsually, when we deploy our application in the cloud, we have to predefine the number of instances and the amount of\nresources (e.g. CPU, memory, etc.) that should be allocated to it. Unless we use a mechanism that adjusts these\nvalues on a per-instance basis, each instance will receive an equal share of the available resources. Now, let’s take\na look at the CPU usage of each \u003cem\u003econsumer\u003c/em\u003e from one of our Hermes production clusters using the workload balancer\nwhich does not account for the disproportions between subscriptions:\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_before.png\"\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_before.png\" alt=\"Initial CPU usage\" /\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eThe figure below shows the difference in CPU usage between the least and most heavily loaded \u003cem\u003econsumers\u003c/em\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_before_least_and_most.png\"\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_before_least_and_most.png\" alt=\"CPU usage of the least and most heavily loaded consumers\" /\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eTaking into account all of the above factors, in order not to compromise system performance, we always had to determine\nthe right allocation based on the most loaded instance. Consequently, less busy instances were wasting resources as\ntheir demands were lower, not making the most of what was available. Knowing that our Hermes production clusters will\ncontinue to grow in terms of both traffic and the number of topics and subscriptions, we decided to develop a new and\nimproved workload balancer, which we will discuss in the next section. We knew that if we didn’t do this, we would have\nto over-allocate even more resources in the future.\u003c/p\u003e\n\n\u003ch2 id=\"solution\"\u003eSolution\u003c/h2\u003e\n\n\u003ch3 id=\"constraints-and-requirements\"\u003eConstraints and Requirements\u003c/h3\u003e\n\n\u003cp\u003eBefore we proceed to the description of the solution we devised, we would like to discuss the requirements and\nconstraints that guided our design process.\u003c/p\u003e\n\n\u003cp\u003eFirst, we wanted to preserve the core responsibilities of the original workload balancer, such as:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAllocating work across newly added \u003cem\u003econsumers\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003eDistributing work from removed \u003cem\u003econsumers\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003eAssigning newly added subscriptions to available \u003cem\u003econsumers\u003c/em\u003e\u003c/li\u003e\n  \u003cli\u003eReclaiming resources previously assigned to removed subscriptions\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSecondly, we decided not to change the existing rule of assigning the same number of subscriptions to each \u003cem\u003econsumer\u003c/em\u003e.\nThe reasoning behind this decision stemmed from the threading model implemented in Hermes Consumers, where every\nsubscription assigned to a \u003cem\u003econsumer\u003c/em\u003e is handled by a separate thread. Having an unequal number of threads between\n\u003cem\u003econsumers\u003c/em\u003e could potentially lead to some consumers being overwhelmed with more\n\u003ca href=\"https://en.wikipedia.org/wiki/Context_switch\"\u003econtext switches\u003c/a\u003e and a higher memory footprint.\u003c/p\u003e\n\n\u003cp\u003eSimilarly, we wanted to preserve the strategy of determining the number of \u003cem\u003econsumers\u003c/em\u003e a subscription should be\nassigned to (fixed and globally configured, but with the option of being overridden on a per-subscription basis by the\nadministrator). Although it may not be optimal, for the reasons mentioned earlier, we chose to narrow down the scope of the\nimprovements, keep it as is, and potentially revisit it in the future.\u003c/p\u003e\n\n\u003cp\u003eThe last and very important factor that we had to consider while designing the new algorithm was the cost tied to\nevery change in the assignment of subscriptions, particularly the cost of rebalancing Kafka’s consumer groups\n(i.e. temporarily suspending the delivery of messages from partitions affected by the rebalance until the process is\ncompleted).\u003c/p\u003e\n\n\u003cp\u003eWith all of the above preconditions met, we were able to augment the capabilities of the workload balancer by making it\naware of the heterogeneity of the subscriptions. We will discuss how we approached this in the following two subsections.\u003c/p\u003e\n\n\u003ch3 id=\"first-attempt\"\u003eFirst Attempt\u003c/h3\u003e\n\n\u003cp\u003eAs we mentioned earlier, the main reason for the imbalance was the fact that the original balancing algorithm was\nunaware of the differences between subscriptions. Therefore, in the first place, we wanted to make subscriptions\ncomparable by associating with each of them an attribute called “weight.” Internally, it’s a vector of metrics\ncharacterizing a subscription. Currently, this vector has only one element, named “operations per second.” A single\noperation is an action executed by a \u003cem\u003econsumer\u003c/em\u003e in the context of a given subscription, such as fetching an event from\nKafka, committing offsets to Kafka, or sending an event to a subscriber. In the future, we may extend the\nweight vector by adding metrics that will allow us to eliminate uneven consumption of resources other than CPU.\u003c/p\u003e\n\n\u003cp\u003eBased on weights reported by individual \u003cem\u003econsumers\u003c/em\u003e, a leader (one of the nodes from the Hermes Consumers cluster)\nbuilds subscription profiles, which are records containing information about subscriptions necessary for making\nbalancing decisions. Among the details included in each profile are the weight and the timestamp of the last rebalance.\nIt’s important to remember that a single subscription can be spanned across multiple \u003cem\u003econsumers\u003c/em\u003e, resulting in multiple\nweight vectors associated with a single subscription. To resolve this, the leader builds a final weight vector (\\(W\\)),\nwhich is used in further calculations:\u003c/p\u003e\n\n\u003cp\u003e\\begin{equation}\nW =\n\\begin{bmatrix}\nmax(m_{11},m_{12},\\cdots,m_{1M}) \u0026amp; max(m_{21},m_{22},\\cdots,m_{2M}) \u0026amp; \\cdots \u0026amp; max(m_{N1},m_{N2},\\cdots,m_{NM})\n\\end{bmatrix}\n\\end{equation}\u003c/p\u003e\n\n\u003cp\u003ewhere:\u003c/p\u003e\n\n\u003cp\u003e\\(m_{ij}\\) is the value of metric \\(i\\) reported by \u003cem\u003econsumer\u003c/em\u003e \\(j\\)\u003c/p\u003e\n\n\u003cp\u003e\\(N\\) is the number of metrics included in the weight vector\u003c/p\u003e\n\n\u003cp\u003e\\(M\\) is the number of \u003cem\u003econsumers\u003c/em\u003e\u003c/p\u003e\n\n\u003cp\u003eIt’s also worth noting that in order to smooth out abrupt changes and short-term fluctuations in traffic, we apply an\n\u003ca href=\"https://en.wikipedia.org/wiki/Exponential_smoothing\"\u003eexponentially weighted moving average (EWMA)\u003c/a\u003e to the collected\nmetrics.\u003c/p\u003e\n\n\u003cp\u003eTo address the requirement regarding the cost of reassigning subscriptions, we introduced a global parameter called\n“stabilization window.” After a subscription is assigned, the stabilization window determines the minimum time before\nthe subscription can be reassigned. This “freezes” the subscription so that it doesn’t get reassigned too quickly,\nallowing the subscription to catch up with the events produced during the rebalancing process.\u003c/p\u003e\n\n\u003cp\u003eEquipped with the necessary terminology, we can now proceed to describe the algorithm itself. The high-level idea is\nfairly simple and boils down to the leader periodically executing the following steps:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eFetch subscription weights from every \u003cem\u003econsumer\u003c/em\u003e.\u003c/li\u003e\n  \u003cli\u003eUsing information from the previous step, rebuild subscription profiles.\u003c/li\u003e\n  \u003cli\u003eCalculate the total weight of the whole Hermes Consumers cluster by summing all subscription weights.\u003c/li\u003e\n  \u003cli\u003eDetermine the target consumer weight as an average of the weights of all \u003cem\u003econsumers\u003c/em\u003e in the cluster.\u003c/li\u003e\n  \u003cli\u003eBuild a set of \u003cem\u003econsumers\u003c/em\u003e whose weights are above the average calculated in step 4.\u003c/li\u003e\n  \u003cli\u003eBuild a set of \u003cem\u003econsumers\u003c/em\u003e whose weights are below the average calculated in step 4.\u003c/li\u003e\n  \u003cli\u003eSwap subscriptions between the sets calculated in the previous two steps while maintaining the following restrictions:\n    \u003cul\u003e\n      \u003cli\u003eThe weight of an instance from the overloaded set (step 5) is smaller than it was before the swap but is not\nsmaller than the target value.\u003c/li\u003e\n      \u003cli\u003eThe weight of an instance from the underloaded set (step 6) is greater than it was before the swap but is not\ngreater than the target value.\u003c/li\u003e\n      \u003cli\u003eIf, for a given subscription, the period of time since the last rebalance is shorter than the stabilization\nwindow, don’t consider the subscription eligible for the swap.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe graph below shows the results we obtained after deploying the implementation of this algorithm to production:\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_first_attempt.png\"\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_first_attempt.png\" alt=\"CPU usage after the first attempt\" /\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eAlthough the graph shows that the new algorithm got us closer to having uniform utilization of CPU, we were not fully\nsatisfied with that outcome. The reason for that is depicted below, where we compare the most loaded instance with the\nleast loaded one. The degree of disparity in terms of CPU usage is still significant. Therefore, we decided to at least\ndetermine the reason for that state of affairs and, if possible, refine the algorithm.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_first_attempt_least_most.png\"\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_first_attempt_least_most.png\" alt=\"CPU usage of the least and most heavily loaded consumers after the first attempt\" /\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eWhile investigating this issue, we noticed a correlation between the class of hardware that \u003cem\u003econsumers\u003c/em\u003e run on and\ntheir CPU usage. This observation led us to the conclusion that, despite the fact that load is evenly distributed,\ninstances running on older generations of hardware utilize a higher percentage of available CPU power than those running\non newer hardware, which is understandable as older machines are typically less performant. In the following section, we\ndescribe how we tackled this issue.\u003c/p\u003e\n\n\u003ch3 id=\"second-attempt\"\u003eSecond Attempt\u003c/h3\u003e\n\n\u003cp\u003eAfter our investigation, it was clear to us that if we wanted to achieve uniform CPU usage across the entire Hermes cluster,\naiming for processing the same number of operations per second on each instance was not the way to go. This led us to the\nquestion of how to determine the ideal number of operations per second that each instance can handle without being either\noverloaded or underloaded. To answer this, we had to take into account the fact that in the cloud environment, it is not\nalways possible to precisely define the hardware that our application will be running on. Potentially, we could put the\nburden of making the right decision on the Hermes administrator. However, this is a very tedious task and also hard to\nmaintain in dynamic environments where applications are almost constantly moved around different physical machines.\u003c/p\u003e\n\n\u003cp\u003eAs we wanted to avoid any manual tuning, we decided to employ a concept well-known in\n\u003ca href=\"https://en.wikipedia.org/wiki/Classical_control_theory\"\u003eControl Theory\u003c/a\u003e, called a\n\u003ca href=\"https://en.wikipedia.org/wiki/Proportional_control\"\u003eproportional controller\u003c/a\u003e. To explain the idea behind this concept,\nlet’s use an example. In the following picture, we see an operator who must adjust a hand valve to achieve the desired\ntemperature in a furnace. The operator doesn’t know upfront what the appropriate degree to which the valve should be\nopen is, therefore it is necessary to use a trial-and-error method to attain the desired outcome.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/proportional_controler.png\" alt=\"Proportional Controller\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNow let’s take a look at how this example relates to Control Theory. Using Control Theory terms, we can say that the\npicture presents a feedback control system, also known as a closed-loop control system. In such systems, the current\nstate and desired state of the system are referred to as the process variable and set point, respectively. In the example,\nthe current temperature in the furnace represents the process variable, while the temperature that the operator wants to\nachieve is the set point. To fully automate the control system and eliminate the need for manual adjustments, we typically\nreplace the operator with two components: an actuator and a controller. The controller calculates an error, which is the\ndifference between the set point and the process variable. Based on the error, the controller proportionally increases\nor decreases its output (in this example, the degree to which the valve is open). The actuator uses the controller’s\noutput to physically adjust the state of the system.\u003c/p\u003e\n\n\u003cp\u003eIf we think about it, we realize that the problem a proportional controller solves is very similar to the one we\nencounter when we run Hermes on heterogeneous hardware. Specifically, our goal is to achieve equal CPU usage across all\n\u003cem\u003econsumers\u003c/em\u003e, with the average usage of all \u003cem\u003econsumers\u003c/em\u003e being our target. Additionally, we know that the number of\noperations per second processed by each \u003cem\u003econsumer\u003c/em\u003e directly affects CPU usage. By utilizing a proportional\ncontroller, we can determine the value of this variable by calculating the error (the difference between the\ntarget and current CPU usage) and then adjusting the target weight accordingly. If we run the controller\nin a continuous loop, where it increases the target weight when current usage is below the target and vice versa, we\nshould eventually reach the set point.\u003c/p\u003e\n\n\u003cp\u003eAfter integrating a proportional controller into our algorithm and deploying it to production, we were able to achieve\nthe following results:\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_second_attempt.png\"\u003e\u003cimg src=\"/img/articles/2023-04-05-dynamic-workload-balancing-in-hermes/cpu_second_attempt.png\" alt=\"CPU usage after the second attempt\" /\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eAs we can see, the current CPU usage of all instances is very similar. This is exactly what we aimed for. If we\nassume that we are targeting 40% CPU utilization (to be able to handle additional traffic in case of a datacenter failover),\nby introducing the new algorithm we reduced the amount of allocated resources by approximately 42%.\u003c/p\u003e\n\n\u003ch2 id=\"conclusion-and-potential-improvements\"\u003eConclusion and Potential Improvements\u003c/h2\u003e\n\n\u003cp\u003eIn this post, we have described the challenges we faced with balancing load in our Hermes clusters, and the steps we\ntook to overcome them. By introducing our new workload balancing algorithm that dynamically adapts to varying\nsubscription loads and heterogeneous hardware, we were able to achieve a more uniform distribution of CPU usage across\nHermes Consumers instances.\u003c/p\u003e\n\n\u003cp\u003eThis approach has allowed us to significantly reduce the amount of allocated resources and avoid performance issues\ncaused by the imbalance that we observed earlier. However, there is still room for improvement. So far, we have been\nfocused on optimizing CPU utilization, but the way the algorithm is designed, enables us to extend the spectrum of\nbalanced resources in the future. For instance, we may consider factoring in memory and network bandwidth as well.\u003c/p\u003e\n\n\u003cp\u003eAdditionally, we plan to improve the ease of operating Hermes clusters. We aim to avoid having tuning knobs and putting\nthe burden of setting them correctly on the user. One such knob that we may want to remove in the future is the parameter\ndefining how many consumers are assigned to a subscription. Currently, the Hermes administrator is responsible for\nchoosing the value of this parameter for subscriptions where the default value is not a good fit. We suspect that this\ntask could be automated by delegating it to the workload balancer.\u003c/p\u003e\n","contentSnippet":"Hermes is a distributed publish-subscribe\nmessage broker that we use at Allegro to facilitate asynchronous communication between our\nmicroservices. As our usage of Hermes has grown over time, we faced a challenge in effectively distributing the\nload it handles to optimize resource utilization. In this blog post, we will present the implementation of a dynamic\nworkload balancing algorithm that we developed to address this challenge. We will describe the approach we took, the\nlessons we learned along the way, and the results we achieved.\nHermes Architecture\nBefore we delve deeper into the article’s topic, let’s first briefly introduce the architecture of Hermes, depicted in\nthe diagram below:\n\nAs we can see, Hermes is composed of two main modules:\nHermes Frontend acts as a gateway, receiving messages from publishers via its REST interface, applying necessary\n  preprocessing, and eventually storing them in Apache Kafka.\nHermes Consumers is a component that constitutes the delivery part of the system. Its role is to fetch messages\n  from Kafka and push them to predefined subscribers while providing reliability mechanisms such as retries, backpressure,\n  and rate limiting. For the sake of brevity, in the latter parts of the article, we’ll refer to a single instance of this\n  module as a consumer.\nSince the rest of this post discusses topics that mainly pertain to the delivery side of the system, let’s turn our\nattention to that now.\nApache Kafka organizes messages, also known as events, into topics. To facilitate parallelism, a topic usually has\nmultiple partitions. Each event is stored in only one partition. When someone wants to receive messages from a given\ntopic via Hermes, they create a new subscription with the defined HTTP endpoint to which messages will be delivered. Under the\nhood, Hermes assigns a group of consumers to that subscription, with each consumer handling at least one of the\ntopic partitions. By default, a fixed number of consumers are assigned to a subscription, but the administrator can\nmanually override this number on a per-subscription basis. It’s also worth noting that a single consumer can handle\nmultiple subscriptions. The following diagram illustrates this:\n\nWorkload Balancer\nThe Hermes Consumers module is designed to operate in highly dynamic environments, e.g. in the cloud, where new\ninstances can be added, restarted, or removed at almost any time. This means that the module can handle these situations\nseamlessly and without disrupting the flow of messages. Additionally, it is horizontally scalable, meaning that\nwhen there is an increase in the number of subscriptions or an increase in outgoing traffic, we can easily scale out\nthe cluster by adding new consumers. This adaptability to changing circumstances is achieved by a mechanism called the\n“workload balancer.” It acts as an arbiter, monitoring the state of the cluster, and if necessary, proposing\nappropriate adjustments in the distribution of subscriptions to the rest of the nodes.\nMotivations for Improving Workload Balancer\nOur first implementation of the workload balancer aimed to always assign the same number of subscriptions to each\nconsumer. This strategy is easy to understand and performs optimally when subscriptions are equal with respect to\ntheir load. However, this is not always the case. For example, imagine that we have two subscriptions. The first\nprocesses 1,000 messages per second, and the second only 10 messages per second. It is highly likely that they will\nnot consume the same number of CPU cores, network bandwidth, etc. Thus, if we want to spread the load evenly,\nwe should not assume that they are equal.\nUsually, when we deploy our application in the cloud, we have to predefine the number of instances and the amount of\nresources (e.g. CPU, memory, etc.) that should be allocated to it. Unless we use a mechanism that adjusts these\nvalues on a per-instance basis, each instance will receive an equal share of the available resources. Now, let’s take\na look at the CPU usage of each consumer from one of our Hermes production clusters using the workload balancer\nwhich does not account for the disproportions between subscriptions:\n\nThe figure below shows the difference in CPU usage between the least and most heavily loaded consumers:\n\nTaking into account all of the above factors, in order not to compromise system performance, we always had to determine\nthe right allocation based on the most loaded instance. Consequently, less busy instances were wasting resources as\ntheir demands were lower, not making the most of what was available. Knowing that our Hermes production clusters will\ncontinue to grow in terms of both traffic and the number of topics and subscriptions, we decided to develop a new and\nimproved workload balancer, which we will discuss in the next section. We knew that if we didn’t do this, we would have\nto over-allocate even more resources in the future.\nSolution\nConstraints and Requirements\nBefore we proceed to the description of the solution we devised, we would like to discuss the requirements and\nconstraints that guided our design process.\nFirst, we wanted to preserve the core responsibilities of the original workload balancer, such as:\nAllocating work across newly added consumers\nDistributing work from removed consumers\nAssigning newly added subscriptions to available consumers\nReclaiming resources previously assigned to removed subscriptions\nSecondly, we decided not to change the existing rule of assigning the same number of subscriptions to each consumer.\nThe reasoning behind this decision stemmed from the threading model implemented in Hermes Consumers, where every\nsubscription assigned to a consumer is handled by a separate thread. Having an unequal number of threads between\nconsumers could potentially lead to some consumers being overwhelmed with more\ncontext switches and a higher memory footprint.\nSimilarly, we wanted to preserve the strategy of determining the number of consumers a subscription should be\nassigned to (fixed and globally configured, but with the option of being overridden on a per-subscription basis by the\nadministrator). Although it may not be optimal, for the reasons mentioned earlier, we chose to narrow down the scope of the\nimprovements, keep it as is, and potentially revisit it in the future.\nThe last and very important factor that we had to consider while designing the new algorithm was the cost tied to\nevery change in the assignment of subscriptions, particularly the cost of rebalancing Kafka’s consumer groups\n(i.e. temporarily suspending the delivery of messages from partitions affected by the rebalance until the process is\ncompleted).\nWith all of the above preconditions met, we were able to augment the capabilities of the workload balancer by making it\naware of the heterogeneity of the subscriptions. We will discuss how we approached this in the following two subsections.\nFirst Attempt\nAs we mentioned earlier, the main reason for the imbalance was the fact that the original balancing algorithm was\nunaware of the differences between subscriptions. Therefore, in the first place, we wanted to make subscriptions\ncomparable by associating with each of them an attribute called “weight.” Internally, it’s a vector of metrics\ncharacterizing a subscription. Currently, this vector has only one element, named “operations per second.” A single\noperation is an action executed by a consumer in the context of a given subscription, such as fetching an event from\nKafka, committing offsets to Kafka, or sending an event to a subscriber. In the future, we may extend the\nweight vector by adding metrics that will allow us to eliminate uneven consumption of resources other than CPU.\nBased on weights reported by individual consumers, a leader (one of the nodes from the Hermes Consumers cluster)\nbuilds subscription profiles, which are records containing information about subscriptions necessary for making\nbalancing decisions. Among the details included in each profile are the weight and the timestamp of the last rebalance.\nIt’s important to remember that a single subscription can be spanned across multiple consumers, resulting in multiple\nweight vectors associated with a single subscription. To resolve this, the leader builds a final weight vector (\\(W\\)),\nwhich is used in further calculations:\n\\begin{equation}\nW =\n\\begin{bmatrix}\nmax(m_{11},m_{12},\\cdots,m_{1M}) \u0026 max(m_{21},m_{22},\\cdots,m_{2M}) \u0026 \\cdots \u0026 max(m_{N1},m_{N2},\\cdots,m_{NM})\n\\end{bmatrix}\n\\end{equation}\nwhere:\n\\(m_{ij}\\) is the value of metric \\(i\\) reported by consumer \\(j\\)\n\\(N\\) is the number of metrics included in the weight vector\n\\(M\\) is the number of consumers\nIt’s also worth noting that in order to smooth out abrupt changes and short-term fluctuations in traffic, we apply an\nexponentially weighted moving average (EWMA) to the collected\nmetrics.\nTo address the requirement regarding the cost of reassigning subscriptions, we introduced a global parameter called\n“stabilization window.” After a subscription is assigned, the stabilization window determines the minimum time before\nthe subscription can be reassigned. This “freezes” the subscription so that it doesn’t get reassigned too quickly,\nallowing the subscription to catch up with the events produced during the rebalancing process.\nEquipped with the necessary terminology, we can now proceed to describe the algorithm itself. The high-level idea is\nfairly simple and boils down to the leader periodically executing the following steps:\nFetch subscription weights from every consumer.\nUsing information from the previous step, rebuild subscription profiles.\nCalculate the total weight of the whole Hermes Consumers cluster by summing all subscription weights.\nDetermine the target consumer weight as an average of the weights of all consumers in the cluster.\nBuild a set of consumers whose weights are above the average calculated in step 4.\nBuild a set of consumers whose weights are below the average calculated in step 4.\nSwap subscriptions between the sets calculated in the previous two steps while maintaining the following restrictions:\n    \nThe weight of an instance from the overloaded set (step 5) is smaller than it was before the swap but is not\nsmaller than the target value.\nThe weight of an instance from the underloaded set (step 6) is greater than it was before the swap but is not\ngreater than the target value.\nIf, for a given subscription, the period of time since the last rebalance is shorter than the stabilization\nwindow, don’t consider the subscription eligible for the swap.\nThe graph below shows the results we obtained after deploying the implementation of this algorithm to production:\n\nAlthough the graph shows that the new algorithm got us closer to having uniform utilization of CPU, we were not fully\nsatisfied with that outcome. The reason for that is depicted below, where we compare the most loaded instance with the\nleast loaded one. The degree of disparity in terms of CPU usage is still significant. Therefore, we decided to at least\ndetermine the reason for that state of affairs and, if possible, refine the algorithm.\n\nWhile investigating this issue, we noticed a correlation between the class of hardware that consumers run on and\ntheir CPU usage. This observation led us to the conclusion that, despite the fact that load is evenly distributed,\ninstances running on older generations of hardware utilize a higher percentage of available CPU power than those running\non newer hardware, which is understandable as older machines are typically less performant. In the following section, we\ndescribe how we tackled this issue.\nSecond Attempt\nAfter our investigation, it was clear to us that if we wanted to achieve uniform CPU usage across the entire Hermes cluster,\naiming for processing the same number of operations per second on each instance was not the way to go. This led us to the\nquestion of how to determine the ideal number of operations per second that each instance can handle without being either\noverloaded or underloaded. To answer this, we had to take into account the fact that in the cloud environment, it is not\nalways possible to precisely define the hardware that our application will be running on. Potentially, we could put the\nburden of making the right decision on the Hermes administrator. However, this is a very tedious task and also hard to\nmaintain in dynamic environments where applications are almost constantly moved around different physical machines.\nAs we wanted to avoid any manual tuning, we decided to employ a concept well-known in\nControl Theory, called a\nproportional controller. To explain the idea behind this concept,\nlet’s use an example. In the following picture, we see an operator who must adjust a hand valve to achieve the desired\ntemperature in a furnace. The operator doesn’t know upfront what the appropriate degree to which the valve should be\nopen is, therefore it is necessary to use a trial-and-error method to attain the desired outcome.\n\nNow let’s take a look at how this example relates to Control Theory. Using Control Theory terms, we can say that the\npicture presents a feedback control system, also known as a closed-loop control system. In such systems, the current\nstate and desired state of the system are referred to as the process variable and set point, respectively. In the example,\nthe current temperature in the furnace represents the process variable, while the temperature that the operator wants to\nachieve is the set point. To fully automate the control system and eliminate the need for manual adjustments, we typically\nreplace the operator with two components: an actuator and a controller. The controller calculates an error, which is the\ndifference between the set point and the process variable. Based on the error, the controller proportionally increases\nor decreases its output (in this example, the degree to which the valve is open). The actuator uses the controller’s\noutput to physically adjust the state of the system.\nIf we think about it, we realize that the problem a proportional controller solves is very similar to the one we\nencounter when we run Hermes on heterogeneous hardware. Specifically, our goal is to achieve equal CPU usage across all\nconsumers, with the average usage of all consumers being our target. Additionally, we know that the number of\noperations per second processed by each consumer directly affects CPU usage. By utilizing a proportional\ncontroller, we can determine the value of this variable by calculating the error (the difference between the\ntarget and current CPU usage) and then adjusting the target weight accordingly. If we run the controller\nin a continuous loop, where it increases the target weight when current usage is below the target and vice versa, we\nshould eventually reach the set point.\nAfter integrating a proportional controller into our algorithm and deploying it to production, we were able to achieve\nthe following results:\n\nAs we can see, the current CPU usage of all instances is very similar. This is exactly what we aimed for. If we\nassume that we are targeting 40% CPU utilization (to be able to handle additional traffic in case of a datacenter failover),\nby introducing the new algorithm we reduced the amount of allocated resources by approximately 42%.\nConclusion and Potential Improvements\nIn this post, we have described the challenges we faced with balancing load in our Hermes clusters, and the steps we\ntook to overcome them. By introducing our new workload balancing algorithm that dynamically adapts to varying\nsubscription loads and heterogeneous hardware, we were able to achieve a more uniform distribution of CPU usage across\nHermes Consumers instances.\nThis approach has allowed us to significantly reduce the amount of allocated resources and avoid performance issues\ncaused by the imbalance that we observed earlier. However, there is still room for improvement. So far, we have been\nfocused on optimizing CPU utilization, but the way the algorithm is designed, enables us to extend the spectrum of\nbalanced resources in the future. For instance, we may consider factoring in memory and network bandwidth as well.\nAdditionally, we plan to improve the ease of operating Hermes clusters. We aim to avoid having tuning knobs and putting\nthe burden of setting them correctly on the user. One such knob that we may want to remove in the future is the parameter\ndefining how many consumers are assigned to a subscription. Currently, the Hermes administrator is responsible for\nchoosing the value of this parameter for subscriptions where the default value is not a good fit. We suspect that this\ntask could be automated by delegating it to the workload balancer.","guid":"https://blog.allegro.tech/2023/04/dynamic-workload-balancing-in-hermes.html","categories":["tech","architecture","hermes","kafka","algorithms","pub/sub","publish-subscribe","load balancing","open source"],"isoDate":"2023-04-04T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How neuroscience can help you as a software engineer - motivation","link":"https://blog.allegro.tech/2023/03/neuroscience-for-software-engineers-motivation.html","pubDate":"Tue, 21 Mar 2023 00:00:00 +0100","authors":{"author":[{"name":["Paulina Szwed"],"photo":["https://blog.allegro.tech/img/authors/paulina.szwed.jpg"],"url":["https://blog.allegro.tech/authors/paulina.szwed"]}]},"content":"\u003cp\u003eMany of us, software engineers, have experienced those days when nothing really sparks joy in coding, debugging,\npreparing spikes or refining tasks for the next sprints. Obviously, we would like to have as few of such days as possible\nand go on with our work effectively. A solution to this definitely is not tormenting our brains with guilt and forced\nlabour. There are other ways, and I would like to invite you to explore them with me and learn a little about our\nnervous systems in the process. We’ll find out where the motivation comes from on a biological and psychological level.\nWe’ll also take a look at the changes you can introduce into your day to take advantage of certain mechanisms working\non a neural level and boost your motivation and productivity.\u003c/p\u003e\n\n\u003ch2 id=\"the-neuroscience-of-motivation\"\u003eThe neuroscience of motivation\u003c/h2\u003e\n\n\u003cp\u003eThe key to maintaining energy to work throughout the day is simple – getting and staying motivated. Even though it seems\nlike something not necessarily in our power, motivation can be, to an extent, consciously modulated once we know\nthe biological and psychological mechanisms behind it.\u003c/p\u003e\n\n\u003ch3 id=\"the-source\"\u003eThe source\u003c/h3\u003e\n\n\u003cp\u003eOn a neural level, the fuel of motivation is \u003cstrong\u003edopamine\u003c/strong\u003e. Dopamine is a hormone (meaning it’s a messenger of the\nbody travelling through the bloodstream) and a neurotransmitter (which indicates its ability to affect communication between\nneurons).\u003c/p\u003e\n\n\u003cp\u003eIn general, high levels of dopamine cause high drive, motivation and willingness to live, do and experience. Low levels\nof dopamine cause the opposite state — a lack of will to do any effort (doomscrolling or examining the contents of the\nfridge is still in our reach). How much dopamine is currently in our system, how much dopamine there was a moment ago\nand how much we remember enjoying a particular state is for our brains a way to set our level of motivation.\u003c/p\u003e\n\n\u003ch3 id=\"the-workspace\"\u003eThe workspace\u003c/h3\u003e\n\n\u003cp\u003eIn the context of motivation our main area of focus across the nervous system should be the mesocorticolimbic system,\nwhich is responsible for the reward mechanism.\u003c/p\u003e\n\n\u003cp\u003eMesocorticolimbic system consists of dopaminergic and dopaminoceptive neurons — the former is a specialised kind of\nneurons that is capable of producing and emitting dopamine into our body; the latter is capable of detecting and\nreacting to the dopamine. The pathways of this system go through different areas of the brain — they extend from the\nventral tegmental area (VTA) to the part of the brain responsible for memory (hippocampus), reward, pleasure and movement\n(nucleus accumbens) and reasoning (prefrontal cortex).\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-03-21-neuroscience-for-software-engineers-motivation/1280px-Dopamine_pathways.svg.png\" alt=\"Dopamine pathways in the brain\" title=\"Dopamine pathways in the brain (Public Domain from [Wikipedia](https://en.wikipedia.org/wiki/Dopamine#/media/File:Dopamine_pathways.svg))\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe complexity of this structure might be a hint that there are a multiple implications of dopamine release, but\nalso many different ways to interact with this area.\u003c/p\u003e\n\n\u003ch3 id=\"the-process\"\u003eThe process\u003c/h3\u003e\n\n\u003cp\u003eWhen we talk about managing dopamine levels in healthy individuals, we actually should consider two different things:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003ebaseline level\u003c/strong\u003e, which is how much dopamine circulates in the body and determines how much dopamine we are capable\nof having,\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003epeak level\u003c/strong\u003e, which tells us how much dopamine we have at the moment or as a result of a rapid change.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBaseline level and peak level are closely related to each other. Evolutionally we are prepared to go out and search for\ndifferent resources, such as food, water or shelter, even though nowadays we might use it to gain slightly different\nthings (like a morning coffee or a salary). The drive to do so is provided by dopamine – which is, as we already know, a\nhormonal fuel of motivation. This mechanism is pretty old and it can be observed widely across the animal kingdom.\u003c/p\u003e\n\n\u003cp\u003eWhen the resources are found we experience a dopamine release — that is an effect of the reward mechanism in our\nmesocorticolimbic pathway. Now, in order to make us go and search for the resources again, the dopamine level must drop,\nso that we feel the lack of those resources as unpleasurable and seek for them again.\u003c/p\u003e\n\n\u003cp\u003eThe dopamine level drops \u003cstrong\u003elower\u003c/strong\u003e than the baseline and the extent of the drop is proportional to the height of the\npeak. Why? The drop of the dopamine level is caused by releasing available dopamine from synaptic vesicles – small\nstructures in the dopaminergic neurons. In time the vesicles get depleted of dopamine – we can only release the hormone\nthat is already there, ready to be deployed. After the release there isn’t enough dopamine in the vesicles to keep the\nbaseline level. It will go back to the baseline eventually, as the neurons produce more of it, but for a period of time\nit’s going to remain low.\u003c/p\u003e\n\n\u003cp\u003eContinuous peaks in dopamine level may eventually lead to drop of the baseline level. In that situation a person’s\nbrain, seeking for another reward to elevate it, will try repeating previous dopamine-increasing behaviours. A cascade\nof peaks and drops with repeated behaviours is a mechanism for addiction. This is something that may happen over\nexcessive usage of certain substances but also with social media. Incoming likes, comments and notifications or\nscrollable, neverending feed of videos — all of these generate peaks of dopamine. When we spend time on social media\nexperiencing peak after peak we may start feeling less and less satisfaction but still feel the urge to scroll further —\ndue to the mechanism I have just described. That would explain why social media addiction is such a great challenge\nfor our society.\u003c/p\u003e\n\n\u003ch2 id=\"how-to-get-and-stay-motivated\"\u003eHow to get and stay motivated\u003c/h2\u003e\n\n\u003cp\u003eAfter that long lecture on neurobiology, those who are still with me may be asking the question “are we there yet”? Yes,\nwe are! That knowledge is going to let us dive into different strategies of managing our dopamine levels, hence\nmodulating our motivation.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-03-21-neuroscience-for-software-engineers-motivation/54e8c52c-c40e-4197-a0ec-d17be266ed90_text.gif\" alt=\"Are we there yet...? Asked Donkey from Shrek\" title=\"(from [Giphy](https://y.yarn.co/54e8c52c-c40e-4197-a0ec-d17be266ed90_text.gif)\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"short-term-increase\"\u003eShort-term increase\u003c/h3\u003e\n\n\u003cp\u003eThere are multiple possibilities to invoke a peak of dopamine and give our bodies that immediate impulse lasting a few\nminutes or even seconds. We should be aware of them, both when we need those peaks and when we want to avoid them — and\nsoon you’ll know why.\u003c/p\u003e\n\n\u003cp\u003eThere is a significant number of substances that may increase the dopamine level above the baseline:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eChocolate might increase it up to 1,5 times, although it only lasts a few seconds.\u003c/li\u003e\n  \u003cli\u003eSmoked nicotine or cocaine may cause a 2,5 times increase, amphetamine causes up to 10-fold increase (knowing that\nand how peak and baseline levels of dopamine work, explains the addictive effect those substances have).\u003c/li\u003e\n  \u003cli\u003eAlcohol in low doses is also known to cause dopamine release.\u003c/li\u003e\n  \u003cli\u003eIngestion of herbs like saffron, rosemary or oregano may lead to elevation of dopamine levels.\u003c/li\u003e\n  \u003cli\u003eCaffeine causes a rather modest increase of dopamine, but also increases sensitivity of some dopamine receptors as\nwell as their number and density. This is worth noting, especially for coffee-fueled machines like programmers – a cup\nof coffee in the morning will make us more susceptible to dopamine changes throughout the day.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThere are also several actions which we might take to induce a peak of dopamine like physical activity or thinking and\ntalking about things we enjoy. The former is rather subjective and the height of the peak depends on whether the person\nenjoys the activity itself. For those who do, it may double the dopamine level.\u003c/p\u003e\n\n\u003cp\u003eThe latter results from involvement of the prefrontal cortex in the mesocorticolimbic system. Do you remember the last\ntime when you’ve been telling somebody about that new thing you’d recently learned? How passionate you’ve felt and how\nhappy and excited you’ve been afterwards? The prefrontal cortex is responsible for assigning rational explanations and\nsubjective experiences to things we engage with. Recalling those interactions might cause a dopamine release and make us\nhappier and more motivated.\u003c/p\u003e\n\n\u003ch3 id=\"long-term-strategies\"\u003eLong-term strategies\u003c/h3\u003e\n\n\u003cp\u003eAs previously said, the peaks of the dopamine, especially one after the other, will cause the dopamine level to drop\nbelow the baseline. In order to maintain high levels of dopamine (hence high motivation) on a daily basis we should act\nlong-term and affect the baseline level as much as possible. There are ways to do it.\u003c/p\u003e\n\n\u003ch4 id=\"exposure-to-cold\"\u003eExposure to cold\u003c/h4\u003e\n\n\u003cp\u003eResearch shows that when a human subject enters cold water (14°C) and stays there for up to an hour it leads to rapid\nincrease in norepinephrine and epinephrine (i.e. adrenaline) and also an increase in dopamine. Dopamine was observed to\ncontinuously rise up to 250% of baseline level and it stayed there for a few hours. It also limited release of cortisol\n— the stress hormone.\u003c/p\u003e\n\n\u003cp\u003eAn hour-long cold bath is too much for an average person, but the same effect (although on a smaller scale) can be\nacquired by more accessible measures. How about a quick cold shower before work to charge up on that dopamine? Or maybe\njoining a winter swimming community?\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-03-21-neuroscience-for-software-engineers-motivation/giphy-downsized-large.gif\" alt=\"A man jumping into snow\" title=\"(from [Giphy](https://media.giphy.com/media/MpJJ7gWng24bjxrMiK/giphy-downsized-large.gif))\" /\u003e\u003c/p\u003e\n\n\u003ch4 id=\"avoiding-layers-of-dopamine-increasing-factors\"\u003eAvoiding layers of dopamine-increasing factors\u003c/h4\u003e\n\n\u003cp\u003eDo you start working only with a big hot cup of coffee in your hand? Do you listen to loud music while programming? Or\nmaybe do you treat yourself with a sweet drink after a workout? Are there a lot of such rituals? You might want to\nconsider not having them on a daily basis.\u003c/p\u003e\n\n\u003cp\u003eEvery one of these rituals is a dopamine-increasing factor. As you’ve already read, multiple peaks of dopamine one on\ntop of the other might not be a great idea. Layering multiple dopamine-increasing factors on a regular basis might\nseriously affect our ability to release dopamine in general. Spiking the dopamine by multiple activities in a short\nperiod of time leads to lowering the baseline by depleting stored, ready-to-deploy dopamine. That may lead to lowering\nthe baseline dopamine level, what will result in lack of motivation and feeling low in general.\u003c/p\u003e\n\n\u003cp\u003eSo what should we do? Are those things bad for us? Well, not really. Sometimes these spikes of dopamine from listening\nto music or sweet treats are exactly what we need to get through the day. The key is to differentiate all these “extras”\nwhile we work. Listen to music while working — just not every time. Have that sweet drink — just not every day. Meet\nwith friends for a workout session — but also have these individual sessions once in a while.\u003c/p\u003e\n\n\u003cp\u003eSome of us spend our time on meetings while trying to do our individual work at the same time. Some listen to podcasts\nwhile coding. We should keep in mind, though, that multitasking is another way of layering dopamine and doing that on a\ndaily basis will have a negative effect on general motivation.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-03-21-neuroscience-for-software-engineers-motivation/giphy.webp\" alt=\"A man multitasking at work \" title=\"(from [Giphy](\nhttps://media3.giphy.com/media/PvvSfSDFoAL5e/giphy.gif?cid=ecf05e47mym26elkkk7obya4xr8zj83hva1jlu12l7cvmglq\u0026amp;rid=giphy.gif\u0026amp;ct=g)\n)\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAnother factor we should keep in mind are distractions. Every email, every slack message, every push notification on\nyour phone will cause a small peak of dopamine. That makes it yet another thing to eliminate if you want to avoid\ndopamine layering.\u003c/p\u003e\n\n\u003ch4 id=\"focusing-on-intrinsic-motivation\"\u003eFocusing on intrinsic motivation\u003c/h4\u003e\n\n\u003cp\u003eWhen we look at the sources of motivation we can divide them in two groups — external sources, such as meeting\nexpectations of other people, fame or financial compensation and intrinsic sources, like enjoyment during an activity or\nfulfilling a personal mission.\u003c/p\u003e\n\n\u003cp\u003eWhat do you like to do after work? Is it playing video games, playing music, baking, practising yoga or maybe watching\nlectures on mathematics on YouTube? (don’t judge me…) Whatever this would be, I bet no one needs to encourage you to do\nthis — you have an intrinsic drive to go and do your thing. You could spend hours on it, and then you’ll feel happy and\nfulfilled. This is how intrinsic motivation works — no external reward is required for you to feel motivated. Wouldn’t\nit be great if you had that in your job?\u003c/p\u003e\n\n\u003cp\u003eResearch shows that intrinsic motivation brings better results than extrinsic motivation. One could ask a question —\nwhat if we brought those two things together? Well, the results might surprise you. Researchers gave out to participants\na rather enjoyable task like assembling the jigsaw puzzle or drawing and measured their motivation to do so. Then they\nintroduced small rewards for finishing the puzzle. Curiously, the participants were no longer as motivated as at the\nbeginning.\u003c/p\u003e\n\n\u003cp\u003eThe conclusion is that when we add an extrinsic motivation to an existing intrinsic one, we observe an overall decrease\nin willingness to do the activity!\nThis is what we call an undermining effect.\u003c/p\u003e\n\n\u003cp\u003eThe effect comes from the reward being perceived as the ultimate goal of the activity. When the reward comes at the end\nof the activity, only then is dopamine release activated, whereas it could be active during the whole time if only we\nmindfully focused on the joyful part of the activity itself. Introducing the reward leads to perceiving the whole\nexperience as less and less pleasurable over time.\u003c/p\u003e\n\n\u003cp\u003eHow to avoid the undermining effect? It’s rather simple. Don’t layer other sources of dopamine, avoid dopamine peaks\nright before and right after the activity and be mindful about your intrinsic motivation. But what if getting it done\nbecomes really hard? It is difficult to have intrinsic motivation at such times. A good tactic to try is\ntelling ourselves that overcoming those difficulties is a kind of pleasure, so we can activate our prefrontal cortex as\na part of the dopaminergic pathway and eventually get ourselves motivated.\u003c/p\u003e\n\n\u003ch4 id=\"thinking-positively-practising-gratitude-meditation\"\u003eThinking positively, practising gratitude, meditation\u003c/h4\u003e\n\n\u003cp\u003eRemember that aspect of the mesocorticolimbic pathway where thinking about something enjoyable caused a dopamine\nrelease? There is more to it! By repeatedly having positive or negative interactions with something we can make a\nsignificant impact on its rewarding or non-rewarding properties. It effectively means that focusing on positive aspects\nof our surroundings and activities will eventually lead to an increase of dopamine releases from engaging with it.\u003c/p\u003e\n\n\u003cp\u003eHaving said that, I would strongly encourage you to use different forms of appreciation or gratitude practice. Maybe try\njournaling a little at the end of the day? I dare you to think about 3 good things that happened to you at work every\nday and write it down.\u003c/p\u003e\n\n\u003cp\u003eResearch also shows that engaging in meditational practices leads to activation of reward-related areas of the brain.\nHaving a long-term habit of meditation and mindfulness will contribute to having sustained feelings of deep joy and\npeace, which is associated with higher dopamine levels.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eYou now know how your dopamine mechanisms work and how to use your biological hardware\nto modulate your overall motivation. What you should remember is that the thing most important for your\nmotivation is maintaining a high baseline dopamine level. Avoid dopamine peaks, differentiate stimuli when you work,\nuse other tactics such as exposing yourself to cold water or meditation. Also, keep your focus\nmostly (preferably only) on your intrinsic motivation to avoid the undermining effect. Now, go conquer the world!\u003c/p\u003e\n\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\n\u003col\u003e\n  \u003cli\u003eLee W, Reeve J, Xue Y, Xiong J. Neural differences between intrinsic reasons for doing versus extrinsic reasons for\ndoing: an fMRI study. Neurosci Res. 2012 May;73(1):68-72. doi: 10.1016/j.neures.2012.02.010. PMID: 23565014; PMCID:\nPMC3614004.\u003c/li\u003e\n  \u003cli\u003eLiu, C., Goel, P. \u0026amp; Kaeser, P.S. Spatial and temporal scales of dopamine transmission. Nat Rev Neurosci 22, 345–358 (\n2021). https://doi.org/10.1038/s41583-021-00455-7\u003c/li\u003e\n  \u003cli\u003eŠrámek, P., Šimečková, M., Janský, L. et al. Human physiological responses to immersion into water of different\ntemperatures. Eur J Appl Physiol 81, 436–442 (2000). https://doi.org/10.1007/s004210050065\u003c/li\u003e\n  \u003cli\u003eVolkow, N., Wang, GJ., Logan, J. et al. Caffeine increases striatal dopamine D2/D3 receptor availability in the human\nbrain. Transl Psychiatry 5, e549 (2015). https://doi.org/10.1038/tp.2015.46\u003c/li\u003e\n  \u003cli\u003eNeuroscience. 2nd edition. Purves D, Augustine GJ, Fitzpatrick D, et al., editors. Sunderland (MA): Sinauer\nAssociates;\u003c/li\u003e\n  \u003cli\u003eVani Pariyadath, Joshua L. Gowin, Elliot A. Stein, Chapter 8 - Resting state functional connectivity analysis for\naddiction medicine: From individual loci to complex networks, Editor(s): Hamed Ekhtiari, Martin P. Paulus, Progress\nin Brain Research, Elsevier, Volume 224, 2016\u003c/li\u003e\n  \u003cli\u003eDeci, E. L., Koestner, R., \u0026amp; Ryan, R. M. (1999). A meta-analytic review of experiments examining the effects of\nextrinsic rewards on intrinsic motivation. Psychological Bulletin, 125(6),\n627–668. https://doi.org/10.1037/0033-2909.125.6.627\u003c/li\u003e\n  \u003cli\u003eChen W. Neural circuits provide insights into reward and aversion. Front Neural Circuits. 2022 Oct 28;16:1002485.\ndoi:\n10.3389/fncir.2022.1002485. PMID: 36389177; PMCID: PMC9650032.\u003c/li\u003e\n  \u003cli\u003eLooby A, Zimmerman L, Livingston NR. Expectation for stimulant type modifies caffeine’s effects on mood and cognition\namong college students. Exp Clin Psychopharmacol. 2022 Oct;30(5):525-535. doi: 10.1037/pha0000448. Epub 2021 Mar 18.\nPMID: 33734725.\u003c/li\u003e\n  \u003cli\u003eArias-Carrión, O., Stamelou, M., Murillo-Rodríguez, E. et al. Dopaminergic reward system: a short integrative\nreview. Int Arch Med 3, 24 (2010)\n. https://doi.org/10.1186/1755-7682-3-24\u003c/li\u003e\n  \u003cli\u003eEsch, Tobias. “The neurobiology of meditation and mindfulness.” Meditation–neuroscientific approaches and\nphilosophical implications. Springer, Cham, 2014. 153-173.\u003c/li\u003e\n  \u003cli\u003eDi Chiara G. Alcohol and dopamine. Alcohol Health Res World. 1997;21(2):\n108-14. PMID: 15704345; PMCID: PMC6826820.\u003c/li\u003e\n  \u003cli\u003eFarahani MS, Bahramsoltani R, Farzaei MH, Abdollahi M, Rahimi R. Plant-derived natural medicines for the management\nof depression: an overview of mechanisms of action. Rev Neurosci. 2015;26(3):305-21. doi:\n10.1515/revneuro-2014-0058. PMID: 25719303.\u003c/li\u003e\n  \u003cli\u003eMechan AO, Fowler A, Seifert N, Rieger H, Wöhrle T, Etheve S, Wyss A, Schüler G, Colletto B, Kilpert C, Aston J,\nElliott JM, Goralczyk R, Mohajeri MH. Monoamine reuptake inhibition and mood-enhancing potential of a specified\noregano extract. Br J Nutr. 2011 Apr;105(8):1150-63. doi:\n10.1017/S0007114510004940. Epub 2010 Dec 21. PMID: 21205415.\u003c/li\u003e\n  \u003cli\u003eKhazdair MR, Boskabady MH, Hosseini M, Rezaee R, M Tsatsakis A. The effects of Crocus sativus (saffron) and its\nconstituents on nervous system: A review. Avicenna J Phytomed. 2015 Sep-Oct;5(5):376-91. PMID: 26468457; PMCID:\nPMC4599112.\u003c/li\u003e\n  \u003cli\u003eMarques A, Marconcin P, Werneck AO, Ferrari G, Gouveia ÉR, Kliegel M, Peralta M, Ihle A. Bidirectional Association\nbetween Physical Activity and Dopamine Across Adulthood-A Systematic Review. Brain Sci. 2021 Jun 23;11(7):829. doi:\n10.3390/brainsci11070829. PMID: 34201523; PMCID:\nPMC8301978.\u003c/li\u003e\n  \u003cli\u003eR. Kotarski “Inaczej”, Altenberg, 2020\u003c/li\u003e\n  \u003cli\u003eA. Huberman, Controlling Your Dopamine For Motivation, Focus \u0026amp; Satisfaction | Huberman Lab Podcast #39,\nYoutube, https://www.youtube.com/watch?v=QmOF0crdyRU\u003c/li\u003e\n\u003c/ol\u003e\n\n","contentSnippet":"Many of us, software engineers, have experienced those days when nothing really sparks joy in coding, debugging,\npreparing spikes or refining tasks for the next sprints. Obviously, we would like to have as few of such days as possible\nand go on with our work effectively. A solution to this definitely is not tormenting our brains with guilt and forced\nlabour. There are other ways, and I would like to invite you to explore them with me and learn a little about our\nnervous systems in the process. We’ll find out where the motivation comes from on a biological and psychological level.\nWe’ll also take a look at the changes you can introduce into your day to take advantage of certain mechanisms working\non a neural level and boost your motivation and productivity.\nThe neuroscience of motivation\nThe key to maintaining energy to work throughout the day is simple – getting and staying motivated. Even though it seems\nlike something not necessarily in our power, motivation can be, to an extent, consciously modulated once we know\nthe biological and psychological mechanisms behind it.\nThe source\nOn a neural level, the fuel of motivation is dopamine. Dopamine is a hormone (meaning it’s a messenger of the\nbody travelling through the bloodstream) and a neurotransmitter (which indicates its ability to affect communication between\nneurons).\nIn general, high levels of dopamine cause high drive, motivation and willingness to live, do and experience. Low levels\nof dopamine cause the opposite state — a lack of will to do any effort (doomscrolling or examining the contents of the\nfridge is still in our reach). How much dopamine is currently in our system, how much dopamine there was a moment ago\nand how much we remember enjoying a particular state is for our brains a way to set our level of motivation.\nThe workspace\nIn the context of motivation our main area of focus across the nervous system should be the mesocorticolimbic system,\nwhich is responsible for the reward mechanism.\nMesocorticolimbic system consists of dopaminergic and dopaminoceptive neurons — the former is a specialised kind of\nneurons that is capable of producing and emitting dopamine into our body; the latter is capable of detecting and\nreacting to the dopamine. The pathways of this system go through different areas of the brain — they extend from the\nventral tegmental area (VTA) to the part of the brain responsible for memory (hippocampus), reward, pleasure and movement\n(nucleus accumbens) and reasoning (prefrontal cortex).\n\nThe complexity of this structure might be a hint that there are a multiple implications of dopamine release, but\nalso many different ways to interact with this area.\nThe process\nWhen we talk about managing dopamine levels in healthy individuals, we actually should consider two different things:\nbaseline level, which is how much dopamine circulates in the body and determines how much dopamine we are capable\nof having,\npeak level, which tells us how much dopamine we have at the moment or as a result of a rapid change.\nBaseline level and peak level are closely related to each other. Evolutionally we are prepared to go out and search for\ndifferent resources, such as food, water or shelter, even though nowadays we might use it to gain slightly different\nthings (like a morning coffee or a salary). The drive to do so is provided by dopamine – which is, as we already know, a\nhormonal fuel of motivation. This mechanism is pretty old and it can be observed widely across the animal kingdom.\nWhen the resources are found we experience a dopamine release — that is an effect of the reward mechanism in our\nmesocorticolimbic pathway. Now, in order to make us go and search for the resources again, the dopamine level must drop,\nso that we feel the lack of those resources as unpleasurable and seek for them again.\nThe dopamine level drops lower than the baseline and the extent of the drop is proportional to the height of the\npeak. Why? The drop of the dopamine level is caused by releasing available dopamine from synaptic vesicles – small\nstructures in the dopaminergic neurons. In time the vesicles get depleted of dopamine – we can only release the hormone\nthat is already there, ready to be deployed. After the release there isn’t enough dopamine in the vesicles to keep the\nbaseline level. It will go back to the baseline eventually, as the neurons produce more of it, but for a period of time\nit’s going to remain low.\nContinuous peaks in dopamine level may eventually lead to drop of the baseline level. In that situation a person’s\nbrain, seeking for another reward to elevate it, will try repeating previous dopamine-increasing behaviours. A cascade\nof peaks and drops with repeated behaviours is a mechanism for addiction. This is something that may happen over\nexcessive usage of certain substances but also with social media. Incoming likes, comments and notifications or\nscrollable, neverending feed of videos — all of these generate peaks of dopamine. When we spend time on social media\nexperiencing peak after peak we may start feeling less and less satisfaction but still feel the urge to scroll further —\ndue to the mechanism I have just described. That would explain why social media addiction is such a great challenge\nfor our society.\nHow to get and stay motivated\nAfter that long lecture on neurobiology, those who are still with me may be asking the question “are we there yet”? Yes,\nwe are! That knowledge is going to let us dive into different strategies of managing our dopamine levels, hence\nmodulating our motivation.\n\nShort-term increase\nThere are multiple possibilities to invoke a peak of dopamine and give our bodies that immediate impulse lasting a few\nminutes or even seconds. We should be aware of them, both when we need those peaks and when we want to avoid them — and\nsoon you’ll know why.\nThere is a significant number of substances that may increase the dopamine level above the baseline:\nChocolate might increase it up to 1,5 times, although it only lasts a few seconds.\nSmoked nicotine or cocaine may cause a 2,5 times increase, amphetamine causes up to 10-fold increase (knowing that\nand how peak and baseline levels of dopamine work, explains the addictive effect those substances have).\nAlcohol in low doses is also known to cause dopamine release.\nIngestion of herbs like saffron, rosemary or oregano may lead to elevation of dopamine levels.\nCaffeine causes a rather modest increase of dopamine, but also increases sensitivity of some dopamine receptors as\nwell as their number and density. This is worth noting, especially for coffee-fueled machines like programmers – a cup\nof coffee in the morning will make us more susceptible to dopamine changes throughout the day.\nThere are also several actions which we might take to induce a peak of dopamine like physical activity or thinking and\ntalking about things we enjoy. The former is rather subjective and the height of the peak depends on whether the person\nenjoys the activity itself. For those who do, it may double the dopamine level.\nThe latter results from involvement of the prefrontal cortex in the mesocorticolimbic system. Do you remember the last\ntime when you’ve been telling somebody about that new thing you’d recently learned? How passionate you’ve felt and how\nhappy and excited you’ve been afterwards? The prefrontal cortex is responsible for assigning rational explanations and\nsubjective experiences to things we engage with. Recalling those interactions might cause a dopamine release and make us\nhappier and more motivated.\nLong-term strategies\nAs previously said, the peaks of the dopamine, especially one after the other, will cause the dopamine level to drop\nbelow the baseline. In order to maintain high levels of dopamine (hence high motivation) on a daily basis we should act\nlong-term and affect the baseline level as much as possible. There are ways to do it.\nExposure to cold\nResearch shows that when a human subject enters cold water (14°C) and stays there for up to an hour it leads to rapid\nincrease in norepinephrine and epinephrine (i.e. adrenaline) and also an increase in dopamine. Dopamine was observed to\ncontinuously rise up to 250% of baseline level and it stayed there for a few hours. It also limited release of cortisol\n— the stress hormone.\nAn hour-long cold bath is too much for an average person, but the same effect (although on a smaller scale) can be\nacquired by more accessible measures. How about a quick cold shower before work to charge up on that dopamine? Or maybe\njoining a winter swimming community?\n\nAvoiding layers of dopamine-increasing factors\nDo you start working only with a big hot cup of coffee in your hand? Do you listen to loud music while programming? Or\nmaybe do you treat yourself with a sweet drink after a workout? Are there a lot of such rituals? You might want to\nconsider not having them on a daily basis.\nEvery one of these rituals is a dopamine-increasing factor. As you’ve already read, multiple peaks of dopamine one on\ntop of the other might not be a great idea. Layering multiple dopamine-increasing factors on a regular basis might\nseriously affect our ability to release dopamine in general. Spiking the dopamine by multiple activities in a short\nperiod of time leads to lowering the baseline by depleting stored, ready-to-deploy dopamine. That may lead to lowering\nthe baseline dopamine level, what will result in lack of motivation and feeling low in general.\nSo what should we do? Are those things bad for us? Well, not really. Sometimes these spikes of dopamine from listening\nto music or sweet treats are exactly what we need to get through the day. The key is to differentiate all these “extras”\nwhile we work. Listen to music while working — just not every time. Have that sweet drink — just not every day. Meet\nwith friends for a workout session — but also have these individual sessions once in a while.\nSome of us spend our time on meetings while trying to do our individual work at the same time. Some listen to podcasts\nwhile coding. We should keep in mind, though, that multitasking is another way of layering dopamine and doing that on a\ndaily basis will have a negative effect on general motivation.\n\nAnother factor we should keep in mind are distractions. Every email, every slack message, every push notification on\nyour phone will cause a small peak of dopamine. That makes it yet another thing to eliminate if you want to avoid\ndopamine layering.\nFocusing on intrinsic motivation\nWhen we look at the sources of motivation we can divide them in two groups — external sources, such as meeting\nexpectations of other people, fame or financial compensation and intrinsic sources, like enjoyment during an activity or\nfulfilling a personal mission.\nWhat do you like to do after work? Is it playing video games, playing music, baking, practising yoga or maybe watching\nlectures on mathematics on YouTube? (don’t judge me…) Whatever this would be, I bet no one needs to encourage you to do\nthis — you have an intrinsic drive to go and do your thing. You could spend hours on it, and then you’ll feel happy and\nfulfilled. This is how intrinsic motivation works — no external reward is required for you to feel motivated. Wouldn’t\nit be great if you had that in your job?\nResearch shows that intrinsic motivation brings better results than extrinsic motivation. One could ask a question —\nwhat if we brought those two things together? Well, the results might surprise you. Researchers gave out to participants\na rather enjoyable task like assembling the jigsaw puzzle or drawing and measured their motivation to do so. Then they\nintroduced small rewards for finishing the puzzle. Curiously, the participants were no longer as motivated as at the\nbeginning.\nThe conclusion is that when we add an extrinsic motivation to an existing intrinsic one, we observe an overall decrease\nin willingness to do the activity!\nThis is what we call an undermining effect.\nThe effect comes from the reward being perceived as the ultimate goal of the activity. When the reward comes at the end\nof the activity, only then is dopamine release activated, whereas it could be active during the whole time if only we\nmindfully focused on the joyful part of the activity itself. Introducing the reward leads to perceiving the whole\nexperience as less and less pleasurable over time.\nHow to avoid the undermining effect? It’s rather simple. Don’t layer other sources of dopamine, avoid dopamine peaks\nright before and right after the activity and be mindful about your intrinsic motivation. But what if getting it done\nbecomes really hard? It is difficult to have intrinsic motivation at such times. A good tactic to try is\ntelling ourselves that overcoming those difficulties is a kind of pleasure, so we can activate our prefrontal cortex as\na part of the dopaminergic pathway and eventually get ourselves motivated.\nThinking positively, practising gratitude, meditation\nRemember that aspect of the mesocorticolimbic pathway where thinking about something enjoyable caused a dopamine\nrelease? There is more to it! By repeatedly having positive or negative interactions with something we can make a\nsignificant impact on its rewarding or non-rewarding properties. It effectively means that focusing on positive aspects\nof our surroundings and activities will eventually lead to an increase of dopamine releases from engaging with it.\nHaving said that, I would strongly encourage you to use different forms of appreciation or gratitude practice. Maybe try\njournaling a little at the end of the day? I dare you to think about 3 good things that happened to you at work every\nday and write it down.\nResearch also shows that engaging in meditational practices leads to activation of reward-related areas of the brain.\nHaving a long-term habit of meditation and mindfulness will contribute to having sustained feelings of deep joy and\npeace, which is associated with higher dopamine levels.\nSummary\nYou now know how your dopamine mechanisms work and how to use your biological hardware\nto modulate your overall motivation. What you should remember is that the thing most important for your\nmotivation is maintaining a high baseline dopamine level. Avoid dopamine peaks, differentiate stimuli when you work,\nuse other tactics such as exposing yourself to cold water or meditation. Also, keep your focus\nmostly (preferably only) on your intrinsic motivation to avoid the undermining effect. Now, go conquer the world!\nReferences\nLee W, Reeve J, Xue Y, Xiong J. Neural differences between intrinsic reasons for doing versus extrinsic reasons for\ndoing: an fMRI study. Neurosci Res. 2012 May;73(1):68-72. doi: 10.1016/j.neures.2012.02.010. PMID: 23565014; PMCID:\nPMC3614004.\nLiu, C., Goel, P. \u0026 Kaeser, P.S. Spatial and temporal scales of dopamine transmission. Nat Rev Neurosci 22, 345–358 (\n2021). https://doi.org/10.1038/s41583-021-00455-7\nŠrámek, P., Šimečková, M., Janský, L. et al. Human physiological responses to immersion into water of different\ntemperatures. Eur J Appl Physiol 81, 436–442 (2000). https://doi.org/10.1007/s004210050065\nVolkow, N., Wang, GJ., Logan, J. et al. Caffeine increases striatal dopamine D2/D3 receptor availability in the human\nbrain. Transl Psychiatry 5, e549 (2015). https://doi.org/10.1038/tp.2015.46\nNeuroscience. 2nd edition. Purves D, Augustine GJ, Fitzpatrick D, et al., editors. Sunderland (MA): Sinauer\nAssociates;\nVani Pariyadath, Joshua L. Gowin, Elliot A. Stein, Chapter 8 - Resting state functional connectivity analysis for\naddiction medicine: From individual loci to complex networks, Editor(s): Hamed Ekhtiari, Martin P. Paulus, Progress\nin Brain Research, Elsevier, Volume 224, 2016\nDeci, E. L., Koestner, R., \u0026 Ryan, R. M. (1999). A meta-analytic review of experiments examining the effects of\nextrinsic rewards on intrinsic motivation. Psychological Bulletin, 125(6),\n627–668. https://doi.org/10.1037/0033-2909.125.6.627\nChen W. Neural circuits provide insights into reward and aversion. Front Neural Circuits. 2022 Oct 28;16:1002485.\ndoi:\n10.3389/fncir.2022.1002485. PMID: 36389177; PMCID: PMC9650032.\nLooby A, Zimmerman L, Livingston NR. Expectation for stimulant type modifies caffeine’s effects on mood and cognition\namong college students. Exp Clin Psychopharmacol. 2022 Oct;30(5):525-535. doi: 10.1037/pha0000448. Epub 2021 Mar 18.\nPMID: 33734725.\nArias-Carrión, O., Stamelou, M., Murillo-Rodríguez, E. et al. Dopaminergic reward system: a short integrative\nreview. Int Arch Med 3, 24 (2010)\n. https://doi.org/10.1186/1755-7682-3-24\nEsch, Tobias. “The neurobiology of meditation and mindfulness.” Meditation–neuroscientific approaches and\nphilosophical implications. Springer, Cham, 2014. 153-173.\nDi Chiara G. Alcohol and dopamine. Alcohol Health Res World. 1997;21(2):\n108-14. PMID: 15704345; PMCID: PMC6826820.\nFarahani MS, Bahramsoltani R, Farzaei MH, Abdollahi M, Rahimi R. Plant-derived natural medicines for the management\nof depression: an overview of mechanisms of action. Rev Neurosci. 2015;26(3):305-21. doi:\n10.1515/revneuro-2014-0058. PMID: 25719303.\nMechan AO, Fowler A, Seifert N, Rieger H, Wöhrle T, Etheve S, Wyss A, Schüler G, Colletto B, Kilpert C, Aston J,\nElliott JM, Goralczyk R, Mohajeri MH. Monoamine reuptake inhibition and mood-enhancing potential of a specified\noregano extract. Br J Nutr. 2011 Apr;105(8):1150-63. doi:\n10.1017/S0007114510004940. Epub 2010 Dec 21. PMID: 21205415.\nKhazdair MR, Boskabady MH, Hosseini M, Rezaee R, M Tsatsakis A. The effects of Crocus sativus (saffron) and its\nconstituents on nervous system: A review. Avicenna J Phytomed. 2015 Sep-Oct;5(5):376-91. PMID: 26468457; PMCID:\nPMC4599112.\nMarques A, Marconcin P, Werneck AO, Ferrari G, Gouveia ÉR, Kliegel M, Peralta M, Ihle A. Bidirectional Association\nbetween Physical Activity and Dopamine Across Adulthood-A Systematic Review. Brain Sci. 2021 Jun 23;11(7):829. doi:\n10.3390/brainsci11070829. PMID: 34201523; PMCID:\nPMC8301978.\nR. Kotarski “Inaczej”, Altenberg, 2020\nA. Huberman, Controlling Your Dopamine For Motivation, Focus \u0026 Satisfaction | Huberman Lab Podcast #39,\nYoutube, https://www.youtube.com/watch?v=QmOF0crdyRU","guid":"https://blog.allegro.tech/2023/03/neuroscience-for-software-engineers-motivation.html","categories":["tech","soft skills","neuroscience","productivity"],"isoDate":"2023-03-20T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999912833698","name":"Front-End Software Engineer - Merchant Experience","uuid":"ae8d3a17-99a8-491e-aff9-6e26ae3c4819","jobAdId":"db0fc952-c047-4dd0-9c96-c4b6650982fc","defaultJobAd":false,"refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-06-13T10:34:31.101Z","location":{"city":"Warszawa, Poznań, Wrocław","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999912833698","creator":{"name":"Martyna Maziarska"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999912830523","name":"Front-End Software Engineer","uuid":"1890289b-cd2a-4bab-8111-ecf377984fbd","jobAdId":"05b6643a-43dd-46a2-b998-bfbaa0a31c68","defaultJobAd":true,"refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-06-13T09:59:55.327Z","location":{"city":"Warszawa, Poznań, Wrocław","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999912830523","creator":{"name":"Martyna Maziarska"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999912829875","name":"Front-End Software Engineer - Delivery Experience","uuid":"08ebd3ca-f6fd-4017-a254-de47cd70590a","jobAdId":"2c5bb846-0902-4b0e-8821-6dda5cbfafa9","defaultJobAd":false,"refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-06-13T09:58:08.274Z","location":{"city":"Warszawa","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999912829875","creator":{"name":"Martyna Maziarska"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999910309044","name":"Mid/Senior Software Engineer (Java/Kotlin)","uuid":"44664606-87f7-4644-949a-7a28c3db72b2","jobAdId":"104e2540-4115-4a17-8545-001ee1df0a60","defaultJobAd":true,"refNumber":"REF4181Y","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-05-31T14:53:58.556Z","location":{"city":"Warszawa, Kraków, Poznań, Wrocław, Gdańsk","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999910309044","creator":{"name":"Paulina Siwek"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999910043146","name":"Engineering Manager (Java/Kotlin) - Merchant Experience","uuid":"b2882fca-c920-4127-8a3c-35b606577d02","jobAdId":"a4b71e28-0033-47e9-a924-0861d515c6fc","defaultJobAd":true,"refNumber":"REF4165Y","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-05-30T09:46:16.495Z","location":{"city":"Warszawa, Poznań, Wrocław, Kraków, Gdańsk","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"63c62385-eb34-4cd3-a7ad-a2479d862d05","valueLabel":"Technology Manager (CL 4-6)"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"78540d72-e837-4a9d-af54-6a8def1cc2bc","valueLabel":"6"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999910043146","language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1685697967000,"duration":7200000,"id":"293929321","name":"Allegro Tech Talks #38 - Mobile: o iOS bez spinki","date_in_series_pattern":false,"status":"past","time":1686760200000,"local_date":"2023-06-14","local_time":"18:30","updated":1686773845000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":17,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":0,"lon":0,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293929321/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwą wakacyjną, stacjonarne spotkanie z cyklu Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Fabryki Norblina (wejście Plater 3 od ul. Żelaznej). W niedalekiej odległości znajdują się dwie stacje metra linii M2, Rondo Daszyńskiego i Rondo ONZ. Autobusy, tramwaje i inne środki transportu sprawdzisz też na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false},{"created":1678978572000,"duration":111600000,"id":"292278882","name":"UX Research Confetti - III edycja ","date_in_series_pattern":false,"status":"past","time":1684915200000,"local_date":"2023-05-24","local_time":"10:00","updated":1685029049000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":33,"is_online_event":true,"eventType":"ONLINE","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/292278882/","description":"**Rejestracja na wydarzenie ➡ [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **🎉 Przedstawiamy 3. edycję UX Research Confetti organizowaną przez Allegro - bezpłatną, polską konferencję poświęconą badaniom…","visibility":"public","member_pay_fee":false},{"created":1683275557000,"duration":7200000,"id":"293341234","name":"Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu","date_in_series_pattern":false,"status":"past","time":1684425600000,"local_date":"2023-05-18","local_time":"18:00","updated":1684437308000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":19,"venue":{"id":27570147,"name":"Allegro Office - Poznań (Nowy Rynek)","lat":52.40021514892578,"lon":16.92083168029785,"repinned":true,"address_1":"Wierzbięcice 1B - budynek D","city":"Poznań","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293341234/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-37/](https://app.evenea.pl/event/allegro-tech-talk-37/) Ciąg dalszy naszych stacjonarnych spotkań Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów w kuluarach. 📌…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Nowy Rynek w budynku D. Najbliższy przystanek to Wierzbięcice i kursują tu linie tramwajowe numer 2, 5, 6, 10, 12, 18. ","visibility":"public","member_pay_fee":false},{"created":1682779438000,"duration":9000000,"id":"293215214","name":"AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć","date_in_series_pattern":false,"status":"past","time":1684252800000,"local_date":"2023-05-16","local_time":"18:00","updated":1684266490000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":41,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293215214/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-kwanty/](https://app.evenea.pl/event/allegro-tech-kwanty/) Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie…","how_to_find_us":"The Allegro office is located in Norblin Factory (entrance Plater 3, from Żelazna Street). You can check the details of the journey (buses, trams, metro) at: https://fabrykanorblina.pl/dojazd/","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"O Developer Experience, czyli jak pracuje się programist(k)om w Allegro","link":"https://podcast.allegro.tech/o-developer-experience-czyli-jak-pracuje-sie-programistkom-w-allegro/","pubDate":"Thu, 15 Jun 2023 00:00:00 GMT","content":"Czym jest Developer Experience (DX) w Allegro i kto za nie odpowiada? Co robimy, jako firma, aby nasi developerzy(-ki) mieli(-ły) jak największy komfort pracy? Czym jest Allegro Developer Platform i jak dużą swobodę w wyborze technologii pozostawia developer(k)om? Jak mierzymy zadowolenie użytkowników i efektywność naszych narzędzi? Skąd wiemy co naprawdę się sprawdza na różnych płaszczyznach?","contentSnippet":"Czym jest Developer Experience (DX) w Allegro i kto za nie odpowiada? Co robimy, jako firma, aby nasi developerzy(-ki) mieli(-ły) jak największy komfort pracy? Czym jest Allegro Developer Platform i jak dużą swobodę w wyborze technologii pozostawia developer(k)om? Jak mierzymy zadowolenie użytkowników i efektywność naszych narzędzi? Skąd wiemy co naprawdę się sprawdza na różnych płaszczyznach?","guid":"https://podcast.allegro.tech/o-developer-experience-czyli-jak-pracuje-sie-programistkom-w-allegro/","isoDate":"2023-06-15T00:00:00.000Z"},{"title":"O pracy liderek i liderów w Allegro","link":"https://podcast.allegro.tech/o-pracy-liderek-i-liderow-w-allegro/","pubDate":"Thu, 23 Feb 2023 00:00:00 GMT","content":"Jakimi umiejętnościami powinny wyróżniać się osoby na stanowiskach liderskich? Czy wykształcenie techniczne to “must have”, aby dołączyć do zespołów Tech \u0026 Data w Allegro? Czym charakteryzuje się praca liderek i liderów w Allegro oraz jak wspieramy ich rozwój? Jaki wpływ na produkt oraz organizację mają liderki i liderzy w Allegro? Jak zacząć budowanie swojej ścieżki kariery w roli liderskiej? Jakich wyzwań się spodziewać i jak sobie z nimi poradzić? Posłuchajcie rozmowy z udziałem Aliny Magowskiej - Dyrektorki obszaru User Experience i Agnieszki Jagusiak - Senior Managerki w zespole Group IT Services w Allegro.","contentSnippet":"Jakimi umiejętnościami powinny wyróżniać się osoby na stanowiskach liderskich? Czy wykształcenie techniczne to “must have”, aby dołączyć do zespołów Tech \u0026 Data w Allegro? Czym charakteryzuje się praca liderek i liderów w Allegro oraz jak wspieramy ich rozwój? Jaki wpływ na produkt oraz organizację mają liderki i liderzy w Allegro? Jak zacząć budowanie swojej ścieżki kariery w roli liderskiej? Jakich wyzwań się spodziewać i jak sobie z nimi poradzić? Posłuchajcie rozmowy z udziałem Aliny Magowskiej - Dyrektorki obszaru User Experience i Agnieszki Jagusiak - Senior Managerki w zespole Group IT Services w Allegro.","guid":"https://podcast.allegro.tech/o-pracy-liderek-i-liderow-w-allegro/","isoDate":"2023-02-23T00:00:00.000Z"},{"title":"O pracy i rozwoju w zespole IT Support","link":"https://podcast.allegro.tech/o-pracy-i-rozwoju-w-zespole-it-support/","pubDate":"Thu, 26 Jan 2023 00:00:00 GMT","content":"Jak wygląda praca w zespole, który zawsze udziela odpowiedzi na zadane pytania? Co można zautomatyzować w obszarze wsparcia IT i jaki to może mieć cel? Czy praca w zespole IT Support jest bramą do kariery w IT, może dawać możliwości rozwoju i przynosić satysfakcję? Jakie wyzwania przed tym zespołem w Allegro postawiła pandemia koronawirusa? O umożliwianiu pracownikom Grupy Allegro sprawnej pracy na narzędziach i usługach IT dostarczanych przez zespół Business Services \u0026 Automation opowiada Bartosz Kaczyński - IT Service Operations Manager w Allegro.","contentSnippet":"Jak wygląda praca w zespole, który zawsze udziela odpowiedzi na zadane pytania? Co można zautomatyzować w obszarze wsparcia IT i jaki to może mieć cel? Czy praca w zespole IT Support jest bramą do kariery w IT, może dawać możliwości rozwoju i przynosić satysfakcję? Jakie wyzwania przed tym zespołem w Allegro postawiła pandemia koronawirusa? O umożliwianiu pracownikom Grupy Allegro sprawnej pracy na narzędziach i usługach IT dostarczanych przez zespół Business Services \u0026 Automation opowiada Bartosz Kaczyński - IT Service Operations Manager w Allegro.","guid":"https://podcast.allegro.tech/o-pracy-i-rozwoju-w-zespole-it-support/","isoDate":"2023-01-26T00:00:00.000Z"},{"title":"O tym jak przygotowujemy rozwiązania dla klientów w oparciu o badania","link":"https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/","pubDate":"Thu, 12 Jan 2023 00:00:00 GMT","content":"W jaki sposób przygotowujemy rozwiązania dla klientów Allegro w oparciu o badania? Jak wygląda ścieżka projektu od eksploracji do wdrożenia i późniejszego monitorowania? Jaką korzyść dają badania usability? Dlaczego warto, aby badanie było prowadzone przez dwoje badaczy? O współpracy między badaczami i projektantami UX rozmawialiśmy z Zofią Śmierzchalską - Design Managerką i Jakubem Dodotem - Senior UX Research Managerem w Allegro.","contentSnippet":"W jaki sposób przygotowujemy rozwiązania dla klientów Allegro w oparciu o badania? Jak wygląda ścieżka projektu od eksploracji do wdrożenia i późniejszego monitorowania? Jaką korzyść dają badania usability? Dlaczego warto, aby badanie było prowadzone przez dwoje badaczy? O współpracy między badaczami i projektantami UX rozmawialiśmy z Zofią Śmierzchalską - Design Managerką i Jakubem Dodotem - Senior UX Research Managerem w Allegro.","guid":"https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/","isoDate":"2023-01-12T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"QeoYl8Gr6fKEkOuhx_7FZ","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>