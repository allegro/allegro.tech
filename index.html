<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="15"/><link rel="preload" href="/_next/static/css/f90f7240cef95199c2b1.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f90f7240cef95199c2b1.css" data-n-g=""/><link rel="preload" href="/_next/static/css/bab806b57e265845b5ec.css" as="style"/><link rel="stylesheet" href="/_next/static/css/bab806b57e265845b5ec.css" data-n-p=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/webpack-189c53927ffd3caf09c3.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework-0441fae7fd130f37dee1.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-547dee26f92077ae29b6.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-f9c1c7bd15b9b2e730cb.js" as="script"/><link rel="preload" href="/_next/static/chunks/805-39af1b772818e37efb44.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/index-fe725e13004e1b56d414.js" as="script"/></head><body><div id="__next"><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__2vWRp m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__n5PN5"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__15JNc"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">O nas</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to również ponad 1000 specjalistów IT, różnych specjalizacji, rozwijających nasz serwis. Unikatowa skala i złożoność problemów, które rozwiązujemy na co dzień, dają nam możliwość rozwoju przy bardzo różnorodnych projektach. Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie – w formie artykułów, podcastów oraz eventów.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html" title="One task — two solutions: Apache Spark or Apache Beam?"><img width="388" src="images/post-headers/default.jpg" alt="One task — two solutions: Apache Spark or Apache Beam?" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html" title="One task — two solutions: Apache Spark or Apache Beam?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">One task — two solutions: Apache Spark or Apache Beam?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">18 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/Apache Spark">#<!-- -->Apache Spark</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/Apache Beam">#<!-- -->Apache Beam</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/BigFlow">#<!-- -->BigFlow</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/BigData">#<!-- -->BigData</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/GCP">#<!-- -->GCP</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Some time ago, our team faced the issue of moving an existing Apache Spark job from an on-premise Hadoop cluster to public cloud.
While working on…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Yevgeniya Li" src="https://blog.allegro.tech/img/authors/yevgeniya.li.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/yevgeniya.li">Yevgeniya Li</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/06/python-logging.html" title="How to make context logging in Python less cumbersome"><img width="388" src="images/post-headers/python.png" alt="How to make context logging in Python less cumbersome" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/06/python-logging.html" title="How to make context logging in Python less cumbersome" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">How to make context logging in Python less cumbersome</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">29 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/python">#<!-- -->python</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/logging">#<!-- -->logging</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">This post is about the reasons behind writing a small (yet practical) library that has just been released as open-source:  LogExtraCtx
Why did I write…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Łukasz Mach" src="https://blog.allegro.tech/img/authors/lukasz.mach.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/lukasz.mach">Łukasz Mach</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/06/python-logging.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/06/python-logging.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/06/measuring-web-performance.html" title="Measuring Web Performance"><img width="388" src="images/post-headers/default.jpg" alt="Measuring Web Performance" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/06/measuring-web-performance.html" title="Measuring Web Performance" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Measuring Web Performance</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około miesiąc temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/webperf">#<!-- -->webperf</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/frontend">#<!-- -->frontend</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/perfmatters">#<!-- -->perfmatters</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/javascript">#<!-- -->javascript</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Some time ago we announced that Allegro passes Core Web Vitals assessment and thanks to that we were awarded in “Core Web Vitals Hall of…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Jerzy Jelinek" src="https://blog.allegro.tech/img/authors/jerzy.jelinek.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/jerzy.jelinek">Jerzy Jelinek</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/06/measuring-web-performance.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/06/measuring-web-performance.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html" title="Domino - financial forecasting in the age of global pandemic"><img width="388" src="images/post-headers/default.jpg" alt="Domino - financial forecasting in the age of global pandemic" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html" title="Domino - financial forecasting in the age of global pandemic" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Domino - financial forecasting in the age of global pandemic</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Accurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions,…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__1CeVw"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:3"><img alt="Piotr Gabryś" src="https://blog.allegro.tech/img/authors/piotr.gabrys.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar MuiAvatar-colorDefault" style="z-index:0">+<!-- -->2</div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/piotr.gabrys">Piotr Gabryś…</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/rola_architekta_w_allegro" title="Rola architekta w Allegro"><img src="images/podcast.png" alt="Rola architekta w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/rola_architekta_w_allegro" title="Rola architekta w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Rola architekta w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około miesiąc temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/rola_architekta_w_allegro">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/infrastruktura_Allegro" title="Infrastruktura Allegro"><img src="images/podcast.png" alt="Infrastruktura Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/infrastruktura_Allegro" title="Infrastruktura Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Infrastruktura Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/infrastruktura_Allegro">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro" title="Praca architekta ekosystemu big data w Allegro"><img src="images/podcast.png" alt="Praca architekta ekosystemu big data w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro" title="Praca architekta ekosystemu big data w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Praca architekta ekosystemu big data w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager &amp; Platform Architect w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro" title="Od inżyniera do lidera w Allegro"><img src="images/podcast.png" alt="Od inżyniera do lidera w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro" title="Od inżyniera do lidera w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Od inżyniera do lidera w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">2 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/278903176/" title="Allegro Tech Live #20: Wydajność Backendu" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #20: Wydajność Backendu"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/278903176/" title="Allegro Tech Live #20: Wydajność Backendu" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #20: Wydajność Backendu</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">17 dni temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my zagościmy…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/278903176/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/278374635/" title="UX Research Confetti" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/278374635/" title="UX Research Confetti" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">23 dni temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">🎉 Niech rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy… Dlatego łączymy siły z ekspertami z…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/278374635/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/278528964/" title="Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/278528964/" title="Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około miesiąc temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/278528964/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/277852879/" title="Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/277852879/" title="Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">2 miesiące temu<!-- -->, <!-- -->Online event</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/277852879/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (Java/Python + DevOps)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999761320996-software-engineer-javapython-devops?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Front-end Software Engineer - IT Business Services</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999761313464-front-end-software-engineer-it-business-services?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Research Engineer - Machine Learning (Reinforcement Learning)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa, Kraków, Poznań, Toruń</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999761311645-research-engineer-machine-learning-reinforcement-learning?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Technical Project Manager</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań, Warszawa</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999761176723-technical-project-manager?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Team Leader (Java/Kotlin) - Merchant Experience</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warszawa,Kraków,Poznań,Toruń,Wrocław</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999761166036-team-leader-javakotlin-merchant-experience?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=638647392538.6915;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"One task — two solutions: Apache Spark or Apache Beam?","link":"https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html","pubDate":"Mon, 28 Jun 2021 00:00:00 +0200","authors":{"author":[{"name":["Yevgeniya Li"],"photo":["https://blog.allegro.tech/img/authors/yevgeniya.li.jpg"],"url":["https://blog.allegro.tech/authors/yevgeniya.li"]}]},"content":"\u003cp\u003eSome time ago, our team faced the issue of moving an existing \u003ca href=\"https://spark.apache.org\"\u003eApache Spark\u003c/a\u003e job from an on-premise Hadoop cluster to public cloud.\nWhile working on the transition we came across another way to process data that is \u003ca href=\"https://beam.apache.org\"\u003eApache Beam\u003c/a\u003e. We were curious whether this tool had\nmore advantages in comparison to traditional Apache Spark. We wanted to find the answer relatively quickly with minimal effort. Hence, we built two projects to\nprocess the same data using these technologies. Below you can get to know the architecture of the jobs written in Apache Spark and Apache Beam.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGiven: 4 input tables (~2.5 TB/day).\u003c/li\u003e\n  \u003cli\u003eTask: join and clean data.\u003c/li\u003e\n  \u003cli\u003eResult: 4 output tables.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-28-1-task-2-solutions-spark-or-beam/bigdata-projects-architecture.png\" alt=\"scripting\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNote: Below I described our solution and used tools and technologies which do not pretend to be 100% the right approach so your results might be different.\u003c/p\u003e\n\n\u003ch3 id=\"programming-model\"\u003eProgramming model\u003c/h3\u003e\n\n\u003cp\u003eThe first comparison is about showing the way these technologies are built as capturing the general concept plays the first role in writing the code for data\nprocessing.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Based on the in-memory \u003ca href=\"https://en.wikipedia.org/wiki/MapReduce\"\u003eMapReduce\u003c/a\u003e model evolved from Hadoop MapReduce. This fact forces developers\n        to take care more of executors' memory, as mapping operations occur there. And you still need to remember about shuffling at the Join moment and how to\n        split data to make right repartitioning (too big data chunk causes overloading on machines, too small means more shuffling over the net). How did we tune\n        this job? The only way we knew how to do it was applying some \"a-la best practices\" Spark configurations, running the job with production data volume\n        (just imagine how it reflects machine resource cost!), looking at the metrics and crossing fingers that executors will stand. Finally after n-th attempt\n        it started working.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        It pretends to be a unified processing model. What does it mean? Apache Beam is based on so-called abstract pipelines that can be run on different\n        executors (you can even switch to the Spark execution environment). In our case we're using a DataFlow runner. This pipeline includes every stage of\n        processing starting from data fetching, transformation, ending with the result output. With these pipelines Apache Beam hides low-level things like\n        shuffling, repartitioning, etc. from the developer. Additionally we used \u003ca href=\"https://github.com/allegro/bigflow\"\u003eBigFlow\u003c/a\u003e open-source framework\n        developed internally in Allegro which is built to support Apache Beam data processing technology (simplifies building packages, configuration and\n        deployment processes).\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Apache Beam looks more like a framework as it abstracts the complexity of processing and hides technical details, and Spark is the\ntechnology where you literally need to dive deeper.\u003c/p\u003e\n\n\u003ch3 id=\"programming-languages-and-build-tools\"\u003eProgramming languages and build tools\u003c/h3\u003e\n\n\u003cp\u003eHere I do not want to spread hate and discuss which programming language is the best one for data processing, it is the matter of taste. I just would like to\nprovide some context of our team’s background to give you better understanding of our preferences: most of us are specialised in the software development in\nKotlin so we were a little bit biased against non JVM-based languages before starting this task.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        For the main processing part we chose Scala as it looks similar to our background in Java, although there is PySpark - version in Python. Python was\n        used only for composing a DAG file, which is basically a description of steps to be performed by \u003ca href=\"https://airflow.apache.org/\"\u003eAirflow\u003c/a\u003e\n        (the tool which automates running the job in the cluster).\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Natively available in Java and Python, but since we took advantage of the BigFlow framework, Python was used for everything.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Because of Scala, \u003ca href=\"https://www.scala-sbt.org\"\u003esbt\u003c/a\u003e is used for building the package and running the tests. This can be more beneficial for\n        Java developers who are familiar with Gradle: it uses the build.sbt configuration file, which like build.gradle declares dependencies, the Scala version\n        to use, etc.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        \u003ca href=\"https://packaging.python.org/key_projects/#setuptools\"\u003eSetuptools\u003c/a\u003e (tool for building Python packages) is run by script.py build script\n        provided by BigFlow. This framework also provides an additional mechanism for managing dependencies which is based on the standard\n        \u003ca href=\"https://github.com/jazzband/pip-tools\"\u003epip-tool\u003c/a\u003e. Additionally running tests with BigFlow CLI is more convenient and faster compared with sbt.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Scala is much closer to our team (mostly Java developers), especially at the beginning when we needed to get used to working with Python\nwithout static typing. Also Scala is more natural for Spark, so all upcoming features will be firstly supported for this programming language, educational\nresources and examples in Scala for Spark are more exhaustive.\u003c/p\u003e\n\n\u003ch3 id=\"batch-and-stream-data-processing\"\u003eBatch and stream data processing\u003c/h3\u003e\n\n\u003cp\u003eIn our projects we did not implement stream data processing, since we have batch processing only, but anyway it is probable that business requirements could\nchange so we must also consider how to do it in a better way.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Two different APIs for batch/stream data processing. You need to split data yourself by grouping by the time and it is not truly real-time processing,\n        as basically Spark divides the data stream into micro batches of X seconds called Dstreams, which is a sequence of RDDs under the hood.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Due to the unified model, processing is carried out in the same way for both batch and stream data. An additional bonus here is the useful feature for\n        windowing (a way to split data during stream processing), watermarks and triggers (handling events that come late or out-of-order).\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: although we didn’t try it ourselves, we’d bet on Apache Beam in this comparison. It looks more adapted for Streaming than Spark.\nAlthough there is a streaming extension since Spark 2.2 but libraries of streaming functions are quite limited. For us this means more efforts to apply.\u003c/p\u003e\n\n\u003ch3 id=\"testing\"\u003eTesting\u003c/h3\u003e\n\n\u003cp\u003eAlong with the development process these projects were getting more complicated - so we realized the unit testing is not enough to make us feel safe. So below\nare our researches how to ensure the jobs will be able to handle big load in a production environment.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:20%;\"\u003e\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%; text-align: center;\"\u003eUnit tests (local environment without access to real infrastructure, validation of business logic)\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e\n        \u003ca href=\"http://spark.apache.org/docs/latest\"\u003eEmbedded Spark\u003c/a\u003e\n        \u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e\n        \u003ca href=\"https://beam.apache.org/documentation/pipelines/test-your-pipeline/\"\u003eTestPipeline\u003c/a\u003e\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%; text-align: center;\"\u003eIntegration tests (validation of proper reads/writes from/to BigQuery)\u003c/td\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eNo official support for mocking BigQuery/Google Storage or embedded/test container version\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%; text-align: center;\"\u003eE2E tests (ensure job is executed, run in Cloud and has integration with GCP infrastructure)\u003c/td\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003e\n        Due to the missing implementation of testing the whole flow, we were running locally jobs to load production data, process them and store into BigQuery\n        tables for development purposes. This way we were ensuring introduced changes did not impact performance and both return identical results.\n        Note: recently BigFlow added a \u003ca href=\"https://github.com/allegro/bigflow/blob/master/docs/e2e_testing.md\"\u003esolution\u003c/a\u003e by setting real Dataflow/BigQuery\n        infrastructure while running E2E tests\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: a solution is to minimize the ratio of missing test coverage and isolate classes responsible for I/O operations to external storages\n(Google Storage and BigQuery) as much as possible. Big issue is to provide these dependencies while running tests inside CI pipeline but this is completely\nanother story. In both cases, it is not trivial task. So, for the sake of simplicity we were ensuring there are no performance issues and jobs are not broken by\nrunning them on a dev environment using the production data which also took us a lot of time.\u003c/p\u003e\n\n\u003ch3 id=\"local-run\"\u003eLocal run\u003c/h3\u003e\n\n\u003cp\u003eLocal run in our case does not mean 100% execution on the laptop: job is triggered from the local machine, but actually it has a place on the real cluster\nin Google Cloud.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Natively via Spark submit command. In our case we automated this by using Terraform: setting necessary infrastructure in GCP and running the job on the\n        cluster.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%; text-align: center;\"\u003e\n        BigFlow CLI\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: no absolute winner here, depends on your preferences. Beside this we found Terraform to be a fine separate tool to set up a local\nenvironment for running the job and could be used for Apache Beam job as well.\u003c/p\u003e\n\n\u003ch3 id=\"running-on-the-gcp\"\u003eRunning on the GCP\u003c/h3\u003e\n\n\u003cp\u003eOnce we are done with the local development, we are good to go to the Cloud! However, even here, not everything is so clear as we have two services to run our\njobs and we described our considerations about them below.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%; vertical-align: top;\"\u003e\n        \u003ca href=\"https://cloud.google.com/dataproc\"\u003eDataproc\u003c/a\u003e (Hadoop under the hood which is required to run Spark). As we do not need to have it running all the time, we deployed it for each job\n        execution, luckily we again have benefited from Terraform to do it. Also we used it to spin up the network, subnetwork, router and other things that are\n        needed to run the Spark job within Dataproc, so I would generally recommend it as a useful tool to automate things in GCP. Dataproc has autoscaling\n        feature, but it requires more actions: creating autoscaling policy in GCP and integrating it into the job. Moreover, to achieve good performance we\n        needed to play a lot with Spark configuration like tuning memory on workers, choosing appropriate number of shuffle partitions, executor instances and\n        so on.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        \u003ca href=\"https://cloud.google.com/dataflow\"\u003eDataflow\u003c/a\u003e. Due to its serverless nature we didn’t need to set up a cluster each time we wanted to process data. The next big advantage\n        of Dataflow is the Shuffle service, which addresses the shuffle issue on Spark executors as it moves heavy operation out of the worker virtual machine\n        to the service backend. Besides, there is autoscaling out-of-the-box, Streaming engine for streaming pipeline support. Generally, Dataflow is supposed\n        to be a self-managed platform, so less effort is required to configure it compared to Dataproc.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Dataflow wins in terms of ease of the setup. However, there is one more drawback here: limitation to run on the public cloud. It was so\nconvenient for us to profit from Dataflow services that it would be hard to find appropriate substitution for them. Furthermore, BigFlow framework\npositions itself as a Python framework for data processing pipelines on GCP. So if we want to migrate to another platform, this would enforce us to\nconfigure another runner so that we are able to run the job properly.\u003c/p\u003e\n\n\u003ch3 id=\"monitoring\"\u003eMonitoring\u003c/h3\u003e\n\n\u003cp\u003eIn addition to the process itself, it is crucial to have an option to observe its workflow, collect metrics and have access to logs from the machines.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:50%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Familiar dashboard with Yarn metrics at our disposal - Spark UI is the same that we had in the Hadoop cluster on-premise. The inconvenience was that we\n        had to switch between different tabs in the browser. Another issue was with Spark logs UI view, as if you open it by clicking on the particular running job, you\n        can see only the part of them. The rest can be found in GCP Logger where you need to know how to build queries to fetch them. Also Dataproc does not\n        keep the history of metrics once the cluster is shut down. You need to spin up a separate Spark history server to collect them and then configure its\n        visualisation in GCP Monitoring.\n        \u003c/td\u003e\n        \u003ctd style=\"width:50%;\"\u003e\n        Logs from workers and metrics are displayed on the same UI and are available even after the job is finished.\n        \u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: Dataflow beats Dataproc here and that is it. No comments are required.\u003c/p\u003e\n\n\u003ch3 id=\"cost\"\u003eCost\u003c/h3\u003e\n\n\u003cp\u003eLast but not least is the pricing of used GCP resources needed to process data like setting and running a cluster, storing data and queries execution in\nBigQuery, etc. Here we are most likely talking not only about the money, but also about the time. As it was basically a pilot project, we calculated the cost of\nnon-optimized jobs to see how much it is without significant tuning.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:20%;\"\u003e\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%;\"\u003eCost\u003c/td\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003e\n        Approximately on the same level, slightly in favour of Apache Spark\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%;\"\u003eTime\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e1.5h\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e1h\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eSummary\u003c/strong\u003e: cost is almost the same, however we need to highlight that Spark job has much more space to optimize while Apache Beam job already\ncontains Dataflow optimizations out-of-the-box. For example: playing more with Spark configuration, experimenting with Dataproc workers number, etc, so\nprobably it would cost less and run faster if you know how to tune it properly.\u003c/p\u003e\n\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\n\u003cp\u003eAt the end we’d like to say that Spark Job could be more beneficial if you know it well. Additionally, Dataproc requires advanced skills close to\nthe experienced DevOps engineer to organize all necessary infrastructure. But if you do not have an engineer with deep BigData experience in your team or you\nrun out of time and you are ready to pay a little bit more for out-of-the-box features, then Apache Beam + Dataflow is your choice. Also remember even if you\npay a little bit more, it means that you are saving developers’ time spent on the Spark tweaking that may bring some value.\u003c/p\u003e\n\n\u003cp\u003eBelow you can find how we’d estimate entry level for skills that were necessary for us to develop using the two technologies.\u003c/p\u003e\n\n\u003ctable\u003e\n    \u003ctr\u003e\n        \u003cth style=\"width:20%;\"\u003e\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Spark\u003c/th\u003e\n        \u003cth style=\"width:40%; text-align: center;\"\u003eApache Beam\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd rowspan=\"3\" style=\"width:20%;\"\u003eProgramming languages\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eScala (Intermediate)\u003c/td\u003e\n        \u003ctd rowspan=\"2\" style=\"width:40%; text-align: center;\"\u003ePython (Intermediate)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003e\n        Python (Basic)\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003e\n        SQL (depends on your data and constraints in the executed query, generally it is recommended to load only necessary data)\n        \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:20%;\" rowspan=\"8\"\u003eTechnology stack\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\" rowspan=\"2\"\u003eApache Spark (Advanced)\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eApache Beam (Intermediate)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eBigFlow (Advanced)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eGoogle Dataproc (Intermediate)\u003c/td\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eGoogle DataFlow (Intermediate) \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eGoogle Logging (Basic)\u003c/td\u003e\n        \u003ctd style=\"width:40%;\"\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:40%; text-align: center;\"\u003eTerraform (Intermediate)\u003c/td\u003e\n        \u003ctd style=\"width:40%;\"\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eGoogle BigQuery (Basic)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eGoogle Composer to schedule jobs in GCP (Basic)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd style=\"width:80%; text-align: center;\" colspan=\"2\"\u003eGoogle Cloud Storage (Basic)\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/table\u003e\n\u003cstyle type=\"text/css\"\u003e\n.post img{margin: 0 auto;display: block;}\ntd {\n  vertical-align: top;\n}\n@media (max-width: 400px) {\n  table {\n    font-size: 0.8em;\n  }\n}\n@media (max-width: 350px) {\n  table {\n    font-size: 0.6em;\n  }\n}\n\u003c/style\u003e\n\n","contentSnippet":"Some time ago, our team faced the issue of moving an existing Apache Spark job from an on-premise Hadoop cluster to public cloud.\nWhile working on the transition we came across another way to process data that is Apache Beam. We were curious whether this tool had\nmore advantages in comparison to traditional Apache Spark. We wanted to find the answer relatively quickly with minimal effort. Hence, we built two projects to\nprocess the same data using these technologies. Below you can get to know the architecture of the jobs written in Apache Spark and Apache Beam.\nGiven: 4 input tables (~2.5 TB/day).\nTask: join and clean data.\nResult: 4 output tables.\n\nNote: Below I described our solution and used tools and technologies which do not pretend to be 100% the right approach so your results might be different.\nProgramming model\nThe first comparison is about showing the way these technologies are built as capturing the general concept plays the first role in writing the code for data\nprocessing.\nApache Spark\n        Apache Beam\n    \nMapReduce model evolved from Hadoop MapReduce. This fact forces developers\n        to take care more of executors' memory, as mapping operations occur there. And you still need to remember about shuffling at the Join moment and how to\n        split data to make right repartitioning (too big data chunk causes overloading on machines, too small means more shuffling over the net). How did we tune\n        this job? The only way we knew how to do it was applying some \"a-la best practices\" Spark configurations, running the job with production data volume\n        (just imagine how it reflects machine resource cost!), looking at the metrics and crossing fingers that executors will stand. Finally after n-th attempt\n        it started working.\n        \n        \n        It pretends to be a unified processing model. What does it mean? Apache Beam is based on so-called abstract pipelines that can be run on different\n        executors (you can even switch to the Spark execution environment). In our case we're using a DataFlow runner. This pipeline includes every stage of\n        processing starting from data fetching, transformation, ending with the result output. With these pipelines Apache Beam hides low-level things like\n        shuffling, repartitioning, etc. from the developer. Additionally we used BigFlow open-source framework\n        developed internally in Allegro which is built to support Apache Beam data processing technology (simplifies building packages, configuration and\n        deployment processes).\n        \n    \nSummary: Apache Beam looks more like a framework as it abstracts the complexity of processing and hides technical details, and Spark is the\ntechnology where you literally need to dive deeper.\nProgramming languages and build tools\nHere I do not want to spread hate and discuss which programming language is the best one for data processing, it is the matter of taste. I just would like to\nprovide some context of our team’s background to give you better understanding of our preferences: most of us are specialised in the software development in\nKotlin so we were a little bit biased against non JVM-based languages before starting this task.\nApache Spark\n        Apache Beam\n    \nAirflow\n        (the tool which automates running the job in the cluster).\n        \n        \n        Natively available in Java and Python, but since we took advantage of the BigFlow framework, Python was used for everything.\n        \n    \nsbt is used for building the package and running the tests. This can be more beneficial for\n        Java developers who are familiar with Gradle: it uses the build.sbt configuration file, which like build.gradle declares dependencies, the Scala version\n        to use, etc.\n        \n        \n        Setuptools (tool for building Python packages) is run by script.py build script\n        provided by BigFlow. This framework also provides an additional mechanism for managing dependencies which is based on the standard\n        pip-tool. Additionally running tests with BigFlow CLI is more convenient and faster compared with sbt.\n        \n    \nSummary: Scala is much closer to our team (mostly Java developers), especially at the beginning when we needed to get used to working with Python\nwithout static typing. Also Scala is more natural for Spark, so all upcoming features will be firstly supported for this programming language, educational\nresources and examples in Scala for Spark are more exhaustive.\nBatch and stream data processing\nIn our projects we did not implement stream data processing, since we have batch processing only, but anyway it is probable that business requirements could\nchange so we must also consider how to do it in a better way.\nApache Spark\n        Apache Beam\n    \nSummary: although we didn’t try it ourselves, we’d bet on Apache Beam in this comparison. It looks more adapted for Streaming than Spark.\nAlthough there is a streaming extension since Spark 2.2 but libraries of streaming functions are quite limited. For us this means more efforts to apply.\nTesting\nAlong with the development process these projects were getting more complicated - so we realized the unit testing is not enough to make us feel safe. So below\nare our researches how to ensure the jobs will be able to handle big load in a production environment.\n\n        Apache Spark\n        Apache Beam\n    \nUnit tests (local environment without access to real infrastructure, validation of business logic)\n        \n        Embedded Spark\n        \n        \n        TestPipeline\n        \n    \nIntegration tests (validation of proper reads/writes from/to BigQuery)\n        No official support for mocking BigQuery/Google Storage or embedded/test container version\n    \nE2E tests (ensure job is executed, run in Cloud and has integration with GCP infrastructure)\n        \n        Due to the missing implementation of testing the whole flow, we were running locally jobs to load production data, process them and store into BigQuery\n        tables for development purposes. This way we were ensuring introduced changes did not impact performance and both return identical results.\n        Note: recently BigFlow added a solution by setting real Dataflow/BigQuery\n        infrastructure while running E2E tests\n        \n    \nSummary: a solution is to minimize the ratio of missing test coverage and isolate classes responsible for I/O operations to external storages\n(Google Storage and BigQuery) as much as possible. Big issue is to provide these dependencies while running tests inside CI pipeline but this is completely\nanother story. In both cases, it is not trivial task. So, for the sake of simplicity we were ensuring there are no performance issues and jobs are not broken by\nrunning them on a dev environment using the production data which also took us a lot of time.\nLocal run\nLocal run in our case does not mean 100% execution on the laptop: job is triggered from the local machine, but actually it has a place on the real cluster\nin Google Cloud.\nApache Spark\n        Apache Beam\n    \nSummary: no absolute winner here, depends on your preferences. Beside this we found Terraform to be a fine separate tool to set up a local\nenvironment for running the job and could be used for Apache Beam job as well.\nRunning on the GCP\nOnce we are done with the local development, we are good to go to the Cloud! However, even here, not everything is so clear as we have two services to run our\njobs and we described our considerations about them below.\nApache Spark\n        Apache Beam\n    \nDataproc (Hadoop under the hood which is required to run Spark). As we do not need to have it running all the time, we deployed it for each job\n        execution, luckily we again have benefited from Terraform to do it. Also we used it to spin up the network, subnetwork, router and other things that are\n        needed to run the Spark job within Dataproc, so I would generally recommend it as a useful tool to automate things in GCP. Dataproc has autoscaling\n        feature, but it requires more actions: creating autoscaling policy in GCP and integrating it into the job. Moreover, to achieve good performance we\n        needed to play a lot with Spark configuration like tuning memory on workers, choosing appropriate number of shuffle partitions, executor instances and\n        so on.\n        \n        \n        Dataflow. Due to its serverless nature we didn’t need to set up a cluster each time we wanted to process data. The next big advantage\n        of Dataflow is the Shuffle service, which addresses the shuffle issue on Spark executors as it moves heavy operation out of the worker virtual machine\n        to the service backend. Besides, there is autoscaling out-of-the-box, Streaming engine for streaming pipeline support. Generally, Dataflow is supposed\n        to be a self-managed platform, so less effort is required to configure it compared to Dataproc.\n        \n    \nSummary: Dataflow wins in terms of ease of the setup. However, there is one more drawback here: limitation to run on the public cloud. It was so\nconvenient for us to profit from Dataflow services that it would be hard to find appropriate substitution for them. Furthermore, BigFlow framework\npositions itself as a Python framework for data processing pipelines on GCP. So if we want to migrate to another platform, this would enforce us to\nconfigure another runner so that we are able to run the job properly.\nMonitoring\nIn addition to the process itself, it is crucial to have an option to observe its workflow, collect metrics and have access to logs from the machines.\nApache Spark\n        Apache Beam\n    \nSummary: Dataflow beats Dataproc here and that is it. No comments are required.\nCost\nLast but not least is the pricing of used GCP resources needed to process data like setting and running a cluster, storing data and queries execution in\nBigQuery, etc. Here we are most likely talking not only about the money, but also about the time. As it was basically a pilot project, we calculated the cost of\nnon-optimized jobs to see how much it is without significant tuning.\n\n        Apache Spark\n        Apache Beam\n    \nCost\n        \n        Approximately on the same level, slightly in favour of Apache Spark\n        \n    \nTime\n        1.5h\n        1h\n    \nSummary: cost is almost the same, however we need to highlight that Spark job has much more space to optimize while Apache Beam job already\ncontains Dataflow optimizations out-of-the-box. For example: playing more with Spark configuration, experimenting with Dataproc workers number, etc, so\nprobably it would cost less and run faster if you know how to tune it properly.\nConclusion\nAt the end we’d like to say that Spark Job could be more beneficial if you know it well. Additionally, Dataproc requires advanced skills close to\nthe experienced DevOps engineer to organize all necessary infrastructure. But if you do not have an engineer with deep BigData experience in your team or you\nrun out of time and you are ready to pay a little bit more for out-of-the-box features, then Apache Beam + Dataflow is your choice. Also remember even if you\npay a little bit more, it means that you are saving developers’ time spent on the Spark tweaking that may bring some value.\nBelow you can find how we’d estimate entry level for skills that were necessary for us to develop using the two technologies.\n\n        Apache Spark\n        Apache Beam\n    \nProgramming languages\n        Scala (Intermediate)\n        Python (Intermediate)\n    \nTechnology stack\n        Apache Spark (Advanced)\n        Apache Beam (Intermediate)\n    \nBigFlow (Advanced)\n    \nGoogle Dataproc (Intermediate)\n        Google DataFlow (Intermediate) \n    \nGoogle Logging (Basic)\n        \n    \nTerraform (Intermediate)\n        \n    \nGoogle BigQuery (Basic)\n    \nGoogle Composer to schedule jobs in GCP (Basic)\n    \nGoogle Cloud Storage (Basic)\n    \n\n\n.post img{margin: 0 auto;display: block;}\ntd {\n  vertical-align: top;\n}\n@media (max-width: 400px) {\n  table {\n    font-size: 0.8em;\n  }\n}\n@media (max-width: 350px) {\n  table {\n    font-size: 0.6em;\n  }\n}","guid":"https://blog.allegro.tech/2021/06/1-task-2-solutions-spark-or-beam.html","categories":["tech","Apache Spark","Apache Beam","BigFlow","BigData","GCP"],"isoDate":"2021-06-27T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How to make context logging in Python less cumbersome","link":"https://blog.allegro.tech/2021/06/python-logging.html","pubDate":"Thu, 17 Jun 2021 00:00:00 +0200","authors":{"author":[{"name":["Łukasz Mach"],"photo":["https://blog.allegro.tech/img/authors/lukasz.mach.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.mach"]}]},"content":"\u003cp\u003eThis post is about the reasons behind writing a small (yet practical) library that has just been released as open-source:  \u003ca href=\"https://github.com/allegro/logextractx\"\u003eLogExtraCtx\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2 id=\"why-did-i-write-this-library\"\u003eWhy did I write this library?\u003c/h2\u003e\n\n\u003cp\u003eI’m a big fan of logging. I like to log as much extra data as possible, and I’m\nfond of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogging.debug\u003c/code\u003e entries.\u003c/p\u003e\n\n\u003cp\u003eI’m also a DRY approach believer. I feel strong anxiety when I see repetition in code.\nAnd my mind literally hangs when I need to do \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCtrl-C/V\u003c/code\u003e, even if it’s justified by\ncircumstances. In such cases I start to focus on getting rid of copy-pastes instead of\nwriting new code.\u003c/p\u003e\n\n\u003cp\u003eCombining all these “passions” is not always easy. It’s hard to log everything without\nrepeating things. Even if it’s possible, it usually leads to inelegant code.\u003c/p\u003e\n\n\u003cp\u003eFor example, if you want to log that some error occured:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Foo happened: %s'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThen so far it’s clean and easy.\u003c/p\u003e\n\n\u003cp\u003eIt would be nice to add some extra details though, like \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euser\u003c/code\u003e\n(then you could search by \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euser\u003c/code\u003e in \u003ca href=\"https://www.elastic.co/kibana\"\u003eKibana\u003c/a\u003e, if your logs goes there):\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Foo happened: %s'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                           \u003cspan class=\"s\"\u003e'action_type'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'bar'\u003c/span\u003e\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eNext, during debugging of a problem, you will notice that it’s not nearly enough and it’s worth\nadding some \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogging.debug\u003c/code\u003e\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edebug\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"We're going to do SOMETHING in thread\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n             \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'action_type'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'bar'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'thread_num'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003ethread_num\u003c/span\u003e\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAfter that you will notice that your code has \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eextra=\u003c/code\u003e with duplicated \u003ccode class=\"language-plaintext highlighter-rouge\"\u003euser\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eaction_type\u003c/code\u003e.\nIt’s a Bad Thing! Imagine what would happen if there was another \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogger.debug\u003c/code\u003e, and another?\nLots of repeated code that should be written only once…\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"https://i.imgflip.com/54peqd.jpg\" alt=\"I see copypastes in code\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eSo in other words, the more details you log, the more cumbersome the code. What can we\ndo?\u003c/p\u003e\n\n\u003ch3 id=\"method-1-dont-log-extra\"\u003eMethod 1: don’t log \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eextra\u003c/code\u003e\u003c/h3\u003e\n\n\u003cp\u003eIt’s not what I like. I \u003cstrong\u003edo\u003c/strong\u003e want to log them!\u003c/p\u003e\n\n\u003ch3 id=\"method-2-store-extra-in-a-variable\"\u003eMethod 2: store \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eextra\u003c/code\u003e in a variable\u003c/h3\u003e\n\n\u003cp\u003eIt’s what I used to do sometimes, before \u003ca href=\"https://github.com/allegro/logextractx\"\u003eLogExtraCtx\u003c/a\u003e. It could go like that:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eextra\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'action_type'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'bar'\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edebug\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"We're going to do SOMETHING in thread\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n             \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003edict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ethread_num\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ethread_num\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e\u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Foo happened: %s'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eQuite tricky and not very elegant.\u003c/p\u003e\n\n\u003ch3 id=\"method-3-use-logextractx\"\u003eMethod 3: use \u003ca href=\"https://github.com/allegro/logextractx\"\u003eLogExtraCtx\u003c/a\u003e\u003c/h3\u003e\n\n\u003cp\u003eBefore I describe the aforementioned library, let me show you a more realistic example.\u003c/p\u003e\n\n\u003cp\u003eConsider the following code:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003elogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elogging\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\n\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esend_message\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"\"\" Function send_message sends message to the specified recipient.  \"\"\"\u003c/span\u003e\n    \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erequests\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epost\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esettings\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eMSG_PROVIDER\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'content'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n                          \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eother\u003c/span\u003e \u003cspan class=\"n\"\u003eparams\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e....)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eraise_for_status\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"n\"\u003erequests\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexceptions\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRequestException\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Sending message failed. Response text: \"%s\"'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                     \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# extra data to be logged and indexed by Kibana\n\u003c/span\u003e                         \u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                         \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                         \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                     \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\n    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Sending MSG success.'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# the same extra data to be logged/indexed\n\u003c/span\u003e                    \u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThere are two log entries, both with extra data for easy-finding in Kibana (there is a bit\nof redundancy in the code, so I already feel the desire to DRY it).\u003c/p\u003e\n\n\u003cp\u003eThen I need to add additional logging:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edebug\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"headers=%r\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheaders\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n             \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\n                    \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAnd then, I need even more context in every log entry:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eextra\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n         \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n         \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n         \u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n         \u003cspan class=\"s\"\u003e'environment'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eenv_type\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n         \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eCombined all together, it becomes a big, unreadable blob of code. A very simple piece of logic has been\nspoiled by 3 log entries.\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003elogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elogging\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\n\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esend_message\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eenvironment\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"\u003c/span\u003e\n    \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erequests\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epost\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esettings\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eMSG_PROVIDER\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'content'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n                          \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eother\u003c/span\u003e \u003cspan class=\"n\"\u003eparams\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e....)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eraise_for_status\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"n\"\u003erequests\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexceptions\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRequestException\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Sending MSG failed. Response text: \"%s\"'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                     \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e   \u003cspan class=\"c1\"\u003e# extra data to be logged and indexed by Kibana\n\u003c/span\u003e                         \u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                         \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                         \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                         \u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                         \u003cspan class=\"s\"\u003e'environment'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eenv_type\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                     \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n        \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edebug\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"headers=%r\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheaders\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                     \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                            \u003cspan class=\"s\"\u003e'environment'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eenv_type\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\n    \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Sending MSG success.'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e  \u003cspan class=\"c1\"\u003e# the same extra data to be logged/indexed\n\u003c/span\u003e                    \u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                    \u003cspan class=\"s\"\u003e'environment'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eenv_type\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"p\"\u003e})\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch4 id=\"my-solution--logextractx\"\u003eMy solution — \u003cem\u003eLogExtraCtx\u003c/em\u003e:\u003c/h4\u003e\n\n\u003cp\u003eIn order to use it, just replace \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogging.getLogger\u003c/code\u003e with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetLogger\u003c/code\u003e from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogextractx.logger\u003c/code\u003e,\nand then create local logger with local context:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elogextractx.logger\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\n\u003cspan class=\"n\"\u003elogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e[...]\u003c/span\u003e\n\u003cspan class=\"n\"\u003eloclogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'DATA_IN'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'CURRENT_CONTEXT'\u003c/span\u003e\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eso the previous example is reduced to the following clean code:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elogextractx.logger\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\n\u003cspan class=\"n\"\u003elogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003esend_message\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eenvironment\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e \u003cspan class=\"nb\"\u003ebool\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e# extra data to be logged/indexed\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eloclogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocal\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eextra\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'ACTION_TYPE'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'SEND_MSG'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                    \u003cspan class=\"s\"\u003e'requester'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erequester\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                    \u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                                    \u003cspan class=\"s\"\u003e'user'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nb\"\u003estr\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003erequest\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euser\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n                                    \u003cspan class=\"s\"\u003e'environment'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003eenv_type\u003c/span\u003e\u003cspan class=\"p\"\u003e})\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003etry\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003er\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erequests\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epost\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esettings\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eMSG_PROVIDER\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ejson\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e{\u003c/span\u003e\u003cspan class=\"s\"\u003e'recipient'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003erecipient\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'content'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"p\"\u003e},\u003c/span\u003e\n                          \u003cspan class=\"p\"\u003e...\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"n\"\u003eother\u003c/span\u003e \u003cspan class=\"n\"\u003eparams\u003c/span\u003e \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e....)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eraise_for_status\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eexcept\u003c/span\u003e \u003cspan class=\"n\"\u003erequests\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexceptions\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eRequestException\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloclogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eerror\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Sending MSG failed. Response text: \"%s\"'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ee\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eloclogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edebug\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"headers=%r\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003er\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eresult\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheaders\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eloclogger\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einfo\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e'Sending MSG success.'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eTrue\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2 id=\"interesting-and-useful-side-effect\"\u003eInteresting and useful “side effect”\u003c/h2\u003e\n\n\u003cp\u003eUsually, it’s hard to distinguish log entries from various users. For example when you have an error in\nyour code and you find \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eIndexError\u003c/code\u003e, you cannot be \u003cstrong\u003ereally sure\u003c/strong\u003e to which request it belongs.\u003c/p\u003e\n\n\u003cp\u003eOf course, you can guess, based on chronology and many other symptoms,\nbut if you have many concurrent requests, then it’s hard or even impossible to associate \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eERROR\u003c/code\u003e log\nwith previous \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eINFO\u003c/code\u003e or \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDEBUG\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eSo it’s nice to have some kind of tracking ID (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erequest-id\u003c/code\u003e), that sticks to the request,\nfollows it and is added to every log entry until the end of request processing. It’s also worth having\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esession-id\u003c/code\u003e attached to all requests that belong to given HTTP session.\u003c/p\u003e\n\n\u003cp\u003eTo use it in your \u003ca href=\"https://www.djangoproject.com/\"\u003eDjango\u003c/a\u003e project, you should use the following:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eappend \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogextractx.middleware.LogCtxDjangoMiddleware\u003c/code\u003e to your \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMIDDLEWARE\u003c/code\u003e in settings:\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\n\u003cspan class=\"n\"\u003eMIDDLEWARE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e[...]\u003c/span\u003e\n     \u003cspan class=\"s\"\u003e'django.contrib.sessions.middleware.SessionMiddleware'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e[...]\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e'logextractx.middleware.LogCtxDjangoMiddleware'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n \u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAnd instead of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogextractx.logger\u003c/code\u003e use \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogextractx.middleware\u003c/code\u003e:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003elogextractx.middleware\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\n\u003cspan class=\"n\"\u003elogger\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003egetLogger\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003e__name__\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e[...]\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAlso, you need to add filter into logging\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\n    \u003cspan class=\"s\"\u003e'filters'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"s\"\u003e'RidFilter'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"s\"\u003e'()'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'logextractx.middleware.RidFilter'\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAnd that’s all. Now every log entry will contain \u003ccode class=\"language-plaintext highlighter-rouge\"\u003erequest-id\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esession-id\u003c/code\u003e fields,\nwhat looks so nice in Kibana:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-17-python-logging/kibana-clean.png\" alt=\"kibana-example\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"extra-formatter\"\u003eExtra Formatter\u003c/h2\u003e\n\n\u003cp\u003eIf you use plain logging format, instead of Kibana + JSON formatter, then you may be interested in using\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003elogextractx.formatter.ExtraFormatter\u003c/code\u003e. Just add following in your formatter definition (DictConfig):\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\n        \u003cspan class=\"s\"\u003e'formatters'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"s\"\u003e'simple'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n                \u003cspan class=\"s\"\u003e'()'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'logextractx.formatter.ExtraFormatter'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                \u003cspan class=\"s\"\u003e'fmt'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003e'%(levelname)s %(asctime)s %(name)s: %(message)s [%(extras)s]'\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAnd then you will have all \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eextra\u003c/code\u003e in a single log line.\u003c/p\u003e\n\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\n\u003cp\u003eLogging a lot of details is good, but when it leads to breaching the DRY approach, I encourage you\nto use \u003ca href=\"https://github.com/allegro/logextractx\"\u003eLogExtraCtx\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAlso feel free to contribute — PRs are welcome.\u003c/p\u003e\n\n","contentSnippet":"This post is about the reasons behind writing a small (yet practical) library that has just been released as open-source:  LogExtraCtx\nWhy did I write this library?\nI’m a big fan of logging. I like to log as much extra data as possible, and I’m\nfond of logging.debug entries.\nI’m also a DRY approach believer. I feel strong anxiety when I see repetition in code.\nAnd my mind literally hangs when I need to do Ctrl-C/V, even if it’s justified by\ncircumstances. In such cases I start to focus on getting rid of copy-pastes instead of\nwriting new code.\nCombining all these “passions” is not always easy. It’s hard to log everything without\nrepeating things. Even if it’s possible, it usually leads to inelegant code.\nFor example, if you want to log that some error occured:\n\nlogger.error('Foo happened: %s', e)\n\n\nThen so far it’s clean and easy.\nIt would be nice to add some extra details though, like user\n(then you could search by user in Kibana, if your logs goes there):\n\nlogger.error('Foo happened: %s', e, extra={'user': user,\n                                           'action_type': 'bar'})\n\n\nNext, during debugging of a problem, you will notice that it’s not nearly enough and it’s worth\nadding some logging.debug\n\nlogger.debug(\"We're going to do SOMETHING in thread\",\n             extra={'user': user,\n                    'action_type': 'bar',\n                    'thread_num': thread_num})\n\n\nAfter that you will notice that your code has extra= with duplicated user and action_type.\nIt’s a Bad Thing! Imagine what would happen if there was another logger.debug, and another?\nLots of repeated code that should be written only once…\n\nSo in other words, the more details you log, the more cumbersome the code. What can we\ndo?\nMethod 1: don’t log extra\nIt’s not what I like. I do want to log them!\nMethod 2: store extra in a variable\nIt’s what I used to do sometimes, before LogExtraCtx. It could go like that:\n\nextra = {'user': user, 'action_type': 'bar'}\n\nlogger.debug(\"We're going to do SOMETHING in thread\",\n             extra=dict(thread_num=thread_num, **extra))\n\nlogger.error('Foo happened: %s', e, extra=extra)\n\n\nQuite tricky and not very elegant.\nMethod 3: use LogExtraCtx\nBefore I describe the aforementioned library, let me show you a more realistic example.\nConsider the following code:\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_message(requester: str, recipient: str, text: str) -\u003e bool:\n    \"\"\" Function send_message sends message to the specified recipient.  \"\"\"\n    try:\n        r = requests.post(settings.MSG_PROVIDER, json={'recipient': recipient, 'content': text},\n                          ... \u003c other params \u003e ....)\n        r.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        logger.error('Sending message failed. Response text: \"%s\"', e,\n                     extra={  # extra data to be logged and indexed by Kibana\n                         'ACTION_TYPE': 'SEND_MSG',\n                         'requester': requester,\n                         'recipient': recipient,\n                     })\n        return False\n    logger.info('Sending MSG success.',\n                extra={  # the same extra data to be logged/indexed\n                    'ACTION_TYPE': 'SEND_MSG',\n                    'requester': requester,\n                    'recipient': recipient,\n                })\n    return True\n\n\nThere are two log entries, both with extra data for easy-finding in Kibana (there is a bit\nof redundancy in the code, so I already feel the desire to DRY it).\nThen I need to add additional logging:\n\nlogger.debug(\"headers=%r\", r.result.headers,\n             extra={'ACTION_TYPE': 'SEND_MSG',\n                    'requester': requester,\n                    'recipient': recipient\n                    })\n\n\nAnd then, I need even more context in every log entry:\n\nextra = {'ACTION_TYPE': 'SEND_MSG',\n         'requester': requester,\n         'recipient': recipient,\n         'user': str(request.user),\n         'environment': env_type,\n         }\n\n\nCombined all together, it becomes a big, unreadable blob of code. A very simple piece of logic has been\nspoiled by 3 log entries.\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_message(environment: str, requester: str, recipient: str, text: str) -\u003e bool:\n    \"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"\n    try:\n        r = requests.post(settings.MSG_PROVIDER, json={'recipient': recipient, 'content': text},\n                          ... \u003c other params \u003e ....)\n        r.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        logger.error('Sending MSG failed. Response text: \"%s\"', e,\n                     extra={   # extra data to be logged and indexed by Kibana\n                         'ACTION_TYPE': 'SEND_MSG',\n                         'requester': requester,\n                         'recipient': recipient,\n                         'user': str(request.user),\n                         'environment': env_type,\n                     })\n        logger.debug(\"headers=%r\", r.result.headers,\n                     extra={'ACTION_TYPE': 'SEND_MSG',\n                            'requester': requester,\n                            'recipient': recipient,\n                            'user': str(request.user),\n                            'environment': env_type,\n                            })\n        return False\n    logger.info('Sending MSG success.',\n                extra={  # the same extra data to be logged/indexed\n                    'ACTION_TYPE': 'SEND_MSG',\n                    'requester': requester,\n                    'recipient': recipient,\n                    'user': str(request.user),\n                    'environment': env_type,\n                })\n    return True\n\n\nMy solution — LogExtraCtx:\nIn order to use it, just replace logging.getLogger with getLogger from logextractx.logger,\nand then create local logger with local context:\n\nfrom logextractx.logger import getLogger\nlogger = getLogger(__name__)\n[...]\nloclogger = logger.local(extra={'DATA_IN': 'CURRENT_CONTEXT'})\n\n\nso the previous example is reduced to the following clean code:\n\nfrom logextractx.logger import getLogger\nlogger = getLogger(__name__)\n\ndef send_message(environment: str, requester: str, recipient: str, text: str) -\u003e bool:\n    \"\"\" Function send_message sends MSG to the specified recipient.  \"\"\"\n\n    # extra data to be logged/indexed\n    loclogger = logger.local(extra={'ACTION_TYPE': 'SEND_MSG',\n                                    'requester': requester,\n                                    'recipient': recipient,\n                                    'user': str(request.user),\n                                    'environment': env_type})\n\n    try:\n        r = requests.post(settings.MSG_PROVIDER, json={'recipient': recipient, 'content': text},\n                          ... \u003c other params \u003e ....)\n        r.raise_for_status()\n    except requests.exceptions.RequestException as e:\n        loclogger.error('Sending MSG failed. Response text: \"%s\"', e)\n        loclogger.debug(\"headers=%r\", r.result.headers)\n        return False\n    loclogger.info('Sending MSG success.')\n    return True\n\n\nInteresting and useful “side effect”\nUsually, it’s hard to distinguish log entries from various users. For example when you have an error in\nyour code and you find IndexError, you cannot be really sure to which request it belongs.\nOf course, you can guess, based on chronology and many other symptoms,\nbut if you have many concurrent requests, then it’s hard or even impossible to associate ERROR log\nwith previous INFO or DEBUG.\nSo it’s nice to have some kind of tracking ID (request-id), that sticks to the request,\nfollows it and is added to every log entry until the end of request processing. It’s also worth having\nsession-id attached to all requests that belong to given HTTP session.\nTo use it in your Django project, you should use the following:\nappend logextractx.middleware.LogCtxDjangoMiddleware to your MIDDLEWARE in settings:\n\n\nMIDDLEWARE = [\n    [...]\n     'django.contrib.sessions.middleware.SessionMiddleware',\n    [...]\n    'logextractx.middleware.LogCtxDjangoMiddleware',\n ]\n\n\nAnd instead of logextractx.logger use logextractx.middleware:\n\n\nfrom logextractx.middleware import getLogger\nlogger = getLogger(__name__)\n[...]\n\n\nAlso, you need to add filter into logging\n\n\n    'filters': {\n        'RidFilter': {\n            '()': 'logextractx.middleware.RidFilter'\n        }\n    }\n\n\nAnd that’s all. Now every log entry will contain request-id and session-id fields,\nwhat looks so nice in Kibana:\n\nExtra Formatter\nIf you use plain logging format, instead of Kibana + JSON formatter, then you may be interested in using\nlogextractx.formatter.ExtraFormatter. Just add following in your formatter definition (DictConfig):\n\n\n        'formatters': {\n            'simple': {\n                '()': 'logextractx.formatter.ExtraFormatter',\n                'fmt': '%(levelname)s %(asctime)s %(name)s: %(message)s [%(extras)s]'\n            }\n        }\n\n\nAnd then you will have all extra in a single log line.\nConclusion\nLogging a lot of details is good, but when it leads to breaching the DRY approach, I encourage you\nto use LogExtraCtx.\nAlso feel free to contribute — PRs are welcome.","guid":"https://blog.allegro.tech/2021/06/python-logging.html","categories":["tech","python","logging"],"isoDate":"2021-06-16T22:00:00.000Z","thumbnail":"images/post-headers/python.png"},{"title":"Measuring Web Performance","link":"https://blog.allegro.tech/2021/06/measuring-web-performance.html","pubDate":"Tue, 08 Jun 2021 00:00:00 +0200","authors":{"author":[{"name":["Jerzy Jelinek"],"photo":["https://blog.allegro.tech/img/authors/jerzy.jelinek.jpg"],"url":["https://blog.allegro.tech/authors/jerzy.jelinek"]}]},"content":"\u003cp\u003eSome time ago we announced that Allegro passes Core Web Vitals assessment and thanks to that we were awarded in “\u003ca href=\"https://www.thinkwithgoogle.com/intl/en-cee/marketing-strategies/app-and-mobile/why-should-73-of-polish-websites-have-a-closer-look-at-their-mobile-user-experience/\" title=\"Core Web Vitals Hall of Fame\"\u003eCore Web Vitals Hall of Fame\u003c/a\u003e”.\nIt means that Allegro is in the group of the 27% fastest websites in Polish Internet.\u003c/p\u003e\n\n\u003cp\u003eIn this series of articles, Webperf team and I want to tell you what our daily work has been like over the years,\nwhat we’ve optimized and what we’ve failed at, and how the perception of web performance has changed at our company.\u003c/p\u003e\n\n\u003cdiv class=\"twitter-tweet twitter-tweet-rendered\" style=\"width: 100%; margin: 10px auto; display: flex; max-width: 550px;\"\u003e\u003ciframe id=\"twitter-widget-0\" scrolling=\"no\" allowtransparency=\"true\" allowfullscreen=\"true\" class=\"\" style=\"position: static; visibility: visible; width: 550px; height: 544px; display: block; flex-grow: 1;\" title=\"Twitter Tweet\" src=\"https://platform.twitter.com/embed/Tweet.html?creatorScreenName=allegrotech\u0026amp;dnt=false\u0026amp;embedId=twitter-widget-0\u0026amp;features=eyJ0ZndfZXhwZXJpbWVudHNfY29va2llX2V4cGlyYXRpb24iOnsiYnVja2V0IjoxMjA5NjAwLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X2hvcml6b25fdHdlZXRfZW1iZWRfOTU1NSI6eyJidWNrZXQiOiJodGUiLCJ2ZXJzaW9uIjpudWxsfSwidGZ3X3R3ZWV0X2VtYmVkX2NsaWNrYWJpbGl0eV8xMjEwMiI6eyJidWNrZXQiOiJjb250cm9sIiwidmVyc2lvbiI6bnVsbH19\u0026amp;frame=false\u0026amp;hideCard=false\u0026amp;hideThread=false\u0026amp;id=1331547521139822592\u0026amp;lang=en\u0026amp;origin=https%3A%2F%2Fblog.allegro.tech%2F2021%2F06%2Fmeasuring-web-performance.html\u0026amp;sessionId=1ab4cf28b64fcefdb722376a0651b436e188c01b\u0026amp;theme=light\u0026amp;widgetsVersion=82e1070%3A1619632193066\u0026amp;width=550px\" data-tweet-id=\"1331547521139822592\" frameborder=\"0\"\u003e\u003c/iframe\u003e\u003c/div\u003e\n\n\u003cp\u003eOur path to a quite fast page (we’re still hoping for more) was winding, bumpy, more than once ended in a dead end and forced us to rethink our solutions.\nWe want to show that there is no magical \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e{ perf: true }\u003c/code\u003e option and that some things you just have to figure out by trial and error.\u003c/p\u003e\n\n\u003ch2 id=\"beginnings\"\u003eBeginnings\u003c/h2\u003e\n\n\u003cp\u003eIt all started with a group of enthusiasts concerned about the poor performance of \u003ca href=\"https://allegro.pl\" title=\"Allegro.pl\"\u003eAllegro\u003c/a\u003e and the lack of actions to improve it.\nTheir grassroots initiative was appreciated and the core of the technical team (Webperf) was formed.\nThere was one major problem — it is relatively easy to make micro-optimizations in the code of one component,\nhowever, it is much more difficult to push through a major change involving different teams or business areas.\nThe company needed to know how the change would affect not only performance but also the business.\nAt that time there were many success stories from various companies on the internet about how improvements in loading speed had impacted their business results.\nHowever, at Allegro, we had never seen correlation between performance and business. It was our holy grail to be found and as a first step,\nthe decision was made to collect performance measures that could be linked to business data in the future.\u003c/p\u003e\n\n\u003ch2 id=\"measurement\"\u003eMeasurement\u003c/h2\u003e\n\n\u003cp\u003eThe idea was simple, we wanted to:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ecreate a library which would use Performance API to create marks on the client side,\u003c/li\u003e\n  \u003cli\u003euse an existing mechanism for sending events to the backend,\u003c/li\u003e\n  \u003cli\u003estore the information in a database so we could easily operate on it,\u003c/li\u003e\n  \u003cli\u003eand finally display basic metrics on a dashboard.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBut first things first.\u003c/p\u003e\n\n\u003ch3 id=\"metrics-library\"\u003eMetrics Library\u003c/h3\u003e\n\n\u003cp\u003eThe first commits to the library collecting performance measures (called Pinter) took place on June 5, 2017. Since then, it has been actively developed.\u003c/p\u003e\n\n\u003cp\u003eWe collect two types of measures in Pinter:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eStandard, e.g., Web Vitals,\u003c/li\u003e\n  \u003cli\u003eCustom, e.g., Time To Component Interactive.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"principle-of-operation-of-the-pinter\"\u003ePrinciple of operation of the Pinter\u003c/h4\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-08-measuring-web-performance/pinter-diagram.jpg\" alt=\"Pinter principle of operation of Pinter\" title=\"Pinter principle of operation of Pinter\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIn general, metrics’ changes are tracked using PerformanceObserver from which values are collected, processed into a performance event and sent to the backend.\u003c/p\u003e\n\n\u003cp\u003eHowever, there are several metrics, e.g., Navigation Timing, Resource Timing or Benchmark that are only sent once, after the document has loaded.\u003c/p\u003e\n\n\u003cp\u003eOur script, like any other, can affect web performance. This is why the traffic is sampled and the library itself is not served to all users.\u003c/p\u003e\n\n\u003ch4 id=\"collected-measures\"\u003eCollected measures\u003c/h4\u003e\n\n\u003cp\u003eThe browser provides a whole bunch of APIs to analyze resources, connections etc. Combined with data from the DOM tree, we have a general picture of what the user experience was like.\u003c/p\u003e\n\n\u003cp\u003eBelow is a slice of what we are collecting and why:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003ca href=\"https://web.dev/learn-web-vitals/\"\u003eWeb Vitals\u003c/a\u003e\u003c/strong\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eFirst Contentful Paint\u003c/strong\u003e — when the first content on the page was rendered.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eLargest Contentful Paint\u003c/strong\u003e — what is the largest image or text block on the page and when it appeared on the screen.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eCumulative Layout Shift\u003c/strong\u003e — layout stability.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eFirst Input Delay\u003c/strong\u003e — how quickly the first user interaction is handled.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCustom Marks\u003c/strong\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eTime To Component Interactive\u003c/strong\u003e — when the critical component is fully interactive and can handle all user actions, e.g. after React rehydration.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eNavigation\u003c/strong\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eType\u003c/strong\u003e — what \u003ca href=\"https://developer.mozilla.org/en-US/docs/Web/API/PerformanceNavigationTiming/type\"\u003etype of navigation\u003c/a\u003e the user was using. Useful when analyzing metrics.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eTiming\u003c/strong\u003e — data from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ewindow.performance.timing\u003c/code\u003e about connection, response time, load time etc.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eResources\u003c/strong\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eTransfer Size\u003c/strong\u003e — the total size of scripts and styles transferred over the network.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eTotal Encoded Body Size\u003c/strong\u003e — the total size of scripts and styles on the page. Is not distorted by cache.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eResource count\u003c/strong\u003e — number of assets with breakdown into styles, internal scripts and 3rd party scripts.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eBenchmark\u003c/strong\u003e — information about the performance of a given device. We want to know if weaker devices perform worse and if our fixes have a positive impact on them.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"the-backend\"\u003eThe backend\u003c/h3\u003e\n\n\u003cp\u003eAll collected performance data is sent to the backend where it is anonymized, aggregated and prepared to be displayed on charts.\nIt is a complex system which allows us to operate only on the necessary portion of data.\u003c/p\u003e\n\n\u003ch4 id=\"principle-of-operation-of-the-backend\"\u003ePrinciple of operation of the backend\u003c/h4\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-08-measuring-web-performance/backend-diagram.jpg\" alt=\"backend principle of operation\" title=\"backend principle of operation\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eInitially, all events (including performance ones) are gathered and stored in a single \u003ca href=\"https://hive.apache.org/\"\u003eHIVE\u003c/a\u003e table.\nWe want to be able to quickly analyze as well as compare historical records, but this amount of data would effectively prevent us from doing so.\nTherefore, we need a whole process to extract the most relevant information. We transform the filtered performance events combined\nwith more general data (page route, device details etc.) to a new, smaller Hive table. Then we index this data in \u003ca href=\"https://druid.apache.org/\"\u003eDruid\u003c/a\u003e\n(high performance real-time analytics database), which is consumed by \u003ca href=\"/2018/10/turnilo-lets-change-the-way-people-explore-big-data.html\"\u003eTurnilo\u003c/a\u003e and Grafana.\nOnce the entire process is complete, we are able to filter, split, plot and generally process about 2TB of data in real time as needed.\u003c/p\u003e\n\n\u003ch4 id=\"visualizations\"\u003eVisualizations\u003c/h4\u003e\n\n\u003cp\u003eWe use two independent systems to present the data:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://grafana.com/\"\u003eGrafana\u003c/a\u003e, which is used for daily monitoring.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/allegro/turnilo\"\u003eTurnilo\u003c/a\u003e, which is used for analyzing anomalies or testing the impact of A/B experiments.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch5 id=\"grafana\"\u003eGrafana\u003c/h5\u003e\n\n\u003cp\u003eOur dashboard gathers the most important metrics which allow us to catch potential performance problems but it is not used for analysis.\nIt is worth noting that we display data only for mobile devices. We do this for a reason: in general those devices\nare not as efficient as desktops and the share of phones in Allegro traffic is growing day by day.\nWe assume that improving performance on mobile devices would have a positive impact on desktops as well.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-08-measuring-web-performance/grafana-screen.png\" alt=\"Grafana screenshot\" title=\"Grafana screenshot\" /\u003e\u003c/p\u003e\n\n\u003ch5 id=\"turnilo\"\u003eTurnilo\u003c/h5\u003e\n\n\u003cp\u003eIt is a business intelligence, data exploration and visualization web application. Thanks to the wide range of available dimensions\nand metrics we are able to pinpoint found issues to particular pages, device types or even browser versions\nand then check if the applied solution actually worked.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-08-measuring-web-performance/turnilo-screen.png\" alt=\"Turnilo screenshot\" title=\"Turnilo screenshot\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"monitoring\"\u003eMonitoring\u003c/h3\u003e\n\n\u003cp\u003eChecking measures on the dashboard is our daily routine, but we are only humans and sometimes we can miss certain anomalies\nor we won’t be able to notice a changing trend so we decided to automate our work as much as possible.\nWe have created a range of detectors that notify us on Slack or mail when a predetermined threshold is exceeded.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-06-08-measuring-web-performance/monitoring-screen.png\" alt=\"Monitoring screenshot\" title=\"Monitoring screenshot\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eBefore we started our optimization work, we needed to know:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eWhat do we want to measure?\u003c/li\u003e\n  \u003cli\u003eHow will we collect this data?\u003c/li\u003e\n  \u003cli\u003eHow will we visualize and compare them?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAnswers to those questions and implementation of their results allow us to keep track of performance regression from our users. We are able to analyze how the implemented optimizations, A/B tests or content changes affect performance metrics.\u003c/p\u003e\n\n\u003cp\u003eIn the next article, we will tell you what we were able to optimize, how our metrics changed over the years and what were the failures from which we have learned a lot.\u003c/p\u003e\n","contentSnippet":"Some time ago we announced that Allegro passes Core Web Vitals assessment and thanks to that we were awarded in “Core Web Vitals Hall of Fame”.\nIt means that Allegro is in the group of the 27% fastest websites in Polish Internet.\nIn this series of articles, Webperf team and I want to tell you what our daily work has been like over the years,\nwhat we’ve optimized and what we’ve failed at, and how the perception of web performance has changed at our company.\n\nOur path to a quite fast page (we’re still hoping for more) was winding, bumpy, more than once ended in a dead end and forced us to rethink our solutions.\nWe want to show that there is no magical { perf: true } option and that some things you just have to figure out by trial and error.\nBeginnings\nIt all started with a group of enthusiasts concerned about the poor performance of Allegro and the lack of actions to improve it.\nTheir grassroots initiative was appreciated and the core of the technical team (Webperf) was formed.\nThere was one major problem — it is relatively easy to make micro-optimizations in the code of one component,\nhowever, it is much more difficult to push through a major change involving different teams or business areas.\nThe company needed to know how the change would affect not only performance but also the business.\nAt that time there were many success stories from various companies on the internet about how improvements in loading speed had impacted their business results.\nHowever, at Allegro, we had never seen correlation between performance and business. It was our holy grail to be found and as a first step,\nthe decision was made to collect performance measures that could be linked to business data in the future.\nMeasurement\nThe idea was simple, we wanted to:\ncreate a library which would use Performance API to create marks on the client side,\nuse an existing mechanism for sending events to the backend,\nstore the information in a database so we could easily operate on it,\nand finally display basic metrics on a dashboard.\nBut first things first.\nMetrics Library\nThe first commits to the library collecting performance measures (called Pinter) took place on June 5, 2017. Since then, it has been actively developed.\nWe collect two types of measures in Pinter:\nStandard, e.g., Web Vitals,\nCustom, e.g., Time To Component Interactive.\nPrinciple of operation of the Pinter\n\nIn general, metrics’ changes are tracked using PerformanceObserver from which values are collected, processed into a performance event and sent to the backend.\nHowever, there are several metrics, e.g., Navigation Timing, Resource Timing or Benchmark that are only sent once, after the document has loaded.\nOur script, like any other, can affect web performance. This is why the traffic is sampled and the library itself is not served to all users.\nCollected measures\nThe browser provides a whole bunch of APIs to analyze resources, connections etc. Combined with data from the DOM tree, we have a general picture of what the user experience was like.\nBelow is a slice of what we are collecting and why:\nWeb Vitals\n    \nFirst Contentful Paint — when the first content on the page was rendered.\nLargest Contentful Paint — what is the largest image or text block on the page and when it appeared on the screen.\nCumulative Layout Shift — layout stability.\nFirst Input Delay — how quickly the first user interaction is handled.\nCustom Marks\n    \nTime To Component Interactive — when the critical component is fully interactive and can handle all user actions, e.g. after React rehydration.\nNavigation\n    \nType — what type of navigation the user was using. Useful when analyzing metrics.\nTiming — data from window.performance.timing about connection, response time, load time etc.\nResources\n    \nTransfer Size — the total size of scripts and styles transferred over the network.\nTotal Encoded Body Size — the total size of scripts and styles on the page. Is not distorted by cache.\nResource count — number of assets with breakdown into styles, internal scripts and 3rd party scripts.\nBenchmark — information about the performance of a given device. We want to know if weaker devices perform worse and if our fixes have a positive impact on them.\nThe backend\nAll collected performance data is sent to the backend where it is anonymized, aggregated and prepared to be displayed on charts.\nIt is a complex system which allows us to operate only on the necessary portion of data.\nPrinciple of operation of the backend\n\nInitially, all events (including performance ones) are gathered and stored in a single HIVE table.\nWe want to be able to quickly analyze as well as compare historical records, but this amount of data would effectively prevent us from doing so.\nTherefore, we need a whole process to extract the most relevant information. We transform the filtered performance events combined\nwith more general data (page route, device details etc.) to a new, smaller Hive table. Then we index this data in Druid\n(high performance real-time analytics database), which is consumed by Turnilo and Grafana.\nOnce the entire process is complete, we are able to filter, split, plot and generally process about 2TB of data in real time as needed.\nVisualizations\nWe use two independent systems to present the data:\nGrafana, which is used for daily monitoring.\nTurnilo, which is used for analyzing anomalies or testing the impact of A/B experiments.\nGrafana\nOur dashboard gathers the most important metrics which allow us to catch potential performance problems but it is not used for analysis.\nIt is worth noting that we display data only for mobile devices. We do this for a reason: in general those devices\nare not as efficient as desktops and the share of phones in Allegro traffic is growing day by day.\nWe assume that improving performance on mobile devices would have a positive impact on desktops as well.\n\nTurnilo\nIt is a business intelligence, data exploration and visualization web application. Thanks to the wide range of available dimensions\nand metrics we are able to pinpoint found issues to particular pages, device types or even browser versions\nand then check if the applied solution actually worked.\n\nMonitoring\nChecking measures on the dashboard is our daily routine, but we are only humans and sometimes we can miss certain anomalies\nor we won’t be able to notice a changing trend so we decided to automate our work as much as possible.\nWe have created a range of detectors that notify us on Slack or mail when a predetermined threshold is exceeded.\n\nSummary\nBefore we started our optimization work, we needed to know:\nWhat do we want to measure?\nHow will we collect this data?\nHow will we visualize and compare them?\nAnswers to those questions and implementation of their results allow us to keep track of performance regression from our users. We are able to analyze how the implemented optimizations, A/B tests or content changes affect performance metrics.\nIn the next article, we will tell you what we were able to optimize, how our metrics changed over the years and what were the failures from which we have learned a lot.","guid":"https://blog.allegro.tech/2021/06/measuring-web-performance.html","categories":["tech","webperf","frontend","performance","perfmatters","javascript"],"isoDate":"2021-06-07T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Domino - financial forecasting in the age of global pandemic","link":"https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html","pubDate":"Fri, 21 May 2021 00:00:00 +0200","authors":{"author":[{"name":["Piotr Gabryś"],"photo":["https://blog.allegro.tech/img/authors/piotr.gabrys.jpg"],"url":["https://blog.allegro.tech/authors/piotr.gabrys"]},{"name":["Julia Bluszcz"],"photo":["https://blog.allegro.tech/img/authors/julia.bluszcz.jpg"],"url":["https://blog.allegro.tech/authors/julia.bluszcz"]},{"name":["Klaudia Walewska-Łubian"],"photo":["https://blog.allegro.tech/img/authors/klaudia.walewska-lubian.jpg"],"url":["https://blog.allegro.tech/authors/klaudia.walewska-lubian"]}]},"content":"\u003cp\u003eAccurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions, and prepare adequate resources for what is coming.\u003c/p\u003e\n\n\u003cp\u003eYet, many companies struggle with efficient and accurate revenue forecasting. For most of them the task still rests on the financial department’s shoulders, performing manual analyses in Excel, lacking data science know-how, and relying on a set of arbitrary assumptions concerning the future. This process is often inefficient and prone to error, making it hard to distinguish trend-based patterns from manual overrides. In the end, the forecasts often turn out to be inaccurate, but it is difficult to diagnose the source of divergence - was it the fault of the model itself, wrong business assumptions, or perhaps unusual circumstances.\u003c/p\u003e\n\n\u003cp\u003eIn order to overcome this problem, we decided to develop an AI tool which would allow us to forecast Allegro’s GMV (Gross Merchandise Value) based on a set of business inputs defined by the financial team. The purpose of the tool was twofold: to create automatic, monthly forecasts, and to enable creating scenarios with different input values. Examples of such inputs would be the costs of online advertising or the number of users enrolled in the loyalty program (Allegro Smart!).\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eDisclaimer:\u003c/strong\u003e analysis and graphs presented in article are based on artificial data\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"approach\"\u003eApproach\u003c/h2\u003e\n\n\u003cp\u003eWe named our project domino after Marvel’s universe foreseer-heroine Domino, but the name reflects also how the model is executed.\u003c/p\u003e\n\n\u003cp\u003eThe model works as a graph of dependencies where each element is either a cause or effect of other steps (in the technical nomenclature known as \u003ca href=\"https://en.wikipedia.org/wiki/Directed_acyclic_graph\"\u003eDAG\u003c/a\u003e). Let’s go through an artificial example. Below you can find a diagram of the model. In the training phase we need to provide historical data for every node in the graph, i.e. we need to know actual online advertising spending (\u003ca href=\"https://en.wikipedia.org/wiki/Pay-per-click\"\u003ePPC\u003c/a\u003e), important holidays, and actual number of paid and organic visits to be able to accurately model the relationship between these variables.\u003cimg src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/01-approach.png\" alt=\"diagram\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eDuring the prediction phase, we only need actual data for violet nodes. These nodes usually represent our key business assumptions which are reflected in the company’s yearly budget. Next, our goal is to recreate all the succeeding values. Some of these subsequent steps will be just arithmetic operations like adding, summing, or applying logarithms (yellow and red). Other steps, here represented with blue nodes, may be Machine Learning models (e.g. \u003ca href=\"https://facebook.github.io/prophet/\"\u003eFacebook’s Prophet\u003c/a\u003e). They can learn non-trivial effects like seasonalities, trends, holidays, and the influence of the preceding yellow nodes (i.e. explanatory variables).\u003c/p\u003e\n\n\u003cp\u003eYou may wonder why we bothered with creation of such a complex graph of dependencies instead of making a single model taking all the business inputs as explanatory variables and returning future GMV values as the output. It’s what we want to model in the end, right? Why are we concerned with reconstructing the values of organic or paid traffic along the way? Well, we found out that following business logic yields much better results than inserting all the inputs into a huge single pot. Not only were the results less accurate in the latter case, but also the impact of explanatory variables on GMV was often not clearly distinguishable or even misleading. Instead, we recreated the business scheme by creating and training specialized models to reconstruct all the intermediate steps (e.g. predicting the number of paid visits) and merging their outputs with arithmetic operations or other models. However, remember that this approach comes at a cost! Using many steps in the modeling process may be both a blessing and a curse, since you need to train and maintain multiple models simultaneously.\u003c/p\u003e\n\n\u003cp\u003eOne of the biggest advantages of this approach is that it allows us to capture very sophisticated non-linear relationships between inputs and the final target variable (GMV in our case). Following the business logic allows us to verify these non-trivial assumptions at each step.\u003c/p\u003e\n\n\u003cp\u003eAnother advantage of this approach is that every model can be fitted separately with any model class you want. Having tested a few alternatives, we’ve chosen the Prophet library, but potentially any ML algorithm could be used (e.g. ARIMA, Gradient Boosting Trees, or Artificial Neural Networks).\u003c/p\u003e\n\n\u003cp\u003eA disadvantage is that the error in prediction propagates downstream the graph. So, if we make a mistake in a prediction in an early step, it will influence all the models and transformations dependent on it. The issue can be mitigated by creating accurate models at each step of the process.\u003c/p\u003e\n\n\u003cp\u003eAnother slight disadvantage is that our Domino of models is not intrinsically interpretable (as most of the modern model classes). You have to use some post hoc methods to gather information on how the model does process data.\u003c/p\u003e\n\n\u003ch2 id=\"technical-implementation\"\u003eTechnical implementation\u003c/h2\u003e\n\n\u003cp\u003eTo develop and iterate over the DAG-type model we had to implement a custom Python framework. It allows for training and running models as well as arithmetic transformations in a predefined order.\u003c/p\u003e\n\n\u003cp\u003eThe implementation allows us to utilize various model frameworks like Facebook’s Prophet, any Scikit-learn’s regressor, or an Artificial NN. For operational purposes, MAPE (Mean Absolute Percentage Error) can be easily calculated for each model in the graph, as well as for the whole DAG (again, the errors propagate downstream).\u003c/p\u003e\n\n\u003cp\u003eBelow you can see how the above DAG can be implemented.\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003edomino.pipeline\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eModel\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eTransformer\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eCombinator\u003c/span\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003efbprophet\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003eProphet\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ePipeline\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput_list\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e'ppc_cost'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'smart_users'\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd_step\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eTransformer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebase_var\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'ppc_cost'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                             \u003cspan class=\"n\"\u003etarget_variable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'ppc_cost_ln'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                             \u003cspan class=\"n\"\u003eoperation_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'ln'\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n             \u003cspan class=\"n\"\u003estep_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'transformer1'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd_step\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eTransformer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebase_var\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'smart_users'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                             \u003cspan class=\"n\"\u003etarget_variable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'smart_users_add1'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                             \u003cspan class=\"n\"\u003eoperation_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'add1'\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n             \u003cspan class=\"n\"\u003estep_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'transformer2'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd_step\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eTransformer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebase_var\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'smart_users_add1'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                             \u003cspan class=\"n\"\u003etarget_variable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'smart_users_add1_ln'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                             \u003cspan class=\"n\"\u003eoperation_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'ln'\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n             \u003cspan class=\"n\"\u003estep_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'transformer3'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd_step\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eModel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eProphet\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n                  \u003cspan class=\"n\"\u003etarget_variable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'paid_visits'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                  \u003cspan class=\"n\"\u003eexplanatory_variables\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e'ppc_cost_ln'\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e\n         \u003cspan class=\"n\"\u003estep_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'model1'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd_step\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eModel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eProphet\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n                  \u003cspan class=\"n\"\u003etarget_variable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'non_paid_visits'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                  \u003cspan class=\"n\"\u003eexplanatory_variables\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e'smart_users_add1_ln'\u003c/span\u003e\u003cspan class=\"p\"\u003e]),\u003c/span\u003e\n             \u003cspan class=\"n\"\u003estep_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'model2'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eadd_step\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003estep\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eCombinator\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebase_var1\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'paid_visits'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"n\"\u003ebase_var2\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'non_paid_visits'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"n\"\u003etarget_variable\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'all_visits'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                            \u003cspan class=\"n\"\u003eoperation_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'add'\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n             \u003cspan class=\"n\"\u003estep_name\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'combination'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eNow we can use the dag object as a single model.\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efit\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edf_train\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003epredictions\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epredict\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edf_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eTo understand what’s happening inside the DAG, we implemented two methods of calculating MAPE on every step:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecalculate_mape_for_models\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edf_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecalculate_mape_for_dag\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edf_test\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThey both return dictionaries of model-MAPE pairs. The calculate_mape_for_model method checks each model separately, and calculate_mape_for_dag takes into account the errors propagating from preceding steps. These are examples of results:\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003emodels_mape\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'transformer1'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'transformer2'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'transformer3'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'model1'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'model2'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.05\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'combination'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edag_mape\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'transformer1'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'transformer2'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'transformer3'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'model1'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'model2'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.05\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n   \u003cspan class=\"s\"\u003e'combination'\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.09\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eNote that the combination step has MAPE equal to zero in models_mape and a positive one in dag_mape. That’s because it does not generate any error, as it’s an arithmetic operation, but it can propagate errors from previous steps.\u003c/p\u003e\n\n\u003cp\u003eLast but not least, there is an explainability method calculate_variable_impact that helps to evaluate how changes in initial inputs impact the subsequent steps in the graph. For example, we can check what is going to happen if we decrease PPC costs by 10% and increase the number of Smart users by 5%.\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edag\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecalculate_variable_impact\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003edf_test\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n                              \u003cspan class=\"n\"\u003evariables_list\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e'ppc_cost'\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s\"\u003e'smart_users'\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n                              \u003cspan class=\"n\"\u003evariables_multiplier_list\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mf\"\u003e0.9\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mf\"\u003e1.05\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e\n                              \u003cspan class=\"n\"\u003ediff_type\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s\"\u003e'relative'\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe percent change will be calculated on every node, i.e. smart_users_add1_ln, paid_visits, and all_visits. We will be able to evaluate how such changes affect not only the GMV, but also all intermediary KPIs.\u003c/p\u003e\n\n\u003ch2 id=\"facebooks-prophet\"\u003eFacebook’s Prophet\u003c/h2\u003e\n\n\u003cp\u003eHaving tested various modelling techniques, we chose the forecasting procedure offered by Facebook’s Prophet library (\u003ca href=\"https://facebook.github.io/prophet/\"\u003ehttps://facebook.github.io/prophet/\u003c/a\u003e). It uses a decomposable Bayesian time series model with three main components: seasonalities, trends and errors, hence it works well for our time series that have strong seasonal effects. Moreover, the Prophet model is robust to outliers and shifts in the trend, which proved very useful in some models. Mostly, however, we assumed the trend to be flat. The piecewise linear trend explains some of the variance of the dependent variable which could otherwise be explained by the regressor variables/inputs. Given that the purpose of the tool is to allow testing scenarios with different values of the inputs, we needed our models to estimate the relationship between the explanatory variables and the dependent variable, but accounting for seasonality, holidays and some external events (e.g. COVID) only. The graphs below show an example of the forecast components generated by the Prophet, with linear trend, effect of holidays, weekly and yearly seasonality, as well as effect of explanatory variables.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/02-chart.png\" alt=\"chart\" /\u003e\n\u003cimg src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/03-chart.png\" alt=\"chart\" /\u003e\n\u003cimg src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/04-chart.png\" alt=\"chart\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAdding our domain knowledge about the analysed time series (e.g. calendar effects) and possibility of tuning the parameters of the model (e.g. the strength of the weekly seasonality effect) makes Prophet a perfect fit for our purpose.\u003c/p\u003e\n\n\u003ch2 id=\"modelling-framework\"\u003eModelling framework\u003c/h2\u003e\n\n\u003cp\u003eBoth the patterns and relationships between the variables change slightly over time, hence it would be naive to expect that once all the models are tuned they will give best forecasts forever. It is therefore necessary to analyze the results every time the forecasts are made (every month) and apply necessary tweaks to the models.\u003c/p\u003e\n\n\u003cp\u003eAs in every machine learning project, we split our time series into:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eTraining dataset: the actual dataset that we use to train the model, i.e. the model learns from this data.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eValidation dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters; in time series, this is a time period following the training dataset time series.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eTest dataset: the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. In time series, this is a time period following the validation dataset time series.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe COVID-19 pandemic wreaked havoc on our time series, not only changing the patterns temporarily, but often introducing shifts in the trend. Given that we worked on the tool during summer 2020, we were forced to use quite a non-standard approach to hyperparameter tuning and model testing (e.g. to maximize the length of the training period, so that it includes at least a month of the data showing post-COVID comeback to a new normal).\u003c/p\u003e\n\n\u003cp\u003eIn the long run, we expect that the process will stabilize and we’ll be able to conduct the following adjustment procedure each month: training all the models using the parameters set in the previous month, testing them on the last 2 months of observed data, evaluating monthly and daily MAPE. When forecast errors in GMV prediction or any intermediate model are too large, we scrutinize the graphs of observed vs forecasted values. It is also helpful to compare the predictions vs observed values for the same period of the previous year. This step allows us to verify whether there are any seasonalities or patterns that were not detected by the tuned model. We can fine-tune the models either manually, or using the automatic hyperparameter optimization framework.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eHyperparameter tuning using Optuna (\u003ca href=\"https://optuna.org/\"\u003ehttps://optuna.org/\u003c/a\u003e), half a year’s worth of data and expanding window approach (see visualisation below). This means that we will fine-tune our models using 6 sets of validation datasets, each consisting of 1, 2, 3, 4, 5 and 6 months. The Optuna framework will suggest parameters that minimize the average of MAPE over these datasets.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eTesting the tuned models on 2 last months of observed data, measuring MAPE on the forecasted vs observed values of GMV, as well as on all intermediate models.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eIf any of the MAPE is not satisfactory, again scrutinizing the graphs and fine-tuning the models manually.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOnce we are satisfied with the results, we always check if the changes made to the models do not result in some explanatory variables having unexpected signs of impact on GMV.\u003c/p\u003e\n\n\u003cp\u003eDespite the changes in time series, we are expecting that in the long run fewer and fewer tweaks to the models will be necessary, and less work will be required from the analysts to maintain the tool.\u003c/p\u003e\n\n\u003ch2 id=\"user-interface\"\u003eUser interface\u003c/h2\u003e\n\n\u003cp\u003eTo make the model easily accessible by business users, an interactive application was prepared. The user has default inputs set for upcoming months. They can change their values and get model predictions by clicking the “RUN SCENARIO” button. The predictions can be seen in daily, weekly and monthly granularities. If the user chooses to, they can export the predictions in CSV format. You can find an anonymised print screen of the tool below.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2021-05-21-domino-financial-forecasting-in-the-age-of-global-pandemic/05-domino.png\" alt=\"Domino UI\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eAs a result of the project, we developed a solution providing incredible business value. The main features of the tool are:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eGreat forecast accuracy - we managed to get below 2% MAPE\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eStability - the structure of the model remains the same and the inputs have the same impact direction over time\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eResponsiveness - the forecasts change with changes in the business inputs\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eInterpretation - though the model is not intrinsically interpretable, we developed methods to check how well it works\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eInteractive UI - stakeholders can experiment with various business scenarios online\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDomino proved its effectiveness in hard and demanding times while giving us a lot of practical knowledge related to modeling of such a complex business metric. And, we already started using these lessons in new upcoming projects.\u003c/p\u003e\n","contentSnippet":"Accurate forecasting is key for any successful business. It allows one to set realistic financial goals for the next quarters, evaluate impact of business decisions, and prepare adequate resources for what is coming.\nYet, many companies struggle with efficient and accurate revenue forecasting. For most of them the task still rests on the financial department’s shoulders, performing manual analyses in Excel, lacking data science know-how, and relying on a set of arbitrary assumptions concerning the future. This process is often inefficient and prone to error, making it hard to distinguish trend-based patterns from manual overrides. In the end, the forecasts often turn out to be inaccurate, but it is difficult to diagnose the source of divergence - was it the fault of the model itself, wrong business assumptions, or perhaps unusual circumstances.\nIn order to overcome this problem, we decided to develop an AI tool which would allow us to forecast Allegro’s GMV (Gross Merchandise Value) based on a set of business inputs defined by the financial team. The purpose of the tool was twofold: to create automatic, monthly forecasts, and to enable creating scenarios with different input values. Examples of such inputs would be the costs of online advertising or the number of users enrolled in the loyalty program (Allegro Smart!).\nDisclaimer: analysis and graphs presented in article are based on artificial data\nApproach\nWe named our project domino after Marvel’s universe foreseer-heroine Domino, but the name reflects also how the model is executed.\nThe model works as a graph of dependencies where each element is either a cause or effect of other steps (in the technical nomenclature known as DAG). Let’s go through an artificial example. Below you can find a diagram of the model. In the training phase we need to provide historical data for every node in the graph, i.e. we need to know actual online advertising spending (PPC), important holidays, and actual number of paid and organic visits to be able to accurately model the relationship between these variables.\nDuring the prediction phase, we only need actual data for violet nodes. These nodes usually represent our key business assumptions which are reflected in the company’s yearly budget. Next, our goal is to recreate all the succeeding values. Some of these subsequent steps will be just arithmetic operations like adding, summing, or applying logarithms (yellow and red). Other steps, here represented with blue nodes, may be Machine Learning models (e.g. Facebook’s Prophet). They can learn non-trivial effects like seasonalities, trends, holidays, and the influence of the preceding yellow nodes (i.e. explanatory variables).\nYou may wonder why we bothered with creation of such a complex graph of dependencies instead of making a single model taking all the business inputs as explanatory variables and returning future GMV values as the output. It’s what we want to model in the end, right? Why are we concerned with reconstructing the values of organic or paid traffic along the way? Well, we found out that following business logic yields much better results than inserting all the inputs into a huge single pot. Not only were the results less accurate in the latter case, but also the impact of explanatory variables on GMV was often not clearly distinguishable or even misleading. Instead, we recreated the business scheme by creating and training specialized models to reconstruct all the intermediate steps (e.g. predicting the number of paid visits) and merging their outputs with arithmetic operations or other models. However, remember that this approach comes at a cost! Using many steps in the modeling process may be both a blessing and a curse, since you need to train and maintain multiple models simultaneously.\nOne of the biggest advantages of this approach is that it allows us to capture very sophisticated non-linear relationships between inputs and the final target variable (GMV in our case). Following the business logic allows us to verify these non-trivial assumptions at each step.\nAnother advantage of this approach is that every model can be fitted separately with any model class you want. Having tested a few alternatives, we’ve chosen the Prophet library, but potentially any ML algorithm could be used (e.g. ARIMA, Gradient Boosting Trees, or Artificial Neural Networks).\nA disadvantage is that the error in prediction propagates downstream the graph. So, if we make a mistake in a prediction in an early step, it will influence all the models and transformations dependent on it. The issue can be mitigated by creating accurate models at each step of the process.\nAnother slight disadvantage is that our Domino of models is not intrinsically interpretable (as most of the modern model classes). You have to use some post hoc methods to gather information on how the model does process data.\nTechnical implementation\nTo develop and iterate over the DAG-type model we had to implement a custom Python framework. It allows for training and running models as well as arithmetic transformations in a predefined order.\nThe implementation allows us to utilize various model frameworks like Facebook’s Prophet, any Scikit-learn’s regressor, or an Artificial NN. For operational purposes, MAPE (Mean Absolute Percentage Error) can be easily calculated for each model in the graph, as well as for the whole DAG (again, the errors propagate downstream).\nBelow you can see how the above DAG can be implemented.\n\nfrom domino.pipeline import Pipeline, Model, Transformer, Combinator\nfrom fbprophet import Prophet\n\ndag = Pipeline(input_list=['ppc_cost', 'smart_users'])\n\ndag.add_step(step=Transformer(base_var='ppc_cost',\n                             target_variable='ppc_cost_ln',\n                             operation_name='ln'),\n             step_name='transformer1')\n\ndag.add_step(step=Transformer(base_var='smart_users',\n                             target_variable='smart_users_add1',\n                             operation_name='add1'),\n             step_name='transformer2')\n\ndag.add_step(step=Transformer(base_var='smart_users_add1',\n                             target_variable='smart_users_add1_ln',\n                             operation_name='ln'),\n             step_name='transformer3')\n\ndag.add_step(step=Model(model=Prophet(),\n                  target_variable='paid_visits',\n                  explanatory_variables=['ppc_cost_ln']),\n         step_name='model1')\n\ndag.add_step(step=Model(model=Prophet(),\n                  target_variable='non_paid_visits',\n                  explanatory_variables=['smart_users_add1_ln']),\n             step_name='model2')\n\ndag.add_step(step=Combinator(base_var1='paid_visits',\n                            base_var2='non_paid_visits',\n                            target_variable='all_visits',\n                            operation_name='add'),\n             step_name='combination')\n\n\nNow we can use the dag object as a single model.\n\ndag.fit(df_train)\npredictions = dag.predict(df_test)\n\n\nTo understand what’s happening inside the DAG, we implemented two methods of calculating MAPE on every step:\n\ndag.calculate_mape_for_models(x=df_test)\ndag.calculate_mape_for_dag(x=df_test)\n\n\nThey both return dictionaries of model-MAPE pairs. The calculate_mape_for_model method checks each model separately, and calculate_mape_for_dag takes into account the errors propagating from preceding steps. These are examples of results:\n\nmodels_mape = {\n   'transformer1': 0.0,\n   'transformer2': 0.0,\n   'transformer3': 0.0,\n   'model1': 0.1,\n   'model2': 0.05,\n   'combination': 0.0\n}\n\ndag_mape = {\n   'transformer1': 0.0,\n   'transformer2': 0.0,\n   'transformer3': 0.0,\n   'model1': 0.1,\n   'model2': 0.05,\n   'combination': 0.09\n}\n\n\nNote that the combination step has MAPE equal to zero in models_mape and a positive one in dag_mape. That’s because it does not generate any error, as it’s an arithmetic operation, but it can propagate errors from previous steps.\nLast but not least, there is an explainability method calculate_variable_impact that helps to evaluate how changes in initial inputs impact the subsequent steps in the graph. For example, we can check what is going to happen if we decrease PPC costs by 10% and increase the number of Smart users by 5%.\n\ndag.calculate_variable_impact(x=df_test,\n                              variables_list=['ppc_cost', 'smart_users'],\n                              variables_multiplier_list=[0.9, 1.05],\n                              diff_type='relative')\n\n\nThe percent change will be calculated on every node, i.e. smart_users_add1_ln, paid_visits, and all_visits. We will be able to evaluate how such changes affect not only the GMV, but also all intermediary KPIs.\nFacebook’s Prophet\nHaving tested various modelling techniques, we chose the forecasting procedure offered by Facebook’s Prophet library (https://facebook.github.io/prophet/). It uses a decomposable Bayesian time series model with three main components: seasonalities, trends and errors, hence it works well for our time series that have strong seasonal effects. Moreover, the Prophet model is robust to outliers and shifts in the trend, which proved very useful in some models. Mostly, however, we assumed the trend to be flat. The piecewise linear trend explains some of the variance of the dependent variable which could otherwise be explained by the regressor variables/inputs. Given that the purpose of the tool is to allow testing scenarios with different values of the inputs, we needed our models to estimate the relationship between the explanatory variables and the dependent variable, but accounting for seasonality, holidays and some external events (e.g. COVID) only. The graphs below show an example of the forecast components generated by the Prophet, with linear trend, effect of holidays, weekly and yearly seasonality, as well as effect of explanatory variables.\n\n\n\nAdding our domain knowledge about the analysed time series (e.g. calendar effects) and possibility of tuning the parameters of the model (e.g. the strength of the weekly seasonality effect) makes Prophet a perfect fit for our purpose.\nModelling framework\nBoth the patterns and relationships between the variables change slightly over time, hence it would be naive to expect that once all the models are tuned they will give best forecasts forever. It is therefore necessary to analyze the results every time the forecasts are made (every month) and apply necessary tweaks to the models.\nAs in every machine learning project, we split our time series into:\nTraining dataset: the actual dataset that we use to train the model, i.e. the model learns from this data.\nValidation dataset: the sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters; in time series, this is a time period following the training dataset time series.\nTest dataset: the sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. In time series, this is a time period following the validation dataset time series.\nThe COVID-19 pandemic wreaked havoc on our time series, not only changing the patterns temporarily, but often introducing shifts in the trend. Given that we worked on the tool during summer 2020, we were forced to use quite a non-standard approach to hyperparameter tuning and model testing (e.g. to maximize the length of the training period, so that it includes at least a month of the data showing post-COVID comeback to a new normal).\nIn the long run, we expect that the process will stabilize and we’ll be able to conduct the following adjustment procedure each month: training all the models using the parameters set in the previous month, testing them on the last 2 months of observed data, evaluating monthly and daily MAPE. When forecast errors in GMV prediction or any intermediate model are too large, we scrutinize the graphs of observed vs forecasted values. It is also helpful to compare the predictions vs observed values for the same period of the previous year. This step allows us to verify whether there are any seasonalities or patterns that were not detected by the tuned model. We can fine-tune the models either manually, or using the automatic hyperparameter optimization framework.\nHyperparameter tuning using Optuna (https://optuna.org/), half a year’s worth of data and expanding window approach (see visualisation below). This means that we will fine-tune our models using 6 sets of validation datasets, each consisting of 1, 2, 3, 4, 5 and 6 months. The Optuna framework will suggest parameters that minimize the average of MAPE over these datasets.\nTesting the tuned models on 2 last months of observed data, measuring MAPE on the forecasted vs observed values of GMV, as well as on all intermediate models.\nIf any of the MAPE is not satisfactory, again scrutinizing the graphs and fine-tuning the models manually.\nOnce we are satisfied with the results, we always check if the changes made to the models do not result in some explanatory variables having unexpected signs of impact on GMV.\nDespite the changes in time series, we are expecting that in the long run fewer and fewer tweaks to the models will be necessary, and less work will be required from the analysts to maintain the tool.\nUser interface\nTo make the model easily accessible by business users, an interactive application was prepared. The user has default inputs set for upcoming months. They can change their values and get model predictions by clicking the “RUN SCENARIO” button. The predictions can be seen in daily, weekly and monthly granularities. If the user chooses to, they can export the predictions in CSV format. You can find an anonymised print screen of the tool below.\n\nSummary\nAs a result of the project, we developed a solution providing incredible business value. The main features of the tool are:\nGreat forecast accuracy - we managed to get below 2% MAPE\nStability - the structure of the model remains the same and the inputs have the same impact direction over time\nResponsiveness - the forecasts change with changes in the business inputs\nInterpretation - though the model is not intrinsically interpretable, we developed methods to check how well it works\nInteractive UI - stakeholders can experiment with various business scenarios online\nDomino proved its effectiveness in hard and demanding times while giving us a lot of practical knowledge related to modeling of such a complex business metric. And, we already started using these lessons in new upcoming projects.","guid":"https://blog.allegro.tech/2021/05/domino-financial-forecasting-in-the-age-of-global-pandemic.html","categories":["tech"],"isoDate":"2021-05-20T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999761320996","name":"Software Engineer (Java/Python + DevOps)","uuid":"8447e33e-81fe-490f-b387-2c1d17eb50e3","refNumber":"REF2919X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-07-15T11:40:31.000Z","location":{"city":"Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"5a71d24e-2c10-42e2-96ac-85d5ca2a65b6","valueLabel":"IT - Technical Platform"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Software Engineering,  JAVA, Python, DevOps, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999761320996","language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999761313464","name":"Front-end Software Engineer - IT Business Services","uuid":"ee5c5ecf-e4e6-4e42-83f8-7dd5cd00e0c7","refNumber":"REF2903S","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-07-15T10:03:40.000Z","location":{"city":"Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572872","label":"IT - IT Business Solutions"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572872","valueLabel":"IT - IT Business Solutions"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999761313464","creator":{"name":"Paulina Tynecka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999761311645","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"8488a6d1-d506-49b3-bf3d-8e5bd085b115","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-07-15T09:48:14.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999761311645","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999761176723","name":"Technical Project Manager","uuid":"983e6f70-8e4d-4ba0-a5b7-a79d5751569f","refNumber":"REF2758O","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-07-14T13:47:09.000Z","location":{"city":"Poznań, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572889","label":"IT - Product/Project Management"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572889","valueLabel":"IT - Product/Project Management"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"wnik projektów/projektu, koordynator projektów/projektu, technical project manager, project coordinator, agile project manager, IT project manager, specjalista ds. projektów"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999761176723","language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999761166036","name":"Team Leader (Java/Kotlin) - Merchant Experience","uuid":"81917b59-5701-415c-b3d3-8c078f8d53d1","refNumber":"REF2604H","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-07-14T13:10:41.000Z","location":{"city":"Warszawa,Kraków,Poznań,Toruń,Wrocław","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"bf8669ea-7cc9-445c-9713-25ba37d96657","valueLabel":"IT - Management"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"team leader, java, kotlin, scala"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999761166036","creator":{"name":"Katarzyna Faber"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1623957759000,"duration":7200000,"id":"278903176","name":"Allegro Tech Live #20: Wydajność Backendu","date_in_series_pattern":false,"status":"past","time":1624982400000,"local_date":"2021-06-29","local_time":"18:00","updated":1624994207000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":125,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278903176/","description":"Allegro Tech Live w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my zagościmy…","how_to_find_us":"https://youtu.be/VklKR_fO5OI","visibility":"public","member_pay_fee":false},{"created":1621842668000,"duration":100800000,"id":"278374635","name":"UX Research Confetti","date_in_series_pattern":false,"status":"past","time":1624456800000,"local_date":"2021-06-23","local_time":"16:00","updated":1624563213000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278374635/","description":"🎉 Niech rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy… Dlatego łączymy siły z ekspertami z…","visibility":"public","member_pay_fee":false},{"created":1622474681000,"duration":5400000,"id":"278528964","name":"Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?","date_in_series_pattern":false,"status":"past","time":1623340800000,"local_date":"2021-06-10","local_time":"18:00","updated":1623349290000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":52,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278528964/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":" https://www.youtube.com/watch?v=8sLX0ExSq7E","visibility":"public","member_pay_fee":false},{"created":1619620661000,"duration":5400000,"id":"277852879","name":"Allegro Tech Live #18 PM w Allegro, jak do nas dołączyć i czerpać radość z pracy","date_in_series_pattern":false,"status":"past","time":1620921600000,"local_date":"2021-05-13","local_time":"18:00","updated":1620932668000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":46,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/277852879/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":"https://youtu.be/WNOQJxPKweM","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager \u0026 Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager \u0026 Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager \u0026 Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true},"page":"/","query":{},"buildId":"oqFSTwvrlGjGsIWW1FYwG","isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-eef578260fd80f8fff94.js"></script><script src="/_next/static/chunks/webpack-189c53927ffd3caf09c3.js" async=""></script><script src="/_next/static/chunks/framework-0441fae7fd130f37dee1.js" async=""></script><script src="/_next/static/chunks/main-547dee26f92077ae29b6.js" async=""></script><script src="/_next/static/chunks/pages/_app-f9c1c7bd15b9b2e730cb.js" async=""></script><script src="/_next/static/chunks/805-39af1b772818e37efb44.js" async=""></script><script src="/_next/static/chunks/pages/index-fe725e13004e1b56d414.js" async=""></script><script src="/_next/static/oqFSTwvrlGjGsIWW1FYwG/_buildManifest.js" async=""></script><script src="/_next/static/oqFSTwvrlGjGsIWW1FYwG/_ssgManifest.js" async=""></script></body></html>