<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w ktÃ³rym nasi inÅ¼ynierowie dzielÄ… siÄ™ wiedzÄ… oraz case study z wybranych projektÃ³w w firmie - w formie artykuÅ‚Ã³w, podcastÃ³w oraz eventÃ³w."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><link rel="author" href="humans.txt"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/c4277531f90028a4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c4277531f90028a4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/79db8b1e27b0a093.css" as="style"/><link rel="stylesheet" href="/_next/static/css/79db8b1e27b0a093.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-179adf437ae674f2.js" defer=""></script><script src="/_next/static/chunks/206-3a56e5ded293e83e.js" defer=""></script><script src="/_next/static/chunks/pages/index-9e9857288e1eab25.js" defer=""></script><script src="/_next/static/465mJBxcn3yxedGbDTDVL/_buildManifest.js" defer=""></script><script src="/_next/static/465mJBxcn3yxedGbDTDVL/_ssgManifest.js" defer=""></script><script src="/_next/static/465mJBxcn3yxedGbDTDVL/_middlewareManifest.js" defer=""></script></head><body class="m-color-bg_desk"><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="OtwÃ³rz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">About us</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro is one of the most technologically advanced companies in our part of Europe. Allegro is also over 1700Â IT specialists of various specializations, developing our website. The unique scale and complexity of the problems that we solve on a daily basis give us the opportunity to develop on a wide variety of projects. Allegro Tech is a place where our engineers share knowledge and case studies from selected projects in the companyÂ â€“ in the form of articles, podcasts and events.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html" title="The Acrobatics of Switching Between Management and Engineering"><img width="388" src="images/post-headers/default.jpg" alt="The Acrobatics of Switching Between Management and Engineering" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html" title="The Acrobatics of Switching Between Management and Engineering" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">The Acrobatics of Switching Between Management and Engineering</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/coding">#<!-- -->coding</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/management">#<!-- -->management</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/developer">#<!-- -->developer</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/team leader">#<!-- -->team leader</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/career path">#<!-- -->career path</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">After six years as a Team Leader, I went back to hands-on engineering work, and Iâ€™m very happy about taking
this step. While it may appearâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="MichaÅ‚ Kosmulski" src="https://blog.allegro.tech/img/authors/michal.kosmulski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/michal.kosmulski">MichaÅ‚ Kosmulski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html" title="From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance"><img width="388" src="images/post-headers/default.jpg" alt="From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html" title="From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o 2 miesiÄ…ce temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/azure">#<!-- -->azure</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/sql">#<!-- -->sql</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/saving">#<!-- -->saving</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/cloud">#<!-- -->cloud</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">In the era of ubiquitous cloud services and an increasingly growing PaaS and serverless-oriented approach, performance
and resources seem to be becoming less and less important.
Afterâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Mateusz Stolecki" src="https://blog.allegro.tech/img/authors/mateusz.stolecki.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/mateusz.stolecki">Mateusz Stolecki</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/05/debugging-hangs.html" title="Debugging hangs - piecing together why nothing happens"><img width="388" src="images/post-headers/java.png" alt="Debugging hangs - piecing together why nothing happens" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/05/debugging-hangs.html" title="Debugging hangs - piecing together why nothing happens" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Debugging hangs - piecing together why nothing happens</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiÄ…ce temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/java">#<!-- -->java</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/jvm">#<!-- -->jvm</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/debugging">#<!-- -->debugging</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/dependency hell">#<!-- -->dependency hell</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">As a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.
The taskâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Åukasz Rokita" src="https://blog.allegro.tech/img/authors/lukasz.rokita.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/lukasz.rokita">Åukasz Rokita</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/05/debugging-hangs.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/05/debugging-hangs.html">przejdÅº do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html" title="Trust no one, not even your training data! Machine learning from noisy data"><img width="388" src="images/post-headers/default.jpg" alt="Trust no one, not even your training data! Machine learning from noisy data" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html" title="Trust no one, not even your training data! Machine learning from noisy data" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Trust no one, not even your training data! Machine learning from noisy data</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 miesiÄ…ce temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mlr">#<!-- -->mlr</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/robustness">#<!-- -->robustness</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/research">#<!-- -->research</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ml">#<!-- -->ml</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/machine-learning">#<!-- -->machine-learning</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ai">#<!-- -->ai</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Label noise is ever-present in machine learning practice.
Allegro datasets are no exception.
We compared 7 methods for training classifiers robust to label noise.
All of them improvedâ€¦</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:5"><img alt="Åukasz RÄ…czkowski" src="https://blog.allegro.tech/img/authors/lukasz.raczkowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar MuiAvatar-colorDefault" style="z-index:0">+<!-- -->4</div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/lukasz.raczkowski">Åukasz RÄ…czkowskiâ€¦</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/04/learning-from-noisy-data.html">przejdÅº do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz wiÄ™cej wpisÃ³w</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/" title="O roli analitykÃ³w biznesowych w Allegro"><img src="images/podcast.png" alt="O roli analitykÃ³w biznesowych w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/" title="O roli analitykÃ³w biznesowych w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O roli analitykÃ³w biznesowych w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">dzieÅ„ temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym zajmujÄ… siÄ™ analitycy danych w Allegro i za jakie projekty odpowiadajÄ…? Z jakich rodzajÃ³w danych i narzÄ™dzi korzystajÄ… w codziennej pracy? Jakie (przykÅ‚adowe) obszary tematyczne pokrywamy danymi, ktÃ³re analizujemy w Allegro? Jakich umiejÄ™tnoÅ›ci szukamy u analitykÃ³w biznesowych w Allegro i jak moÅ¼na do nas doÅ‚Ä…czyÄ‡? O roli analitykÃ³w biznesowych i pracy w skali Allegro opowiadajÄ… Jakub KrÃ³l i Mateusz Falkowski - Senior Data Analysts w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/" title="O spoÅ‚ecznoÅ›ci Allegro Tech i rozwoju inÅ¼ynierÃ³w w Allegro"><img src="images/podcast.png" alt="O spoÅ‚ecznoÅ›ci Allegro Tech i rozwoju inÅ¼ynierÃ³w w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/" title="O spoÅ‚ecznoÅ›ci Allegro Tech i rozwoju inÅ¼ynierÃ³w w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O spoÅ‚ecznoÅ›ci Allegro Tech i rozwoju inÅ¼ynierÃ³w w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">29 dni temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Na czym polega rola Principal Software Engineera w Allegro oraz co ma wspÃ³lnego z rozwijaniem siebie i dzieleniem siÄ™ wiedzÄ…? Co warto wiedzieÄ‡ o turystyce, ktÃ³ra pojawia siÄ™ niemal w kaÅ¼dym odcinku naszych podcastÃ³w? Na czym polega, kto, kiedy i jak moÅ¼e z niej skorzystaÄ‡? Jak pracujemy z talentami Gallupa (takÅ¼e w zespoÅ‚ach technicznych)?  Co dajÄ… nam wewnÄ™trzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy siÄ™ doÅ›wiadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przyÅ›wieca? O spoÅ‚ecznoÅ›ci Allegro Tech i moÅ¼liwoÅ›ciach rozwoju w Allegro z perspektywy inÅ¼ynierÃ³w rozmawialiÅ›my z Marcinem Turkiem i MichaÅ‚em Kosmulskim.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-data-science-hub-w-allegro/" title="O Data Science Hub w Allegro"><img src="images/podcast.png" alt="O Data Science Hub w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-data-science-hub-w-allegro/" title="O Data Science Hub w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O Data Science Hub w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o miesiÄ…c temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Co kryje siÄ™ pod pojÄ™ciem Data Science Hub w Allegro? Jakie dziaÅ‚ania rozwijamy w tym obszarze i jak oceniamy ich potencjaÅ‚? O czym jest projekt Wilson i na czym skupiamy siÄ™ w projekcie przewidywania zakupÃ³w cyklicznych? Jak wykorzystujemy sztucznÄ… inteligencjÄ™ i gdzie jest dla niej miejsce wÅ›rÃ³d naszych kierunkÃ³w rozwoju? O AI Transformation, poczuciu sprawczoÅ›ci, mieszance kompetencji i talentÃ³w zamkniÄ™tej w rolach Data Scientist, Data Engineer i Data Analyst rozmawialiÅ›my z KarolinÄ… NieradkÄ… i Kamilem Konikiewiczem.,</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-data-science-hub-w-allegro/">PosÅ‚uchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/" title="O technologiach i projektach w Allegro Pay"><img src="images/podcast.png" alt="O technologiach i projektach w Allegro Pay" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/" title="O technologiach i projektach w Allegro Pay" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O technologiach i projektach w Allegro Pay</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">okoÅ‚o 2 miesiÄ…ce temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak powstaÅ‚a usÅ‚uga Allegro Pay i co ma wspÃ³lnego z ratatouille? Jakie projekty i technologie stojÄ… za tym rozwiÄ…zaniem? Jak to jest pracowaÄ‡ w Azure i obsÅ‚ugiwaÄ‡ ruch, ktÃ³ry generuje Allegro? Czym inÅ¼ynierÃ³w moÅ¼e zaskoczyÄ‡ praca w Allegro Pay i co czeka na nich (na przykÅ‚ad) w programie All4Customer? O migrowaniu baz CosmosDB, wymaganiach skali i dostÄ™pnoÅ›ci, a takÅ¼e o rozwijaniu ludzi i technologii rozmawialiÅ›my z Mariuszem Budzynem i Tomaszem SzczerbÄ…. Zapraszamy do sÅ‚uchania! na rÃ³Å¼nych pÅ‚aszczyznach?</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/">PosÅ‚uchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz wiÄ™cej podcastÃ³w</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #38 - Mobile: o iOS bez spinki"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #38 - Mobile: o iOS bez spinki</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">2 miesiÄ…ce temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwÄ… wakacyjnÄ…, stacjonarne spotkanie z cyklu Allegro Tech Talks, na ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³wâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293929321/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/292278882/" title="UX Research Confetti - III edycja " class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - III edycja "/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/292278882/" title="UX Research Confetti - III edycja " class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - III edycja </h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiÄ…ce temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**Rejestracja na wydarzenie â¡ [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **ğŸ‰ Przedstawiamy 3. edycjÄ™ UX Research Confetti organizowanÄ… przezÂ Allegro - bezpÅ‚atnÄ…, polskÄ… konferencjÄ™ poÅ›wiÄ™conÄ… badaniomâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/292278882/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293341234/" title="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeÅ„stwa wspÃ³Å‚dzielonego stanu" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeÅ„stwa wspÃ³Å‚dzielonego stanu"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293341234/" title="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeÅ„stwa wspÃ³Å‚dzielonego stanu" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #37 - Kotlin Native i niebezpieczeÅ„stwa wspÃ³Å‚dzielonego stanu</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiÄ…ce temu<!-- -->, Allegro Office - PoznaÅ„ (Nowy Rynek)</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-37/](https://app.evenea.pl/event/allegro-tech-talk-37/) CiÄ…g dalszy naszych stacjonarnych spotkaÅ„ Allegro Tech Talks, na ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³w w kuluarach. ğŸ“Œâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293341234/">SzczegÃ³Å‚y</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293215214/" title="AlleKwanty: o komputerach przyszÅ‚oÅ›ci, ktÃ³re na Allegro dopiero bÄ™dÄ… mieÄ‡" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="AlleKwanty: o komputerach przyszÅ‚oÅ›ci, ktÃ³re na Allegro dopiero bÄ™dÄ… mieÄ‡"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293215214/" title="AlleKwanty: o komputerach przyszÅ‚oÅ›ci, ktÃ³re na Allegro dopiero bÄ™dÄ… mieÄ‡" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">AlleKwanty: o komputerach przyszÅ‚oÅ›ci, ktÃ³re na Allegro dopiero bÄ™dÄ… mieÄ‡</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiÄ…ce temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-kwanty/](https://app.evenea.pl/event/allegro-tech-kwanty/) Allegro Tech to miejsce, w ktÃ³rym nasi inÅ¼ynierowie dzielÄ… siÄ™ wiedzÄ… oraz case study z wybranych projektÃ³w w firmie - w formieâ€¦</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293215214/">SzczegÃ³Å‚y</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz wiÄ™cej wydarzeÅ„</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer - Allegro Ads</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, Warsaw, WrocÅ‚aw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999926482206-software-engineer-allegro-ads?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Big Data Engineer - Allegro Ads</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, Warsaw, WrocÅ‚aw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999926450974-big-data-engineer-allegro-ads?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Big Data Engineer - Data &amp; AI</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, Warsaw, GdaÅ„sk, ToruÅ„, Cracow</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999926136626-big-data-engineer-data-ai?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Mobile Software Engineer iOS - Mobile Core</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">PoznaÅ„, WrocÅ‚aw, Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999925616021-mobile-software-engineer-ios-mobile-core?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer - Java</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Prague, Remote</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999925170914-software-engineer-java?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">SprawdÅº</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz wiÄ™cej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=8166480887997.205;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"The Acrobatics of Switching Between Management and Engineering","link":"https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html","pubDate":"Tue, 22 Aug 2023 00:00:00 +0200","authors":{"author":[{"name":["MichaÅ‚ Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"\u003cp\u003eAfter six years as a Team Leader, I went back to hands-on engineering work, and Iâ€™m very happy about taking\nthis step. While it may appear surprising at first, it was a well-thought-out decision, and actually Iâ€™ve already\nperformed such a maneuver once before.\u003c/p\u003e\n\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eA few years ago I stumbled upon \u003ca href=\"https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/\"\u003eThe Engineer/Manager Pendulum\u003c/a\u003e\nby Charity Majors, and the follow-up post \u003ca href=\"https://charity.wtf/2019/01/04/engineering-management-the-pendulum-or-the-ladder/\"\u003eEngineering Management: The Pendulum Or The Ladder\u003c/a\u003e.\nI found both pieces interesting, and quite in line with my own experiences: in my previous job some time earlier I\nhad been a team leader, but consciously joined \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e as a software engineer. After a while, I\nbecame a team leader again, and recently, another few years down the line, I went back to engineering yet again.\u003c/p\u003e\n\n\u003cp\u003eBoth above-mentioned posts make very good points, so I recommend you read them first. What I want to add on top of\nthem are my personal experiences and some tips on organizing a transition between a management and an individual\ncontributor role. The journey in the other direction (from developer to team leader) has been discussed in depth\nelsewhere, so I wonâ€™t delve into that.\u003c/p\u003e\n\n\u003ch2 id=\"why\"\u003eWhy\u003c/h2\u003e\n\n\u003cp\u003eWhy would I want to switch between developer and team leader roles? The problem is they are both interesting and have\n\u003ca href=\"/2019/06/allegro-culture-tech-leaders-meeting.html\"\u003etheir own highlights\u003c/a\u003e, but you canâ€™t do both at the\nsame time. When you become a manager, not only do you have less time for technical tasks, but you also pretty much\nlose the ability to focus on these tasks even if you do find a time slot. This is because, like it or not, you end\nup with a \u003ca href=\"http://www.paulgraham.com/makersschedule.html\"\u003emanagerâ€™s schedule\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-08-22-management-engineering-acrobatics/flying-trapeze-performers.jpg\" alt=\"Circus performers on the flying trapeze, Public Domain image from https://commons.wikimedia.org/wiki/File:Programma_van_Circus_Krone_in_Rotterdam_drie_Alizes_,_vliegende_trapeze_met_o.a.,_Bestanddeelnr_910-4372.jpg\" class=\"small-image\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eOver time, your technical skills start deteriorating, and if you miss the right moment, you may find yourself at\na point of no return. Like a circus artist on a flying trapeze, you have to time your actions right to avoid\ndisaster. I think this metaphor fits the situation better than that of the pendulum, which moves in its own rhythm,\nindependent of external influences.\u003c/p\u003e\n\n\u003cp\u003eObviously, you need to ask yourself whether you actually want to prevent your technical skills from deteriorating. Some\npeople move on to management, wave goodbye to getting their hands dirty, and are completely fine with that. Some tinker\nwith technology in their spare time. As for me, I like technology and would not only prefer to not lose what I have\nlearned so far, but actually want to learn something new. However, due to a number of other hobbies and already\nspending more time in front of the screen than I would like, doing tech in the afternoons was not a viable option for\nme. Hence, the decision to make technology an important part of my job again.\u003c/p\u003e\n\n\u003ch2 id=\"how\"\u003eHow\u003c/h2\u003e\n\n\u003cp\u003eA crucial factor to take into account when planning such a change is that it will take time. If you take your\nteam seriously, you canâ€™t just disappear overnight. You need to think ahead. Do not ask yourself â€œhave I already\nlost my tech skills beyond repair?â€. Ask yourself â€œhow will my tech skills be a year from now?â€.\u003c/p\u003e\n\n\u003cp\u003eAlso, keep a cool head. Just as when moving in the opposite direction, from engineer to manager, consider all\nconsequences, both positive and negative. It has been repeated many times that becoming a manager is not a promotion\n(being a lateral move to another career path), but in practice sometimes it is. In particular, it may come with a\nhigher level or salary. Make sure you check all details of your target role in order to avoid unpleasant surprises when\ngoing back. In my case, I was aiming for the Principal Software Engineer role, which is the same level as the Team\nLeader role at Allegro, so there were no issues in this regard.\u003c/p\u003e\n\n\u003cp\u003eWhen I started thinking about making the switch, once I had a rough idea of what I wanted, I talked to my superior.\nThis was an important step: it allowed him to plan ahead, and also to look for opportunities for making the\nreorganization easier. Some elements of the process would depend only on our actions, but some, such as finding a good\nreplacement team leader, would also depend on a number of factors outside our control. Knowing that my boss understood\nmy need, and supported it, mattered a lot, and made the wait and preparations easier.\u003c/p\u003e\n\n\u003cp\u003eChance favors the prepared mind, as Louis Pasteur supposedly said. There happened to be a team leader in\nanother part of the company who was thinking about moving on to a different area. Thanks to being aware of my plan,\nmy boss was able to grab the chance, and we had a perfect match. We discussed with the potential new leader the team\nand the project, and he found them interesting. We planned a transition period, as short as possible, but long enough\nfor me to transfer to him a reasonable part of my knowledge about the team and its work.\u003c/p\u003e\n\n\u003cp\u003eNow that we had a specific plan, we could tell the team. It was important to let everyone know as soon as possible, but\nnot before we had a specific plan. Without it, this information would only stir uncertainty. Apart\nfrom telling the team as a whole, I also talked to each person individually, in order to resolve any questions or doubts\nand to try to reduce any problems resulting from the transition as much as possible.\u003c/p\u003e\n\n\u003cp\u003eWaiting for the switch date, we kept meeting online with the new leader, transferring knowledge and preparing him for\nworking with the team. Thereâ€™s actually quite a lot of stuff a leader needs to know: not only how the project\nworks on technical and business levels, but also current plans, who the stakeholders are and how to work with them,\nand each team memberâ€™s individual strengths and development plan. The new leader himself also started meeting\npeople he would now work with, both team members and our productâ€™s stakeholders, and attending team meetings such as\nthe daily stand-up. Despite gradually moving on to other tasks after the switch date, I was still available to clarify\nany doubts, and our boss would also help out when necessary, so the new leader knew he would not be left on his own.\nWhile it required quite a bit of work, the switch went smoothly, and we didnâ€™t notice any serious disturbance to the\nteamâ€™s functioning.\u003c/p\u003e\n\n\u003ch2 id=\"the-aftermath\"\u003eThe Aftermath\u003c/h2\u003e\n\n\u003cp\u003eItâ€™s been several months since the switch now. Me changing back to a technical role has certainly required extra work,\nfor me, my boss, and the new team leader. Despite our best efforts, it probably put a little extra strain on the team as\nwell. Nonetheless, I think it was a win-win, even more so thanks to us being able to spot and exploit a happy\ncoincidence. I am glad to be closer to technology again, and the new leader also got to try something new, just as he\nwanted.\u003c/p\u003e\n\n\u003cp\u003eThere is one more subtle advantage to the whole process. When people leave the team, some knowledge inevitably gets\nlost. One of the reasons is \u003cem\u003etacit knowledge\u003c/em\u003e: there are always things you know, but are not aware of knowing. You can\nuse this knowledge when itâ€™s needed, but you will probably not transfer it to others because you are not even aware of\nits existence in the first place. Removing someone from the team in a controlled manner as happened here (and being\nstill able to reach out to them if needed) causes such latent knowledge to be discovered, and once discovered, to be\npropagated. This causes a little disruption short-term, but in the long run it reduces\n\u003ca href=\"https://en.wikipedia.org/wiki/Information_silo\"\u003eknowledge silos\u003c/a\u003e and increases the\n\u003ca href=\"https://en.wikipedia.org/wiki/Bus_factor\"\u003ebus factor\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhen I first started thinking about going back to hands-on technical work for the second time, I had some doubts about\nhow much my technical skills had already deteriorated and how difficult it would be to go back. It seems I made it, but\nnot by much. Had I delayed by one more year, I might have really struggled. Itâ€™s not a matter of knowledge: theory,\nespecially generic things that do not change that fast with technology, is not lost so quickly. Also, while a team\nleader, I tried to stay in touch with technology by taking part in task refinement, architecture discussions, on-call\nrotation, etc.\u003c/p\u003e\n\n\u003cp\u003eHowever, I really felt, and to some degree still feel, a difference in practical, hands-on work, such as actually\nwriting code. There are many small quirks that you need to be aware of in order to accomplish things quickly that you\ndonâ€™t even notice if you use them every day and know inside-out. Knowing all the little useful tools, the less often\nused features of your IDE, or what to do when something breaks unexpectedly, make a world of a difference, but this\npractical knowledge gets lost when not used and I had to rebuild it almost from scratch. Another thing that I still\nexperience is the difficulty in focusing on a single topic. Working on a managerâ€™s schedule for several years has taken\nits toll, and now that I often have large contiguous blocks of time, I find myself not using them as effectively as I\ncould, because I have become accustomed to always doing multiple things at once and without a chance to stay focused\nanyway. Itâ€™s gradually getting better, but I still feel the impact, and this is probably my biggest surprise of the\nwhole process.\u003c/p\u003e\n\n\u003ch2 id=\"about-the-principal-software-engineer-role\"\u003eAbout the Principal Software Engineer role\u003c/h2\u003e\n\n\u003cp\u003eMy current role is that of Principal Software Engineer (PSE). It is a relatively new addition to the junior, mid,\nand senior roles weâ€™ve had so far. It has gone through a number of revisions, and is still evolving. Most people in this\nrole come from a Senior Software Engineer background, so my case of getting there after being a Team Leader is a bit\nuntypical. At many companies, roles like this are called Staff Software Engineer or similar. While still an individual\ncontributor role, a PSE differs from a senior in several ways.\u003c/p\u003e\n\n\u003cp\u003eFirst of all, a PSE is expected to spend significant time on topics whose scope is much larger than a single team can\nhandle. Seniors can also do this, but itâ€™s not a requirement for them. Such topics may be area-wide, such as planning a\nmajor change to a single subsystemâ€™s architecture, or have a company-wide scope. Much work on this level consists of\ncoming up with ideas and discussing them while implementation is often left to others. So, while still technical, this\nrole encompasses less coding than that of a senior. Not very surprising given that generally moving up the career ladder\nmeans more coming up with ideas, teaching others, and planning work, while coding less yourself.\u003c/p\u003e\n\n\u003cp\u003eSecondly, a PSE should be very autonomous. Most PSEs are not members of regular development teams since they move from\ntask to task depending on where they can help most. This means you cooperate with more people from different parts of\nthe company, but you donâ€™t have the few peers you work with every day that most people have. You donâ€™t get a backlog\nof tasks to work on, but have to plan your work yourself. People do come to you, asking for support or doing something\nfor their project, but thatâ€™s just one of many inputs.\u003c/p\u003e\n\n\u003cp\u003eThirdly, since there are few PSEs compared to other positions, for each person the role is a little different. On one\nhand this means you canâ€™t fully know what to expect when you start. On the other, you get to shape the role yourself,\nand personally I enjoy this flexibility.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eI think the idea of swinging back and forth between engineering and management described in\n\u003ca href=\"https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/\"\u003eThe Engineer/Manager Pendulum\u003c/a\u003e,\nis valid, and my own experience backs it up fully. One element which I want to additionally stress, however, is that\nthe switch is more like a flying trapeze than a pendulum: timing is crucial, and missing the right moment can have\nserious consequences. Becoming a Principal Software Engineer was a unique experience, both on a technical\nlevel and as my de facto last task as a team leader. Who knows what the future holds? Perhaps some time from now Iâ€™ll\ntake another swing on the flying trapeze and go back to management?\u003c/p\u003e\n","contentSnippet":"After six years as a Team Leader, I went back to hands-on engineering work, and Iâ€™m very happy about taking\nthis step. While it may appear surprising at first, it was a well-thought-out decision, and actually Iâ€™ve already\nperformed such a maneuver once before.\nBackground\nA few years ago I stumbled upon The Engineer/Manager Pendulum\nby Charity Majors, and the follow-up post Engineering Management: The Pendulum Or The Ladder.\nI found both pieces interesting, and quite in line with my own experiences: in my previous job some time earlier I\nhad been a team leader, but consciously joined Allegro as a software engineer. After a while, I\nbecame a team leader again, and recently, another few years down the line, I went back to engineering yet again.\nBoth above-mentioned posts make very good points, so I recommend you read them first. What I want to add on top of\nthem are my personal experiences and some tips on organizing a transition between a management and an individual\ncontributor role. The journey in the other direction (from developer to team leader) has been discussed in depth\nelsewhere, so I wonâ€™t delve into that.\nWhy\nWhy would I want to switch between developer and team leader roles? The problem is they are both interesting and have\ntheir own highlights, but you canâ€™t do both at the\nsame time. When you become a manager, not only do you have less time for technical tasks, but you also pretty much\nlose the ability to focus on these tasks even if you do find a time slot. This is because, like it or not, you end\nup with a managerâ€™s schedule.\n\nOver time, your technical skills start deteriorating, and if you miss the right moment, you may find yourself at\na point of no return. Like a circus artist on a flying trapeze, you have to time your actions right to avoid\ndisaster. I think this metaphor fits the situation better than that of the pendulum, which moves in its own rhythm,\nindependent of external influences.\nObviously, you need to ask yourself whether you actually want to prevent your technical skills from deteriorating. Some\npeople move on to management, wave goodbye to getting their hands dirty, and are completely fine with that. Some tinker\nwith technology in their spare time. As for me, I like technology and would not only prefer to not lose what I have\nlearned so far, but actually want to learn something new. However, due to a number of other hobbies and already\nspending more time in front of the screen than I would like, doing tech in the afternoons was not a viable option for\nme. Hence, the decision to make technology an important part of my job again.\nHow\nA crucial factor to take into account when planning such a change is that it will take time. If you take your\nteam seriously, you canâ€™t just disappear overnight. You need to think ahead. Do not ask yourself â€œhave I already\nlost my tech skills beyond repair?â€. Ask yourself â€œhow will my tech skills be a year from now?â€.\nAlso, keep a cool head. Just as when moving in the opposite direction, from engineer to manager, consider all\nconsequences, both positive and negative. It has been repeated many times that becoming a manager is not a promotion\n(being a lateral move to another career path), but in practice sometimes it is. In particular, it may come with a\nhigher level or salary. Make sure you check all details of your target role in order to avoid unpleasant surprises when\ngoing back. In my case, I was aiming for the Principal Software Engineer role, which is the same level as the Team\nLeader role at Allegro, so there were no issues in this regard.\nWhen I started thinking about making the switch, once I had a rough idea of what I wanted, I talked to my superior.\nThis was an important step: it allowed him to plan ahead, and also to look for opportunities for making the\nreorganization easier. Some elements of the process would depend only on our actions, but some, such as finding a good\nreplacement team leader, would also depend on a number of factors outside our control. Knowing that my boss understood\nmy need, and supported it, mattered a lot, and made the wait and preparations easier.\nChance favors the prepared mind, as Louis Pasteur supposedly said. There happened to be a team leader in\nanother part of the company who was thinking about moving on to a different area. Thanks to being aware of my plan,\nmy boss was able to grab the chance, and we had a perfect match. We discussed with the potential new leader the team\nand the project, and he found them interesting. We planned a transition period, as short as possible, but long enough\nfor me to transfer to him a reasonable part of my knowledge about the team and its work.\nNow that we had a specific plan, we could tell the team. It was important to let everyone know as soon as possible, but\nnot before we had a specific plan. Without it, this information would only stir uncertainty. Apart\nfrom telling the team as a whole, I also talked to each person individually, in order to resolve any questions or doubts\nand to try to reduce any problems resulting from the transition as much as possible.\nWaiting for the switch date, we kept meeting online with the new leader, transferring knowledge and preparing him for\nworking with the team. Thereâ€™s actually quite a lot of stuff a leader needs to know: not only how the project\nworks on technical and business levels, but also current plans, who the stakeholders are and how to work with them,\nand each team memberâ€™s individual strengths and development plan. The new leader himself also started meeting\npeople he would now work with, both team members and our productâ€™s stakeholders, and attending team meetings such as\nthe daily stand-up. Despite gradually moving on to other tasks after the switch date, I was still available to clarify\nany doubts, and our boss would also help out when necessary, so the new leader knew he would not be left on his own.\nWhile it required quite a bit of work, the switch went smoothly, and we didnâ€™t notice any serious disturbance to the\nteamâ€™s functioning.\nThe Aftermath\nItâ€™s been several months since the switch now. Me changing back to a technical role has certainly required extra work,\nfor me, my boss, and the new team leader. Despite our best efforts, it probably put a little extra strain on the team as\nwell. Nonetheless, I think it was a win-win, even more so thanks to us being able to spot and exploit a happy\ncoincidence. I am glad to be closer to technology again, and the new leader also got to try something new, just as he\nwanted.\nThere is one more subtle advantage to the whole process. When people leave the team, some knowledge inevitably gets\nlost. One of the reasons is tacit knowledge: there are always things you know, but are not aware of knowing. You can\nuse this knowledge when itâ€™s needed, but you will probably not transfer it to others because you are not even aware of\nits existence in the first place. Removing someone from the team in a controlled manner as happened here (and being\nstill able to reach out to them if needed) causes such latent knowledge to be discovered, and once discovered, to be\npropagated. This causes a little disruption short-term, but in the long run it reduces\nknowledge silos and increases the\nbus factor.\nWhen I first started thinking about going back to hands-on technical work for the second time, I had some doubts about\nhow much my technical skills had already deteriorated and how difficult it would be to go back. It seems I made it, but\nnot by much. Had I delayed by one more year, I might have really struggled. Itâ€™s not a matter of knowledge: theory,\nespecially generic things that do not change that fast with technology, is not lost so quickly. Also, while a team\nleader, I tried to stay in touch with technology by taking part in task refinement, architecture discussions, on-call\nrotation, etc.\nHowever, I really felt, and to some degree still feel, a difference in practical, hands-on work, such as actually\nwriting code. There are many small quirks that you need to be aware of in order to accomplish things quickly that you\ndonâ€™t even notice if you use them every day and know inside-out. Knowing all the little useful tools, the less often\nused features of your IDE, or what to do when something breaks unexpectedly, make a world of a difference, but this\npractical knowledge gets lost when not used and I had to rebuild it almost from scratch. Another thing that I still\nexperience is the difficulty in focusing on a single topic. Working on a managerâ€™s schedule for several years has taken\nits toll, and now that I often have large contiguous blocks of time, I find myself not using them as effectively as I\ncould, because I have become accustomed to always doing multiple things at once and without a chance to stay focused\nanyway. Itâ€™s gradually getting better, but I still feel the impact, and this is probably my biggest surprise of the\nwhole process.\nAbout the Principal Software Engineer role\nMy current role is that of Principal Software Engineer (PSE). It is a relatively new addition to the junior, mid,\nand senior roles weâ€™ve had so far. It has gone through a number of revisions, and is still evolving. Most people in this\nrole come from a Senior Software Engineer background, so my case of getting there after being a Team Leader is a bit\nuntypical. At many companies, roles like this are called Staff Software Engineer or similar. While still an individual\ncontributor role, a PSE differs from a senior in several ways.\nFirst of all, a PSE is expected to spend significant time on topics whose scope is much larger than a single team can\nhandle. Seniors can also do this, but itâ€™s not a requirement for them. Such topics may be area-wide, such as planning a\nmajor change to a single subsystemâ€™s architecture, or have a company-wide scope. Much work on this level consists of\ncoming up with ideas and discussing them while implementation is often left to others. So, while still technical, this\nrole encompasses less coding than that of a senior. Not very surprising given that generally moving up the career ladder\nmeans more coming up with ideas, teaching others, and planning work, while coding less yourself.\nSecondly, a PSE should be very autonomous. Most PSEs are not members of regular development teams since they move from\ntask to task depending on where they can help most. This means you cooperate with more people from different parts of\nthe company, but you donâ€™t have the few peers you work with every day that most people have. You donâ€™t get a backlog\nof tasks to work on, but have to plan your work yourself. People do come to you, asking for support or doing something\nfor their project, but thatâ€™s just one of many inputs.\nThirdly, since there are few PSEs compared to other positions, for each person the role is a little different. On one\nhand this means you canâ€™t fully know what to expect when you start. On the other, you get to shape the role yourself,\nand personally I enjoy this flexibility.\nSummary\nI think the idea of swinging back and forth between engineering and management described in\nThe Engineer/Manager Pendulum,\nis valid, and my own experience backs it up fully. One element which I want to additionally stress, however, is that\nthe switch is more like a flying trapeze than a pendulum: timing is crucial, and missing the right moment can have\nserious consequences. Becoming a Principal Software Engineer was a unique experience, both on a technical\nlevel and as my de facto last task as a team leader. Who knows what the future holds? Perhaps some time from now Iâ€™ll\ntake another swing on the flying trapeze and go back to management?","guid":"https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html","categories":["tech","coding","management","developer","team leader","career path"],"isoDate":"2023-08-21T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance","link":"https://blog.allegro.tech/2023/07/save-money-on-large-database.html","pubDate":"Mon, 10 Jul 2023 00:00:00 +0200","authors":{"author":[{"name":["Mateusz Stolecki"],"photo":["https://blog.allegro.tech/img/authors/mateusz.stolecki.jpg"],"url":["https://blog.allegro.tech/authors/mateusz.stolecki"]}]},"content":"\u003cp\u003eIn the era of ubiquitous cloud services and an increasingly growing PaaS and serverless-oriented approach, performance\nand resources seem to be becoming less and less important.\nAfter all, we can scale horizontally and vertically at any time, without worrying about potential performance challenges\nthat the business may introduce.\u003c/p\u003e\n\n\u003cp\u003eHowever, there is also another side to the coin â€“ rising costs. While it can be argued that in many situations it is simply\ncheaper to add another instance of the service than to engage a developer who will work tirelessly to diagnose\nand optimize performance problems, the problem will persist and intensify as the business and its requirements grow.\u003c/p\u003e\n\n\u003cp\u003eA similar situation arises with databases. We often store huge amounts of data for auditing or historical purposes.\nWhile the cost of maintaining such databases is negligible at a small scale,\nover time it can become a notable burden on our budget.\u003c/p\u003e\n\n\u003cp\u003eI wanted to talk about such a case and how we managed to reduce the cost of maintaining a database nearly 30-fold.\u003c/p\u003e\n\n\u003ch2 id=\"the-problem\"\u003eThe problem\u003c/h2\u003e\n\u003cp\u003eAs the amount of data grows, the need for scaling arises. In the case of \u003cstrong\u003eAzure\u003c/strong\u003e services, scaling also has its \u003ca href=\"https://learn.microsoft.com/en-us/azure/azure-sql/database/purchasing-models?view=azuresql\"\u003elimitations\u003c/a\u003e.\nIt is not always possible to infinitely increase the available disk space without scaling other resources (CPU, RAM, I/O).\nIn our case, this limit became apparent when we exceeded 1TB of data. Our database was based on the vCore model,\nwhere we used \u003cstrong\u003e4 vCores\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eUnfortunately, this number of vCores limited the available disk space to \u003cstrong\u003e1TB\u003c/strong\u003e. Due to the increase in the number of users\nand the demand for disk space, we needed more resources. We continued to scale up, adding not only more disk resources\nbut also computational resources (I will mention that at this point we reached a scale of \u003cstrong\u003e3TB\u003c/strong\u003e of data, which requires\nat least \u003cstrong\u003e12 vCores\u003c/strong\u003e). At some point, the cost of maintaining the database amounted to several thousand euros.\nThis prompted us to look for solutions.\u003c/p\u003e\n\n\u003cp\u003eComparing the cost of storing substantial amounts of data within \u003cstrong\u003eAzure SQL\u003c/strong\u003e and \u003cstrong\u003eStorage Account\u003c/strong\u003e\n(especially blobs in the \u003cstrong\u003earchive\u003c/strong\u003e tier), we concluded that we could achieve significant cost reduction\nby archiving old/unused data and placing it in a cost-optimized container.\u003c/p\u003e\n\n\u003ch3 id=\"monthly-cost-of-storing-3tb-of-data\"\u003eMonthly cost of storing 3TB of data\u003c/h3\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eAzure SQL 12vCore 3TB\u003c/th\u003e\n    \u003cth\u003eStorage Account Archive tier\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e$2,876.18\u003c/td\u003e\n    \u003ctd\u003e$31.12\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003ch2 id=\"analysis\"\u003eAnalysis\u003c/h2\u003e\n\u003cp\u003eAfter some investigation, It turned out that significant part of data could be safely archived,\nwhich would certainly provide\npotential savings and eliminate the problem of an overgrown database. Most of this data was actually historical.\u003c/p\u003e\n\n\u003cp\u003eWe implemented a solution that allows for much more scalable data archiving\nby asynchronously loading data into the warehouse.\nHowever, data from before the implementation of aforementioned solution were still generating considerable storage costs.\u003c/p\u003e\n\n\u003cp\u003eThe idea seemed simple both in concept and execution. However, we immediately encountered several problems.\nExporting such massive amounts of data is a time-consuming process and puts a heavy load on the database\ncausing responsiveness issues.\u003c/p\u003e\n\n\u003cp\u003eDealing with a production system, we could not reduce the reliability and availability of services.\nIn addition, the export functionality offered by the Azure portal is limited to databases up to \u003cstrong\u003e200GB\u003c/strong\u003e in size,\nwhich meant that we had to look for another solution.\u003c/p\u003e\n\n\u003ch2 id=\"action-plan\"\u003eAction plan\u003c/h2\u003e\n\u003ch3 id=\"concept\"\u003eConcept\u003c/h3\u003e\n\u003cp\u003eAs it turned out, there are ways to export even huge databases. After some investigation,\nwe found the \u003cstrong\u003eSQL Package\u003c/strong\u003e tool.\nIt provides \u003cstrong\u003eexport\u003c/strong\u003e option and is great for solving aforementioned problem. It is able to produce a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebacpac\u003c/code\u003e\nfile that contains highly compressed content of the database.\nThe tool also allows you to restore data at any time using the \u003cstrong\u003eimport\u003c/strong\u003e operation,\nif there is ever a need to review it, for example for audit purposes.\u003c/p\u003e\n\n\u003cp\u003eThe next step is to copy the file to the container in the Storage Account using the \u003cstrong\u003eAzCopy\u003c/strong\u003e tool and ensure\nthat it is stored in the \u003cstrong\u003eARCHIVE\u003c/strong\u003e tier, what will massively reduce the costs of maintaining it.\u003c/p\u003e\n\n\u003cp\u003eThe final stage is to delete unnecessary data from the database, then \u003cstrong\u003eSHRINK\u003c/strong\u003e it, what will reduce database resources.\u003c/p\u003e\n\u003ch3 id=\"script-and-tools\"\u003eScript and tools\u003c/h3\u003e\n\u003cp\u003eTo export and archive the database, we used two tools provided by Microsoft: \u003ca href=\"https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage?view=sql-server-ver16\"\u003eSQL Package\u003c/a\u003e\nand \u003ca href=\"https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10\"\u003eAzCopy\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAfter analyzing their documentation, we prepared the appropriate procedure taking\ninto account performance and operation duration.\u003c/p\u003e\n\u003ch3 id=\"infrastructure\"\u003eInfrastructure\u003c/h3\u003e\n\u003cp\u003eDue to the fact that the export and upload process to the Storage Account container with this amount of data may take\na long time, we decided to set up a temporary \u003cstrong\u003eVM\u003c/strong\u003e with the accelerated networking option, which served us\nto execute all required scripts. It should be mentioned that the need to set up a dedicated virtual machine also arises\nfrom the fact that it must be located in an internal network, where it is also possible to connect to the machine that\nhandles the database. Thanks to meeting this condition,\nit was possible to successfully connect to the database and perform the export operation.\u003c/p\u003e\n\n\u003cp\u003eThe virtual machine turned out to be moderately priced, as all performed operations were not computationally demanding\n(both CPU and RAM usage were low), what allowed us to use a very resource-efficient machine. The only notable extension\nof its functionality is \u003cstrong\u003eaccelerated networking\u003c/strong\u003e, as it must work with data transfer over the network\nand we needed good performance.\u003c/p\u003e\n\n\u003ch2 id=\"testing\"\u003eTesting\u003c/h2\u003e\n\u003ch3 id=\"optimization\"\u003eOptimization\u003c/h3\u003e\n\u003cp\u003eBefore we proceeded with the implementation in the production environment, we conducted a series of\ntests using test environments. They mainly involved running all the steps of the process using\ndata packages of approximately \u003cstrong\u003e50GB\u003c/strong\u003e and \u003cstrong\u003e200GB\u003c/strong\u003e in size.\nWe spent the majority of time testing and optimizing the use of the SQL Package tool.\u003c/p\u003e\n\n\u003cp\u003eOur goal was to shorten the export time and obtain an optimal size for the resulting file,\nso it would not generate excessive costs due to the need to store it. We tested several scenarios\n(mostly by manipulating the \u003cstrong\u003ecompression level\u003c/strong\u003e parameter).\u003c/p\u003e\n\n\u003cp\u003eCompression in \u003cstrong\u003eFAST\u003c/strong\u003e mode showed an average of 10-20% faster export time than \u003cstrong\u003eMAXIMUM\u003c/strong\u003e, with the resulting file\nsize varying within \u0026lt;10%.\u003c/p\u003e\n\n\u003ch3 id=\"performance-testing\"\u003ePerformance testing\u003c/h3\u003e\n\u003cp\u003eWe also tested the load on the databases in each environment.\n\u003cstrong\u003eData IO\u003c/strong\u003e and \u003cstrong\u003eCPU\u003c/strong\u003e load were tested using the test environment relying on DTU-based infrastructure utilising \u003cstrong\u003e100 DTU\u003c/strong\u003e\nunits.\u003c/p\u003e\n\n\u003cp\u003eData IO\n\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-test-dev-iops.png\" alt=\"Data IO\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eCPU\n\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-test-dev-cpu.png\" alt=\"CPU\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNotice, that the export operation primarily consumes IO resources.\u003c/p\u003e\n\u003ch3 id=\"data-import\"\u003eData Import\u003c/h3\u003e\n\u003cp\u003eDue to the possible need to reuse archived data, we had to make sure that the data we imported was suitable for re-import.\u003c/p\u003e\n\n\u003cp\u003eInitially, we attempted to import the data using the \u003cstrong\u003eSQL Server Management Studio\u003c/strong\u003e tool provided by Microsoft.\nUnfortunately, this attempt failed due to errors related to file reading during the import operation.\nWe made an additional attempt to import the archive using the SQL Package tool, which, in addition to the export option,\nalso provides import options.\u003c/p\u003e\n\n\u003cp\u003eCommand\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003esqlpackage /Action:Import `\n        /tsn:$ServerName `\n        /tdn:$DatabaseName `\n        /tu:$SqlAdminName `\n        /tp:$SqlAdminPassword `\n        /tec:true `\n        /ttsc:false `\n        /d:true `\n        /sf:$SourceFile `\n        /p:CommandTimeout=999 `\n        /p:LongRunningCommandTimeout=0 `\n        /p:DatabaseLockTimeout=-1 `\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003esolved the problem.\u003c/p\u003e\n\n\u003ch2 id=\"deployment\"\u003eDeployment\u003c/h2\u003e\n\u003ch3 id=\"exporting-the-database-using-sql-package-tool\"\u003eExporting the database using SQL Package tool\u003c/h3\u003e\n\u003cp\u003eThe following script was executed, successfully extracting data from the database and creating the appropriate \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebacpac\u003c/code\u003e file.\nAs a result, we received a compressed file of around 100GB.\nIt is worth pointing out that data in the database occupied about 3TB, so compression was very efficient.\nThe whole process took several hours.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003esqlpackage\n    /Action:Export `\n    /ssn:$ServerName `\n    /sdn:$DatabaseName `\n    /su:$SqlAdminName `\n    /sp:$SqlAdminPassword `\n    /sec:true `\n    /stsc:false `\n    /tf:$TargetFile `\n    /p:CompressionOption=Fast `\n    /p:CommandTimeout=999 `\n    /p:LongRunningCommandTimeout=0 `\n    /p:DatabaseLockTimeout=-1 `\n    /p:TempDirectoryForTableData=$TempDirectory `\n    /d:true `\n    /df:$SqlLogs `\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eMany parameters of this operation were evaluated during trials on test environments.\nThe particularly important ones are:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eCommandTimeout, LongRunningCommandTimeout, DatabaseLockTimeout\u003c/strong\u003e - This set of\nparameters ensures that the connection\nis maintained throughout the entire duration of the export operation (assuming that it will be long-running).\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCompressionOption\u003c/strong\u003e - The degree of data compression in the output file.\nTwo variants were tested:\n\u003cstrong\u003eFAST\u003c/strong\u003e and \u003cstrong\u003eMAXIMUM\u003c/strong\u003e.\n\u003cstrong\u003eFAST\u003c/strong\u003e allowed us to shorten the export time by about 2 hours while showing only slightly lower\ndata compression (in our case, the difference was around 10%).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cdiv class=\"language-powershell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003e/p:TableData\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"dbo.TestTable\"\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe parameter allows us to limit the data export only to the tables selected by us, what significantly shortens\nthe overall operation time. It is also worth mentioning that it is possible to set the parameter multiple times.\u003c/p\u003e\n\n\u003cp\u003eSince the export was launched at night, the procedure had no negative impact on users. The impact of the\nexport operation on the database load (Data I/O percentage) is presented in the graph below. It can be observed that\nthe resource load increased during this operation.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-export-iops.png\" alt=\"Data IO\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"copying-the-archived-database-using-azcopy\"\u003eCopying the archived database using AzCopy\u003c/h3\u003e\n\u003cp\u003eThe following script was executed to copy the exported file to the Storage Account:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e.\\azcopy `\n    copy `\n    $TargetFile `\n\"https://$StorageAccountName.blob.core.windows.net/$StorageContainerName/$StorageBlobName$SAS\" `\n    --recursive `\n    --overwrite=true `\n    --blob-type=BlockBlob `\n    --put-md5 `\n    --log-level=info `\n    --block-blob-tier=archive `\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe process went quickly. Copying the 100GB file took only a few minutes, thanks to the high network throughput.\nIt is worth noting that the archive tier is set immediately.\u003c/p\u003e\n\n\u003ch3 id=\"conducting-a-shrink-operation\"\u003eConducting a SHRINK operation\u003c/h3\u003e\n\u003cp\u003eThe SHRINK operation is, unfortunately, required to downscale the Azure SQL database. It took several hours to complete.\n\u003cstrong\u003eWAIT_AT_LOW_PRIORITY\u003c/strong\u003e was used to reduce the impact of this operation on the database users.\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eDBCC\u003c/span\u003e \u003cspan class=\"n\"\u003eSHRINKDATABASE\u003c/span\u003e \u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"n\"\u003eDB_NAME\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e \u003cspan class=\"k\"\u003eWITH\u003c/span\u003e \u003cspan class=\"n\"\u003eWAIT_AT_LOW_PRIORITY\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe performance chart (Data IO) during the above operation looked as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-shrink.png\" alt=\"Data IO\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWe observed a slight increase in Data IO operations during the SHRINK operation.\u003c/p\u003e\n\u003ch3 id=\"performance-analysis-and-index-rebuild\"\u003ePerformance analysis and index rebuild\u003c/h3\u003e\n\u003cp\u003eThis step appeared quite unexpectedly in our procedure. After performing the SHRINK operation and successfully\nlowering the parameters of the machine responsible for the database, we began to observe\nthe impact of our operations on performance.\u003c/p\u003e\n\n\u003cp\u003eTo our concern, we observed a noticeable performance regression.\nEndpoints that use the database on which we performed \u003cstrong\u003eSHRINK\u003c/strong\u003e operation showed abnormally increased response times.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-rps-before-index.png\" alt=\"RPS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe database load chart also did not look encouraging, with frequent peaks during query execution.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-iops-before-index.png\" alt=\"IOPS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAttempts to scale the machine did not bring spectacular results and only increased costs (considering that our goal was\nto lower them, it was not an optimal solution).\u003c/p\u003e\n\n\u003cp\u003eAs it turned out, the culprit was extraordinarily high index fragmentation. The result of the SHRINK operation was an increase\nin the mentioned fragmentation to almost \u0026gt;90% for practically all existing indexes.\nThis forced us to consider rebuilding all of them.\u003c/p\u003e\n\n\u003cp\u003eEven Microsoft recommends rebuilding indexes in their documentation \u003ca href=\"https://learn.microsoft.com/en-us/sql/relational-databases/databases/shrink-a-database?view=sql-server-ver16\"\u003ehere\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eData that is moved to shrink a file can be scattered to any available location in the file.\nThis causes index fragmentation and can slow the performance of queries that search a range of the index.\nTo eliminate the fragmentation, consider rebuilding the indexes on the file after shrinking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe decided to proceed with the above-mentioned index rebuild process. Here, we also applied possible optimizations\nto avoid negative consequences related to the availability of our services. The \u003cstrong\u003eONLINE\u003c/strong\u003e option is particularly noteworthy,\nas it ensures that existing indexes and tables will not be blocked, what is an important issue in the case\nof continuous operation of our services.\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eINDEX\u003c/span\u003e \u003cspan class=\"k\"\u003eALL\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e \u003cspan class=\"n\"\u003edbo\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eTableName\u003c/span\u003e \u003cspan class=\"n\"\u003eREBUILD\u003c/span\u003e \u003cspan class=\"k\"\u003eWITH\u003c/span\u003e\n\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eFILLFACTOR\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e80\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eSORT_IN_TEMPDB\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eSTATISTICS_NORECOMPUTE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eONLINE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIt should also be noted that this can be a time-consuming operation, but as a result of its execution,\nthe indexes returned to the required consistency level, reaching a level of fragmentation close to 0%.\nThe response time and resource consumption charts of the database also returned to the values closer to the initial ones.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-rps-after-rebuild.png\" alt=\"RPS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-iops-after-index.png\" alt=\"IOPS\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eAfter performing all of the described actions, we achieved a reduction\nin the size of the database from over 3TB to slightly below 100GB.\nBy lowering the required disk space, we could also significantly reduce the computational resources of the database,\ngenerating further serious savings.\u003c/p\u003e\n\n\u003cp\u003eBefore performing all the operations,\nthe monthly cost of maintaining the database was close to â‚¬3000.\nBy switching from a database based on a 12 vCore and 3TB model to a Standard DTU with 100 units and 150GB\nwe managed to cut our monthly spendings to mere â‚¬125.\nAfter all, our effort paid off.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/montly-cost-reduction.png\" alt=\"Cost reduction\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above example demonstrates how to greatly reduce infrastructure costs. Of course,\nthe described procedure will apply to specific cases and data characteristics.\nHowever, if you have a similar problem, I think it is worth considering this approach.\u003c/p\u003e\n","contentSnippet":"In the era of ubiquitous cloud services and an increasingly growing PaaS and serverless-oriented approach, performance\nand resources seem to be becoming less and less important.\nAfter all, we can scale horizontally and vertically at any time, without worrying about potential performance challenges\nthat the business may introduce.\nHowever, there is also another side to the coin â€“ rising costs. While it can be argued that in many situations it is simply\ncheaper to add another instance of the service than to engage a developer who will work tirelessly to diagnose\nand optimize performance problems, the problem will persist and intensify as the business and its requirements grow.\nA similar situation arises with databases. We often store huge amounts of data for auditing or historical purposes.\nWhile the cost of maintaining such databases is negligible at a small scale,\nover time it can become a notable burden on our budget.\nI wanted to talk about such a case and how we managed to reduce the cost of maintaining a database nearly 30-fold.\nThe problem\nAs the amount of data grows, the need for scaling arises. In the case of Azure services, scaling also has its limitations.\nIt is not always possible to infinitely increase the available disk space without scaling other resources (CPU, RAM, I/O).\nIn our case, this limit became apparent when we exceeded 1TB of data. Our database was based on the vCore model,\nwhere we used 4 vCores.\nUnfortunately, this number of vCores limited the available disk space to 1TB. Due to the increase in the number of users\nand the demand for disk space, we needed more resources. We continued to scale up, adding not only more disk resources\nbut also computational resources (I will mention that at this point we reached a scale of 3TB of data, which requires\nat least 12 vCores). At some point, the cost of maintaining the database amounted to several thousand euros.\nThis prompted us to look for solutions.\nComparing the cost of storing substantial amounts of data within Azure SQL and Storage Account\n(especially blobs in the archive tier), we concluded that we could achieve significant cost reduction\nby archiving old/unused data and placing it in a cost-optimized container.\nMonthly cost of storing 3TB of data\nAzure SQL 12vCore 3TB\n    Storage Account Archive tier\n  \n$2,876.18\n    $31.12\n  \nAnalysis\nAfter some investigation, It turned out that significant part of data could be safely archived,\nwhich would certainly provide\npotential savings and eliminate the problem of an overgrown database. Most of this data was actually historical.\nWe implemented a solution that allows for much more scalable data archiving\nby asynchronously loading data into the warehouse.\nHowever, data from before the implementation of aforementioned solution were still generating considerable storage costs.\nThe idea seemed simple both in concept and execution. However, we immediately encountered several problems.\nExporting such massive amounts of data is a time-consuming process and puts a heavy load on the database\ncausing responsiveness issues.\nDealing with a production system, we could not reduce the reliability and availability of services.\nIn addition, the export functionality offered by the Azure portal is limited to databases up to 200GB in size,\nwhich meant that we had to look for another solution.\nAction plan\nConcept\nAs it turned out, there are ways to export even huge databases. After some investigation,\nwe found the SQL Package tool.\nIt provides export option and is great for solving aforementioned problem. It is able to produce a bacpac\nfile that contains highly compressed content of the database.\nThe tool also allows you to restore data at any time using the import operation,\nif there is ever a need to review it, for example for audit purposes.\nThe next step is to copy the file to the container in the Storage Account using the AzCopy tool and ensure\nthat it is stored in the ARCHIVE tier, what will massively reduce the costs of maintaining it.\nThe final stage is to delete unnecessary data from the database, then SHRINK it, what will reduce database resources.\nScript and tools\nTo export and archive the database, we used two tools provided by Microsoft: SQL Package\nand AzCopy.\nAfter analyzing their documentation, we prepared the appropriate procedure taking\ninto account performance and operation duration.\nInfrastructure\nDue to the fact that the export and upload process to the Storage Account container with this amount of data may take\na long time, we decided to set up a temporary VM with the accelerated networking option, which served us\nto execute all required scripts. It should be mentioned that the need to set up a dedicated virtual machine also arises\nfrom the fact that it must be located in an internal network, where it is also possible to connect to the machine that\nhandles the database. Thanks to meeting this condition,\nit was possible to successfully connect to the database and perform the export operation.\nThe virtual machine turned out to be moderately priced, as all performed operations were not computationally demanding\n(both CPU and RAM usage were low), what allowed us to use a very resource-efficient machine. The only notable extension\nof its functionality is accelerated networking, as it must work with data transfer over the network\nand we needed good performance.\nTesting\nOptimization\nBefore we proceeded with the implementation in the production environment, we conducted a series of\ntests using test environments. They mainly involved running all the steps of the process using\ndata packages of approximately 50GB and 200GB in size.\nWe spent the majority of time testing and optimizing the use of the SQL Package tool.\nOur goal was to shorten the export time and obtain an optimal size for the resulting file,\nso it would not generate excessive costs due to the need to store it. We tested several scenarios\n(mostly by manipulating the compression level parameter).\nCompression in FAST mode showed an average of 10-20% faster export time than MAXIMUM, with the resulting file\nsize varying within \u003c10%.\nPerformance testing\nWe also tested the load on the databases in each environment.\nData IO and CPU load were tested using the test environment relying on DTU-based infrastructure utilising 100 DTU\nunits.\nData IO\n\nCPU\n\nNotice, that the export operation primarily consumes IO resources.\nData Import\nDue to the possible need to reuse archived data, we had to make sure that the data we imported was suitable for re-import.\nInitially, we attempted to import the data using the SQL Server Management Studio tool provided by Microsoft.\nUnfortunately, this attempt failed due to errors related to file reading during the import operation.\nWe made an additional attempt to import the archive using the SQL Package tool, which, in addition to the export option,\nalso provides import options.\nCommand\n\nsqlpackage /Action:Import `\n        /tsn:$ServerName `\n        /tdn:$DatabaseName `\n        /tu:$SqlAdminName `\n        /tp:$SqlAdminPassword `\n        /tec:true `\n        /ttsc:false `\n        /d:true `\n        /sf:$SourceFile `\n        /p:CommandTimeout=999 `\n        /p:LongRunningCommandTimeout=0 `\n        /p:DatabaseLockTimeout=-1 `\n\n\nsolved the problem.\nDeployment\nExporting the database using SQL Package tool\nThe following script was executed, successfully extracting data from the database and creating the appropriate bacpac file.\nAs a result, we received a compressed file of around 100GB.\nIt is worth pointing out that data in the database occupied about 3TB, so compression was very efficient.\nThe whole process took several hours.\n\nsqlpackage\n    /Action:Export `\n    /ssn:$ServerName `\n    /sdn:$DatabaseName `\n    /su:$SqlAdminName `\n    /sp:$SqlAdminPassword `\n    /sec:true `\n    /stsc:false `\n    /tf:$TargetFile `\n    /p:CompressionOption=Fast `\n    /p:CommandTimeout=999 `\n    /p:LongRunningCommandTimeout=0 `\n    /p:DatabaseLockTimeout=-1 `\n    /p:TempDirectoryForTableData=$TempDirectory `\n    /d:true `\n    /df:$SqlLogs `\n\n\nMany parameters of this operation were evaluated during trials on test environments.\nThe particularly important ones are:\nCommandTimeout, LongRunningCommandTimeout, DatabaseLockTimeout - This set of\nparameters ensures that the connection\nis maintained throughout the entire duration of the export operation (assuming that it will be long-running).\nCompressionOption - The degree of data compression in the output file.\nTwo variants were tested:\nFAST and MAXIMUM.\nFAST allowed us to shorten the export time by about 2 hours while showing only slightly lower\ndata compression (in our case, the difference was around 10%).\n\n/p:TableData=\"dbo.TestTable\"\n\n\nThe parameter allows us to limit the data export only to the tables selected by us, what significantly shortens\nthe overall operation time. It is also worth mentioning that it is possible to set the parameter multiple times.\nSince the export was launched at night, the procedure had no negative impact on users. The impact of the\nexport operation on the database load (Data I/O percentage) is presented in the graph below. It can be observed that\nthe resource load increased during this operation.\n\nCopying the archived database using AzCopy\nThe following script was executed to copy the exported file to the Storage Account:\n\n.\\azcopy `\n    copy `\n    $TargetFile `\n\"https://$StorageAccountName.blob.core.windows.net/$StorageContainerName/$StorageBlobName$SAS\" `\n    --recursive `\n    --overwrite=true `\n    --blob-type=BlockBlob `\n    --put-md5 `\n    --log-level=info `\n    --block-blob-tier=archive `\n\n\nThe process went quickly. Copying the 100GB file took only a few minutes, thanks to the high network throughput.\nIt is worth noting that the archive tier is set immediately.\nConducting a SHRINK operation\nThe SHRINK operation is, unfortunately, required to downscale the Azure SQL database. It took several hours to complete.\nWAIT_AT_LOW_PRIORITY was used to reduce the impact of this operation on the database users.\n\nDBCC SHRINKDATABASE ([DB_NAME]) WITH WAIT_AT_LOW_PRIORITY\n\n\nThe performance chart (Data IO) during the above operation looked as follows:\n\nWe observed a slight increase in Data IO operations during the SHRINK operation.\nPerformance analysis and index rebuild\nThis step appeared quite unexpectedly in our procedure. After performing the SHRINK operation and successfully\nlowering the parameters of the machine responsible for the database, we began to observe\nthe impact of our operations on performance.\nTo our concern, we observed a noticeable performance regression.\nEndpoints that use the database on which we performed SHRINK operation showed abnormally increased response times.\n\nThe database load chart also did not look encouraging, with frequent peaks during query execution.\n\nAttempts to scale the machine did not bring spectacular results and only increased costs (considering that our goal was\nto lower them, it was not an optimal solution).\nAs it turned out, the culprit was extraordinarily high index fragmentation. The result of the SHRINK operation was an increase\nin the mentioned fragmentation to almost \u003e90% for practically all existing indexes.\nThis forced us to consider rebuilding all of them.\nEven Microsoft recommends rebuilding indexes in their documentation here:\nData that is moved to shrink a file can be scattered to any available location in the file.\nThis causes index fragmentation and can slow the performance of queries that search a range of the index.\nTo eliminate the fragmentation, consider rebuilding the indexes on the file after shrinking.\nWe decided to proceed with the above-mentioned index rebuild process. Here, we also applied possible optimizations\nto avoid negative consequences related to the availability of our services. The ONLINE option is particularly noteworthy,\nas it ensures that existing indexes and tables will not be blocked, what is an important issue in the case\nof continuous operation of our services.\n\nALTER INDEX ALL ON dbo.TableName REBUILD WITH\n(FILLFACTOR = 80, SORT_IN_TEMPDB = ON, STATISTICS_NORECOMPUTE = ON, ONLINE = ON);\n\n\nIt should also be noted that this can be a time-consuming operation, but as a result of its execution,\nthe indexes returned to the required consistency level, reaching a level of fragmentation close to 0%.\nThe response time and resource consumption charts of the database also returned to the values closer to the initial ones.\n\n\nConclusion\nAfter performing all of the described actions, we achieved a reduction\nin the size of the database from over 3TB to slightly below 100GB.\nBy lowering the required disk space, we could also significantly reduce the computational resources of the database,\ngenerating further serious savings.\nBefore performing all the operations,\nthe monthly cost of maintaining the database was close to â‚¬3000.\nBy switching from a database based on a 12 vCore and 3TB model to a Standard DTU with 100 units and 150GB\nwe managed to cut our monthly spendings to mere â‚¬125.\nAfter all, our effort paid off.\n\nThe above example demonstrates how to greatly reduce infrastructure costs. Of course,\nthe described procedure will apply to specific cases and data characteristics.\nHowever, if you have a similar problem, I think it is worth considering this approach.","guid":"https://blog.allegro.tech/2023/07/save-money-on-large-database.html","categories":["tech","azure","sql","saving","cloud"],"isoDate":"2023-07-09T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Debugging hangs - piecing together why nothing happens","link":"https://blog.allegro.tech/2023/05/debugging-hangs.html","pubDate":"Wed, 31 May 2023 00:00:00 +0200","authors":{"author":[{"name":["Åukasz Rokita"],"photo":["https://blog.allegro.tech/img/authors/lukasz.rokita.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.rokita"]}]},"content":"\u003cp\u003eAs a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.\nThe task is daunting and filled with challenges,\nhowever overall progress is steady and thanks to the modular nature of our code it should end in finite time.\nEveryone who has performed such an upgrade knows that you need to expect the unexpected and at the end of the day prepare for lots of debugging.\nNo amount of migration guide would prepare you for whatâ€™s coming in the field.\nIn the words of Donald Rumsfeld there are unknown unknowns and we need to be equipped with the tools to uncover these unknowns and patch them up.\nIn this blog post Iâ€™d like to walk you through a process that should show where the application hangs,\nalthough there seems to be nothing wrong with it. I will also show that you donâ€™t always know what code you have â€“ problem known as dependecy hell,\nplace we got quite cosy in during this upgrade.\u003c/p\u003e\n\n\u003ch2 id=\"the-change\"\u003eThe change\u003c/h2\u003e\n\u003cp\u003eNote that we keep versions as separate keyâ€“value pairs in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebuild.gradle\u003c/code\u003e files and reference them in dependencies by key.\nUpdating often means a single line change. The upgrade is trivial and git diff looks like this.\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eext.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n]\n\next.versions = [\n+        spring         : '6.0.5',\n+        spock          : '2.4-M1-groovy-4.0',\n+        groovy         : '4.0.9',\n]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eNothing much happens. We upgrade Spring and since there are some problems with Spock not working well with the newest Spring\nwe need to upgrade it as well, along with Groovy. This is the easy part.\nNow we run the tests and expect to be either elated with the sight of a successful build or greeted with descriptive error messages\nthat help us quickly patch the issue. Nobody expects anything and in this case this is an unknown unknown.\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e97% EXECUTING [15m 55s]\n\u0026gt; :platform-libraries-webclient:integrationTest \u0026gt; 1 test completed, 1 failed\n\u0026gt; :platform-libraries-webclient:integrationTest \u0026gt; Executing test pl.allegro....WebClientContextContainerInterceptorSpec\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eAfter 15 minutes we expect the process to end. A quick cross-check with the master branch confirms that tests run and execute in less than a minute.\nSomething is wrong and itâ€™s on us. However, no error is presented. Adding logging does not help, nothing streams to standard output.\nSomething hangs and refuses to budge. When that happenes there is only one way to inspect what is going on and\nthat is to pop the hood open and look into JVM to see what the threads are doing or where they are slacking.\u003c/p\u003e\n\n\u003ch2 id=\"thread-theory\"\u003eThread theory\u003c/h2\u003e\n\n\u003cp\u003eLetâ€™s interrupt this story with a short summary of threading in JVM. You can skip this chapter if you are familiar with the topic.\nAs the priceless book Java Concurrency in Practice by Brian Goetz et al. teaches us:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003eâ€œThreads may block, or pause, for several reasons: waiting for I/O completion, waiting to acquire a lock,\nwaiting to wake up from Thread.sleep, or waiting for the result of a computation in another thread.\nWhen a thread blocks, it is usually suspended and placed in one of the blocked thread states\n(BLOCKED, WAITING, or TIMED_WAITING). (â€¦) blocked thread must wait for an event beyond its control before it can proceedâ€.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis sounds exactly like the situation we are in. So there is hope. Letâ€™s educate ourselves further.\nAnother excerpt that would prove insightful reads as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003eâ€œ(â€¦) tasks can block for exteded periods of time, even if deadlock is not a possibility.\n(â€¦) One technique that can mitigate the ill effects of longâ€“running tasks is for tasks to use timed resource waits instead of\nunbound waits.â€\nThis seems like an answer to our woes. However, two mysteries remain.\nWhere to put the timeout? What the thread is waiting for? To answer these questions we need to inspect the threads in the JVM itself.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"the-investigation\"\u003eThe investigation\u003c/h2\u003e\n\u003cp\u003eAt this point we did two things. First we pushed our code to a branch.\nAfter all at any moment our laptops could burst into flames and all the work would go to waste.\nThe remote CI confirmed our suspicion since it also hung. The problem was real and not only confined to the local machine.\nThe second thing is to scout for the offending thread. This is easy with the help of some JDK binaries:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ejps -lv | grep platform-libraries\n38983 worker.org.gradle.process.internal.worker.GradleWorkerMain -Dorg.gradle.internal.worker.tmpdir=/path/to/code/platform-libraries/platform-libraries-webclient/build/tmp/integrationTest/work -Dorg.gradle.native=false -Xmx512m -Dfile.encoding=UTF-8\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eSo we have the a lvmid â€“ local JVM identifier, which will help us locate the offending thread in jconsole.\nIn the screen below we can see that the thread waits on \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMono.block()\u003c/code\u003e which is left unbounded in a happy path scenario.\nWell, we are in the worst case so first of all we add a simple timeout \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMono.block(Duration.ofSeconds(10))\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-05-31-debugging-hangs/jconsole.png\" alt=\"jconsole\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThis fails our tests and for the first time the error appears:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\t08:13:39.556 [Test worker] WARN reactor.core.Exceptions - throwIfFatal detected a jvm fatal exception, which is thrown and logged below:\njava.lang.NoSuchMethodError: 'reactor.core.publisher.Mono reactor.core.publisher.Mono.subscriberContext(reactor.util.context.Context)'\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\tat reactor.netty.internal.shaded.reactor.pool.AbstractPool$Borrower.request(AbstractPool.java:427)\n\tat reactor.netty.resources.PooledConnectionProvider$DisposableAcquire.onSubscribe(PooledConnectionProvider.java:533)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool$QueueBorrowerMono.subscribe(SimpleDequePool.java:676)\n\tat reactor.netty.resources.PooledConnectionProvider.disposableAcquire(PooledConnectionProvider.java:219)\n\tat reactor.netty.resources.PooledConnectionProvider.lambda$acquire$3(PooledConnectionProvider.java:183)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:326)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.core.publisher.FluxRetryWhen.subscribe(FluxRetryWhen.java:77)\n\tat reactor.core.publisher.MonoRetryWhen.subscribeOrReturn(MonoRetryWhen.java:46)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:329)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)\n\tat reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2545)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.request(MonoFlatMap.java:194)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.request(FluxOnAssembly.java:649)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2341)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:2215)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onSubscribe(FluxOnAssembly.java:633)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onSubscribe(MonoFlatMap.java:117)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)\n\tat reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoDeferContextual.subscribe(MonoDeferContextual.java:55)\n\tat reactor.core.publisher.Mono.subscribe(Mono.java:4485)\n\tat reactor.core.publisher.Mono.block(Mono.java:1733)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$Trait$Helper.makeRequest(WebClientContextContainerAdapterConfiguration.groovy:22)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$makeRequest.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)\n\tat pl.allegro....AdapterConfiguration$Trait$Helper.makeRequest(AdapterConfiguration.groovy:11)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....SharedInterceptorSpec$makeRequest.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:203)\n\tat pl.allegro....SharedInterceptorSpec.$spock_feature_0_0(SharedInterceptorSpec.groovy:44)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.spockframework.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:196)\n\tat org.spockframework.runtime.model.MethodInfo.lambda$new$0(MethodInfo.java:49)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeatureMethod(PlatformSpecRunner.java:324)\n\tat org.spockframework.runtime.IterationNode.execute(IterationNode.java:50)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:58)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.IterationNode.lambda$around$0(IterationNode.java:67)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunIteration$5(PlatformSpecRunner.java:236)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.junit4.AbstractRuleInterceptor$1.evaluate(AbstractRuleInterceptor.java:46)\n\tat com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:79)\n\tat org.spockframework.junit4.MethodRuleInterceptor.intercept(MethodRuleInterceptor.java:40)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runIteration(PlatformSpecRunner.java:218)\n\tat org.spockframework.runtime.IterationNode.around(IterationNode.java:67)\n\tat org.spockframework.runtime.SimpleFeatureNode.lambda$around$0(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)\n\tat org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:63)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:11)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eFor the first time we force the entire reactive code to finally execute itself and present us with the result,\neven if it is an error this moves us in the right direction.\u003c/p\u003e\n\n\u003ch2 id=\"result\"\u003eResult\u003c/h2\u003e\n\n\u003cp\u003eLike in any good crime story uncovering one mystery presents another.\nA quick \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egrep\u003c/code\u003e shows that there are no calls to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereactor.core.publisher.Mono.subscriberContext\u003c/code\u003e.\nWhere could this call be hiding, if itâ€™s not present in our code?\u003c/p\u003e\n\n\u003cp\u003eThe answer is simple but I assure you that it took us some time to come up with it.\nIf it isnâ€™t in our code and it runs inside our JVM then this must be dependency code.\nThe observant reader is able to spot it from afar. The stack trace confirms where the error lies:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    at reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWe need to patch \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereactorâ€“netty\u003c/code\u003e which in this version still used deprecated code. Referring back to our diff:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eext.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n-        reactorNetty   : '0.9.25.RELEASE',\n]\n\next.versions = [\n+        spring        : '6.0.5',\n+        spock         : '2.4-M1-groovy-4.0',\n+        groovy        : '4.0.9',\n+        reactorNetty  : '1.1.3',\n]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWe escape the dependency hell and are delighted to see the green letters \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBUILD SUCCESSFUL in 24s\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWell this was quite a thrilling journey one doesnâ€™t often embark on.\nThe odd peculiarity of the problem combined with peculiarity of the task provided us with a great challange and satisfaction.\nDependency hell is no joke, but armed with the JDK tools and thinking the problem through, there is no obstacle that could not be overcome.\nNext time your code hangs with no apparent reason this is a perfect opportunity to dust off the swiss army knife of JDK binaries and dig in.\u003c/p\u003e\n","contentSnippet":"As a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.\nThe task is daunting and filled with challenges,\nhowever overall progress is steady and thanks to the modular nature of our code it should end in finite time.\nEveryone who has performed such an upgrade knows that you need to expect the unexpected and at the end of the day prepare for lots of debugging.\nNo amount of migration guide would prepare you for whatâ€™s coming in the field.\nIn the words of Donald Rumsfeld there are unknown unknowns and we need to be equipped with the tools to uncover these unknowns and patch them up.\nIn this blog post Iâ€™d like to walk you through a process that should show where the application hangs,\nalthough there seems to be nothing wrong with it. I will also show that you donâ€™t always know what code you have â€“ problem known as dependecy hell,\nplace we got quite cosy in during this upgrade.\nThe change\nNote that we keep versions as separate keyâ€“value pairs in build.gradle files and reference them in dependencies by key.\nUpdating often means a single line change. The upgrade is trivial and git diff looks like this.\n\next.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n]\n\next.versions = [\n+        spring         : '6.0.5',\n+        spock          : '2.4-M1-groovy-4.0',\n+        groovy         : '4.0.9',\n]\n\n\nNothing much happens. We upgrade Spring and since there are some problems with Spock not working well with the newest Spring\nwe need to upgrade it as well, along with Groovy. This is the easy part.\nNow we run the tests and expect to be either elated with the sight of a successful build or greeted with descriptive error messages\nthat help us quickly patch the issue. Nobody expects anything and in this case this is an unknown unknown.\n\n97% EXECUTING [15m 55s]\n\u003e :platform-libraries-webclient:integrationTest \u003e 1 test completed, 1 failed\n\u003e :platform-libraries-webclient:integrationTest \u003e Executing test pl.allegro....WebClientContextContainerInterceptorSpec\n\n\nAfter 15 minutes we expect the process to end. A quick cross-check with the master branch confirms that tests run and execute in less than a minute.\nSomething is wrong and itâ€™s on us. However, no error is presented. Adding logging does not help, nothing streams to standard output.\nSomething hangs and refuses to budge. When that happenes there is only one way to inspect what is going on and\nthat is to pop the hood open and look into JVM to see what the threads are doing or where they are slacking.\nThread theory\nLetâ€™s interrupt this story with a short summary of threading in JVM. You can skip this chapter if you are familiar with the topic.\nAs the priceless book Java Concurrency in Practice by Brian Goetz et al. teaches us:\nâ€œThreads may block, or pause, for several reasons: waiting for I/O completion, waiting to acquire a lock,\nwaiting to wake up from Thread.sleep, or waiting for the result of a computation in another thread.\nWhen a thread blocks, it is usually suspended and placed in one of the blocked thread states\n(BLOCKED, WAITING, or TIMED_WAITING). (â€¦) blocked thread must wait for an event beyond its control before it can proceedâ€.\nThis sounds exactly like the situation we are in. So there is hope. Letâ€™s educate ourselves further.\nAnother excerpt that would prove insightful reads as follows:\nâ€œ(â€¦) tasks can block for exteded periods of time, even if deadlock is not a possibility.\n(â€¦) One technique that can mitigate the ill effects of longâ€“running tasks is for tasks to use timed resource waits instead of\nunbound waits.â€\nThis seems like an answer to our woes. However, two mysteries remain.\nWhere to put the timeout? What the thread is waiting for? To answer these questions we need to inspect the threads in the JVM itself.\nThe investigation\nAt this point we did two things. First we pushed our code to a branch.\nAfter all at any moment our laptops could burst into flames and all the work would go to waste.\nThe remote CI confirmed our suspicion since it also hung. The problem was real and not only confined to the local machine.\nThe second thing is to scout for the offending thread. This is easy with the help of some JDK binaries:\n\njps -lv | grep platform-libraries\n38983 worker.org.gradle.process.internal.worker.GradleWorkerMain -Dorg.gradle.internal.worker.tmpdir=/path/to/code/platform-libraries/platform-libraries-webclient/build/tmp/integrationTest/work -Dorg.gradle.native=false -Xmx512m -Dfile.encoding=UTF-8\n\n\nSo we have the a lvmid â€“ local JVM identifier, which will help us locate the offending thread in jconsole.\nIn the screen below we can see that the thread waits on Mono.block() which is left unbounded in a happy path scenario.\nWell, we are in the worst case so first of all we add a simple timeout Mono.block(Duration.ofSeconds(10)).\n\nThis fails our tests and for the first time the error appears:\n\n\t08:13:39.556 [Test worker] WARN reactor.core.Exceptions - throwIfFatal detected a jvm fatal exception, which is thrown and logged below:\njava.lang.NoSuchMethodError: 'reactor.core.publisher.Mono reactor.core.publisher.Mono.subscriberContext(reactor.util.context.Context)'\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\tat reactor.netty.internal.shaded.reactor.pool.AbstractPool$Borrower.request(AbstractPool.java:427)\n\tat reactor.netty.resources.PooledConnectionProvider$DisposableAcquire.onSubscribe(PooledConnectionProvider.java:533)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool$QueueBorrowerMono.subscribe(SimpleDequePool.java:676)\n\tat reactor.netty.resources.PooledConnectionProvider.disposableAcquire(PooledConnectionProvider.java:219)\n\tat reactor.netty.resources.PooledConnectionProvider.lambda$acquire$3(PooledConnectionProvider.java:183)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:326)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.core.publisher.FluxRetryWhen.subscribe(FluxRetryWhen.java:77)\n\tat reactor.core.publisher.MonoRetryWhen.subscribeOrReturn(MonoRetryWhen.java:46)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:329)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)\n\tat reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2545)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.request(MonoFlatMap.java:194)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.request(FluxOnAssembly.java:649)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2341)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:2215)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onSubscribe(FluxOnAssembly.java:633)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onSubscribe(MonoFlatMap.java:117)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)\n\tat reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoDeferContextual.subscribe(MonoDeferContextual.java:55)\n\tat reactor.core.publisher.Mono.subscribe(Mono.java:4485)\n\tat reactor.core.publisher.Mono.block(Mono.java:1733)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$Trait$Helper.makeRequest(WebClientContextContainerAdapterConfiguration.groovy:22)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$makeRequest.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)\n\tat pl.allegro....AdapterConfiguration$Trait$Helper.makeRequest(AdapterConfiguration.groovy:11)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....SharedInterceptorSpec$makeRequest.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:203)\n\tat pl.allegro....SharedInterceptorSpec.$spock_feature_0_0(SharedInterceptorSpec.groovy:44)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.spockframework.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:196)\n\tat org.spockframework.runtime.model.MethodInfo.lambda$new$0(MethodInfo.java:49)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeatureMethod(PlatformSpecRunner.java:324)\n\tat org.spockframework.runtime.IterationNode.execute(IterationNode.java:50)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:58)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.IterationNode.lambda$around$0(IterationNode.java:67)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunIteration$5(PlatformSpecRunner.java:236)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.junit4.AbstractRuleInterceptor$1.evaluate(AbstractRuleInterceptor.java:46)\n\tat com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:79)\n\tat org.spockframework.junit4.MethodRuleInterceptor.intercept(MethodRuleInterceptor.java:40)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runIteration(PlatformSpecRunner.java:218)\n\tat org.spockframework.runtime.IterationNode.around(IterationNode.java:67)\n\tat org.spockframework.runtime.SimpleFeatureNode.lambda$around$0(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)\n\tat org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:63)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:11)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\n\nFor the first time we force the entire reactive code to finally execute itself and present us with the result,\neven if it is an error this moves us in the right direction.\nResult\nLike in any good crime story uncovering one mystery presents another.\nA quick grep shows that there are no calls to reactor.core.publisher.Mono.subscriberContext.\nWhere could this call be hiding, if itâ€™s not present in our code?\nThe answer is simple but I assure you that it took us some time to come up with it.\nIf it isnâ€™t in our code and it runs inside our JVM then this must be dependency code.\nThe observant reader is able to spot it from afar. The stack trace confirms where the error lies:\n\n    at reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\n\nWe need to patch reactorâ€“netty which in this version still used deprecated code. Referring back to our diff:\n\next.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n-        reactorNetty   : '0.9.25.RELEASE',\n]\n\next.versions = [\n+        spring        : '6.0.5',\n+        spock         : '2.4-M1-groovy-4.0',\n+        groovy        : '4.0.9',\n+        reactorNetty  : '1.1.3',\n]\n\n\nWe escape the dependency hell and are delighted to see the green letters BUILD SUCCESSFUL in 24s.\nSummary\nWell this was quite a thrilling journey one doesnâ€™t often embark on.\nThe odd peculiarity of the problem combined with peculiarity of the task provided us with a great challange and satisfaction.\nDependency hell is no joke, but armed with the JDK tools and thinking the problem through, there is no obstacle that could not be overcome.\nNext time your code hangs with no apparent reason this is a perfect opportunity to dust off the swiss army knife of JDK binaries and dig in.","guid":"https://blog.allegro.tech/2023/05/debugging-hangs.html","categories":["tech","java","jvm","debugging","dependency hell"],"isoDate":"2023-05-30T22:00:00.000Z","thumbnail":"images/post-headers/java.png"},{"title":"Trust no one, not even your training data! Machine learning from noisy data","link":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","pubDate":"Tue, 18 Apr 2023 00:00:00 +0200","authors":{"author":[{"name":["Åukasz RÄ…czkowski"],"photo":["https://blog.allegro.tech/img/authors/lukasz.raczkowski.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.raczkowski"]},{"name":["Aleksandra Osowska-Kurczab"],"photo":["https://blog.allegro.tech/img/authors/aleksandra.osowska-kurczab.jpg"],"url":["https://blog.allegro.tech/authors/aleksandra.osowska-kurczab"]},{"name":["Jacek SzczerbiÅ„ski"],"photo":["https://blog.allegro.tech/img/authors/jacek.szczerbinski.jpg"],"url":["https://blog.allegro.tech/authors/jacek.szczerbinski"]},{"name":["Klaudia Nazarko"],"photo":["https://blog.allegro.tech/img/authors/klaudia.nazarko.jpg"],"url":["https://blog.allegro.tech/authors/klaudia.nazarko"]},{"name":["Kalina Kobus"],"photo":["https://blog.allegro.tech/img/authors/kalina.kobus.jpg"],"url":["https://blog.allegro.tech/authors/kalina.kobus"]}]},"content":"\u003cul\u003e\n  \u003cli\u003eLabel noise is ever-present in machine learning practice.\u003c/li\u003e\n  \u003cli\u003eAllegro datasets are no exception.\u003c/li\u003e\n  \u003cli\u003eWe compared 7 methods for training classifiers robust to label noise.\u003c/li\u003e\n  \u003cli\u003eAll of them improved the modelâ€™s performance on noisy datasets.\u003c/li\u003e\n  \u003cli\u003eSome of the methods decreased the modelâ€™s performance in the absence of label noise.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"what-is-label-noise-and-why-does-it-matter\"\u003eWhat is label noise and why does it matter?\u003c/h2\u003e\n\n\u003cp\u003eIn the scope of supervised machine learning, specifically in classification tasks, the problem of label noise\nis of critical importance. It involves cases of incorrectly labelled training data. For example, letâ€™s say that\nwe want to train a classification model to distinguish cats from dogs. For that purpose, we compose a training\ndataset with images labelled as either cat or dog. The labelling process is usually performed by human annotators,\nwho almost certainly produce some labelling errors. Unfortunately, human annotators can be confused by poor image\nquality, ambiguous image contents, or simply click the wrong item. As such, we inevitably end up with a dataset\nwhere some percentage of cats are labelled as dogs and vice versa (\u003ca href=\"#figure1\"\u003e\u003cstrong\u003eFigure 1\u003c/strong\u003e\u003c/a\u003e).\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure1\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Cats and dogs are equally nice.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure1-label-noise-example.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003eFigure 1. An example of label noise in a binary classification dataset.\u003c/b\u003e Some images in both categories were mislabelled by human annotators, which introduces noise to the training dataset. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eConsequently, the model trained with such data learns partially wrong associations, which then can lead to incorrect\npredictions for new images. The more label noise we have, the more we confuse the model during training. We can\nmeasure this by evaluating the classification error on a held-out test dataset (\u003ca href=\"#figure2\"\u003e\u003cstrong\u003eFigure 2\u003c/strong\u003e\u003c/a\u003e). It is clear\nthat for high noise levels, it is very hard to recover the true training signal from the corrupted training data.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure2\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Oh no, please, not the noise!\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure2-test-accuracy.png\" style=\"width:70%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:70%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 2. Test accuracy as a function of label noise percentage. \u003c/b\u003e The X axis indicates the ratio of mislabelled to correctly labelled examples. The dataset used here was ImageNet, corrupted with synthetic label noise. Image source: \u003csup id=\"fnref:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\"\u003e1\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eHow can this problem be mitigated? One approach is to simply put more effort into the labelling process â€” we can let\nmultiple annotators label each data point and then evaluate the cross-annotator agreement. With enough time and effort,\nwe hope to obtain a dataset free of label noise. However, in practice this approach is rarely feasible due to large\nvolumes of training data and the need for efficient turnaround of machine learning projects. Consequently, we need\na different approach for handling corrupted training data, \u003cem\u003ei.e.\u003c/em\u003e ML models robust to label noise.\u003c/p\u003e\n\n\u003cp\u003eIn the context of this blog post, we define robustness as the modelâ€™s ability to efficiently learn in the presence\nof corrupted training data. In other words, a robust model can recover the correct training signal and ignore\nthe noise, so that it does not overfit to the corrupted traning set and can generalise during prediction. A major\nchallenge in this regard is the difficulty to estimate the proportion of label noise in real-world data. As such,\nrobust models are expected to handle varying amounts of label noise.\u003c/p\u003e\n\n\u003ch2 id=\"how-to-train-a-robust-classifier\"\u003eHow to train a robust classifier?\u003c/h2\u003e\n\n\u003cp\u003eWe can improve the robustness of deep neural networks (DNNs) with a few tips and tricks presented in the recent\nliterature on \u003cem\u003eLearning from Noisy Data\u003c/em\u003e. In general, there are three approaches to boosting the modelâ€™s resistance\nto noisy labels (\u003ca href=\"#figure3\"\u003e\u003cstrong\u003eFigure 3\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eRobust loss function\u003c/strong\u003e boosting the training dynamics in the presence of noise.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eImplicit regularisation\u003c/strong\u003e of the network aiming at decreasing the impact of noisy labels.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eFiltration of noisy data samples\u003c/strong\u003e during the training or at the pre-training stage.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure3\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Flat-topped pyramids are better than sharp-topped ones.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure3-robustness-strategies.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 3. Strategies for robustness. \u003c/b\u003e In this blog post, we focused on two main approaches improving model robustness: utilisation of a robust loss function and implicit regularisation.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eIn the scope of this blog post, we present seven different methods that are strong baselines for improving\nthe generalisation of classifiers in the presence of label noise.\u003c/p\u003e\n\n\u003ch3 id=\"robust-loss-function\"\u003eRobust loss function\u003c/h3\u003e\n\n\u003ch4 id=\"self-paced-learning-spl\"\u003eSelf-Paced Learning (SPL)\u003c/h4\u003e\n\u003cp\u003eThe authors of \u003cstrong\u003eSelf-Paced Learning\u003c/strong\u003e\u003csup id=\"fnref:2\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:2\" class=\"footnote\" rel=\"footnote\"\u003e2\u003c/a\u003e\u003c/sup\u003e noticed that large per-sample loss might be an indication of label\ncorruption, especially in the latter stages of training. Clean labels should be easy to learn, while corrupted labels\nwould appear as difficult, resulting in a high per-sample loss.\u003c/p\u003e\n\n\u003cp\u003eSPL proposes to exclude some predefined ratio of examples from the batch depending on their per-sample loss values\n(\u003ca href=\"#figure4\"\u003e\u003cstrong\u003eFigure 4a\u003c/strong\u003e\u003c/a\u003e). Usually, the ratio is set as the estimated noise level in the dataset.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure4\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"PRL makes everything equal.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure4-loss-filtration.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 4. Comparison of loss filtration methods (SPL, PRL and CCE, see below). \u003c/b\u003e While SPL and PRL exclude samples from loss calculation, CCE decreases the impact of potentially corrupted labels by clipping the per-sample loss values. Orange colour indicates candidate noisy samples. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch4 id=\"provably-robust-learning-prl\"\u003eProvably Robust Learning (PRL)\u003c/h4\u003e\n\n\u003cp\u003e\u003cstrong\u003eProvably Robust Learning\u003c/strong\u003e\u003csup id=\"fnref:3\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:3\" class=\"footnote\" rel=\"footnote\"\u003e3\u003c/a\u003e\u003c/sup\u003e derives from the ideas presented in the SPL paper, but the authors state that\ncorrupted labels should be detected depending on the gradient norm, instead of per-sample loss (\u003ca href=\"#figure4\"\u003e\u003cstrong\u003eFigure 4b\u003c/strong\u003e\u003c/a\u003e).\nThe underlying intuition is that corrupted samples provoke the optimiser to make inadequately large steps\nin the optimisation space. The rest of the logic is the same as in SPL.\u003c/p\u003e\n\n\u003ch4 id=\"clipped-cross-entropy-cce\"\u003eClipped Cross-Entropy (CCE)\u003c/h4\u003e\n\n\u003cp\u003eRejection of samples might not be optimal from the trainingâ€™s point of view, because DNNs need vast amounts of data\nto be able to generalise properly. Therefore, \u003cstrong\u003eClipped Cross-Entropy\u003c/strong\u003e doesnâ€™t exclude the most contributing samples\nfrom the batch, but rather alleviates their impact by clipping the per-sample loss to a predefined value (\u003ca href=\"#figure4\"\u003e\u003cstrong\u003eFigure 4c\u003c/strong\u003e\u003c/a\u003e).\u003c/p\u003e\n\n\u003ch4 id=\"early-learning-regularisation-elr\"\u003eEarly Learning Regularisation (ELR)\u003c/h4\u003e\n\n\u003cp\u003eIt has been recently observed that DNNs first fit clean samples, and then start memorising the noisy ones. This\nphenomenon reduces the generalisation properties of the model, distracting it from learning true patterns present\nin the data. \u003cstrong\u003eEarly Learning Regularisation\u003c/strong\u003e\u003csup id=\"fnref:4\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:4\" class=\"footnote\" rel=\"footnote\"\u003e4\u003c/a\u003e\u003c/sup\u003e mitigates memorisation with two tricks:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eTemporal ensembling\u003c/em\u003e of targets: during the training step \\([k]\\), the original targets \\(\\pmb{\\text{t}}\\) are mixed\nwith the modelâ€™s predictions \\(\\pmb{\\text{p}}\\) from previous training steps. This prevents the gradient from diverging\nhugely between subsequent steps. This trick is well-known in semi-supervised learning\u003csup id=\"fnref:5\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:5\" class=\"footnote\" rel=\"footnote\"\u003e5\u003c/a\u003e\u003c/sup\u003e:\u003c/li\u003e\n\u003c/ul\u003e\n\n\\[\\pmb{\\text{t}}^{[k]} = \\left(\\beta\\ \\pmb{\\text{t}}^{[k-1]} + (1-\\beta)\\ \\pmb{\\text{p}}^{[k-1]}\\right)\\]\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eExplicit regularisation\u003c/em\u003e: an extra term is added to the default cross-entropy loss \\(\\mathcal{L}_{CE}(\\Theta)\\) that\nallows refinement of the early-learnt concepts, but penalises drastically contradicting predictions.\u003c/li\u003e\n\u003c/ul\u003e\n\n\\[\\mathcal{L}_{ELR}(\\Theta)=\\mathcal{L}_{CE}(\\Theta) + \\frac{\\lambda}{n} \\sum\\text{log}(1-\\langle \\pmb{\\text{p}}, \\pmb{\\text{t}} \\rangle)\\]\n\n\u003cp\u003eThus, the gradient gets a boost for the clean samples, while the impact of noisy samples is neutralised\nby temporal ensembling.\u003c/p\u003e\n\n\u003ch4 id=\"jensen-shannon-divergence-loss-jsd\"\u003eJensen-Shannon Divergence Loss (JSD)\u003c/h4\u003e\n\n\u003cp\u003eThe authors of \u003cstrong\u003eJensen-Shannon Divergence Loss\u003c/strong\u003e \u003csup id=\"fnref:6\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:6\" class=\"footnote\" rel=\"footnote\"\u003e6\u003c/a\u003e\u003c/sup\u003e take yet another approach to loss construction,\nwhich is inspired by an empirical comparison between Cross-Entropy (CE) and Mean Absolute Error (MAE) loss. CE is known\nfor its fast convergence and brilliant training dynamics, while MAE provides spectacular robustness at the price\nof slow convergence.\u003c/p\u003e\n\n\u003cp\u003eEnglesson et al. came up with the idea to use Jensen-Shannon Divergence, which is a proven generalisation of CE\nand MAE loss (\u003ca href=\"#figure5\"\u003e\u003cstrong\u003eFigure 5\u003c/strong\u003e\u003c/a\u003e). JSD uses Kullback-Leibler Divergence \\(\\text{D}_{\\text{KL}}\\) between the target\nlabels \\(\\pmb{y}\\) and predictions of the model \\(f(\\pmb{x})\\) vs. their averaged distribution \\(\\pmb{m}\\). Summing up, one\ncan think of JSD as a CE with a robustness boost, or MAE with improved convergence.\u003c/p\u003e\n\n\\[\\mathcal{L}_{\\text{JS}}(\\pmb{x}, \\pmb{y}) = \\frac{1}{Z} \\left( \\pi_1 \\text{D}_{\\text{KL}}(\\pmb{y}||\\pmb{m}) + (1-\\pi_1) \\text{D}_{\\text{KL}}(f(\\pmb{x})||\\pmb{m}) \\right)\\]\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure5\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Big proportion of pie makes your weight high.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure5-jsd.png\" style=\"width:70%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:70%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 5. JSD as a generalisation of CE and MAE loss. \u003c/b\u003e Depending on the parameter \\(\\pi_1\\), JSD resembles CE or MAE. Image source: \u003csup id=\"fnref:6:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:6\" class=\"footnote\" rel=\"footnote\"\u003e6\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"implicit-regularisation\"\u003eImplicit regularisation\u003c/h3\u003e\n\n\u003ch4 id=\"co-teaching-ct\"\u003eCo-teaching (CT)\u003c/h4\u003e\n\n\u003cp\u003eIn \u003cstrong\u003eco-teaching\u003c/strong\u003e \u003csup id=\"fnref:7\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:7\" class=\"footnote\" rel=\"footnote\"\u003e7\u003c/a\u003e\u003c/sup\u003e, we simultaneously train two independent DNNs (\u003ca href=\"#figure6\"\u003e\u003cstrong\u003eFigure 6\u003c/strong\u003e\u003c/a\u003e), and let them\nexchange examples during the training. The \u003cem\u003etraining feed\u003c/em\u003e (learning samples) provided by the peer network should\nideally consist only of clean samples. In CT, each network predicts which samples are clean and provides them to its\ncounterpart. Deciding whether a sample is clean relies on the trick known from SPL: the sampleâ€™s label is probably\nclean if its per-sample loss is low.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure6\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Co-operation is key to success, especially when you want to reduce noise in your garage band.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure6-co-teaching.png\" style=\"width:50%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:50%;margin-left:auto;margin-right:auto\"\u003e\u003cb\u003e Figure 6. Exchange of training feed in co-teaching. \u003c/b\u003e Two peer networks exchange samples that are expected\nto be clean from noise. Image source: \u003csup id=\"fnref:7:1\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:7\" class=\"footnote\" rel=\"footnote\"\u003e7\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eCo-teaching is one of the most popular and universal baselines in the domain of learning from noisy data. It has\nwell-established empirical results, offers good performance even in extreme noise scenarios and can be simply\nintegrated into almost any architecture or downstream task. Unfortunately, it also has a few downsides. Firstly, there\nis no theoretical guarantee that such a training setup will eventually converge. Secondly, we may end up with\na consensus between the two networks, causing them to produce identical training feeds, and making the CT redundant.\u003c/p\u003e\n\n\u003ch4 id=\"mixup\"\u003eMixup\u003c/h4\u003e\n\n\u003cp\u003e\u003cstrong\u003eMixup\u003c/strong\u003e\u003csup id=\"fnref:8\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:8\" class=\"footnote\" rel=\"footnote\"\u003e8\u003c/a\u003e\u003c/sup\u003e is a simple augmentation scheme that enforces linear behaviour of the model for in-between\ntraining samples (\u003ca href=\"#figure7\"\u003e\u003cstrong\u003eFigure 7\u003c/strong\u003e\u003c/a\u003e). It linearly combines two training samples \\((\\pmb{x}_i, \\pmb{y}_i)\\)\nand \\((\\pmb{x}_j, \\pmb{y}_j)\\) with weight \\(\\lambda\\) sampled from the \u003cem\u003eBeta\u003c/em\u003e distribution. It results in a new augmented sample with mixed input features \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\):\u003c/p\u003e\n\n\\[\\pmb{x}_{aug} = \\lambda \\pmb{x}_i + (1 - \\lambda)\\pmb{x}_j \\\\\n\\pmb{y}_{aug} = \\lambda \\pmb{y}_i + (1 - \\lambda)\\pmb{y}_j \\\\\\]\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure7\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"When you canâ€™t decide between cats and dogs, why donâ€™t have both?\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure7-mixup.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 7. Augmentation through mixup. \u003c/b\u003e Two samples \\(i\\) and \\(j\\) are linearly combined into a synthetic image \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\). This new augmented input encourages the model to linearly interpolate the predictions between the original samples. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe method is a simple, universal, yet very effective approach. It yields good empirical results while adding\nno severe computational overhead.\u003c/p\u003e\n\n\u003ch2 id=\"cleaning-up-allegro\"\u003eCleaning up Allegro\u003c/h2\u003e\n\n\u003cp\u003eEvery offer has its right place at \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e, belonging to one out of over 23,000 categories. The category structure\nis a tree consisting of:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ethe root (Allegro),\u003c/li\u003e\n  \u003cli\u003eup to 7 levels of intermediate nodes (departments, metacategories, \u003cem\u003eetc.\u003c/em\u003e) â€” over 2,600 nodes in total,\u003c/li\u003e\n  \u003cli\u003eover 23,000 leaves.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOffers located in wrong categories are hard to find and hard to buy. As such, we need a way to properly assign offers\nto correct category leaves. To this end, our Machine Learning Research team has developed a category classifier\nfor Allegro offers.\u003c/p\u003e\n\n\u003cp\u003eThe model in question is a large language model pre-trained on the Allegro catalogue (see more\nin \u003ca href=\"https://www.youtube.com/watch?v=6T-R4kgIbBs\u0026amp;list=PLzveSKBX_3N7yPb4ErB5HJ83eB6XvH37C\u0026amp;index=20\"\u003e\u003ci\u003eDo you speak Allegro?\u003c/i\u003e\u003c/a\u003e) and fine-tuned for offer classification. Specifically, the downstream task here is extreme text classification: each offer is represented by text (title) and is classified into over 23,000 categories â€” hence the word \u003ci\u003eextreme\u003c/i\u003e.\u003c/p\u003e\n\n\u003cp\u003eClassification is particularly challenging for offers listed in ambiguous categories such as \u003cem\u003eOther, Accessories, etc.\u003c/em\u003e\nThese categories are broad and hard to navigate, as they contain a wide variety of products. Most of those products\nactually belong to some well-defined categories, but the merchant couldnâ€™t find the right place for those offers\nat the time of their listing, because of the very rich taxonomy of the category tree. Consequently, we decided\nto clean up the offers in ambiguous categories.\u003c/p\u003e\n\n\u003cp\u003eHereâ€™s the setup (\u003ca href=\"#figure8\"\u003e\u003cstrong\u003eFigure 8\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eWe train the category classifier on offers in well-defined categories: the model learns what lies where at Allegro.\u003c/li\u003e\n  \u003cli\u003eNext, we run inference on offers in ambiguous categories: the model moves the offers to their right destination.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eNote that this task is subject to domain shift: the assortment listed in these ambiguous categories may be harder\nto categorise than the regular assortment in other categories.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure8\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Always trust your friendly neighbourhood language model.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure8-category-classifier.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 8. Category classifier: training \u0026amp; inference. \u003c/b\u003e The model is trained on offers listed in well-defined categories. Then, it is used to move offers from ambiguous categories (\u003ci\u003eOther, Accessories, etc.\u003c/i\u003e) to the well-defined categories. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"real-world-label-noise-at-allegro\"\u003eReal-world label noise at Allegro\u003c/h3\u003e\n\u003cp\u003eThe training set (offers in well-defined categories) is not 100% correct, for several reasons (\u003ca href=\"#figure9\"\u003e\u003cstrong\u003eFigure 9\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ethe merchant may have put the offer in the wrong category,\u003c/li\u003e\n  \u003cli\u003ethere are several similar categories in the catalogue,\u003c/li\u003e\n  \u003cli\u003ethere is no appropriate category for a given offer,\u003c/li\u003e\n  \u003cli\u003ethe taxonomy of the Allegro category tree changes over time.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure9\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"AHHH, FRESH MEAT.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure9-mislabelled-offers.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 9. Examples of mislabelled offers. \u003c/b\u003e With over 23,000 categories at Allegro, listing each offer in its best-matching category can be challenging for merchants. Hence, label noise is an inherent feature of our training dataset. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThe ML model is prone to memorisation of the wrong labels in the training set, \u003cem\u003ei.e.\u003c/em\u003e overfitting. These errors will\nlikely be reproduced at prediction time. Our goal is to train a robust classifier that will learn the true patterns\nand ignore the mislabelled training instances.\u003c/p\u003e\n\n\u003cp\u003eThe training methods described in the previous section were developed and evaluated on computer vision tasks,\n\u003cem\u003ee.g.\u003c/em\u003e image classification, into a relatively small number of categories. Here, we face the problem of extreme text\nclassification. Thus, we need to adapt those methods for textual input and find out which concepts transfer well between\nthe two domains.\u003c/p\u003e\n\n\u003ch3 id=\"synthetic-label-noise\"\u003eSynthetic label noise\u003c/h3\u003e\n\n\u003cp\u003eTo evaluate the modelâ€™s robustness experimentally, we need to know \u003cem\u003ea priori\u003c/em\u003e which training instances were\nmislabelled. For that, we use a generator of controllable noise. The experimental setup consists of five steps\n(\u003ca href=\"#figure10\"\u003e\u003cstrong\u003eFigure 10\u003c/strong\u003e\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003edumping a clean dataset from a curated pool of offers that are \u003cem\u003ecertainly\u003c/em\u003e in the right place,\u003c/li\u003e\n  \u003cli\u003esplitting it into training, validation and test sets,\u003c/li\u003e\n  \u003cli\u003eapplication of synthetic noise to 20% of instances in the training and validation sets (changing the offerâ€™s category\nto a wrong one),\u003c/li\u003e\n  \u003cli\u003etraining the model on the noisy dataset,\u003c/li\u003e\n  \u003cli\u003etesting the model on a held-out fraction of the clean dataset.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure10\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Staying clean has many benefits. Stay clean kids!\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure10-datasets.png\" style=\"width:70%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:70%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003e Figure 10. Testing the modelâ€™s robustness. \u003c/b\u003e The full dataset of clean instances (offers with true category labels) is split into training, validation and test sets. Next, label noise is introduced to the training and validation sets and the model is trained. The model is tested on a held-out fraction of the clean dataset. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003cp\u003eThis setup lets us answer the following question:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eHow much does the noise in the training set hurt the modelâ€™s performance on the clean test set?\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis way, we can evaluate different methods of training classifiers under label noise and choose the most robust\nclassifier, according to accuracy on the test set.\u003c/p\u003e\n\n\u003ch2 id=\"and-it-works\"\u003eAndâ€¦ it works!\u003c/h2\u003e\n\n\u003cp\u003eBelow we present the results of experiments for 1.3M offers listed in the \u003cem\u003eConstruction Work \u0026amp; Equipment\u003c/em\u003e category.\nSymmetric noise was applied to 20% of the training set. This means that the category labels of that percentage\nof offers were changed to different randomly chosen labels. We evaluated the 7 training methods outlined above\nand compared them to the baseline: classification with cross-entropy loss.\u003c/p\u003e\n\n\u003ch3 id=\"baseline-memorising-doesnt-pay-off\"\u003eBaseline: Memorising doesnâ€™t pay off\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eHow does the presence of noise impact the baseline model?\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eThe validation curves for non-corrupted samples clearly show the severe impact of noisy labels on the modelâ€™s\nperformance (\u003ca href=\"#figure11\"\u003e\u003cstrong\u003eFigure 11\u003c/strong\u003e\u003c/a\u003e). In the early stage of training, the performance of the model trained\non noisy data is on par with the metrics of the model trained on clean data. Yet, starting from the 4th epoch,\nthe wrong labels in the noisy dataset appear to prevent the model from discovering the true patterns in the training\ndata, resulting in a 5 p.p. drop in accuracy at the end of the training. We attribute this drop to the \u003cem\u003ememorisation\u003c/em\u003e\nof the wrong labels: instead of refining the originally learnt concepts, the network starts to overfit to the noisy\nlabels. The labels memorised for particular offers donâ€™t help with classifying previously unseen offers at test time.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure11\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Absolute noise corrupts absolutely.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure11-baseline-degradation.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003eFigure 11. Degradation of the baseline model in the presence of noise.\u003c/b\u003e The 20% synthetic noise degrades the model throughout the training. In the end, the model trained on the corrupted dataset exhibits 5 p.p. lower accuracy in comparison to its clean counterpart \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch3 id=\"towards-robust-classification\"\u003eTowards robust classification\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eDoes robustness imply underfitting?\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eTo verify if the evaluated methods have any effect on the modelâ€™s performance when there is no noise in the training\ndata, we tested all of them on a clean dataset without any synthetic noise.\u003c/p\u003e\n\n\u003cp\u003eIn the absence of corrupted data, three of the tested methods (SPL, PRL and CT) are effectively reduced to the baseline\nCross-Entropy. Therefore, the accuracy for those methods was exactly the same as for the baseline (\u003ca href=\"#table1\"\u003e\u003cstrong\u003eTable 1\u003c/strong\u003e\u003c/a\u003e).\nFor mixup, the difference from the baseline was within the standard deviation range, so it was marked as no improvement\nas well.\u003c/p\u003e\n\n\u003cp\u003eFor CCE and JSD the performance degraded, but only slightly â€” by 0.04 p.p. for the former and 0.34 p.p. for the latter.\nThis drop is an acceptable compromise considering the robustness to noise that these methods enable (see below).\u003c/p\u003e\n\n\u003cp\u003eELR was the only method that improved upon the baseline, by 0.07 p.p. As ELR relies on temporal ensembling, which\ndiminishes the impact of corrupted samples during training, we hypothesise that our clean dataset contained a small\nnumber of mislabelled examples. Such paradoxes are a frequent case in machine learning practice, even for renowned\nbenchmark datasets like CIFAR-100\u003csup id=\"fnref:9\" role=\"doc-noteref\"\u003e\u003ca href=\"#fn:9\" class=\"footnote\" rel=\"footnote\"\u003e9\u003c/a\u003e\u003c/sup\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003ca id=\"table1\"\u003e\u003c/a\u003e\u003cstrong\u003eTable 1.\u003c/strong\u003e Test accuracy scores of the models trained on the clean and corrupted\n(20% synthetic noise) datasets for the 8 training methods. Light red highlight indicates deterioration in comparison\nto the baseline, while light blue denotes improvement. \u003cem\u003eNotation\u003c/em\u003e: (mean \\(\\pm\\) std)% from 5 independently seeded runs.\u003c/p\u003e\n\u003ctable\u003e\n    \u003cthead\u003e\n        \u003ctr\u003e\n            \u003cth rowspan=\"2\" colspan=\"2\" style=\"text-align:center\"\u003eMethod\u003c/th\u003e\n            \u003cth style=\"text-align:center\" colspan=\"2\"\u003eTest accuracy [%]\u003c/th\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003cth style=\"text-align:center\"\u003eclean dataset\u003c/th\u003e\n            \u003cth style=\"text-align:center\"\u003enoisy dataset\u003c/th\u003e\n        \u003c/tr\u003e\n    \u003c/thead\u003e\n    \u003ctbody\u003e\n        \u003ctr\u003e\n            \u003ctd colspan=\"2\" style=\"border-bottom-width: thick;text-align:center;font-weight:bold\"\u003eBaseline\u003c/td\u003e\n            \u003ctd style=\"border-bottom-width: thick;\"\u003e90.26 Â± 0.03\u003c/td\u003e\n            \u003ctd style=\"border-bottom-width: thick;\"\u003e85.31 Â± 0.08\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd rowspan=\"5\" style=\"font-weight:bold;transform: rotate(180deg);writing-mode: vertical-rl;text-align: center;vertical-align: middle;width: 3em\"\u003e\n                Robust loss\u003cbr /\u003efunction\n            \u003c/td\u003e\n            \u003ctd\u003eSelf-Paced Learning (SPL)\u003c/td\u003e\n            \u003ctd\u003e90.26 Â± 0.03\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e88.51 Â± 0.02\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eProvably Robust Learning (PRL)\u003c/td\u003e\n            \u003ctd\u003e90.26 Â± 0.03\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e88.31 Â± 0.02\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eClipped Cross-Entropy (CCE)\u003c/td\u003e\n            \u003ctd style=\"background:#ffdecb;color:black\"\u003e90.22 Â± 0.03\u003c/td\u003e\n            \u003ctd style=\"font-weight:bold;background:#E1F4F4;color:black\"\u003e89.51 Â± 0.01\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eEarly Learning Regularisation (ELR)\u003c/td\u003e\n            \u003ctd style=\"font-weight:bold;background:#E1F4F4;color:black\"\u003e90.33 Â± 0.01\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e89.29 Â± 0.03\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eJensen-Shannon Divergence (JSD) \u003c/td\u003e\n            \u003ctd style=\"background:#ffdecb;color:black\"\u003e89.92 Â± 0.02\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e89.24 Â± 0.01\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd rowspan=\"2\" style=\"font-weight:bold;transform: rotate(180deg);writing-mode: vertical-rl;white-space: wrap;text-align: center;vertical-align: middle;width: 3em;height: 8em\"\u003e\n                Implicit regularisation\n            \u003c/td\u003e\n            \u003ctd\u003eCo-teaching (CT)\u003c/td\u003e\n            \u003ctd\u003e90.26 Â± 0.03\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e88.72 Â± 0.03\u003c/td\u003e\n        \u003c/tr\u003e\n        \u003ctr\u003e\n            \u003ctd\u003eMixup\u003c/td\u003e\n            \u003ctd\u003e90.27 Â± 0.02\u003c/td\u003e\n            \u003ctd style=\"background:#E1F4F4;color:black\"\u003e86.02 Â± 0.06\u003c/td\u003e\n        \u003c/tr\u003e\n    \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e\u003cstrong\u003eRobust classification results\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eAll methods discussed in this study improved the modelâ€™s performance on the noisy dataset when compared to the baseline\n(\u003ca href=\"#table1\"\u003e\u003cstrong\u003eTable 1\u003c/strong\u003e\u003c/a\u003e). The best results were obtained with CCE (+4.2 p.p.), ELR (+3.98 p.p.) and JSD (+3.93 p.p.).\nCT, SPL, PRL performed a bit worse, but still proved to be quite robust, improving upon the baseline by 3.41 p.p.,\n3.2 p.p. and 3.0 p.p., respectively.\u003c/p\u003e\n\n\u003cp\u003eMixup is a clear outlier â€” while it does improve upon the baseline by 0.71 p.p., this increase is noticeably smaller\nthan for the other evaluated methods. Our interpretation is that the linear augmentation at the heart of this method\nregularises the DNN, but does not address label noise \u003cem\u003eper se\u003c/em\u003e. Mixup treats all samples equally, even if their labels\nare corrupted. The marginal improvement upon the baseline is evident in the validation accuracy training curve\n(\u003ca href=\"#figure12\"\u003e\u003cstrong\u003eFigure 12\u003c/strong\u003e\u003c/a\u003e). Mixup starts to overfit around the 5th epoch, similarly to the baseline, and unlike all\nthe other methods.\u003c/p\u003e\n\n\u003cfigure style=\"display:block;float:none;margin-left:auto;margin-right:auto\"\u003e\n    \u003ca id=\"figure12\"\u003e\u003c/a\u003e\n    \u003cimg alt=\"Mixing it up doesnâ€™t always work as intended.\" src=\"/img/articles/2023-04-18-learning-from-noisy-data/figure12-validation-accuracy.png\" style=\"width:80%;margin-bottom:10px\" /\u003e\n    \u003cp style=\"width:80%;margin-left:auto;margin-right:auto\"\u003e \u003cb\u003eFigure 12. Validation accuracy during training.\u003c/b\u003e Validation accuracy for all methods was measured during training. It is evident that the best methods are CCE, ELR and JSD, with CT, PRL and SPL trailing slightly behind. Mixup behaves similarly to the baseline. \u003c/p\u003e\n\u003c/figure\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eThe problem of label noise is unavoidable in machine learning practice, and Allegro datasets are no exception.\nFortunately, there exist numerous methods that diminish the impact of label noise on prediction performance\nby increasing the robustness of machine learning models. In our experiments we implemented 7 of those methods\nand showed that they increase prediction accuracy in the presence of 20% synthetic noise when compared to the baseline\n(Cross-Entropy loss), most of them by a significant margin. The simple Clipped Cross-Entropy proved to be the best,\nwith an accuracy score of 89.51% (increase of 4.2 p.p. vs the baseline trained with noisy labels). This result is very\nclose to the baseline trained with clean labels (90.26%). Thus, we showed that for the case of 20% synthetic label\nnoise, it is possible to increase robustness so that the impact of label noise is negligible.\u003c/p\u003e\n\n\u003cp\u003eThese experiments are only a first step in making classifiers at Allegro robust to label noise. The case of synthetic\nnoise presented here is not very realistic: real-world label noise tends to be instance-dependent,\ni.e. it is influenced by individual sample features. As such, we plan to further evaluate the methods for increasing\nmodel robustness with a real-world dataset perturbed by instance-dependent noise.\u003c/p\u003e\n\n\u003cp\u003eIf youâ€™d like to know more about label noise and model robustness, please refer to the papers listed below.\u003c/p\u003e\n\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n  \u003col\u003e\n    \u003cli id=\"fn:1\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1705.10694\"\u003e\u003cem\u003eDeep Learning is Robust to Massive Label Noise\u003c/em\u003e, Rolnick et al., 2018\u003c/a\u003eÂ \u003ca href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:2\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://papers.nips.cc/paper/2010/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html\"\u003e\u003cem\u003eSelf-Paced Learning for Latent Variable Models\u003c/em\u003e, Kumar et al., 2010\u003c/a\u003eÂ \u003ca href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:3\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2102.06735\"\u003e\u003cem\u003eLearning Deep Neural Networks under Agnostic Corrupted Supervision\u003c/em\u003e, Liu et al., 2021\u003c/a\u003eÂ \u003ca href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:4\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2007.00151\"\u003e\u003cem\u003eEarly-Learning Regularization Prevents Memorization of Noisy Labels\u003c/em\u003e, Liu et al., 2020\u003c/a\u003eÂ \u003ca href=\"#fnref:4\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:5\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1610.02242\"\u003e\u003cem\u003eTemporal Ensembling for Semi-Supervised Learning\u003c/em\u003e, Laine et al., 2017\u003c/a\u003eÂ \u003ca href=\"#fnref:5\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:6\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2105.04522\"\u003e\u003cem\u003eGeneralized Jensen-Shannon Divergence Loss for Learning with Noisy Labels\u003c/em\u003e, Englesson et al., 2021\u003c/a\u003eÂ \u003ca href=\"#fnref:6\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003eÂ \u003ca href=\"#fnref:6:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e2\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:7\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1804.06872\"\u003e\u003cem\u003eCo-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels\u003c/em\u003e, Han et al., 2018\u003c/a\u003eÂ \u003ca href=\"#fnref:7\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003eÂ \u003ca href=\"#fnref:7:1\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003csup\u003e2\u003c/sup\u003e\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:8\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1710.09412\"\u003e\u003cem\u003emixup: Beyond Empirical Risk Minimization\u003c/em\u003e, Zhang et al., 2018\u003c/a\u003eÂ \u003ca href=\"#fnref:8\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:9\" role=\"doc-endnote\"\u003e\n      \u003cp\u003e\u003ca href=\"https://arxiv.org/abs/2103.14749\"\u003e\u003cem\u003ePervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks\u003c/em\u003e, Northcutt et al., 2021\u003c/a\u003eÂ \u003ca href=\"#fnref:9\" class=\"reversefootnote\" role=\"doc-backlink\"\u003e\u0026#8617;\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n  \u003c/ol\u003e\n\u003c/div\u003e\n","contentSnippet":"Label noise is ever-present in machine learning practice.\nAllegro datasets are no exception.\nWe compared 7 methods for training classifiers robust to label noise.\nAll of them improved the modelâ€™s performance on noisy datasets.\nSome of the methods decreased the modelâ€™s performance in the absence of label noise.\nWhat is label noise and why does it matter?\nIn the scope of supervised machine learning, specifically in classification tasks, the problem of label noise\nis of critical importance. It involves cases of incorrectly labelled training data. For example, letâ€™s say that\nwe want to train a classification model to distinguish cats from dogs. For that purpose, we compose a training\ndataset with images labelled as either cat or dog. The labelling process is usually performed by human annotators,\nwho almost certainly produce some labelling errors. Unfortunately, human annotators can be confused by poor image\nquality, ambiguous image contents, or simply click the wrong item. As such, we inevitably end up with a dataset\nwhere some percentage of cats are labelled as dogs and vice versa (Figure 1).\n\n    \n    \n Figure 1. An example of label noise in a binary classification dataset. Some images in both categories were mislabelled by human annotators, which introduces noise to the training dataset. \nConsequently, the model trained with such data learns partially wrong associations, which then can lead to incorrect\npredictions for new images. The more label noise we have, the more we confuse the model during training. We can\nmeasure this by evaluating the classification error on a held-out test dataset (Figure 2). It is clear\nthat for high noise levels, it is very hard to recover the true training signal from the corrupted training data.\n\n    \n    \n Figure 2. Test accuracy as a function of label noise percentage.  The X axis indicates the ratio of mislabelled to correctly labelled examples. The dataset used here was ImageNet, corrupted with synthetic label noise. Image source: 1.\nHow can this problem be mitigated? One approach is to simply put more effort into the labelling process â€” we can let\nmultiple annotators label each data point and then evaluate the cross-annotator agreement. With enough time and effort,\nwe hope to obtain a dataset free of label noise. However, in practice this approach is rarely feasible due to large\nvolumes of training data and the need for efficient turnaround of machine learning projects. Consequently, we need\na different approach for handling corrupted training data, i.e. ML models robust to label noise.\nIn the context of this blog post, we define robustness as the modelâ€™s ability to efficiently learn in the presence\nof corrupted training data. In other words, a robust model can recover the correct training signal and ignore\nthe noise, so that it does not overfit to the corrupted traning set and can generalise during prediction. A major\nchallenge in this regard is the difficulty to estimate the proportion of label noise in real-world data. As such,\nrobust models are expected to handle varying amounts of label noise.\nHow to train a robust classifier?\nWe can improve the robustness of deep neural networks (DNNs) with a few tips and tricks presented in the recent\nliterature on Learning from Noisy Data. In general, there are three approaches to boosting the modelâ€™s resistance\nto noisy labels (Figure 3):\nRobust loss function boosting the training dynamics in the presence of noise.\nImplicit regularisation of the network aiming at decreasing the impact of noisy labels.\nFiltration of noisy data samples during the training or at the pre-training stage.\n\n    \n    \n Figure 3. Strategies for robustness.  In this blog post, we focused on two main approaches improving model robustness: utilisation of a robust loss function and implicit regularisation.\nIn the scope of this blog post, we present seven different methods that are strong baselines for improving\nthe generalisation of classifiers in the presence of label noise.\nRobust loss function\nSelf-Paced Learning (SPL)\nThe authors of Self-Paced Learning2 noticed that large per-sample loss might be an indication of label\ncorruption, especially in the latter stages of training. Clean labels should be easy to learn, while corrupted labels\nwould appear as difficult, resulting in a high per-sample loss.\nSPL proposes to exclude some predefined ratio of examples from the batch depending on their per-sample loss values\n(Figure 4a). Usually, the ratio is set as the estimated noise level in the dataset.\n\n    \n    \n  Figure 4. Comparison of loss filtration methods (SPL, PRL and CCE, see below).  While SPL and PRL exclude samples from loss calculation, CCE decreases the impact of potentially corrupted labels by clipping the per-sample loss values. Orange colour indicates candidate noisy samples. \nProvably Robust Learning (PRL)\nProvably Robust Learning3 derives from the ideas presented in the SPL paper, but the authors state that\ncorrupted labels should be detected depending on the gradient norm, instead of per-sample loss (Figure 4b).\nThe underlying intuition is that corrupted samples provoke the optimiser to make inadequately large steps\nin the optimisation space. The rest of the logic is the same as in SPL.\nClipped Cross-Entropy (CCE)\nRejection of samples might not be optimal from the trainingâ€™s point of view, because DNNs need vast amounts of data\nto be able to generalise properly. Therefore, Clipped Cross-Entropy doesnâ€™t exclude the most contributing samples\nfrom the batch, but rather alleviates their impact by clipping the per-sample loss to a predefined value (Figure 4c).\nEarly Learning Regularisation (ELR)\nIt has been recently observed that DNNs first fit clean samples, and then start memorising the noisy ones. This\nphenomenon reduces the generalisation properties of the model, distracting it from learning true patterns present\nin the data. Early Learning Regularisation4 mitigates memorisation with two tricks:\nTemporal ensembling of targets: during the training step \\([k]\\), the original targets \\(\\pmb{\\text{t}}\\) are mixed\nwith the modelâ€™s predictions \\(\\pmb{\\text{p}}\\) from previous training steps. This prevents the gradient from diverging\nhugely between subsequent steps. This trick is well-known in semi-supervised learning5:\nExplicit regularisation: an extra term is added to the default cross-entropy loss \\(\\mathcal{L}_{CE}(\\Theta)\\) that\nallows refinement of the early-learnt concepts, but penalises drastically contradicting predictions.\nThus, the gradient gets a boost for the clean samples, while the impact of noisy samples is neutralised\nby temporal ensembling.\nJensen-Shannon Divergence Loss (JSD)\nThe authors of Jensen-Shannon Divergence Loss 6 take yet another approach to loss construction,\nwhich is inspired by an empirical comparison between Cross-Entropy (CE) and Mean Absolute Error (MAE) loss. CE is known\nfor its fast convergence and brilliant training dynamics, while MAE provides spectacular robustness at the price\nof slow convergence.\nEnglesson et al. came up with the idea to use Jensen-Shannon Divergence, which is a proven generalisation of CE\nand MAE loss (Figure 5). JSD uses Kullback-Leibler Divergence \\(\\text{D}_{\\text{KL}}\\) between the target\nlabels \\(\\pmb{y}\\) and predictions of the model \\(f(\\pmb{x})\\) vs. their averaged distribution \\(\\pmb{m}\\). Summing up, one\ncan think of JSD as a CE with a robustness boost, or MAE with improved convergence.\n\n    \n    \n Figure 5. JSD as a generalisation of CE and MAE loss.  Depending on the parameter \\(\\pi_1\\), JSD resembles CE or MAE. Image source: 6.\nImplicit regularisation\nCo-teaching (CT)\nIn co-teaching 7, we simultaneously train two independent DNNs (Figure 6), and let them\nexchange examples during the training. The training feed (learning samples) provided by the peer network should\nideally consist only of clean samples. In CT, each network predicts which samples are clean and provides them to its\ncounterpart. Deciding whether a sample is clean relies on the trick known from SPL: the sampleâ€™s label is probably\nclean if its per-sample loss is low.\n\n    \n    \n Figure 6. Exchange of training feed in co-teaching.  Two peer networks exchange samples that are expected\nto be clean from noise. Image source: 7.\nCo-teaching is one of the most popular and universal baselines in the domain of learning from noisy data. It has\nwell-established empirical results, offers good performance even in extreme noise scenarios and can be simply\nintegrated into almost any architecture or downstream task. Unfortunately, it also has a few downsides. Firstly, there\nis no theoretical guarantee that such a training setup will eventually converge. Secondly, we may end up with\na consensus between the two networks, causing them to produce identical training feeds, and making the CT redundant.\nMixup\nMixup8 is a simple augmentation scheme that enforces linear behaviour of the model for in-between\ntraining samples (Figure 7). It linearly combines two training samples \\((\\pmb{x}_i, \\pmb{y}_i)\\)\nand \\((\\pmb{x}_j, \\pmb{y}_j)\\) with weight \\(\\lambda\\) sampled from the Beta distribution. It results in a new augmented sample with mixed input features \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\):\n\n    \n    \n  Figure 7. Augmentation through mixup.  Two samples \\(i\\) and \\(j\\) are linearly combined into a synthetic image \\(\\pmb{x}_{aug}\\) and a soft label \\(\\pmb{y}_{aug}\\). This new augmented input encourages the model to linearly interpolate the predictions between the original samples. \nThe method is a simple, universal, yet very effective approach. It yields good empirical results while adding\nno severe computational overhead.\nCleaning up Allegro\nEvery offer has its right place at Allegro, belonging to one out of over 23,000 categories. The category structure\nis a tree consisting of:\nthe root (Allegro),\nup to 7 levels of intermediate nodes (departments, metacategories, etc.) â€” over 2,600 nodes in total,\nover 23,000 leaves.\nOffers located in wrong categories are hard to find and hard to buy. As such, we need a way to properly assign offers\nto correct category leaves. To this end, our Machine Learning Research team has developed a category classifier\nfor Allegro offers.\nThe model in question is a large language model pre-trained on the Allegro catalogue (see more\nin Do you speak Allegro?) and fine-tuned for offer classification. Specifically, the downstream task here is extreme text classification: each offer is represented by text (title) and is classified into over 23,000 categories â€” hence the word extreme.\nClassification is particularly challenging for offers listed in ambiguous categories such as Other, Accessories, etc.\nThese categories are broad and hard to navigate, as they contain a wide variety of products. Most of those products\nactually belong to some well-defined categories, but the merchant couldnâ€™t find the right place for those offers\nat the time of their listing, because of the very rich taxonomy of the category tree. Consequently, we decided\nto clean up the offers in ambiguous categories.\nHereâ€™s the setup (Figure 8):\nWe train the category classifier on offers in well-defined categories: the model learns what lies where at Allegro.\nNext, we run inference on offers in ambiguous categories: the model moves the offers to their right destination.\nNote that this task is subject to domain shift: the assortment listed in these ambiguous categories may be harder\nto categorise than the regular assortment in other categories.\n\n    \n    \n  Figure 8. Category classifier: training \u0026 inference.  The model is trained on offers listed in well-defined categories. Then, it is used to move offers from ambiguous categories (Other, Accessories, etc.) to the well-defined categories. \nReal-world label noise at Allegro\nThe training set (offers in well-defined categories) is not 100% correct, for several reasons (Figure 9):\nthe merchant may have put the offer in the wrong category,\nthere are several similar categories in the catalogue,\nthere is no appropriate category for a given offer,\nthe taxonomy of the Allegro category tree changes over time.\n\n    \n    \n  Figure 9. Examples of mislabelled offers.  With over 23,000 categories at Allegro, listing each offer in its best-matching category can be challenging for merchants. Hence, label noise is an inherent feature of our training dataset. \nThe ML model is prone to memorisation of the wrong labels in the training set, i.e. overfitting. These errors will\nlikely be reproduced at prediction time. Our goal is to train a robust classifier that will learn the true patterns\nand ignore the mislabelled training instances.\nThe training methods described in the previous section were developed and evaluated on computer vision tasks,\ne.g. image classification, into a relatively small number of categories. Here, we face the problem of extreme text\nclassification. Thus, we need to adapt those methods for textual input and find out which concepts transfer well between\nthe two domains.\nSynthetic label noise\nTo evaluate the modelâ€™s robustness experimentally, we need to know a priori which training instances were\nmislabelled. For that, we use a generator of controllable noise. The experimental setup consists of five steps\n(Figure 10):\ndumping a clean dataset from a curated pool of offers that are certainly in the right place,\nsplitting it into training, validation and test sets,\napplication of synthetic noise to 20% of instances in the training and validation sets (changing the offerâ€™s category\nto a wrong one),\ntraining the model on the noisy dataset,\ntesting the model on a held-out fraction of the clean dataset.\n\n    \n    \n  Figure 10. Testing the modelâ€™s robustness.  The full dataset of clean instances (offers with true category labels) is split into training, validation and test sets. Next, label noise is introduced to the training and validation sets and the model is trained. The model is tested on a held-out fraction of the clean dataset. \nThis setup lets us answer the following question:\nHow much does the noise in the training set hurt the modelâ€™s performance on the clean test set?\nThis way, we can evaluate different methods of training classifiers under label noise and choose the most robust\nclassifier, according to accuracy on the test set.\nAndâ€¦ it works!\nBelow we present the results of experiments for 1.3M offers listed in the Construction Work \u0026 Equipment category.\nSymmetric noise was applied to 20% of the training set. This means that the category labels of that percentage\nof offers were changed to different randomly chosen labels. We evaluated the 7 training methods outlined above\nand compared them to the baseline: classification with cross-entropy loss.\nBaseline: Memorising doesnâ€™t pay off\nHow does the presence of noise impact the baseline model?\nThe validation curves for non-corrupted samples clearly show the severe impact of noisy labels on the modelâ€™s\nperformance (Figure 11). In the early stage of training, the performance of the model trained\non noisy data is on par with the metrics of the model trained on clean data. Yet, starting from the 4th epoch,\nthe wrong labels in the noisy dataset appear to prevent the model from discovering the true patterns in the training\ndata, resulting in a 5 p.p. drop in accuracy at the end of the training. We attribute this drop to the memorisation\nof the wrong labels: instead of refining the originally learnt concepts, the network starts to overfit to the noisy\nlabels. The labels memorised for particular offers donâ€™t help with classifying previously unseen offers at test time.\n\n    \n    \n Figure 11. Degradation of the baseline model in the presence of noise. The 20% synthetic noise degrades the model throughout the training. In the end, the model trained on the corrupted dataset exhibits 5 p.p. lower accuracy in comparison to its clean counterpart \nTowards robust classification\nDoes robustness imply underfitting?\nTo verify if the evaluated methods have any effect on the modelâ€™s performance when there is no noise in the training\ndata, we tested all of them on a clean dataset without any synthetic noise.\nIn the absence of corrupted data, three of the tested methods (SPL, PRL and CT) are effectively reduced to the baseline\nCross-Entropy. Therefore, the accuracy for those methods was exactly the same as for the baseline (Table 1).\nFor mixup, the difference from the baseline was within the standard deviation range, so it was marked as no improvement\nas well.\nFor CCE and JSD the performance degraded, but only slightly â€” by 0.04 p.p. for the former and 0.34 p.p. for the latter.\nThis drop is an acceptable compromise considering the robustness to noise that these methods enable (see below).\nELR was the only method that improved upon the baseline, by 0.07 p.p. As ELR relies on temporal ensembling, which\ndiminishes the impact of corrupted samples during training, we hypothesise that our clean dataset contained a small\nnumber of mislabelled examples. Such paradoxes are a frequent case in machine learning practice, even for renowned\nbenchmark datasets like CIFAR-1009.\nTable 1. Test accuracy scores of the models trained on the clean and corrupted\n(20% synthetic noise) datasets for the 8 training methods. Light red highlight indicates deterioration in comparison\nto the baseline, while light blue denotes improvement. Notation: (mean \\(\\pm\\) std)% from 5 independently seeded runs.\nMethod\n            Test accuracy [%]\n        \nclean dataset\n            noisy dataset\n        \nBaseline\n            90.26 Â± 0.03\n            85.31 Â± 0.08\n        \nfunction\n            \n            Self-Paced Learning (SPL)\n            90.26 Â± 0.03\n            88.51 Â± 0.02\n        \nProvably Robust Learning (PRL)\n            90.26 Â± 0.03\n            88.31 Â± 0.02\n        \nClipped Cross-Entropy (CCE)\n            90.22 Â± 0.03\n            89.51 Â± 0.01\n        \nEarly Learning Regularisation (ELR)\n            90.33 Â± 0.01\n            89.29 Â± 0.03\n        \nJensen-Shannon Divergence (JSD) \n            89.92 Â± 0.02\n            89.24 Â± 0.01\n        \nCo-teaching (CT)\n            90.26 Â± 0.03\n            88.72 Â± 0.03\n        \nMixup\n            90.27 Â± 0.02\n            86.02 Â± 0.06\n        \nRobust classification results\nAll methods discussed in this study improved the modelâ€™s performance on the noisy dataset when compared to the baseline\n(Table 1). The best results were obtained with CCE (+4.2 p.p.), ELR (+3.98 p.p.) and JSD (+3.93 p.p.).\nCT, SPL, PRL performed a bit worse, but still proved to be quite robust, improving upon the baseline by 3.41 p.p.,\n3.2 p.p. and 3.0 p.p., respectively.\nMixup is a clear outlier â€” while it does improve upon the baseline by 0.71 p.p., this increase is noticeably smaller\nthan for the other evaluated methods. Our interpretation is that the linear augmentation at the heart of this method\nregularises the DNN, but does not address label noise per se. Mixup treats all samples equally, even if their labels\nare corrupted. The marginal improvement upon the baseline is evident in the validation accuracy training curve\n(Figure 12). Mixup starts to overfit around the 5th epoch, similarly to the baseline, and unlike all\nthe other methods.\n\n    \n    \n Figure 12. Validation accuracy during training. Validation accuracy for all methods was measured during training. It is evident that the best methods are CCE, ELR and JSD, with CT, PRL and SPL trailing slightly behind. Mixup behaves similarly to the baseline. \nConclusions\nThe problem of label noise is unavoidable in machine learning practice, and Allegro datasets are no exception.\nFortunately, there exist numerous methods that diminish the impact of label noise on prediction performance\nby increasing the robustness of machine learning models. In our experiments we implemented 7 of those methods\nand showed that they increase prediction accuracy in the presence of 20% synthetic noise when compared to the baseline\n(Cross-Entropy loss), most of them by a significant margin. The simple Clipped Cross-Entropy proved to be the best,\nwith an accuracy score of 89.51% (increase of 4.2 p.p. vs the baseline trained with noisy labels). This result is very\nclose to the baseline trained with clean labels (90.26%). Thus, we showed that for the case of 20% synthetic label\nnoise, it is possible to increase robustness so that the impact of label noise is negligible.\nThese experiments are only a first step in making classifiers at Allegro robust to label noise. The case of synthetic\nnoise presented here is not very realistic: real-world label noise tends to be instance-dependent,\ni.e. it is influenced by individual sample features. As such, we plan to further evaluate the methods for increasing\nmodel robustness with a real-world dataset perturbed by instance-dependent noise.\nIf youâ€™d like to know more about label noise and model robustness, please refer to the papers listed below.\nDeep Learning is Robust to Massive Label Noise, Rolnick et al., 2018Â â†©\nSelf-Paced Learning for Latent Variable Models, Kumar et al., 2010Â â†©\nLearning Deep Neural Networks under Agnostic Corrupted Supervision, Liu et al., 2021Â â†©\nEarly-Learning Regularization Prevents Memorization of Noisy Labels, Liu et al., 2020Â â†©\nTemporal Ensembling for Semi-Supervised Learning, Laine et al., 2017Â â†©\nGeneralized Jensen-Shannon Divergence Loss for Learning with Noisy Labels, Englesson et al., 2021Â â†©Â â†©2\nCo-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels, Han et al., 2018Â â†©Â â†©2\nmixup: Beyond Empirical Risk Minimization, Zhang et al., 2018Â â†©\nPervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks, Northcutt et al., 2021Â â†©","guid":"https://blog.allegro.tech/2023/04/learning-from-noisy-data.html","categories":["tech","mlr","robustness","research","ml","machine-learning","ai"],"isoDate":"2023-04-17T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999926482206","name":"Software Engineer - Allegro Ads","uuid":"6260a5b3-7630-45af-ade6-2f56bcba0f41","jobAdId":"96623438-8231-4568-9e98-2795876d1401","defaultJobAd":true,"refNumber":"REF4345L","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-08-24T12:28:18.087Z","location":{"city":"PoznaÅ„, Warsaw, WrocÅ‚aw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999926482206","language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999926450974","name":"Big Data Engineer - Allegro Ads","uuid":"1e520a1e-cd95-487a-9694-44342c4db740","jobAdId":"1fb341bc-6b0c-4144-9a6b-a7312367c446","defaultJobAd":true,"refNumber":"REF4328C","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-08-24T09:48:58.228Z","location":{"city":"PoznaÅ„, Warsaw, WrocÅ‚aw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999926450974","language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999926136626","name":"Big Data Engineer - Data \u0026 AI","uuid":"f75d0d51-583a-440c-87d1-baed2e432d7a","jobAdId":"deda3947-a80a-4ee5-afb7-d478820f54eb","defaultJobAd":true,"refNumber":"REF3096X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-08-23T10:58:04.065Z","location":{"city":"PoznaÅ„, Warsaw, GdaÅ„sk, ToruÅ„, Cracow","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999926136626","creator":{"name":"Martyna Stafa"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999925616021","name":"Mobile Software Engineer iOS - Mobile Core","uuid":"d4cc2fe1-0ea5-4db4-9762-efef55118836","jobAdId":"aa1af78d-bb07-47a1-a5b7-e55056cdc865","defaultJobAd":true,"refNumber":"REF4336M","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-08-21T13:32:48.907Z","location":{"city":"PoznaÅ„, WrocÅ‚aw, Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3976147c-fe25-42a8-8c97-78273250960b","valueLabel":"4"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999925616021","creator":{"name":"Agnieszka Adamus"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999925170914","name":"Software Engineer - Java","uuid":"2720def5-35ff-49dc-9dc1-2bb485967c25","jobAdId":"2b53fed0-4b15-488e-a1be-516a97bf5866","defaultJobAd":true,"refNumber":"REF4327W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-08-17T13:28:43.779Z","location":{"city":"Prague, Remote","region":"","country":"cz","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"f2bb5bc2-3fb0-4d5a-96d2-59e7d59ab3d7","valueLabel":"Tech Engineer/Non-Engineer - IC (MG)"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"cz","valueLabel":"Czech republic"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"b61e1897-7104-4a9d-b1cf-04fc2c537081","valueLabel":"N/A"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"0bca93e9-bc16-4156-902e-50465671c8fa","valueLabel":"Mall.cz"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999925170914","creator":{"name":"Natalia GliÅ„ska"},"language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1685697967000,"duration":7200000,"id":"293929321","name":"Allegro Tech Talks #38 - Mobile: o iOS bez spinki","date_in_series_pattern":false,"status":"past","time":1686760200000,"local_date":"2023-06-14","local_time":"18:30","updated":1686773845000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":17,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":0,"lon":0,"repinned":true,"address_1":"ul. Å»elazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293929321/","description":"**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwÄ… wakacyjnÄ…, stacjonarne spotkanie z cyklu Allegro Tech Talks, na ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³wâ€¦","how_to_find_us":"Biuro Allegro znajduje siÄ™ w kompleksie Fabryki Norblina (wejÅ›cie Plater 3 od ul. Å»elaznej). W niedalekiej odlegÅ‚oÅ›ci znajdujÄ… siÄ™ dwie stacje metra linii M2, Rondo DaszyÅ„skiego i Rondo ONZ. Autobusy, tramwaje i inne Å›rodki transportu sprawdzisz teÅ¼ na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false},{"created":1678978572000,"duration":111600000,"id":"292278882","name":"UX Research Confetti - III edycja ","date_in_series_pattern":false,"status":"past","time":1684915200000,"local_date":"2023-05-24","local_time":"10:00","updated":1685029049000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":33,"is_online_event":true,"eventType":"ONLINE","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/292278882/","description":"**Rejestracja na wydarzenie â¡ [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **ğŸ‰ Przedstawiamy 3. edycjÄ™ UX Research Confetti organizowanÄ… przezÂ Allegro - bezpÅ‚atnÄ…, polskÄ… konferencjÄ™ poÅ›wiÄ™conÄ… badaniomâ€¦","visibility":"public","member_pay_fee":false},{"created":1683275557000,"duration":7200000,"id":"293341234","name":"Allegro Tech Talks #37 - Kotlin Native i niebezpieczeÅ„stwa wspÃ³Å‚dzielonego stanu","date_in_series_pattern":false,"status":"past","time":1684425600000,"local_date":"2023-05-18","local_time":"18:00","updated":1684437308000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":19,"venue":{"id":27570147,"name":"Allegro Office - PoznaÅ„ (Nowy Rynek)","lat":52.40021514892578,"lon":16.92083168029785,"repinned":true,"address_1":"WierzbiÄ™cice 1B - budynek D","city":"PoznaÅ„","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293341234/","description":"**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-37/](https://app.evenea.pl/event/allegro-tech-talk-37/) CiÄ…g dalszy naszych stacjonarnych spotkaÅ„ Allegro Tech Talks, na ktÃ³rych dzielimy siÄ™ wiedzÄ…, wzajemnie inspirujemy oraz integrujemy podczas rozmÃ³w w kuluarach. ğŸ“Œâ€¦","how_to_find_us":"Biuro Allegro znajduje siÄ™ w kompleksie Nowy Rynek w budynku D. NajbliÅ¼szy przystanek to WierzbiÄ™cice i kursujÄ… tu linie tramwajowe numer 2, 5, 6, 10, 12, 18. ","visibility":"public","member_pay_fee":false},{"created":1682779438000,"duration":9000000,"id":"293215214","name":"AlleKwanty: o komputerach przyszÅ‚oÅ›ci, ktÃ³re na Allegro dopiero bÄ™dÄ… mieÄ‡","date_in_series_pattern":false,"status":"past","time":1684252800000,"local_date":"2023-05-16","local_time":"18:00","updated":1684266490000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":41,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Å»elazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293215214/","description":"**â¡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-kwanty/](https://app.evenea.pl/event/allegro-tech-kwanty/) Allegro Tech to miejsce, w ktÃ³rym nasi inÅ¼ynierowie dzielÄ… siÄ™ wiedzÄ… oraz case study z wybranych projektÃ³w w firmie - w formieâ€¦","how_to_find_us":"The Allegro office is located in Norblin Factory (entrance Plater 3, from Å»elazna Street). You can check the details of the journey (buses, trams, metro) at: https://fabrykanorblina.pl/dojazd/","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"O roli analitykÃ³w biznesowych w Allegro","link":"https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/","pubDate":"Thu, 24 Aug 2023 00:00:00 GMT","content":"Czym zajmujÄ… siÄ™ analitycy danych w Allegro i za jakie projekty odpowiadajÄ…? Z jakich rodzajÃ³w danych i narzÄ™dzi korzystajÄ… w codziennej pracy? Jakie (przykÅ‚adowe) obszary tematyczne pokrywamy danymi, ktÃ³re analizujemy w Allegro? Jakich umiejÄ™tnoÅ›ci szukamy u analitykÃ³w biznesowych w Allegro i jak moÅ¼na do nas doÅ‚Ä…czyÄ‡? O roli analitykÃ³w biznesowych i pracy w skali Allegro opowiadajÄ… Jakub KrÃ³l i Mateusz Falkowski - Senior Data Analysts w Allegro.","contentSnippet":"Czym zajmujÄ… siÄ™ analitycy danych w Allegro i za jakie projekty odpowiadajÄ…? Z jakich rodzajÃ³w danych i narzÄ™dzi korzystajÄ… w codziennej pracy? Jakie (przykÅ‚adowe) obszary tematyczne pokrywamy danymi, ktÃ³re analizujemy w Allegro? Jakich umiejÄ™tnoÅ›ci szukamy u analitykÃ³w biznesowych w Allegro i jak moÅ¼na do nas doÅ‚Ä…czyÄ‡? O roli analitykÃ³w biznesowych i pracy w skali Allegro opowiadajÄ… Jakub KrÃ³l i Mateusz Falkowski - Senior Data Analysts w Allegro.","guid":"https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/","isoDate":"2023-08-24T00:00:00.000Z"},{"title":"O spoÅ‚ecznoÅ›ci Allegro Tech i rozwoju inÅ¼ynierÃ³w w Allegro","link":"https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/","pubDate":"Thu, 27 Jul 2023 00:00:00 GMT","content":"Na czym polega rola Principal Software Engineera w Allegro oraz co ma wspÃ³lnego z rozwijaniem siebie i dzieleniem siÄ™ wiedzÄ…? Co warto wiedzieÄ‡ o turystyce, ktÃ³ra pojawia siÄ™ niemal w kaÅ¼dym odcinku naszych podcastÃ³w? Na czym polega, kto, kiedy i jak moÅ¼e z niej skorzystaÄ‡? Jak pracujemy z talentami Gallupa (takÅ¼e w zespoÅ‚ach technicznych)?  Co dajÄ… nam wewnÄ™trzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy siÄ™ doÅ›wiadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przyÅ›wieca? O spoÅ‚ecznoÅ›ci Allegro Tech i moÅ¼liwoÅ›ciach rozwoju w Allegro z perspektywy inÅ¼ynierÃ³w rozmawialiÅ›my z Marcinem Turkiem i MichaÅ‚em Kosmulskim.","contentSnippet":"Na czym polega rola Principal Software Engineera w Allegro oraz co ma wspÃ³lnego z rozwijaniem siebie i dzieleniem siÄ™ wiedzÄ…? Co warto wiedzieÄ‡ o turystyce, ktÃ³ra pojawia siÄ™ niemal w kaÅ¼dym odcinku naszych podcastÃ³w? Na czym polega, kto, kiedy i jak moÅ¼e z niej skorzystaÄ‡? Jak pracujemy z talentami Gallupa (takÅ¼e w zespoÅ‚ach technicznych)?  Co dajÄ… nam wewnÄ™trzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy siÄ™ doÅ›wiadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przyÅ›wieca? O spoÅ‚ecznoÅ›ci Allegro Tech i moÅ¼liwoÅ›ciach rozwoju w Allegro z perspektywy inÅ¼ynierÃ³w rozmawialiÅ›my z Marcinem Turkiem i MichaÅ‚em Kosmulskim.","guid":"https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/","isoDate":"2023-07-27T00:00:00.000Z"},{"title":"O Data Science Hub w Allegro","link":"https://podcast.allegro.tech/o-data-science-hub-w-allegro/","pubDate":"Fri, 14 Jul 2023 00:00:00 GMT","content":"Co kryje siÄ™ pod pojÄ™ciem Data Science Hub w Allegro? Jakie dziaÅ‚ania rozwijamy w tym obszarze i jak oceniamy ich potencjaÅ‚? O czym jest projekt Wilson i na czym skupiamy siÄ™ w projekcie przewidywania zakupÃ³w cyklicznych? Jak wykorzystujemy sztucznÄ… inteligencjÄ™ i gdzie jest dla niej miejsce wÅ›rÃ³d naszych kierunkÃ³w rozwoju? O AI Transformation, poczuciu sprawczoÅ›ci, mieszance kompetencji i talentÃ³w zamkniÄ™tej w rolach Data Scientist, Data Engineer i Data Analyst rozmawialiÅ›my z KarolinÄ… NieradkÄ… i Kamilem Konikiewiczem.,","contentSnippet":"Co kryje siÄ™ pod pojÄ™ciem Data Science Hub w Allegro? Jakie dziaÅ‚ania rozwijamy w tym obszarze i jak oceniamy ich potencjaÅ‚? O czym jest projekt Wilson i na czym skupiamy siÄ™ w projekcie przewidywania zakupÃ³w cyklicznych? Jak wykorzystujemy sztucznÄ… inteligencjÄ™ i gdzie jest dla niej miejsce wÅ›rÃ³d naszych kierunkÃ³w rozwoju? O AI Transformation, poczuciu sprawczoÅ›ci, mieszance kompetencji i talentÃ³w zamkniÄ™tej w rolach Data Scientist, Data Engineer i Data Analyst rozmawialiÅ›my z KarolinÄ… NieradkÄ… i Kamilem Konikiewiczem.,","guid":"https://podcast.allegro.tech/o-data-science-hub-w-allegro/","isoDate":"2023-07-14T00:00:00.000Z"},{"title":"O technologiach i projektach w Allegro Pay","link":"https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/","pubDate":"Thu, 29 Jun 2023 00:00:00 GMT","content":"Jak powstaÅ‚a usÅ‚uga Allegro Pay i co ma wspÃ³lnego z ratatouille? Jakie projekty i technologie stojÄ… za tym rozwiÄ…zaniem? Jak to jest pracowaÄ‡ w Azure i obsÅ‚ugiwaÄ‡ ruch, ktÃ³ry generuje Allegro? Czym inÅ¼ynierÃ³w moÅ¼e zaskoczyÄ‡ praca w Allegro Pay i co czeka na nich (na przykÅ‚ad) w programie All4Customer? O migrowaniu baz CosmosDB, wymaganiach skali i dostÄ™pnoÅ›ci, a takÅ¼e o rozwijaniu ludzi i technologii rozmawialiÅ›my z Mariuszem Budzynem i Tomaszem SzczerbÄ…. Zapraszamy do sÅ‚uchania! na rÃ³Å¼nych pÅ‚aszczyznach?","contentSnippet":"Jak powstaÅ‚a usÅ‚uga Allegro Pay i co ma wspÃ³lnego z ratatouille? Jakie projekty i technologie stojÄ… za tym rozwiÄ…zaniem? Jak to jest pracowaÄ‡ w Azure i obsÅ‚ugiwaÄ‡ ruch, ktÃ³ry generuje Allegro? Czym inÅ¼ynierÃ³w moÅ¼e zaskoczyÄ‡ praca w Allegro Pay i co czeka na nich (na przykÅ‚ad) w programie All4Customer? O migrowaniu baz CosmosDB, wymaganiach skali i dostÄ™pnoÅ›ci, a takÅ¼e o rozwijaniu ludzi i technologii rozmawialiÅ›my z Mariuszem Budzynem i Tomaszem SzczerbÄ…. Zapraszamy do sÅ‚uchania! na rÃ³Å¼nych pÅ‚aszczyznach?","guid":"https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/","isoDate":"2023-06-29T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"465mJBxcn3yxedGbDTDVL","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>