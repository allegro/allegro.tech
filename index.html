<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="15"/><link rel="preload" href="/_next/static/css/66933eaa547aae51.css" as="style"/><link rel="stylesheet" href="/_next/static/css/66933eaa547aae51.css" data-n-g=""/><link rel="preload" href="/_next/static/css/79db8b1e27b0a093.css" as="style"/><link rel="stylesheet" href="/_next/static/css/79db8b1e27b0a093.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-179adf437ae674f2.js" defer=""></script><script src="/_next/static/chunks/206-3a56e5ded293e83e.js" defer=""></script><script src="/_next/static/chunks/pages/index-f037c91132ed6a0a.js" defer=""></script><script src="/_next/static/pEqYuwKWlWmbs4B1d7Ex0/_buildManifest.js" defer=""></script><script src="/_next/static/pEqYuwKWlWmbs4B1d7Ex0/_ssgManifest.js" defer=""></script><script src="/_next/static/pEqYuwKWlWmbs4B1d7Ex0/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">O nas</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to również ponad 1000 specjalistów IT, różnych specjalizacji, rozwijających nasz serwis. Unikatowa skala i złożoność problemów, które rozwiązujemy na co dzień, dają nam możliwość rozwoju przy bardzo różnorodnych projektach. Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie – w formie artykułów, podcastów oraz eventów.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html" title="How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study"><img width="388" src="images/post-headers/default.jpg" alt="How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html" title="How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">10 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/cloud">#<!-- -->cloud</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/azure">#<!-- -->azure</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/cosmosdb">#<!-- -->cosmosdb</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,
as these features are indeed significant advantages…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Kamil Starczak" src="https://blog.allegro.tech/img/authors/kamil.starczak.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/kamil.starczak">Kamil Starczak</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html" title="MBox: server-driven UI for mobile apps"><img width="388" src="images/post-headers/default.jpg" alt="MBox: server-driven UI for mobile apps" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html" title="MBox: server-driven UI for mobile apps" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">MBox: server-driven UI for mobile apps</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/Server-driven UI">#<!-- -->Server-driven UI</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mobile">#<!-- -->mobile</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mbox">#<!-- -->mbox</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">In this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the
first version of the…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Paulina Sadowska" src="https://blog.allegro.tech/img/authors/paulina.sadowska.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/paulina.sadowska">Paulina Sadowska</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2022/07/event-storming-workshops.html" title="How to facilitate EventStorming workshops"><img width="388" src="images/post-headers/eventstorming.png" alt="How to facilitate EventStorming workshops" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2022/07/event-storming-workshops.html" title="How to facilitate EventStorming workshops" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">How to facilitate EventStorming workshops</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/eventstorming">#<!-- -->eventstorming</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/communication">#<!-- -->communication</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">With this article, I would like to introduce you to EventStorming and explain to you how to get started. I am not discovering
anything new, just…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Krzysztof Przychodzki" src="https://blog.allegro.tech/img/authors/krzysztof.przychodzki.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/krzysztof.przychodzki">Krzysztof Przychodzki</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2022/07/event-storming-workshops.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2022/07/event-storming-workshops.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html" title="GC, hands off my data!"><img width="388" src="images/post-headers/default.jpg" alt="GC, hands off my data!" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html" title="GC, hands off my data!" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">GC, hands off my data!</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/cache">#<!-- -->cache</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/off-heap">#<!-- -->off-heap</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/garbage collectors">#<!-- -->garbage collectors</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Certainly one of the main distinguishing features of the Java world is the Garbage Collector.
Using it is safe and convenient, it allows us to forget…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Michał Knasiecki" src="https://blog.allegro.tech/img/authors/michal.knasiecki.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/michal.knasiecki">Michał Knasiecki</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/" title="S03E03 - Paweł Marcinkowski - O Data &amp; AI w Allegro Pay"><img src="images/podcast.png" alt="S03E03 - Paweł Marcinkowski - O Data &amp; AI w Allegro Pay" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/" title="S03E03 - Paweł Marcinkowski - O Data &amp; AI w Allegro Pay" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">S03E03 - Paweł Marcinkowski - O Data &amp; AI w Allegro Pay</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 10 godzin temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak zbudowany jest obszar Data &amp; AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data &amp; AI w Allegro Pay.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/" title="S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box"><img src="images/podcast.png" alt="S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/" title="S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">14 dni temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-quality-assurance-w-allegro/" title="S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro"><img src="images/podcast.png" alt="S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-quality-assurance-w-allegro/" title="S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">28 dni temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-quality-assurance-w-allegro/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/rola_architekta_w_allegro/" title="S02E12 - Piotr Betkier - Rola architekta w Allegro"><img src="images/podcast.png" alt="S02E12 - Piotr Betkier - Rola architekta w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/rola_architekta_w_allegro/" title="S02E12 - Piotr Betkier - Rola architekta w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">S02E12 - Piotr Betkier - Rola architekta w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">ponad rok temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/rola_architekta_w_allegro/">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/287035383/" title="Allegro Tech Labs #10 Online: Poskromić stan w React" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Labs #10 Online: Poskromić stan w React"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/287035383/" title="Allegro Tech Labs #10 Online: Poskromić stan w React" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Labs #10 Online: Poskromić stan w React</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około 2 miesiące temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: [https://app.evenea.pl/event/allegro-tech-labs-10/](https://app.evenea.pl/event/allegro-tech-labs-10/?fbclid=IwAR1Zj3sIcfx3WEWiFfS_hgiW6BJQD6stYouSGuSqfxDq9YVeom8fTFcrE1Q) ❗ **Allegro Tech Labs** to w 100% zdalna odsłona naszych stacjonarnych spotkań warsztatowych. Zazwyczaj spotykaliśmy się…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/287035383/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/286545395/" title="Allegro Tech Live #29 - Wyzwania Product Managera" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #29 - Wyzwania Product Managera"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/286545395/" title="Allegro Tech Live #29 - Wyzwania Product Managera" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #29 - Wyzwania Product Managera</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiące temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/286545395/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/285416318/" title="UX Research Confetti - II edycja" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - II edycja"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/285416318/" title="UX Research Confetti - II edycja" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - II edycja</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">4 miesiące temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">REJESTRACJA NA WYDARZENIE -&amp;gt; https://app.evenea.pl/event/ux-research-confetti-2/ 🎉 Niech ponownie rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy……</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/285416318/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/285691203/" title="Allegro Tech Live #28 - Mobile: Architektura softu i architektura sprzętu" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Live #28 - Mobile: Architektura softu i architektura sprzętu"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/285691203/" title="Allegro Tech Live #28 - Mobile: Architektura softu i architektura sprzętu" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Live #28 - Mobile: Architektura softu i architektura sprzętu</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">4 miesiące temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Kiedyś spotykaliśmy się w naszych biurach, a teraz to my gościmy…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/285691203/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=8181270447539.144;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study","link":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","pubDate":"Tue, 13 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Kamil Starczak"],"photo":["https://blog.allegro.tech/img/authors/kamil.starczak.jpg"],"url":["https://blog.allegro.tech/authors/kamil.starczak"]}]},"content":"\u003cp\u003eCloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with \u003ca href=\"https://azure.microsoft.com/services/cosmos-db/\"\u003eAzure\nCosmos DB\u003c/a\u003e and batch processing.\u003c/p\u003e\n\n\u003ch2 id=\"our-story\"\u003eOur story\u003c/h2\u003e\n\n\u003cp\u003eAt Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.\u003c/p\u003e\n\n\u003cp\u003eIn this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eThe overall time of such a batch operation cannot exceed 5 minutes per 1 million records.\u003c/li\u003e\n  \u003cli\u003eThe processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.\u003c/li\u003e\n  \u003cli\u003eThe solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.\u003c/li\u003e\n  \u003cli\u003eThe solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe outcomes of this case study are published as an open source repository (see \u003ca href=\"#our-library\"\u003eOur library\u003c/a\u003e).\u003c/p\u003e\n\n\u003ch2 id=\"cosmos-db--the-basics\"\u003eCosmos DB — the basics\u003c/h2\u003e\n\n\u003cp\u003eBefore going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the \u003ca href=\"#database-utilization\"\u003eDatabase utilization\u003c/a\u003e chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (\u003ca href=\"https://azure.microsoft.com/en-us/services/cosmos-db/#features\"\u003esource\u003c/a\u003e). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.\u003c/p\u003e\n\n\u003cp\u003eHow does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eforeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFor each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.\u003c/p\u003e\n\n\u003cp\u003eWhat’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.\u003c/p\u003e\n\n\u003ch2 id=\"cosmos-db--scaling-and-provisioning\"\u003eCosmos DB — scaling and provisioning\u003c/h2\u003e\n\n\u003cp\u003eCosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img01.png\" alt=\"Cosmos DB Request Units overview\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eSource: \u003ca href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\"\u003ehttps://docs.microsoft.com/en-us/azure/cosmos-db/request-units\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eBut what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.\u003c/p\u003e\n\n\u003cp\u003eAt this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.\u003c/p\u003e\n\n\u003ch3 id=\"manual\"\u003eManual\u003c/h3\u003e\n\n\u003cp\u003eIn the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.\u003c/p\u003e\n\n\u003cp\u003eWhat happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img02.png\" alt=\"Manual mode visualized\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"autoscale\"\u003eAutoscale\u003c/h3\u003e\n\n\u003cp\u003eThe second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img03.png\" alt=\"Autoscale mode visualized\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWhat’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.\u003c/p\u003e\n\n\u003cp\u003eNevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.\u003c/p\u003e\n\n\u003ch3 id=\"serverless\"\u003eServerless\u003c/h3\u003e\n\n\u003cp\u003eThe last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img04.png\" alt=\"Serverless mode visualized\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eUnfortunately, if we read the docs, we can find some additional information:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eThe maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.\u003c/li\u003e\n  \u003cli\u003eThe guarantees for the operation latencies are worse — 30ms instead of 10ms.\u003c/li\u003e\n  \u003cli\u003eServerless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.\u003c/li\u003e\n  \u003cli\u003eMoreover, the maximum throughput during a single second is 5000 RUs.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAs we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.\u003c/p\u003e\n\n\u003cp\u003eTo sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.\u003c/p\u003e\n\n\u003ch2 id=\"database-utilization\"\u003eDatabase utilization\u003c/h2\u003e\n\n\u003cp\u003eLet’s go back to the sample code I was running.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eForeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIt processed 50k records in about 10 minutes. How loaded was the database?\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img05.png\" alt=\"Normalized RU Consumption metric\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNormalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.\u003c/p\u003e\n\n\u003cp\u003eIf we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.\u003c/p\u003e\n\n\u003cp\u003eThis time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.\u003c/p\u003e\n\n\u003ch3 id=\"ru-limiter\"\u003eRU limiter\u003c/h3\u003e\n\n\u003cp\u003eIs there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img06.png\" alt=\"RU limiter visualized\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img07.png\" alt=\"Total Request Units metric\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"partition-key-ranges\"\u003ePartition key ranges\u003c/h3\u003e\n\n\u003cp\u003eIt almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img08.png\" alt=\"Normalized RU Consumption metric\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eSomething is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ePartitionKeyRangeId\u003c/code\u003e. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img09.png\" alt=\"Normalized RU Consumption metric split by Partition Key ranges\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIf we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.\u003c/p\u003e\n\n\u003ch3 id=\"a-bit-of-reverse-engineering\"\u003eA bit of reverse engineering\u003c/h3\u003e\n\n\u003cp\u003eIs there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.\u003c/p\u003e\n\n\u003ch2 id=\"the-autoscaler-auto-scaler\"\u003eThe “Autoscaler auto scaler”\u003c/h2\u003e\n\n\u003cp\u003eThe problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.\u003c/p\u003e\n\n\u003cp\u003eThe one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.\u003c/p\u003e\n\n\u003cp\u003eBut what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img10.png\" alt=\"Autoscaler visualized during batch processing\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWith this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.\u003c/p\u003e\n\n\u003cp\u003eThis way, we have fulfilled all the requirements that were set at the beginning:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBatch processing time — 5 minutes per 1 million records.\u003c/li\u003e\n  \u003cli\u003eNo risk of starving other processes, thanks to the RU limiter.\u003c/li\u003e\n  \u003cli\u003eCost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.\u003c/li\u003e\n  \u003cli\u003eScalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"our-library\"\u003eOur library\u003c/h2\u003e\n\n\u003cp\u003eThe outcomes of this work have been published as an opensource .NET library on our GitHub page:\n\u003ca href=\"https://github.com/allegro/cosmosdb-utils/tree/main/src/Allegro.CosmosDb.BatchUtilities\"\u003eAllegro.CosmosDb.BatchUtilities\u003c/a\u003e.\nFeel free to use it or even contribute new features.\u003c/p\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eAnd here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eCosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.\u003c/li\u003e\n  \u003cli\u003eCosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.\u003c/li\u003e\n  \u003cli\u003eYou must pay close attention to the costs, as it is very easy to get high bills by misusing the service.\u003c/li\u003e\n  \u003cli\u003eIt’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.\u003c/li\u003e\n\u003c/ul\u003e\n","contentSnippet":"Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with Azure\nCosmos DB and batch processing.\nOur story\nAt Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.\nIn this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:\nThe overall time of such a batch operation cannot exceed 5 minutes per 1 million records.\nThe processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.\nThe solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.\nThe solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.\nThe outcomes of this case study are published as an open source repository (see Our library).\nCosmos DB — the basics\nBefore going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the Database utilization chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (source). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.\nHow does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:\n\nforeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nFor each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.\nWhat’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.\nCosmos DB — scaling and provisioning\nCosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.\n\nSource: https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\nBut what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.\nAt this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.\nManual\nIn the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.\nWhat happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.\n\nAutoscale\nThe second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.\n\nWhat’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.\nNevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.\nServerless\nThe last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.\n\nUnfortunately, if we read the docs, we can find some additional information:\nThe maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.\nThe guarantees for the operation latencies are worse — 30ms instead of 10ms.\nServerless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.\nMoreover, the maximum throughput during a single second is 5000 RUs.\nAs we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.\nTo sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.\nDatabase utilization\nLet’s go back to the sample code I was running.\n\nForeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nIt processed 50k records in about 10 minutes. How loaded was the database?\n\nNormalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.\nIf we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.\nThis time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.\nRU limiter\nIs there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.\n\nThe mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.\n\nPartition key ranges\nIt almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.\n\nSomething is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled PartitionKeyRangeId. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.\n\nIf we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.\nA bit of reverse engineering\nIs there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.\nThe “Autoscaler auto scaler”\nThe problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.\nThe one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.\nBut what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).\n\nWith this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.\nThis way, we have fulfilled all the requirements that were set at the beginning:\nBatch processing time — 5 minutes per 1 million records.\nNo risk of starving other processes, thanks to the RU limiter.\nCost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.\nScalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.\nOur library\nThe outcomes of this work have been published as an opensource .NET library on our GitHub page:\nAllegro.CosmosDb.BatchUtilities.\nFeel free to use it or even contribute new features.\nConclusions\nAnd here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:\nCosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.\nCosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.\nYou must pay close attention to the costs, as it is very easy to get high bills by misusing the service.\nIt’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.","guid":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","categories":["tech","cloud","azure","cosmosdb"],"isoDate":"2022-09-12T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"MBox: server-driven UI for mobile apps","link":"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html","pubDate":"Wed, 03 Aug 2022 00:00:00 +0200","authors":{"author":[{"name":["Paulina Sadowska"],"photo":["https://blog.allegro.tech/img/authors/paulina.sadowska.jpg"],"url":["https://blog.allegro.tech/authors/paulina.sadowska"]}]},"content":"\u003cp\u003eIn this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the\nfirst version of the in-house server-driven rendering tool called MBox and used it to render the\nhomepage in the Allegro app on \u003ca href=\"https://play.google.com/store/apps/details?id=pl.allegro\"\u003eAndroid\u003c/a\u003e\nand \u003ca href=\"https://apps.apple.com/pl/app/allegro/id305659772\"\u003eiOS\u003c/a\u003e. We have come a long way since then, and now we use this\ntool to render more and more screens in the Allegro app.\u003c/p\u003e\n\n\u003cp\u003eAfter over three years of working on MBox, we want to share how it works and the key advantages and challenges of using this approach.\u003c/p\u003e\n\n\u003ch2 id=\"why-server-driven-ui\"\u003eWhy server-driven UI?\u003c/h2\u003e\n\n\u003cp\u003eThe idea behind MBox was to make mobile development faster without compromising the app quality. Implementing a\nfeature twice for Android and iOS takes a lot of time and requires two people with unique skill sets (knowledge of\nAndroid and iOS frameworks). There is also the risk that both apps will not behave consistently because each person may\ninterpret the requirements slightly differently.\u003c/p\u003e\n\n\u003cp\u003eUsing a server-driven UI solves that problem because each business feature is implemented only once on the backend.\nThat gives us consistency out of the box and shortens the time needed to implement the feature.\nAlso, developers don’t need to know mobile frameworks to develop for mobile anymore.\u003c/p\u003e\n\n\u003cp\u003eAnother advantage of server-driven UI is that it allows releasing features independently of the release train. We\ncan deploy changes multiple times a day and when something goes wrong — roll back to the previous version immediately.\nIt gives teams a lot more flexibility and allows them to experiment and iterate much faster. What’s more, deployed\nchanges are visible to all clients, no matter which app version they use.\u003c/p\u003e\n\n\u003ch2 id=\"how-does-mbox-work\"\u003eHow does MBox work?\u003c/h2\u003e\n\n\u003ch3 id=\"defining-the-screen-layout\"\u003eDefining the screen layout\u003c/h3\u003e\n\n\u003cp\u003eWhile designing MBox, we wanted to create a tool that would give developers total flexibility to implement any layout\nthey need — as long as it’s consistent with our design system, Metrum.\u003c/p\u003e\n\n\u003cp\u003eThat’s why MBox screens are built using primitive components, which our rendering libraries map to native views.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/1_MBox_SG.png\" alt=\"MessageWidget structure\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eDevelopers can arrange MBox components freely using different types of containers that MBox supports (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eflex-container\u003c/code\u003e,\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003estack-container\u003c/code\u003e, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eabsolute-container\u003c/code\u003e, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elist-container\u003c/code\u003e, etc.). Those components can be styled and configured to match\ndifferent business scenarios.\u003c/p\u003e\n\n\u003cp\u003eMBox renders components on mobile apps consistently, but it also respects slight differences unique to Android and\niOS platforms.\nFor example, dialog action in MBox supports the same functionalities on both platforms, but the dialog itself looks\ndifferent on Android and iOS:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/6_alert.png\" alt=\"MBox dialog action on Android and iOS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThat gives MBox screens a native look and feel and perfectly blends in with parts of the app developed\nnatively, without MBox. We had to add a label that shows which parts of the app are rendered by MBox, because\neven mobile developers couldn’t tell where native screens ended and MBox started.\u003c/p\u003e\n\n\u003ch3 id=\"what-about-more-complex-views\"\u003eWhat about more complex views?\u003c/h3\u003e\n\n\u003cp\u003eCreating more complex, reusable views is also possible. For example, our design system specifies something called the\nmessage: an element with a vertical line, an icon, and some texts and buttons. However, because this element is complex\nand its requirements may change over time, it’s defined on the backend service as a widget — the element that developers\ncan reuse across different screens.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/2_MessageWidget.png\" alt=\"MessageWidget structure\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIf the requirements for the message widget change, we can easily modify it on the backend side without the need to\nrelease the app. That’s because it’s not defined directly in the MBox libraries included in the mobile apps, but\nspecified on the backend using MBox components.\u003c/p\u003e\n\n\u003ch3 id=\"unified-tracking\"\u003eUnified tracking\u003c/h3\u003e\n\n\u003cp\u003eBesides defining layouts, MBox also allows us to specify tracking events on the backend. For tracking events,\nconsistency is crucial. If events are not triggered under the same scenarios and with the same data\non both platforms, it’s hard to compare the data and make business decisions.\u003c/p\u003e\n\n\u003cp\u003eMBox solves that problem. All events tracked on MBox screens are defined on the backend, meaning unified tracking\nbetween Android and iOS and across different app versions.\u003c/p\u003e\n\n\u003ch3 id=\"testing\"\u003eTesting\u003c/h3\u003e\n\n\u003cp\u003eSince the MBox rendering engine is a core of more and more screens in the app, it had to be thoroughly covered by unit\ntests and integration tests. We also have screenshot tests that ensure that MBox components render correctly. That\nallows us to find out early about possible regressions.\u003c/p\u003e\n\n\u003cp\u003eTeams that develop screens using MBox also have various tools that allow them to test their features. They can write\nunit tests in the MBox backend service and check if correct MBox components are created for the given data.\nThey can also add a URL of their page to Visual Regression — the tool that creates a screenshot of\nthe page whenever someone commits anything to the MBox backend and if any change in the page is detected, the author is\nautomatically notified in their pull request.\u003c/p\u003e\n\n\u003cp\u003eFeature teams can also write UI tests for the native apps to test how their page integrates with the rest of the app and\nif all interactions work as expected. However, those tests have to be written on both platforms by the mobile developers\nand should take into account that the content of the page under tests can be changed on the backend.\u003c/p\u003e\n\n\u003ch2 id=\"the-journey-to-make-mbox-interactive\"\u003eThe journey to make MBox interactive\u003c/h2\u003e\n\n\u003cp\u003eWhen we started working on MBox, we were focused mainly on pages that contain a lot of frequently changing content but\nnot many interactions with users. In the first version of MBox, it was possible to define only basic actions like\nopening a new screen or adding an offer to the cart. That changed gradually when new teams started using MBox.\u003c/p\u003e\n\n\u003cp\u003eTo make MBox more interactive, we used the same atomic approach we adopted when designing MBox layout components. We\ngradually added generic actions that were not custom-made to serve specific business features but were reusable across\ndifferent use cases.\u003c/p\u003e\n\n\u003ch3 id=\"for-example\"\u003eFor example:\u003c/h3\u003e\n\n\u003cp\u003eOne of the first challenges that we faced was allowing the implementation of an “add to watchlist” star in MBox. We\ncould’ve just added the ”watchlist star” component that checks if a user is logged in (redirects to the login page if it’s\nnot), adds an offer to the watchlist, and changes the star icon from empty to full. In the short term, it should have\nbeen easier. But it’s not a way that would allow MBox to scale.\u003c/p\u003e\n\n\u003cp\u003eInstead, we designed a couple of atomic mechanisms that allow building this feature on the backend and could be reused\nin the future in different use cases.\u003c/p\u003e\n\n\u003cp\u003eWe added a logic component called \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emultivariant\u003c/code\u003e that allows changing one component into another thanks to the\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003echangeVariant\u003c/code\u003e action. That enabled us to switch the star icon from empty to full. Next, we added the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esendRequest\u003c/code\u003e\naction\nthat sends requests with given URL, headers, and other data to our services. That allows adding and removing an offer to\nand from the watchlist. Lastly, we added the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eloginIfNeeded\u003c/code\u003e action that allows checking if a user is logged in and\nredirecting to the login screen if needed. That allows ensuring the user is logged in before making the request.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/3_add_to_watched.png\" alt=\"Add to watchlist: scheme\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eOf course, doing it this way took much more time than just implementing the ”add to watchlist” component in MBox libraries\nnatively. But this is the way that scales and gives us flexibility.\u003c/p\u003e\n\n\u003cp\u003eOver time mechanisms that we designed earlier were reused on other screens. And more and more often, when the new team\nwanted to use MBox on their screen, most of the building blocks they needed were already there. It definitely\nwouldn’t be the case if not for our atomic approach.\u003c/p\u003e\n\n\u003ch2 id=\"the-challenges\"\u003eThe challenges\u003c/h2\u003e\n\n\u003cp\u003eWe also encountered many challenges while working on MBox.\u003c/p\u003e\n\n\u003ch3 id=\"consistency-of-the-engines\"\u003eConsistency of the engines\u003c/h3\u003e\n\n\u003cp\u003eWe create two separate rendering engines for mobile platforms, so we must be extra cautious to ensure everything works consistently.\nEven a tiny inconsistency in the behavior of the engines may be hugely problematic for the developers that use MBox.\nIt may force them to, for example, define different layouts for each mobile platform.\u003c/p\u003e\n\n\u003cp\u003eTo make sure the engines are consistent, each feature in MBox is implemented synchronously by a pair of developers\n(Android and iOS) who consult with each other regularly. During the work, they make sure that they interpret the\nrequirements and cover edge cases in the same way.\nThe new features are ready to merge only after thorough tests on both platforms that check both correctness and\nconsistency.\u003c/p\u003e\n\n\u003ch3 id=\"versioning\"\u003eVersioning\u003c/h3\u003e\n\n\u003cp\u003eOn MBox, we also have to pay close attention to versioning. We use semantic versioning in the engines. Each new feature\nhas to be marked with the same minor and major version on both platforms. We also prepare changelogs containing\ninformation about what\nfunctionalities are available in which version.\u003c/p\u003e\n\n\u003cp\u003eOn the backend, we allow checking the version of the MBox engine that the user has and serve different content depending\non it.\nFor example, when the screen contains the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eswitch\u003c/code\u003e component, supported since version \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.21\u003c/code\u003e,\nwe can define that for users who have the app with the older versions of MBox, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003echeckbox\u003c/code\u003e will be displayed instead.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/5_fallback.png\" alt=\"Fallback mechanism\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"testing-changes-introduced-to-the-engines\"\u003eTesting changes introduced to the engines\u003c/h3\u003e\n\n\u003cp\u003eAnd last but not least: testing. Because MBox is used to render various screens in Allegro mobile apps, we must be\ncautious whenever we introduce engine changes to avoid negatively impacting existing MBox screens.\nThe screenshot and UI tests cover every MBox component and action. We’re also encouraging feature teams to add their\nscreens to the Visual Regression and cover their screens with UI tests in the mobile repositories. All those things\nallow us to minimize the risk of introducing a regression.\u003c/p\u003e\n\n\u003ch2 id=\"how-does-mbox-connect-to-other-parts-of-the-allegro-ecosystem\"\u003eHow does MBox connect to other parts of the Allegro ecosystem?\u003c/h2\u003e\n\n\u003cp\u003eConsistency across mobile platforms is not everything. Another important aspect of our work is making sure mobile and\nweb platforms are as consistent as possible, respecting native differences that make each platform unique.\u003c/p\u003e\n\n\u003cp\u003eMBox integrates with our content management system, also used for the web (\u003ca href=\"https://blog.allegro.tech/2016/03/Managing-Frontend-in-the-microservices-architecture.html\"\u003eOpbox\u003c/a\u003e Page Manager). The screen’s content\nconfigured in the Opbox admin panel is sent through the Opbox services to the MBox backend service. The MBox service\nmaps the\ndata into MBox components that make up the MBox screen. Then the screen definition in JSON format is sent to apps and is\nrendered using native views.\u003c/p\u003e\n\n\u003cp\u003eThe same data from Opbox is also used to render the web equivalent of the same screen. Opbox defines its own mappings\nfor the web: Opbox Components, which describe how to map the data into HTML elements that make up the Allegro web pages.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/4_architecture.png\" alt=\"Add to watchlist: scheme\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIntegration with Opbox gives us a lot of advantages. Very often, to change the content in the app and web, you don’t\nneed to change the code at all — all you need to do is change the content in the Opbox admin panel.\u003c/p\u003e\n\n\u003cp\u003eAnother huge advantage is that we have unified tracking between all platforms and can use the same tools for A/B testing\nthat are used for the web. Previously, code for A/B tests had to be written for each mobile platform separately in\nnative\ncode and then cleaned up after the finished experiment. Now, some experiments work out of the box since Opbox sends\ndifferent data to MBox depending on the experiment variant the user falls into. Sometimes a little bit of code in the\nMBox backend is required to conduct an experiment, but it’s not comparable to the amount of work A/B tests take when\nthey’re performed in the native code without MBox.\u003c/p\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eMBox is a tool that changed how we work on mobile apps in Allegro. It allowed us to shorten the development time without\ncompromising the quality and stability of the app and without losing the native look and feel of the Allegro apps.\u003c/p\u003e\n\n\u003cp\u003eWe have come a long way during those three years since we started working on MBox. At first, our ambition was to create\na tool that would be used on content screens with very few interactions. Over time, we pushed the boundaries of what\nMBox\nis capable of and entered screens with more and more interactions with the user.\u003c/p\u003e\n\n\u003cp\u003eCurrently MBox is used in over 25 screens in Allegro mobile apps and the number is still growing. In the first half of\n2022 alone, 27 teams made changes to the app using MBox and created about 300 pull requests. We deployed changes over\n100 times which means ~4.15 releases a week.\u003c/p\u003e\n\n\u003cp\u003eWe’re confident that it’s not the end of the possibilities ahead of us. We still see how we can make MBox even more\npowerful. We’d love to shorten development time even more by providing tools that allow defining MBox screens in\nTypeScript. That’ll enable developers to reuse some parts of the code between mobile and web and take advantage of\nbetter tools such as hot reload. Another thing we’re currently focused on is adding the binding mechanism to MBox and\nthe client-side logic to allow defining the business logic on the backend. Implementing those mechanisms will allow\nintroducing even more interactivity into MBox screens.\u003c/p\u003e\n\n\u003cp\u003eBut that is the topic for the next articles. Stay tuned!\u003c/p\u003e\n","contentSnippet":"In this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the\nfirst version of the in-house server-driven rendering tool called MBox and used it to render the\nhomepage in the Allegro app on Android\nand iOS. We have come a long way since then, and now we use this\ntool to render more and more screens in the Allegro app.\nAfter over three years of working on MBox, we want to share how it works and the key advantages and challenges of using this approach.\nWhy server-driven UI?\nThe idea behind MBox was to make mobile development faster without compromising the app quality. Implementing a\nfeature twice for Android and iOS takes a lot of time and requires two people with unique skill sets (knowledge of\nAndroid and iOS frameworks). There is also the risk that both apps will not behave consistently because each person may\ninterpret the requirements slightly differently.\nUsing a server-driven UI solves that problem because each business feature is implemented only once on the backend.\nThat gives us consistency out of the box and shortens the time needed to implement the feature.\nAlso, developers don’t need to know mobile frameworks to develop for mobile anymore.\nAnother advantage of server-driven UI is that it allows releasing features independently of the release train. We\ncan deploy changes multiple times a day and when something goes wrong — roll back to the previous version immediately.\nIt gives teams a lot more flexibility and allows them to experiment and iterate much faster. What’s more, deployed\nchanges are visible to all clients, no matter which app version they use.\nHow does MBox work?\nDefining the screen layout\nWhile designing MBox, we wanted to create a tool that would give developers total flexibility to implement any layout\nthey need — as long as it’s consistent with our design system, Metrum.\nThat’s why MBox screens are built using primitive components, which our rendering libraries map to native views.\n\nDevelopers can arrange MBox components freely using different types of containers that MBox supports (flex-container,\nstack-container, absolute-container, list-container, etc.). Those components can be styled and configured to match\ndifferent business scenarios.\nMBox renders components on mobile apps consistently, but it also respects slight differences unique to Android and\niOS platforms.\nFor example, dialog action in MBox supports the same functionalities on both platforms, but the dialog itself looks\ndifferent on Android and iOS:\n\nThat gives MBox screens a native look and feel and perfectly blends in with parts of the app developed\nnatively, without MBox. We had to add a label that shows which parts of the app are rendered by MBox, because\neven mobile developers couldn’t tell where native screens ended and MBox started.\nWhat about more complex views?\nCreating more complex, reusable views is also possible. For example, our design system specifies something called the\nmessage: an element with a vertical line, an icon, and some texts and buttons. However, because this element is complex\nand its requirements may change over time, it’s defined on the backend service as a widget — the element that developers\ncan reuse across different screens.\n\nIf the requirements for the message widget change, we can easily modify it on the backend side without the need to\nrelease the app. That’s because it’s not defined directly in the MBox libraries included in the mobile apps, but\nspecified on the backend using MBox components.\nUnified tracking\nBesides defining layouts, MBox also allows us to specify tracking events on the backend. For tracking events,\nconsistency is crucial. If events are not triggered under the same scenarios and with the same data\non both platforms, it’s hard to compare the data and make business decisions.\nMBox solves that problem. All events tracked on MBox screens are defined on the backend, meaning unified tracking\nbetween Android and iOS and across different app versions.\nTesting\nSince the MBox rendering engine is a core of more and more screens in the app, it had to be thoroughly covered by unit\ntests and integration tests. We also have screenshot tests that ensure that MBox components render correctly. That\nallows us to find out early about possible regressions.\nTeams that develop screens using MBox also have various tools that allow them to test their features. They can write\nunit tests in the MBox backend service and check if correct MBox components are created for the given data.\nThey can also add a URL of their page to Visual Regression — the tool that creates a screenshot of\nthe page whenever someone commits anything to the MBox backend and if any change in the page is detected, the author is\nautomatically notified in their pull request.\nFeature teams can also write UI tests for the native apps to test how their page integrates with the rest of the app and\nif all interactions work as expected. However, those tests have to be written on both platforms by the mobile developers\nand should take into account that the content of the page under tests can be changed on the backend.\nThe journey to make MBox interactive\nWhen we started working on MBox, we were focused mainly on pages that contain a lot of frequently changing content but\nnot many interactions with users. In the first version of MBox, it was possible to define only basic actions like\nopening a new screen or adding an offer to the cart. That changed gradually when new teams started using MBox.\nTo make MBox more interactive, we used the same atomic approach we adopted when designing MBox layout components. We\ngradually added generic actions that were not custom-made to serve specific business features but were reusable across\ndifferent use cases.\nFor example:\nOne of the first challenges that we faced was allowing the implementation of an “add to watchlist” star in MBox. We\ncould’ve just added the ”watchlist star” component that checks if a user is logged in (redirects to the login page if it’s\nnot), adds an offer to the watchlist, and changes the star icon from empty to full. In the short term, it should have\nbeen easier. But it’s not a way that would allow MBox to scale.\nInstead, we designed a couple of atomic mechanisms that allow building this feature on the backend and could be reused\nin the future in different use cases.\nWe added a logic component called multivariant that allows changing one component into another thanks to the\nchangeVariant action. That enabled us to switch the star icon from empty to full. Next, we added the sendRequest\naction\nthat sends requests with given URL, headers, and other data to our services. That allows adding and removing an offer to\nand from the watchlist. Lastly, we added the loginIfNeeded action that allows checking if a user is logged in and\nredirecting to the login screen if needed. That allows ensuring the user is logged in before making the request.\n\nOf course, doing it this way took much more time than just implementing the ”add to watchlist” component in MBox libraries\nnatively. But this is the way that scales and gives us flexibility.\nOver time mechanisms that we designed earlier were reused on other screens. And more and more often, when the new team\nwanted to use MBox on their screen, most of the building blocks they needed were already there. It definitely\nwouldn’t be the case if not for our atomic approach.\nThe challenges\nWe also encountered many challenges while working on MBox.\nConsistency of the engines\nWe create two separate rendering engines for mobile platforms, so we must be extra cautious to ensure everything works consistently.\nEven a tiny inconsistency in the behavior of the engines may be hugely problematic for the developers that use MBox.\nIt may force them to, for example, define different layouts for each mobile platform.\nTo make sure the engines are consistent, each feature in MBox is implemented synchronously by a pair of developers\n(Android and iOS) who consult with each other regularly. During the work, they make sure that they interpret the\nrequirements and cover edge cases in the same way.\nThe new features are ready to merge only after thorough tests on both platforms that check both correctness and\nconsistency.\nVersioning\nOn MBox, we also have to pay close attention to versioning. We use semantic versioning in the engines. Each new feature\nhas to be marked with the same minor and major version on both platforms. We also prepare changelogs containing\ninformation about what\nfunctionalities are available in which version.\nOn the backend, we allow checking the version of the MBox engine that the user has and serve different content depending\non it.\nFor example, when the screen contains the switch component, supported since version 1.21,\nwe can define that for users who have the app with the older versions of MBox, checkbox will be displayed instead.\n\nTesting changes introduced to the engines\nAnd last but not least: testing. Because MBox is used to render various screens in Allegro mobile apps, we must be\ncautious whenever we introduce engine changes to avoid negatively impacting existing MBox screens.\nThe screenshot and UI tests cover every MBox component and action. We’re also encouraging feature teams to add their\nscreens to the Visual Regression and cover their screens with UI tests in the mobile repositories. All those things\nallow us to minimize the risk of introducing a regression.\nHow does MBox connect to other parts of the Allegro ecosystem?\nConsistency across mobile platforms is not everything. Another important aspect of our work is making sure mobile and\nweb platforms are as consistent as possible, respecting native differences that make each platform unique.\nMBox integrates with our content management system, also used for the web (Opbox Page Manager). The screen’s content\nconfigured in the Opbox admin panel is sent through the Opbox services to the MBox backend service. The MBox service\nmaps the\ndata into MBox components that make up the MBox screen. Then the screen definition in JSON format is sent to apps and is\nrendered using native views.\nThe same data from Opbox is also used to render the web equivalent of the same screen. Opbox defines its own mappings\nfor the web: Opbox Components, which describe how to map the data into HTML elements that make up the Allegro web pages.\n\nIntegration with Opbox gives us a lot of advantages. Very often, to change the content in the app and web, you don’t\nneed to change the code at all — all you need to do is change the content in the Opbox admin panel.\nAnother huge advantage is that we have unified tracking between all platforms and can use the same tools for A/B testing\nthat are used for the web. Previously, code for A/B tests had to be written for each mobile platform separately in\nnative\ncode and then cleaned up after the finished experiment. Now, some experiments work out of the box since Opbox sends\ndifferent data to MBox depending on the experiment variant the user falls into. Sometimes a little bit of code in the\nMBox backend is required to conduct an experiment, but it’s not comparable to the amount of work A/B tests take when\nthey’re performed in the native code without MBox.\nConclusions\nMBox is a tool that changed how we work on mobile apps in Allegro. It allowed us to shorten the development time without\ncompromising the quality and stability of the app and without losing the native look and feel of the Allegro apps.\nWe have come a long way during those three years since we started working on MBox. At first, our ambition was to create\na tool that would be used on content screens with very few interactions. Over time, we pushed the boundaries of what\nMBox\nis capable of and entered screens with more and more interactions with the user.\nCurrently MBox is used in over 25 screens in Allegro mobile apps and the number is still growing. In the first half of\n2022 alone, 27 teams made changes to the app using MBox and created about 300 pull requests. We deployed changes over\n100 times which means ~4.15 releases a week.\nWe’re confident that it’s not the end of the possibilities ahead of us. We still see how we can make MBox even more\npowerful. We’d love to shorten development time even more by providing tools that allow defining MBox screens in\nTypeScript. That’ll enable developers to reuse some parts of the code between mobile and web and take advantage of\nbetter tools such as hot reload. Another thing we’re currently focused on is adding the binding mechanism to MBox and\nthe client-side logic to allow defining the business logic on the backend. Implementing those mechanisms will allow\nintroducing even more interactivity into MBox screens.\nBut that is the topic for the next articles. Stay tuned!","guid":"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html","categories":["tech","Server-driven UI","mobile","mbox"],"isoDate":"2022-08-02T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How to facilitate EventStorming workshops","link":"https://blog.allegro.tech/2022/07/event-storming-workshops.html","pubDate":"Tue, 19 Jul 2022 00:00:00 +0200","authors":{"author":[{"name":["Krzysztof Przychodzki"],"photo":["https://blog.allegro.tech/img/authors/krzysztof.przychodzki.jpg"],"url":["https://blog.allegro.tech/authors/krzysztof.przychodzki"]}]},"content":"\u003cp\u003eWith this article, I would like to introduce you to EventStorming and explain to you how to get started. I am not discovering\nanything new, just gathering available knowledge in one place. What I will show you is a few tips on how to conduct\nand facilitate EventStorming workshops.\u003c/p\u003e\n\n\u003ch2 id=\"guide-to-big-picture-eventstorming\"\u003eGuide to Big Picture EventStorming\u003c/h2\u003e\n\n\u003ch3 id=\"introducing-eventstorming\"\u003eIntroducing EventStorming\u003c/h3\u003e\n\n\u003cp\u003eIn 2013 Alberto Brandolini posted an \u003ca href=\"https://ziobrando.blogspot.com/2013/11/introducing-event-storming.html\"\u003earticle\u003c/a\u003e\nabout a new workshop format for quick exploration of complex business domains. It was warmly welcomed by the DDD community.\nIn 2015 \u003ca href=\"https://www.thoughtworks.com/radar/techniques/event-storming\"\u003eTechnology Radar\u003c/a\u003e described EventStorming as \u003cem\u003eworthy of attention\u003c/em\u003e\nand three years later as \u003cem\u003ea recommended method\u003c/em\u003e for business domain modelling in information systems.\u003c/p\u003e\n\n\u003cp\u003eDuring the years a lot has changed, the technique has developed and matured but the main idea remained the same:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003eEventStorming is a flexible workshop format that allows a massive collaborative exploration of complex domains (…)\nwhere software and business practitioners are building together a behavioural model of the whole business line.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe above definition is from Alberto Brandolini’s \u003cem\u003e\u003ca href=\"https://leanpub.com/introducing_eventstorming\"\u003eIntroducing EventStorming\u003c/a\u003e\u003c/em\u003e,\nto which I will be referring in this article.\u003c/p\u003e\n\n\u003ch2 id=\"before-launching\"\u003eBefore launching\u003c/h2\u003e\n\n\u003ch3 id=\"provide-unlimited-modelling-space\"\u003eProvide unlimited modelling space\u003c/h3\u003e\n\n\u003cp\u003eWhy is it important? Because you want participants to explore and experiment during the workshop. You don’t want to\nimpose limits on them or to allow a situation where someone doesn’t add an event because there is no space left.\u003c/p\u003e\n\n\u003cp\u003eFor stationary session you need:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ewall where you attach plotter paper (it is easier to stick post-its on plotter paper),\u003c/li\u003e\n  \u003cli\u003estickies in different colours, shapes and quantity (will discuss it later),\u003c/li\u003e\n  \u003cli\u003emarkers\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhen it has to be online, you can use a virtual boards such as:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://miro.com/\"\u003emiro\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.mural.co/\"\u003emural\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"approach\"\u003eApproach\u003c/h3\u003e\n\n\u003cp\u003eThere are two approaches to facilitate and that depends on general participants’ understanding of the business process.\u003c/p\u003e\n\n\u003cp\u003eIf your team does not know the domain it is good to conduct a workshop in an exploratory way, because there are a lot\nof unknowns. You can start with adding a central event, or if the domain is large - several events. Then look at\nwhat is happening before and after those events regarding time flow.\u003c/p\u003e\n\n\u003cp\u003eHowever, if your participants are familiar with the system (domain) and the goal is to discover only a part of it, see\nhow something works or immerse into a specific \u003cem\u003euse-case\u003c/em\u003e. You may want to impose certain boundaries - e.g. by\ninitial and final events.\u003c/p\u003e\n\n\u003cp\u003eDepending on what you want to achieve and how deep you want to explore your business, we can distinguish three possible formats:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBig Picture EventStorming - when you want to look at your business from above (\u003cem\u003ea helicopter view\u003c/em\u003e),\u003c/li\u003e\n  \u003cli\u003eProcess Level EventStorming - going deeper with details but you still focus on whole view,\u003c/li\u003e\n  \u003cli\u003eDesign Level EventStorming - you break down your current process into smaller areas and then model them step by step using DDD, CQRS and/or Event Sourcing.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn his book Alberto Brandolini is mentioning also other formats, however, I would like to narrow the scope for the most\nimportant ones. In this article I focus on the \u003cem\u003eBig Picture\u003c/em\u003e approach as it is the first and crucial step to start exploring our business.\u003c/p\u003e\n\n\u003ch2 id=\"building-blocks\"\u003eBuilding blocks\u003c/h2\u003e\n\n\u003cp\u003eI focus here on the main building blocks without going into details. A comprehensive description can be found in the\nbook mentioned earlier.\u003c/p\u003e\n\n\u003ch3 id=\"invite-the-right-people---business-ux-it\"\u003eInvite the right people - business, UX, IT\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003ebut how do you describe the right people?\u003c/em\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ethose who have questions\n    \u003cul\u003e\n      \u003cli\u003edevelopers, architects, designers etc.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eand those who know the answers\n    \u003cul\u003e\n      \u003cli\u003eyou will need people that care about the problem\u003c/li\u003e\n      \u003cli\u003epeople who know the business. Try to gather people who know and understand it. Don’t confuse them with users —\npeople who are using our business/system (I mean these two words interchangeably and will use \u003cem\u003ebusiness\u003c/em\u003e across the\narticle) — these two are totally opposite.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"orange-sticky-note\"\u003eOrange sticky note\u003c/h3\u003e\n\n\u003cp\u003eOn which we will write down our events in the following form:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eVerb in past tense\u003c/em\u003e to indicate that it already happened\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cem\u003eRelevant for domain experts\u003c/em\u003e - describing specific and pertinent events or changes in our business - these\nare changes that at the end of the day we want to save in the database.\u003c/p\u003e\n\n    \u003cp\u003e\u003cimg src=\"/img/articles/2022-07-19-eventstorming/image1.png\" alt=\"domain event\" /\u003e\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: It is a good practice to define the concept of an event together (with participants) at the beginning of the\nworkshop. Then we can verify our definition with events that are appearing on the wall.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eFor example:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-07-19-eventstorming/image5.png\" alt=\"example of events\" /\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ewe have verbs in past tense,\u003c/li\u003e\n  \u003cli\u003ethey are all relevant changes in our \u003cem\u003eblog business\u003c/em\u003e,\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"phases-of-big-picture-eventstorming-workshop\"\u003ePhases of Big Picture EventStorming workshop\u003c/h2\u003e\n\n\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\n\u003cp\u003eIt is good to start the workshop with a short introduction of all participants - but it has to be rather quick before\neverybody gets bored. Generally you can omit this step and ask only new participants to introduce themselves.\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003eWe are going to explore the business process as a whole by placing all the relevant events along a timeline. We will\nhighlight ideas, risks and opportunities along the way.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWhat is necessary - we need to set a goal. What will we model? Say “What is our goal? What we will model?” and try to\nanalyse.\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: Remember - EventStorming is not a goal by itself - it is only a tool / framework.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"because-the-big-picture-is-all-about-events\"\u003eBecause the Big Picture is all about events\u003c/h3\u003e\n\n\u003cp\u003eProvide participants with an idea of a domain event, why it is important and that it has to be a relevant change in our\nsystem. Imagine you do not have a computer and by the end of the day every event in our system needs to be written\ndown in a notebook, by hand. Is the event ‘offer was shown’ a relevant change you want or is it worth noting?\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: A good ice-breaker is also demonstrating how to peel the sticky note so it would not curl\nup… \u003ca href=\"https://www.youtube.com/watch?v=rPHLxOLuyLY\"\u003efor example here\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"phase-1-chaotic-exploration-brain-dump\"\u003ePhase 1 Chaotic exploration (brain dump)\u003c/h2\u003e\n\n\u003ch3 id=\"what-is-happening\"\u003eWhat is happening\u003c/h3\u003e\n\n\u003cp\u003eAll participants are using orange sticky notes, writing down events and putting them on the board. When events start\nappearing on the board, a discussion will naturally start about what kind of events they are, when they are happening or\nhow or what is triggering them.\u003c/p\u003e\n\n\u003ch3 id=\"your-role-as-a-facilitator\"\u003eYour role as a facilitator\u003c/h3\u003e\n\n\u003cp\u003eExplain that we treat our whole board as a timeline and time is passing from left to right - it helps to see what is\nhappening before and after. Sometimes it is worth showing the importance of time in an example:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ea locker was opened,\u003c/li\u003e\n  \u003cli\u003ea package was taken out,\u003c/li\u003e\n  \u003cli\u003ethe door was closed.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn a different order it does not make sense.\u003c/p\u003e\n\n\u003cp\u003eYour role as a facilitator is to listen and observe - how fast new stickies are appearing, where discussion is taking\nplace (try to capture events people are arguing about). Encourage the team to try to identify as many events as possible.\nIf somebody is wrong, it’s okay and others will correct.\u003c/p\u003e\n\n\u003cp\u003eWhen someone is mentioning some mysterious term, capture its definition. As a facilitator you can and you should ask\nobvious questions as it takes the burden off the other participants.\u003c/p\u003e\n\n\u003cp\u003eThis is also a phase where divergent thinking takes place as a part of \u003cem\u003echaotic exploration\u003c/em\u003e. So on the board we have a lot\nof events (ideas). Some of them are better and some are worse but we do not judge them at this point — later we will see\nwhere they lead us. Once again you should encourage the participants to generate new ideas and set aside critical thinking and judgement.\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTips\u003c/strong\u003e:\nAs an icebreaker you can place the first event or events - to show how easy it is, and help draw participants into\nworkshops.\u003c/p\u003e\n\n  \u003cp\u003eTry to eliminate actors from the events - because we don’t want to impose mental boundaries as we may not notice that\nthere is some other case. For example instead of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBuyer added item to cart\u003c/code\u003e use \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eItem added to cart\u003c/code\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"how-to-manage-people\"\u003eHow to manage people\u003c/h3\u003e\n\n\u003cp\u003eSometimes it is a good idea to divide them into smaller groups and make them work together on the same issue\nor, quite the opposite, to focus on different areas of the system.\u003c/p\u003e\n\n\u003cp\u003eDepending on whether we are exploring or modelling the process, especially during online sessions, I think it is good to\nhave boundaries — like a start event and an end event — among which everybody can create their vision. Then\nthe most difficult part is to merge it. Another approach is to give a free hand to your participants and see how the process is going to develop.\u003c/p\u003e\n\n\u003ch3 id=\"how-long-should-it-take\"\u003eHow long should it take?\u003c/h3\u003e\n\n\u003cp\u003eWhen the speed of new events showing up dramatically slows down, it is a good time to proceed to the next phase.\nUsually chaotic exploration takes about 5 to 15 minutes, but I have noticed that after about 8 minutes people are getting\nbored and busy with other things. So especially during online meetings, when you do not control the environment (like\ncomputers, phones, chat, mails…) it is easy to lose attention. And if you add to it a \u003cem\u003ezoom fatigue\u003c/em\u003e syndrome, you can\nspoil the whole session when key participants leave.\u003c/p\u003e\n\n\u003ch2 id=\"phase-2-timeline\"\u003ePhase 2 Timeline\u003c/h2\u003e\n\n\u003cp\u003eAfter the divergent step, now it is the time for the emergent phase where we want to explore and experiment - this is what\nwe will be doing during the next phases.\u003c/p\u003e\n\n\u003ch3 id=\"what-is-happening-1\"\u003eWhat is happening\u003c/h3\u003e\n\n\u003cp\u003eNow our goal is to make sure we are actually following the timeline - we would like the flow of events to be consistent\nfrom the beginning to the end.\u003c/p\u003e\n\n\u003ch3 id=\"your-role-as-a-facilitator-1\"\u003eYour role as a facilitator:\u003c/h3\u003e\n\n\u003cp\u003eA lot of events are going to change their place, also participants will find them irrelevant or duplicated and that is\nokay. Remove the duplicates, but be careful — ask if those duplicated events mean the same thing for everybody! Do\nnot hesitate to add, remove or change some sticky notes on the board.\u003c/p\u003e\n\n\u003cp\u003eAt this step some issue points may appear, so it is good to mark them as \u003cstrong\u003ehotspots\u003c/strong\u003e. Use red sticky notes and\nwrite the issue down but this is not a good time to deliberate about it now. Try to postpone this discussion until we have\nstructured the whole process.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-07-19-eventstorming/image2.png\" alt=\"hot-spot\" /\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: During the online session when everybody is working solo, it is hard to merge all events and, including\nattention problems, you may be left alone. So my solution is to introduce the next phase right now.\u003c/p\u003e\n\n  \u003cp\u003eDepending on the team - you can pick some random person who is going to start creating a timeline based\non available events. To sustain attention, replace this person with another one. In case of inconsistencies with the timeline,\nwe complete it with the missing events.\u003c/p\u003e\n\n  \u003cp\u003eHowever, you can do all of it — if among participants there are some shy people or your participants’ supervisor is in\nthe room, when you tell the story you can make intentional errors or ask silly questions. All of this eventually will\nhelp to explore the domain.\u003c/p\u003e\n\n  \u003cp\u003eBecause as a facilitator you do not have to know everything — especially the domain or business your participants are\nexploring — you help them effectively and safely discover processes, find new solutions or define problems.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"phase-3-explicit-walk-through-and-reverse-narrative\"\u003ePhase 3 Explicit walk-through and reverse narrative\u003c/h2\u003e\n\n\u003ch3 id=\"what-is-happening-2\"\u003eWhat is happening:\u003c/h3\u003e\n\n\u003cp\u003eNext step is to do a walk-through by creating some sort of a story that can be told based on the events placed on the board.\nDuring this step a lot of discussions (arguments) are going to take place. Maybe some events are missing, so do not hesitate to add,\nremove or change some sticky notes on the board. We should focus on the happy path in the first place.\u003c/p\u003e\n\n\u003ch3 id=\"1-explicit-walk-through\"\u003e1. Explicit walk-through\u003c/h3\u003e\n\n\u003ch3 id=\"your-role-as-a-facilitator-2\"\u003eYour role as a facilitator:\u003c/h3\u003e\n\n\u003cp\u003ePick some random person who is going to start telling a story based on available events according to timeflow (from left\nto right). Sometimes the team gets blocked. In this situation you can add or move an event and place it in an obviously\nwrong place. Your error will be fixed quickly and help the team to move on.\u003c/p\u003e\n\n\u003ch3 id=\"how-to-help-the-participants-discover-more\"\u003eHow to help the participants discover more?\u003c/h3\u003e\n\n\u003cp\u003eThe answer is simple — by asking questions. There are some useful questions that you can ask when discussing\nalmost every event, e.g.:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eWhy did this event happen?\u003c/li\u003e\n  \u003cli\u003eWhat are the consequences of this event?\u003c/li\u003e\n  \u003cli\u003eWhat has to / needs to happen next?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eGoing deeper (of course that depends on how deep you want to go)\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eWhat, how, when, why is it changing?\u003c/li\u003e\n  \u003cli\u003eWhen it can’t change?\u003c/li\u003e\n  \u003cli\u003eHow does this affect the business?\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: Also in this phase it can be convenient to introduce actors (phase 4 - people and systems) — if it\nhelps to tell a story or better understand the process do not hesitate (remember I told you that EventStorming is a tool?)\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"2-reverse-narrative\"\u003e2. Reverse narrative\u003c/h3\u003e\n\n\u003ch3 id=\"your-role-as-a-facilitator-3\"\u003eYour role as a facilitator:\u003c/h3\u003e\n\n\u003cp\u003eSometimes it is good to propose a reverse narrative / reverse chronology. Pick an event from the end of the flow and\nlook for the event that made it possible - it must be consistent - no magic gaps between events. Again if we miss some\nevents - add them.\u003c/p\u003e\n\n\u003cp\u003eSome questions you can ask:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBefore\n    \u003cul\u003e\n      \u003cli\u003e\u003cem\u003eWhat has happened before X\u003c/em\u003e\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eWhat else has to happen for X to happen\u003c/em\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eBetween - we take two corresponding events\n    \u003cul\u003e\n      \u003cli\u003e\u003cem\u003eIs there anything else happening between X and Y\u003c/em\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eAlternative - ask about alternative events\n    \u003cul\u003e\n      \u003cli\u003e\u003cem\u003eWhat if X did not happen\u003c/em\u003e\u003c/li\u003e\n      \u003cli\u003e\u003cem\u003eWhat if 10% of X happened or 150% of X happened\u003c/em\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"phase-4-people-and-systems\"\u003ePhase 4 People and systems\u003c/h2\u003e\n\n\u003ch3 id=\"what-is-happening-3\"\u003eWhat is happening\u003c/h3\u003e\n\n\u003cp\u003eWhen we finish enforcing the timeline and we have a consistent flow of our business we can add people and external\nsystems. We need them for clarity and better understanding of events and forces governing our business.\u003c/p\u003e\n\n\u003cp\u003eFor marking people we use a yellow sticky note with a symbolic drawing of a person or clock if we want to show that time\nmatters. External systems may be represented by large pink stickies with their names on it. By an external system I mean\na piece of the whole process which is beyond our control e.g. an application, a department, other companies.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-07-19-eventstorming/image3.png\" alt=\"actor\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-07-19-eventstorming/image4.png\" alt=\"system\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"who-is-an-actor\"\u003eWho is an actor?\u003c/h3\u003e\n\n\u003cp\u003eIn his book Alberto Brandolini explains that\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003eThe goal is not to match the right yellow sticky note to every event in the flow. Adding significant people adds more\nclarity, but the goal is to trigger some insightful conversation: wherever the behaviour depends on a different type\nof user, wherever special actions need to be taken, and so on.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe lack of precision is helping in discussion and exploration. It can be a specific person for example:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003ein our business model only Mrs. Smith can issue an invoice\u003c/em\u003e.\u003c/li\u003e\n  \u003cli\u003eor \u003cem\u003eafter some time reservation is cancelled\u003c/em\u003e so even \u003cem\u003etime\u003c/em\u003e can be an actor.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eAnother example:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eorder cancellation\u003c/em\u003e can have two actors: client and CEX worker.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"phase-5-opportunities-and-risks\"\u003ePhase 5 Opportunities and risks\u003c/h2\u003e\n\n\u003cp\u003eIn this phase we can literally take three steps back and look at the whole business flow as it is.\n\u003cstrong\u003eHot spots\u003c/strong\u003e are the most conspicuous things - and it is easy to say where the biggest impediment is. This\nis a great occasion for additional discussion and a subject for further exploration.\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003eTip\u003c/strong\u003e: Remember that each \u003cem\u003ehot spot\u003c/em\u003e should be addressed and resolved before the next session takes place.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAnother way to find where problems might lay is voting for a specific event or marking events that indicate where in our\nflow we are generating / losing money or value. For example by green stripes we indicate events where we are earning money,\nby red stripes where we are losing money or value.\u003c/p\u003e\n\n\u003ch2 id=\"it-is-like-pizzas\"\u003eIt is like Pizzas\u003c/h2\u003e\n\n\u003cp\u003eWhen all hotspots are addressed, you have found the biggest impediment, or you know on what part you have to focus on\nduring next session. The only thing left to do is to close the workshop, thank all stakeholders and participants,\nschedule the next session and ask for feedback.\u003c/p\u003e\n\n\u003cp\u003eAfter the session you will have a clear business narrative on the board. What is more important, participants will\nshare general understanding of the process. They have gone through the massive learning process, gained experience and\nshared the common knowledge — everybody uses the domain language. Due to the fact that we used simple building\nblocks, the outcome is understandable to everyone PMs, UX designers, developers etc.\u003c/p\u003e\n\n\u003cp\u003eThe steps described above and their sequence should be regarded as optional during the session.\nThere is no such thing as one recipe. For example, if during the \u003cem\u003etimeline step\u003c/em\u003e you feel that introduction of\npeople and systems is going to help, do not hesitate to do so. In other cases you will be interested only in\nfinding impediments or where your system is delivering values and do not feel obligated to use all the steps.\nAs Alberto says:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003eI like to think about it like Pizzas: there’s a common base, but different toppings.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"nobody-is-excluded\"\u003eNobody is excluded\u003c/h3\u003e\n\n\u003cp\u003eBig Picture EventStorming is the first and crucial step, its outcome is visible and valuable. Depending on what the team\nneeds, it can be sufficient, but if we want to go deeper and explore more, next there are Process Level and Design Level\nEventStorming. We use the same stickies’ grammar enhanced with more colours to explain the complexity of our system. Due\nto the fact that we use the same grammar, developers and businesses can speak the same language — nobody is\nexcluded, isn’t that great?\u003c/p\u003e\n\n\u003cp\u003eThose further steps (Process/Design Level) are getting us closer into the domain-driven-design and implementation. We (\ndevelopers/architects) can start thinking how to change what we have learned into working code, because\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003e(…) it’s developer understanding that gets captured in code and released in production.\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"call-for-action\"\u003eCall for action\u003c/h2\u003e\n\n\u003cp\u003eIf you are Allegro worker and you are interested in EventStorming, you want to develop, participate in workshops\nor help as a facilitator I strongly encourage you to join the guild.\nIf you are not yet working at Allegro but are interested in how we use EventStorming maybe it is good opportunity to\njoin\nus — \u003ca href=\"https://www.linkedin.com/company/allegro-pl/life/team\"\u003e#goodtobehere\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"more-about-eventstroming\"\u003eMore about EventStroming\u003c/h2\u003e\n\n\u003cp\u003eOn the Internet you can find a lot of materials about EventStorming. Below is a list of those I found most valuable.\u003c/p\u003e\n\n\u003ch3 id=\"books\"\u003eBooks\u003c/h3\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://leanpub.com/introducing_eventstorming\"\u003eIntroducing EventStorming\u003c/a\u003e Alberto Brandolini’s book —\n\u003cem\u003eEventStorming Bible\u003c/em\u003e — mandatory book!\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://leanpub.com/eventstorming_handbook\"\u003eThe EventStorming Handbook\u003c/a\u003e by Paul Rayner — a great summary of\n\u003cem\u003eIntroducing EventStorming\u003c/em\u003e with a lot of valuable tips, tricks and recipes. After that you will be able to explain\nEventStorming even to your own child.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://gamestorming.com/\"\u003eGameStorming A Playbook for Innovators, Rulebreakers, and Changemakers\u003c/a\u003e by Dave Gray,\nSunni Brown, James Macanufo — if you want to use the full potential of your storming sessions.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://allegro.pl/oferta/facilitator-s-guide-to-participatory-decision-maki-10017700512\"\u003eFacilitator’s Guide to Participatory Decision-making\u003c/a\u003e\nby Sam Kaner, Lenny Lind — how to be a better facilitator, not only for EventStorming. You will find precious\ninformation about divergent, emergent and convergent thinking and why it is important.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"link\"\u003eLink\u003c/h3\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/mariuszgil/awesome-eventstorming\"\u003eAwesome EventStorming\u003c/a\u003e by Mariusz Gil — I belive this is\nthe biggest source of links about EventStorming topics.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"thanks\"\u003eThanks\u003c/h2\u003e\n\n\u003cp\u003eI would like to thank all of my colleagues from \u003cem\u003eAllegro EventStorming Guild\u003c/em\u003e for their help in creating this article.\u003c/p\u003e\n","contentSnippet":"With this article, I would like to introduce you to EventStorming and explain to you how to get started. I am not discovering\nanything new, just gathering available knowledge in one place. What I will show you is a few tips on how to conduct\nand facilitate EventStorming workshops.\nGuide to Big Picture EventStorming\nIntroducing EventStorming\nIn 2013 Alberto Brandolini posted an article\nabout a new workshop format for quick exploration of complex business domains. It was warmly welcomed by the DDD community.\nIn 2015 Technology Radar described EventStorming as worthy of attention\nand three years later as a recommended method for business domain modelling in information systems.\nDuring the years a lot has changed, the technique has developed and matured but the main idea remained the same:\nEventStorming is a flexible workshop format that allows a massive collaborative exploration of complex domains (…)\nwhere software and business practitioners are building together a behavioural model of the whole business line.\nThe above definition is from Alberto Brandolini’s Introducing EventStorming,\nto which I will be referring in this article.\nBefore launching\nProvide unlimited modelling space\nWhy is it important? Because you want participants to explore and experiment during the workshop. You don’t want to\nimpose limits on them or to allow a situation where someone doesn’t add an event because there is no space left.\nFor stationary session you need:\nwall where you attach plotter paper (it is easier to stick post-its on plotter paper),\nstickies in different colours, shapes and quantity (will discuss it later),\nmarkers\nWhen it has to be online, you can use a virtual boards such as:\nmiro\nmural\nApproach\nThere are two approaches to facilitate and that depends on general participants’ understanding of the business process.\nIf your team does not know the domain it is good to conduct a workshop in an exploratory way, because there are a lot\nof unknowns. You can start with adding a central event, or if the domain is large - several events. Then look at\nwhat is happening before and after those events regarding time flow.\nHowever, if your participants are familiar with the system (domain) and the goal is to discover only a part of it, see\nhow something works or immerse into a specific use-case. You may want to impose certain boundaries - e.g. by\ninitial and final events.\nDepending on what you want to achieve and how deep you want to explore your business, we can distinguish three possible formats:\nBig Picture EventStorming - when you want to look at your business from above (a helicopter view),\nProcess Level EventStorming - going deeper with details but you still focus on whole view,\nDesign Level EventStorming - you break down your current process into smaller areas and then model them step by step using DDD, CQRS and/or Event Sourcing.\nIn his book Alberto Brandolini is mentioning also other formats, however, I would like to narrow the scope for the most\nimportant ones. In this article I focus on the Big Picture approach as it is the first and crucial step to start exploring our business.\nBuilding blocks\nI focus here on the main building blocks without going into details. A comprehensive description can be found in the\nbook mentioned earlier.\nInvite the right people - business, UX, IT\nbut how do you describe the right people?\nthose who have questions\n    \ndevelopers, architects, designers etc.\nand those who know the answers\n    \nyou will need people that care about the problem\npeople who know the business. Try to gather people who know and understand it. Don’t confuse them with users —\npeople who are using our business/system (I mean these two words interchangeably and will use business across the\narticle) — these two are totally opposite.\nOrange sticky note\nOn which we will write down our events in the following form:\nVerb in past tense to indicate that it already happened\nRelevant for domain experts - describing specific and pertinent events or changes in our business - these\nare changes that at the end of the day we want to save in the database.\n\nTip: It is a good practice to define the concept of an event together (with participants) at the beginning of the\nworkshop. Then we can verify our definition with events that are appearing on the wall.\nFor example:\n\nwe have verbs in past tense,\nthey are all relevant changes in our blog business,\nPhases of Big Picture EventStorming workshop\nIntroduction\nIt is good to start the workshop with a short introduction of all participants - but it has to be rather quick before\neverybody gets bored. Generally you can omit this step and ask only new participants to introduce themselves.\nWe are going to explore the business process as a whole by placing all the relevant events along a timeline. We will\nhighlight ideas, risks and opportunities along the way.\nWhat is necessary - we need to set a goal. What will we model? Say “What is our goal? What we will model?” and try to\nanalyse.\nTip: Remember - EventStorming is not a goal by itself - it is only a tool / framework.\nBecause the Big Picture is all about events\nProvide participants with an idea of a domain event, why it is important and that it has to be a relevant change in our\nsystem. Imagine you do not have a computer and by the end of the day every event in our system needs to be written\ndown in a notebook, by hand. Is the event ‘offer was shown’ a relevant change you want or is it worth noting?\nTip: A good ice-breaker is also demonstrating how to peel the sticky note so it would not curl\nup… for example here.\nPhase 1 Chaotic exploration (brain dump)\nWhat is happening\nAll participants are using orange sticky notes, writing down events and putting them on the board. When events start\nappearing on the board, a discussion will naturally start about what kind of events they are, when they are happening or\nhow or what is triggering them.\nYour role as a facilitator\nExplain that we treat our whole board as a timeline and time is passing from left to right - it helps to see what is\nhappening before and after. Sometimes it is worth showing the importance of time in an example:\na locker was opened,\na package was taken out,\nthe door was closed.\nIn a different order it does not make sense.\nYour role as a facilitator is to listen and observe - how fast new stickies are appearing, where discussion is taking\nplace (try to capture events people are arguing about). Encourage the team to try to identify as many events as possible.\nIf somebody is wrong, it’s okay and others will correct.\nWhen someone is mentioning some mysterious term, capture its definition. As a facilitator you can and you should ask\nobvious questions as it takes the burden off the other participants.\nThis is also a phase where divergent thinking takes place as a part of chaotic exploration. So on the board we have a lot\nof events (ideas). Some of them are better and some are worse but we do not judge them at this point — later we will see\nwhere they lead us. Once again you should encourage the participants to generate new ideas and set aside critical thinking and judgement.\nTips:\nAs an icebreaker you can place the first event or events - to show how easy it is, and help draw participants into\nworkshops.\nTry to eliminate actors from the events - because we don’t want to impose mental boundaries as we may not notice that\nthere is some other case. For example instead of Buyer added item to cart use Item added to cart.\nHow to manage people\nSometimes it is a good idea to divide them into smaller groups and make them work together on the same issue\nor, quite the opposite, to focus on different areas of the system.\nDepending on whether we are exploring or modelling the process, especially during online sessions, I think it is good to\nhave boundaries — like a start event and an end event — among which everybody can create their vision. Then\nthe most difficult part is to merge it. Another approach is to give a free hand to your participants and see how the process is going to develop.\nHow long should it take?\nWhen the speed of new events showing up dramatically slows down, it is a good time to proceed to the next phase.\nUsually chaotic exploration takes about 5 to 15 minutes, but I have noticed that after about 8 minutes people are getting\nbored and busy with other things. So especially during online meetings, when you do not control the environment (like\ncomputers, phones, chat, mails…) it is easy to lose attention. And if you add to it a zoom fatigue syndrome, you can\nspoil the whole session when key participants leave.\nPhase 2 Timeline\nAfter the divergent step, now it is the time for the emergent phase where we want to explore and experiment - this is what\nwe will be doing during the next phases.\nWhat is happening\nNow our goal is to make sure we are actually following the timeline - we would like the flow of events to be consistent\nfrom the beginning to the end.\nYour role as a facilitator:\nA lot of events are going to change their place, also participants will find them irrelevant or duplicated and that is\nokay. Remove the duplicates, but be careful — ask if those duplicated events mean the same thing for everybody! Do\nnot hesitate to add, remove or change some sticky notes on the board.\nAt this step some issue points may appear, so it is good to mark them as hotspots. Use red sticky notes and\nwrite the issue down but this is not a good time to deliberate about it now. Try to postpone this discussion until we have\nstructured the whole process.\n\nTip: During the online session when everybody is working solo, it is hard to merge all events and, including\nattention problems, you may be left alone. So my solution is to introduce the next phase right now.\nDepending on the team - you can pick some random person who is going to start creating a timeline based\non available events. To sustain attention, replace this person with another one. In case of inconsistencies with the timeline,\nwe complete it with the missing events.\nHowever, you can do all of it — if among participants there are some shy people or your participants’ supervisor is in\nthe room, when you tell the story you can make intentional errors or ask silly questions. All of this eventually will\nhelp to explore the domain.\nBecause as a facilitator you do not have to know everything — especially the domain or business your participants are\nexploring — you help them effectively and safely discover processes, find new solutions or define problems.\nPhase 3 Explicit walk-through and reverse narrative\nWhat is happening:\nNext step is to do a walk-through by creating some sort of a story that can be told based on the events placed on the board.\nDuring this step a lot of discussions (arguments) are going to take place. Maybe some events are missing, so do not hesitate to add,\nremove or change some sticky notes on the board. We should focus on the happy path in the first place.\n1. Explicit walk-through\nYour role as a facilitator:\nPick some random person who is going to start telling a story based on available events according to timeflow (from left\nto right). Sometimes the team gets blocked. In this situation you can add or move an event and place it in an obviously\nwrong place. Your error will be fixed quickly and help the team to move on.\nHow to help the participants discover more?\nThe answer is simple — by asking questions. There are some useful questions that you can ask when discussing\nalmost every event, e.g.:\nWhy did this event happen?\nWhat are the consequences of this event?\nWhat has to / needs to happen next?\nGoing deeper (of course that depends on how deep you want to go)\nWhat, how, when, why is it changing?\nWhen it can’t change?\nHow does this affect the business?\nTip: Also in this phase it can be convenient to introduce actors (phase 4 - people and systems) — if it\nhelps to tell a story or better understand the process do not hesitate (remember I told you that EventStorming is a tool?)\n2. Reverse narrative\nYour role as a facilitator:\nSometimes it is good to propose a reverse narrative / reverse chronology. Pick an event from the end of the flow and\nlook for the event that made it possible - it must be consistent - no magic gaps between events. Again if we miss some\nevents - add them.\nSome questions you can ask:\nBefore\n    \nWhat has happened before X\nWhat else has to happen for X to happen\nBetween - we take two corresponding events\n    \nIs there anything else happening between X and Y\nAlternative - ask about alternative events\n    \nWhat if X did not happen\nWhat if 10% of X happened or 150% of X happened\nPhase 4 People and systems\nWhat is happening\nWhen we finish enforcing the timeline and we have a consistent flow of our business we can add people and external\nsystems. We need them for clarity and better understanding of events and forces governing our business.\nFor marking people we use a yellow sticky note with a symbolic drawing of a person or clock if we want to show that time\nmatters. External systems may be represented by large pink stickies with their names on it. By an external system I mean\na piece of the whole process which is beyond our control e.g. an application, a department, other companies.\n\n\nWho is an actor?\nIn his book Alberto Brandolini explains that\nThe goal is not to match the right yellow sticky note to every event in the flow. Adding significant people adds more\nclarity, but the goal is to trigger some insightful conversation: wherever the behaviour depends on a different type\nof user, wherever special actions need to be taken, and so on.\nThe lack of precision is helping in discussion and exploration. It can be a specific person for example:\nin our business model only Mrs. Smith can issue an invoice.\nor after some time reservation is cancelled so even time can be an actor.\nAnother example:\norder cancellation can have two actors: client and CEX worker.\nPhase 5 Opportunities and risks\nIn this phase we can literally take three steps back and look at the whole business flow as it is.\nHot spots are the most conspicuous things - and it is easy to say where the biggest impediment is. This\nis a great occasion for additional discussion and a subject for further exploration.\nTip: Remember that each hot spot should be addressed and resolved before the next session takes place.\nAnother way to find where problems might lay is voting for a specific event or marking events that indicate where in our\nflow we are generating / losing money or value. For example by green stripes we indicate events where we are earning money,\nby red stripes where we are losing money or value.\nIt is like Pizzas\nWhen all hotspots are addressed, you have found the biggest impediment, or you know on what part you have to focus on\nduring next session. The only thing left to do is to close the workshop, thank all stakeholders and participants,\nschedule the next session and ask for feedback.\nAfter the session you will have a clear business narrative on the board. What is more important, participants will\nshare general understanding of the process. They have gone through the massive learning process, gained experience and\nshared the common knowledge — everybody uses the domain language. Due to the fact that we used simple building\nblocks, the outcome is understandable to everyone PMs, UX designers, developers etc.\nThe steps described above and their sequence should be regarded as optional during the session.\nThere is no such thing as one recipe. For example, if during the timeline step you feel that introduction of\npeople and systems is going to help, do not hesitate to do so. In other cases you will be interested only in\nfinding impediments or where your system is delivering values and do not feel obligated to use all the steps.\nAs Alberto says:\nI like to think about it like Pizzas: there’s a common base, but different toppings.\nNobody is excluded\nBig Picture EventStorming is the first and crucial step, its outcome is visible and valuable. Depending on what the team\nneeds, it can be sufficient, but if we want to go deeper and explore more, next there are Process Level and Design Level\nEventStorming. We use the same stickies’ grammar enhanced with more colours to explain the complexity of our system. Due\nto the fact that we use the same grammar, developers and businesses can speak the same language — nobody is\nexcluded, isn’t that great?\nThose further steps (Process/Design Level) are getting us closer into the domain-driven-design and implementation. We (\ndevelopers/architects) can start thinking how to change what we have learned into working code, because\n(…) it’s developer understanding that gets captured in code and released in production.\nCall for action\nIf you are Allegro worker and you are interested in EventStorming, you want to develop, participate in workshops\nor help as a facilitator I strongly encourage you to join the guild.\nIf you are not yet working at Allegro but are interested in how we use EventStorming maybe it is good opportunity to\njoin\nus — #goodtobehere.\nMore about EventStroming\nOn the Internet you can find a lot of materials about EventStorming. Below is a list of those I found most valuable.\nBooks\nIntroducing EventStorming Alberto Brandolini’s book —\nEventStorming Bible — mandatory book!\nThe EventStorming Handbook by Paul Rayner — a great summary of\nIntroducing EventStorming with a lot of valuable tips, tricks and recipes. After that you will be able to explain\nEventStorming even to your own child.\nGameStorming A Playbook for Innovators, Rulebreakers, and Changemakers by Dave Gray,\nSunni Brown, James Macanufo — if you want to use the full potential of your storming sessions.\nFacilitator’s Guide to Participatory Decision-making\nby Sam Kaner, Lenny Lind — how to be a better facilitator, not only for EventStorming. You will find precious\ninformation about divergent, emergent and convergent thinking and why it is important.\nLink\nAwesome EventStorming by Mariusz Gil — I belive this is\nthe biggest source of links about EventStorming topics.\nThanks\nI would like to thank all of my colleagues from Allegro EventStorming Guild for their help in creating this article.","guid":"https://blog.allegro.tech/2022/07/event-storming-workshops.html","categories":["eventstorming","tech","communication"],"isoDate":"2022-07-18T22:00:00.000Z","thumbnail":"images/post-headers/eventstorming.png"},{"title":"GC, hands off my data!","link":"https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html","pubDate":"Thu, 30 Jun 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"\u003cp\u003eCertainly one of the main distinguishing features of the Java world is the Garbage Collector.\nUsing it is safe and convenient, it allows us to forget about many tedious responsibilities, letting us focus on the\npure joy of coding. Yet sometimes it can cause a headache too, especially when we notice that GC uses our resources\ntoo intensively. Each of us has probably experienced a time in our career when we wanted to get\nrid of the Garbage Collector from our application because it was running too long, too often, and perhaps even led to temporary system freezes.\u003c/p\u003e\n\n\u003cp\u003eWhat if we could still benefit from the GC, but in special cases, also be able to store data beyond its control? We\ncould still take advantage of its convenience and, at the same time, be able to easily get rid of long GC pauses.\u003c/p\u003e\n\n\u003cp\u003eIt turns out that it is possible. In this article, we will look at whether and when it is worth storing data\nbeyond the reach of the Garbage Collector’s greedy hands.\u003c/p\u003e\n\n\u003ch2 id=\"comfort-comes-at-a-price\"\u003eComfort comes at a price\u003c/h2\u003e\n\n\u003cp\u003eAt \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e we are very keen on metrics. We measure anything that can tell us something about the condition of\nour services. Apart from the most obvious metrics directly related to the application, such as throughput, the number of\nerrors, CPU and memory usage, we also pay a great deal of attention to metrics related to the garbage collecting — GC working\ntime and number of its cycles. Too much time spent on releasing the memory or too frequent GC launches may signal problems with\nmemory leaks or indicate that it is worth considering optimising memory usage or switching to a different GC strategy.\u003c/p\u003e\n\n\u003cp\u003eFollowing the example of large technology companies, we have been organising company meetups within the so-called guilds\nfor some time now. In one of such guilds, over a hundred engineers meet regularly once a month and discuss various\ntopics related to performance, scaling and service optimisation. At one of these meetings, our colleague\ndiscussed the method of determining the actual size of data stored in a cache. Apparently, this is not a\nsimple matter, as internal mechanisms for optimising memory usage, such as deduplication or compression, must be taken\ninto account. After the presentation, an interesting discussion ensued about how much memory\non the heap is actually used by the cache and how long it takes to clean it up. Someone pointed out that there is a hidden cost of using the cache\nthat takes the form of time needed to free the memory of expired cache items, which not everyone is aware of. What is more, the\nmanner in which the cache works does not quite fit the\n\u003ca href=\"http://insightfullogic.com/2013/Feb/20/garbage-collection-java-1/\"\u003egenerational hypothesis\u003c/a\u003e and may mislead the JVM by preventing it\nfrom properly tuning the GC mechanism. I then began to wonder whether it might not be worth keeping the cache in an area\nexcluded from the GC’s control? I knew this is possible, although I had never seen a practical implementation of this\ntechnique. This topic was bothering me for some time, so I decided to investigate.\u003c/p\u003e\n\n\u003ch2 id=\"memory-architecture\"\u003eMemory architecture\u003c/h2\u003e\n\n\u003cp\u003eAny skilled Java programmer knows the division of memory into young and old generation areas. People interested in\ndetails are probably also familiar with the more precise division into eden, survivor, tenured and perm.\nThere are many excellent articles discussing this topic\n(like \u003ca href=\"https://www.betsol.com/blog/java-memory-management-for-java-virtual-machine-jvm/\"\u003ethis one\u003c/a\u003e), so we won’t go\ninto details. Instead, we will focus on a very specialised area of memory that the GC\nhas no control over, which is the off-heap memory, sometimes also called native memory. This is a special area under the\ndirect control of the operating system, which the JVM uses for its own purposes. It stores information about classes and\nmethods, internal thread data and cached code necessary for operation. As I mentioned earlier, off-heap memory is not\nsubject to the GC. In particular, it is excluded from garbage collection processes, which means that programmers\ncreating the JVM code using this area are wholly responsible for freeing memory allocated for\nvariables. There is also a dedicated area to which we — the programmers — have access as well.\nThere is a possibility to write and read data from this space, remembering of course, that the responsibility\nfor cleaning up after unnecessary variables lies entirely with us.\u003c/p\u003e\n\n\u003cp\u003eThis area can be accessed using a simple API.\nThe following code allocates 100 bytes of off-heap memory and stores a String and an Integer.\nAt the end the data are loaded from the off-heap memory and then printed out.\u003c/p\u003e\n\n\u003cdiv class=\"language-java highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"n\"\u003esize\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e100\u003c/span\u003e\u003cspan class=\"o\"\u003e;\u003c/span\u003e\n\n\u003cspan class=\"nc\"\u003eByteBuffer\u003c/span\u003e \u003cspan class=\"n\"\u003ebuff\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eByteBuffer\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eallocateDirect\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\u003cspan class=\"n\"\u003ebuff\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eput\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"Michal\"\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003egetBytes\u003c/span\u003e\u003cspan class=\"o\"\u003e());\u003c/span\u003e\n\u003cspan class=\"n\"\u003ebuff\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eputInt\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e42\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003ebuff\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eposition\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e \u003cspan class=\"c1\"\u003e// set the pointer back to the beginning\u003c/span\u003e\n\n\u003cspan class=\"kt\"\u003ebyte\u003c/span\u003e\u003cspan class=\"o\"\u003e[]\u003c/span\u003e \u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"kt\"\u003ebyte\u003c/span\u003e\u003cspan class=\"o\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e6\u003c/span\u003e\u003cspan class=\"o\"\u003e];\u003c/span\u003e \u003cspan class=\"c1\"\u003e// length of my name\u003c/span\u003e\n\u003cspan class=\"n\"\u003ebuff\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eget\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e);\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eout\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e));\u003c/span\u003e\n\u003cspan class=\"n\"\u003eout\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eprintln\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ebuff\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003egetInt\u003c/span\u003e\u003cspan class=\"o\"\u003e());\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eNote the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eallocateDirect\u003c/code\u003e method that allocates off-heap memory unlike a similar method: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eallocate\u003c/code\u003e that allocates\non-heap memory. The behavior of both methods can be compared with the help of a profiler\n(I will use \u003ca href=\"https://openjdk.java.net/tools/svc/jconsole/\"\u003ejConsole\u003c/a\u003e). The following programs allocate 1GB of memory,\nrespectively, on-heap and off-heap:\u003c/p\u003e\n\n\u003cdiv class=\"language-java highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nc\"\u003eByteBuffer\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eallocate\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1000000000\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cdiv class=\"language-java highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nc\"\u003eByteBuffer\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"na\"\u003eallocateDirect\u003c/span\u003e\u003cspan class=\"o\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1000000000\u003c/span\u003e\u003cspan class=\"o\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe chart below shows heap memory profile comparison for both programs (on-heap on the left vs. off-heap on the right):\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/compare.png\" alt=\"on-heap vs off-heap\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eSuch a possibility to bypass Garbage Collector may seem extremely tempting to\ndevelopers struggling with long working time of the GC. However, this raises the question: what type of usage justifies\nthe extra effort involved in manually freeing the memory and the potential risk of error? What are the advantages of\nusing off-heap memory? Is it faster? How much time will we save by bypassing the GC? Why is this method so uncommon?\nTo put it simply: is it worth doing and if so, when?\u003c/p\u003e\n\n\u003ch2 id=\"be-gone-gc\"\u003eBe gone GC!\u003c/h2\u003e\n\n\u003cp\u003eGC is a wonderful tool. It allows us – although sometimes only for a while – to forget about the problems related\nto painful memory management. We can create variables of any type and any scope almost freely, and not worry about what\nhappens to memory once we stop using them. This task is handled by the GC, which does it brilliantly. In each successive\nversion of the JDK we get a new algorithm, which in some specific cases is even better than the previous one.\u003c/p\u003e\n\n\u003cp\u003eHowever, I’m more than sure that many of us have once encountered the problem of long GC time or too frequent GC\ncalls. Every developer has their own ideas on how to deal with this issue - we look for memory leaks, profile the\napplication in search of hot spots, examine the scope of created variables, use object pools, verify the system\nbehaviour with different GC algorithms, and check the cache configuration.\u003c/p\u003e\n\n\u003cp\u003eIn my case, it is the cache that is often responsible for long GC time. Sometimes it stores large numbers of objects, usually\ncomplex ones, containing references to other objects. What is more, the way cache objects are accessed is often not\nuniform. Some objects are never queried after being inserted into the cache, others are read throughout their whole\nlifecycle. This causes the cache to disrupt the somewhat ideal world order defined by the generational hypothesis. Then,\nGC algorithms are faced with a very difficult task of determining the optimal way to clean up the memory freed by the\nitems removed from the cache. All this causes the cache cleanup to be expensive. This made me wonder if there was\nany benefit in storing cache data outside the heap?\u003c/p\u003e\n\n\u003ch2 id=\"off-heap-space-pros-and-cons\"\u003eOff-heap space: Pros and cons\u003c/h2\u003e\n\n\u003cp\u003eIn a sense, the off-heap space lies outside the control of the JVM (though it belongs to the Java process),\nand for this reason, it is not possible to write\ncomplex structures used in JVM languages into it. This raises the need for an intermediate step of serializing the\ndata into a plain byte array, which can then be stored in the off-heap area. When the data is loaded, the reverse\nprocess must be performed: deserialization into a form that we can use in Java. These additional steps will of\ncourse come at an extra cost, which is why accessing off-heap data will, for obvious reasons, take longer than accessing\non-heap data directly.\u003c/p\u003e\n\n\u003cp\u003eSince writing and reading data in the off-heap space takes longer, what is the benefit of this approach then? Well, the data\nstored in the off-heap space are not subject to GC processes, so on the one hand we – the programmers – are responsible\nfor each freeing of memory after a given variable is no longer useful. On the other hand, we relieve the management\nprocesses in the JVM by releasing CPU’s time for the rest of the application, so, theoretically, it should\nresult in some resource savings. The question is, do these differences balance each other out to any degree? Will the savings\nassociated with the GC process balance out our longer data access time? If so, does it depend only on the amount of\ndata, or is there a specific usage scenario? To answer these questions, it is necessary to run a few experiments.\u003c/p\u003e\n\n\u003ch2 id=\"experiments\"\u003eExperiments\u003c/h2\u003e\n\n\u003cp\u003eWe can store any data structure in the on-heap area, which means that the advantage of this approach lies in the fact\nthat there is no overhead involved in transforming the data to another form, while its disadvantage consists of the\nadditional cost related to the GC. On the other hand, in the case of off-heap storage, there is no GC extra cost,\nbut there is the cost of serialising the data to a byte array.\u003c/p\u003e\n\n\u003cp\u003eOver the last years, significant\nprogress has been made in the field of GC and with the right matching of the algorithm to the application profile, its\ntime can be very short. But is there any case where it is worth reaching into the unmanaged space after all?\u003c/p\u003e\n\n\u003cp\u003eI decided to start with an overview of what open-source options are currently available. When it comes to the implementation of the\non-heap cache mechanism, the options are numerous – there is well known:\n\u003ca href=\"https://guava.dev/releases/21.0/api/docs/com/google/common/cache/Cache.html\"\u003eguava\u003c/a\u003e,\n\u003ca href=\"https://www.ehcache.org/\"\u003eehcache\u003c/a\u003e, \u003ca href=\"https://github.com/ben-manes/caffeine\"\u003ecaffeine\u003c/a\u003e and many other solutions. However,\nwhen I began researching cache mechanisms offering the possibility of storing data outside GC control, I found out\nthat there are very few solutions left. Out of the popular ones, only \u003ca href=\"https://www.terracotta.org/\"\u003eTerracotta\u003c/a\u003e is supported.\nIt seems that this is a very niche solution and we do not have many options to choose\nfrom. In terms of less-known projects, I came across \u003ca href=\"https://github.com/OpenHFT/Chronicle-Map\"\u003eChronicle-Map\u003c/a\u003e,\n\u003ca href=\"https://github.com/jankotek/MapDB\"\u003eMapDB\u003c/a\u003e and \u003ca href=\"https://github.com/snazy/ohc\"\u003eOHC\u003c/a\u003e. I chose the\nlast one because it was created as part of the Cassandra project, which I had some experience with and was curious\nabout how this component worked:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eOHC was developed in 2014/15 for Apache Cassandra 2.2 and 3.0 to be used as the new row-cache backend.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eTo run the experiment, I decided to use a service built to provide the offer description based on its unique number. After\ndownloading the offer description from the repository, it is placed in the cache to speed up future calls. Obviously, the\ncache has a limited capacity, which is chosen in such a way that it forces the deletion of items that have been placed\nin it for the longest time ago.\u003c/p\u003e\n\n\u003cp\u003eIn our cache, the offer number is the key, while its description in the form of a string of characters is the\nvalue. This allows us to easily simulate almost any size of data in the cache (all we have to do is to make the\noffer description longer), and additionally, it makes the overhead related to the aforementioned serialisation\nrelatively small – serialisation of a text string is obviously faster than a complex DTO object.\u003c/p\u003e\n\n\u003cp\u003eIn my project, I used the \u003ca href=\"https://github.com/ben-manes/caffeine\"\u003eCaffeine cache\u003c/a\u003e to store the data in the on-heap area\nand OHC library to store it in the off-heap area.\u003c/p\u003e\n\n\u003cp\u003eThe test scenario consists of querying for descriptions of different offers. During the test, I will\ncollect data on memory and GC parameters using jConsole. I will run the test scenario using \u003ca href=\"https://jmeter.apache.org/\"\u003ejMeter\u003c/a\u003e,\nwhich additionally will allow me to measure response times.\u003c/p\u003e\n\n\u003cp\u003eFrom my preliminary research I know that this method is only applicable to memory-intensive systems.\nHowever, for the sake of order, let’s first run an experiment on a small cache size with element set to 5 KB:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003emaximum number of cached elements: 10000\u003c/li\u003e\n  \u003cli\u003ecached element size: 5.000 bytes\u003c/li\u003e\n  \u003cli\u003e10 threads querying for random offers in a loop of 100000 iterations each\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eTake a look at the screenshots from jConsole below. The results are in line with expectations: no benefit from the use\nof off-heap space. Both the number of garbage collection cycles (63 vs. 65) and GC run time (0.182s vs 0.235s)\nare nearly identical in both cases:\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eThe GC profile of on-heap variant:\u003c/em\u003e\n\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/on-heap-small-gc.png\" alt=\"on-heap GC chart\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eThe GC profile of off-heap variant:\u003c/em\u003e\n\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/off-heap-small-gc.png\" alt=\"on-heap GC chart\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNot much of an improvement for small to medium cache size. However, this result is not disappointing to me because\nI expected it. GC is designed to handle much more memory than 400 MB, it would therefore be strange if we obtained\nan improvement at such an early stage.\u003c/p\u003e\n\n\u003cp\u003eNow let’s see how the comparison looks for a much larger cache element size, let’s increase it up to 100 KB.\nAt the same time, due to the fact that I am running the tests on a laptop with limited resources, I will reduce\nthreads configuration and cache maximum element size.\u003c/p\u003e\n\n\u003cp\u003eThe configuration of the second test is as follows:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003emaximum number of cached elements: 5000\u003c/li\u003e\n  \u003cli\u003ecached element size: 100.000 bytes\u003c/li\u003e\n  \u003cli\u003e10 threads querying for random offers in a loop of 1000 iterations each\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eLet’s take a look at the results.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eThe GC profile of on-heap variant:\u003c/em\u003e\n\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/on-heap-gc.png\" alt=\"on-heap GC chart\" /\u003e\nMemory usage increases throughout the test, there are 40 GC collection cycles that together last 0.212s.\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003eThe GC profile of off-heap variant:\u003c/em\u003e\n\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/off-heap-gc.png\" alt=\"on-heap GC chart\" /\u003e\nThis time heap memory usage chart definitely looks different, is shaped like a saw, and reaches half of the previous value.\nPlease note also, that this time there are only 13 GC cycles with total time of 0.108s.\u003c/p\u003e\n\n\u003cp\u003eThe results of the GC profile comparison are therefore as expected, and what about the response times?\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003ejMeter metrics of on-heap variant:\u003c/em\u003e\n\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/on-heap-jmeter.png\" alt=\"on-heap GC chart\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cem\u003ejMeter metrics of off-heap variant:\u003c/em\u003e\n\u003cimg src=\"/img/articles/2022-06-30-gc-hands-off-my-data/off-heap-jmeter.png\" alt=\"on-heap GC chart\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eRequest time metrics data is also in line with predictions, off-heap variant proved to be slightly slower than on-heap.\u003c/p\u003e\n\n\u003cp\u003eNow let’s see what effect increasing the data size will have on the results. Let’s do tests for the following sizes:\n100.000 B, 200.000 B and 300.000 B, jMeter configuration stays unchanged: 10 threads with 1000 iterations each.\nThis time, for the sake of clarity, the results are summarized in a table:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eCached item size\u003c/th\u003e\n      \u003cth\u003eVariant\u003c/th\u003e\n      \u003cth\u003eGC cycles count\u003c/th\u003e\n      \u003cth\u003eGC time\u003c/th\u003e\n      \u003cth\u003eRequest time (median)\u003c/th\u003e\n      \u003cth\u003eThroughput\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e100.000 B\u003c/td\u003e\n      \u003ctd\u003eon-heap\u003c/td\u003e\n      \u003ctd\u003e40\u003c/td\u003e\n      \u003ctd\u003e0.212 s\u003c/td\u003e\n      \u003ctd\u003e171 ms\u003c/td\u003e\n      \u003ctd\u003e83.2 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e100.000 B\u003c/td\u003e\n      \u003ctd\u003eoff-heap\u003c/td\u003e\n      \u003ctd\u003e13\u003c/td\u003e\n      \u003ctd\u003e0.108 s\u003c/td\u003e\n      \u003ctd\u003e179 ms\u003c/td\u003e\n      \u003ctd\u003e78.1 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e200.000 B\u003c/td\u003e\n      \u003ctd\u003eon-heap\u003c/td\u003e\n      \u003ctd\u003e84\u003c/td\u003e\n      \u003ctd\u003e0.453 s\u003c/td\u003e\n      \u003ctd\u003e396 ms\u003c/td\u003e\n      \u003ctd\u003e38.2 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e200.000 B\u003c/td\u003e\n      \u003ctd\u003eoff-heap\u003c/td\u003e\n      \u003ctd\u003e19\u003c/td\u003e\n      \u003ctd\u003e0.182 s\u003c/td\u003e\n      \u003ctd\u003e355 ms\u003c/td\u003e\n      \u003ctd\u003e40.2 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e300.000 B\u003c/td\u003e\n      \u003ctd\u003eon-heap\u003c/td\u003e\n      \u003ctd\u003e114\u003c/td\u003e\n      \u003ctd\u003e0.6s\u003c/td\u003e\n      \u003ctd\u003e543 ms\u003c/td\u003e\n      \u003ctd\u003e27.3 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e300.000 B\u003c/td\u003e\n      \u003ctd\u003eoff-heap\u003c/td\u003e\n      \u003ctd\u003e27\u003c/td\u003e\n      \u003ctd\u003e0.185s\u003c/td\u003e\n      \u003ctd\u003e528 ms\u003c/td\u003e\n      \u003ctd\u003e27.9 rps\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eIt turns out that as the size of cache item increases, the benefits of using off-heap space grow – all metrics are improved.\u003c/p\u003e\n\n\u003cp\u003eWhat about cache maximum elements? Let’s use 200.000B item size and check what happens when we increase the maximum cache\nelement size, we will test cache for 5000, 10.000 and 15.000 elements:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eCache max elements\u003c/th\u003e\n      \u003cth\u003eVariant\u003c/th\u003e\n      \u003cth\u003eGC cycles count\u003c/th\u003e\n      \u003cth\u003eGC time\u003c/th\u003e\n      \u003cth\u003eRequest time (median)\u003c/th\u003e\n      \u003cth\u003eThroughput\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e5000\u003c/td\u003e\n      \u003ctd\u003eon-heap\u003c/td\u003e\n      \u003ctd\u003e84\u003c/td\u003e\n      \u003ctd\u003e0.453 s\u003c/td\u003e\n      \u003ctd\u003e396 ms\u003c/td\u003e\n      \u003ctd\u003e38.2 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e5000\u003c/td\u003e\n      \u003ctd\u003eoff-heap\u003c/td\u003e\n      \u003ctd\u003e19\u003c/td\u003e\n      \u003ctd\u003e0.182 s\u003c/td\u003e\n      \u003ctd\u003e355 ms\u003c/td\u003e\n      \u003ctd\u003e40.2 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e10000\u003c/td\u003e\n      \u003ctd\u003eon-heap\u003c/td\u003e\n      \u003ctd\u003e81\u003c/td\u003e\n      \u003ctd\u003e0.46 s\u003c/td\u003e\n      \u003ctd\u003e393 ms\u003c/td\u003e\n      \u003ctd\u003e38.8 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e10000\u003c/td\u003e\n      \u003ctd\u003eoff-heap\u003c/td\u003e\n      \u003ctd\u003e19\u003c/td\u003e\n      \u003ctd\u003e0.173 s\u003c/td\u003e\n      \u003ctd\u003e345 ms\u003c/td\u003e\n      \u003ctd\u003e42.6 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e15000\u003c/td\u003e\n      \u003ctd\u003eon-heap\u003c/td\u003e\n      \u003ctd\u003e84\u003c/td\u003e\n      \u003ctd\u003e0.462 s\u003c/td\u003e\n      \u003ctd\u003e355 ms\u003c/td\u003e\n      \u003ctd\u003e41.8 rps\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e15000\u003c/td\u003e\n      \u003ctd\u003eoff-heap\u003c/td\u003e\n      \u003ctd\u003e19\u003c/td\u003e\n      \u003ctd\u003e0.167 s\u003c/td\u003e\n      \u003ctd\u003e344 ms\u003c/td\u003e\n      \u003ctd\u003e42.6 rps\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eNo surprise here either, increasing cache size has a positive impact on both variants. Of course in case of on-heap cache,\nsome of the benefits are offset by the need for cleaning larger memory area.\u003c/p\u003e\n\n\u003cp\u003eWith the experiments conducted, we can conclude that the more data we store in memory, the greater the benefit of using\nthe off-heap area may be. At the same time, it should be added that these benefits are not huge, just a few RPS more.\nIn the case of systems that store tremendous amounts of data, this method may bring some improvements in terms of resource utilization.\nHowever, for most of our apps and services, that’s probably not the way to go, a code audit is a better idea.\u003c/p\u003e\n\n\u003cp\u003eThis is probably a good time to highlight how well implemented the current memory sweeper algorithms are. Well done GC!\u003c/p\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eEveryone has probably come across a case when an application froze as a result of GC’s operation. As the above data\nshow, there is a relationship between the amount of data stored in memory and the time the GC requires to clean it up –\nthe more data we store on the heap, the longer it takes to free the memory. That is why the cases where we process large\namounts of data provide us with a potential benefit of using the off-heap area. There are some very specialised uses of\nthis technique, such as Spark, which can store large amounts of data for subsequent processing steps and can do so using\nthe off-heap space (you can read more about Spark memory model \u003ca href=\"https://medium.com/walmartglobaltech/decoding-memory-in-spark-parameters-that-are-often-confused-c11be7488a24\"\u003ehere\u003c/a\u003e).\nAnother example of the use of the off-heap approach is the Apache Cassandra database. The OHC used\nin this post was developed from this particular project.\u003c/p\u003e\n\n\u003cp\u003eThere is a very narrow group of cases where storing data outside of GC control is justifiable. However, for the\nvast majority of applications, a much better approach is to take advantage of ever-improving GC\nimplementations. If you have experienced problems with the slow performance of the GC while developing your business\nservice, you should definitely audit your code first and experiment with different heap size settings and the GC\nalgorithm. When all other methods fail, you can give the off-heap area a try.\u003c/p\u003e\n\n\u003cp\u003eHowever, if you are working on a server that processes massive amounts of data, it is worth considering off-heap\nstorage earlier, similar to Spark or Cassandra solutions.\u003c/p\u003e\n\n","contentSnippet":"Certainly one of the main distinguishing features of the Java world is the Garbage Collector.\nUsing it is safe and convenient, it allows us to forget about many tedious responsibilities, letting us focus on the\npure joy of coding. Yet sometimes it can cause a headache too, especially when we notice that GC uses our resources\ntoo intensively. Each of us has probably experienced a time in our career when we wanted to get\nrid of the Garbage Collector from our application because it was running too long, too often, and perhaps even led to temporary system freezes.\nWhat if we could still benefit from the GC, but in special cases, also be able to store data beyond its control? We\ncould still take advantage of its convenience and, at the same time, be able to easily get rid of long GC pauses.\nIt turns out that it is possible. In this article, we will look at whether and when it is worth storing data\nbeyond the reach of the Garbage Collector’s greedy hands.\nComfort comes at a price\nAt Allegro we are very keen on metrics. We measure anything that can tell us something about the condition of\nour services. Apart from the most obvious metrics directly related to the application, such as throughput, the number of\nerrors, CPU and memory usage, we also pay a great deal of attention to metrics related to the garbage collecting — GC working\ntime and number of its cycles. Too much time spent on releasing the memory or too frequent GC launches may signal problems with\nmemory leaks or indicate that it is worth considering optimising memory usage or switching to a different GC strategy.\nFollowing the example of large technology companies, we have been organising company meetups within the so-called guilds\nfor some time now. In one of such guilds, over a hundred engineers meet regularly once a month and discuss various\ntopics related to performance, scaling and service optimisation. At one of these meetings, our colleague\ndiscussed the method of determining the actual size of data stored in a cache. Apparently, this is not a\nsimple matter, as internal mechanisms for optimising memory usage, such as deduplication or compression, must be taken\ninto account. After the presentation, an interesting discussion ensued about how much memory\non the heap is actually used by the cache and how long it takes to clean it up. Someone pointed out that there is a hidden cost of using the cache\nthat takes the form of time needed to free the memory of expired cache items, which not everyone is aware of. What is more, the\nmanner in which the cache works does not quite fit the\ngenerational hypothesis and may mislead the JVM by preventing it\nfrom properly tuning the GC mechanism. I then began to wonder whether it might not be worth keeping the cache in an area\nexcluded from the GC’s control? I knew this is possible, although I had never seen a practical implementation of this\ntechnique. This topic was bothering me for some time, so I decided to investigate.\nMemory architecture\nAny skilled Java programmer knows the division of memory into young and old generation areas. People interested in\ndetails are probably also familiar with the more precise division into eden, survivor, tenured and perm.\nThere are many excellent articles discussing this topic\n(like this one), so we won’t go\ninto details. Instead, we will focus on a very specialised area of memory that the GC\nhas no control over, which is the off-heap memory, sometimes also called native memory. This is a special area under the\ndirect control of the operating system, which the JVM uses for its own purposes. It stores information about classes and\nmethods, internal thread data and cached code necessary for operation. As I mentioned earlier, off-heap memory is not\nsubject to the GC. In particular, it is excluded from garbage collection processes, which means that programmers\ncreating the JVM code using this area are wholly responsible for freeing memory allocated for\nvariables. There is also a dedicated area to which we — the programmers — have access as well.\nThere is a possibility to write and read data from this space, remembering of course, that the responsibility\nfor cleaning up after unnecessary variables lies entirely with us.\nThis area can be accessed using a simple API.\nThe following code allocates 100 bytes of off-heap memory and stores a String and an Integer.\nAt the end the data are loaded from the off-heap memory and then printed out.\n\nint size = 100;\n\nByteBuffer buff = ByteBuffer.allocateDirect(size);\nbuff.put(\"Michal\".getBytes());\nbuff.putInt(42);\n\nbuff.position(0); // set the pointer back to the beginning\n\nbyte[] name = new byte[6]; // length of my name\nbuff.get(name);\n\nout.println(new String(name));\nout.println(buff.getInt());\n\n\nNote the allocateDirect method that allocates off-heap memory unlike a similar method: allocate that allocates\non-heap memory. The behavior of both methods can be compared with the help of a profiler\n(I will use jConsole). The following programs allocate 1GB of memory,\nrespectively, on-heap and off-heap:\n\nByteBuffer.allocate(1000000000)\n\n\n\nByteBuffer.allocateDirect(1000000000)\n\n\nThe chart below shows heap memory profile comparison for both programs (on-heap on the left vs. off-heap on the right):\n\nSuch a possibility to bypass Garbage Collector may seem extremely tempting to\ndevelopers struggling with long working time of the GC. However, this raises the question: what type of usage justifies\nthe extra effort involved in manually freeing the memory and the potential risk of error? What are the advantages of\nusing off-heap memory? Is it faster? How much time will we save by bypassing the GC? Why is this method so uncommon?\nTo put it simply: is it worth doing and if so, when?\nBe gone GC!\nGC is a wonderful tool. It allows us – although sometimes only for a while – to forget about the problems related\nto painful memory management. We can create variables of any type and any scope almost freely, and not worry about what\nhappens to memory once we stop using them. This task is handled by the GC, which does it brilliantly. In each successive\nversion of the JDK we get a new algorithm, which in some specific cases is even better than the previous one.\nHowever, I’m more than sure that many of us have once encountered the problem of long GC time or too frequent GC\ncalls. Every developer has their own ideas on how to deal with this issue - we look for memory leaks, profile the\napplication in search of hot spots, examine the scope of created variables, use object pools, verify the system\nbehaviour with different GC algorithms, and check the cache configuration.\nIn my case, it is the cache that is often responsible for long GC time. Sometimes it stores large numbers of objects, usually\ncomplex ones, containing references to other objects. What is more, the way cache objects are accessed is often not\nuniform. Some objects are never queried after being inserted into the cache, others are read throughout their whole\nlifecycle. This causes the cache to disrupt the somewhat ideal world order defined by the generational hypothesis. Then,\nGC algorithms are faced with a very difficult task of determining the optimal way to clean up the memory freed by the\nitems removed from the cache. All this causes the cache cleanup to be expensive. This made me wonder if there was\nany benefit in storing cache data outside the heap?\nOff-heap space: Pros and cons\nIn a sense, the off-heap space lies outside the control of the JVM (though it belongs to the Java process),\nand for this reason, it is not possible to write\ncomplex structures used in JVM languages into it. This raises the need for an intermediate step of serializing the\ndata into a plain byte array, which can then be stored in the off-heap area. When the data is loaded, the reverse\nprocess must be performed: deserialization into a form that we can use in Java. These additional steps will of\ncourse come at an extra cost, which is why accessing off-heap data will, for obvious reasons, take longer than accessing\non-heap data directly.\nSince writing and reading data in the off-heap space takes longer, what is the benefit of this approach then? Well, the data\nstored in the off-heap space are not subject to GC processes, so on the one hand we – the programmers – are responsible\nfor each freeing of memory after a given variable is no longer useful. On the other hand, we relieve the management\nprocesses in the JVM by releasing CPU’s time for the rest of the application, so, theoretically, it should\nresult in some resource savings. The question is, do these differences balance each other out to any degree? Will the savings\nassociated with the GC process balance out our longer data access time? If so, does it depend only on the amount of\ndata, or is there a specific usage scenario? To answer these questions, it is necessary to run a few experiments.\nExperiments\nWe can store any data structure in the on-heap area, which means that the advantage of this approach lies in the fact\nthat there is no overhead involved in transforming the data to another form, while its disadvantage consists of the\nadditional cost related to the GC. On the other hand, in the case of off-heap storage, there is no GC extra cost,\nbut there is the cost of serialising the data to a byte array.\nOver the last years, significant\nprogress has been made in the field of GC and with the right matching of the algorithm to the application profile, its\ntime can be very short. But is there any case where it is worth reaching into the unmanaged space after all?\nI decided to start with an overview of what open-source options are currently available. When it comes to the implementation of the\non-heap cache mechanism, the options are numerous – there is well known:\nguava,\nehcache, caffeine and many other solutions. However,\nwhen I began researching cache mechanisms offering the possibility of storing data outside GC control, I found out\nthat there are very few solutions left. Out of the popular ones, only Terracotta is supported.\nIt seems that this is a very niche solution and we do not have many options to choose\nfrom. In terms of less-known projects, I came across Chronicle-Map,\nMapDB and OHC. I chose the\nlast one because it was created as part of the Cassandra project, which I had some experience with and was curious\nabout how this component worked:\nOHC was developed in 2014/15 for Apache Cassandra 2.2 and 3.0 to be used as the new row-cache backend.\nTo run the experiment, I decided to use a service built to provide the offer description based on its unique number. After\ndownloading the offer description from the repository, it is placed in the cache to speed up future calls. Obviously, the\ncache has a limited capacity, which is chosen in such a way that it forces the deletion of items that have been placed\nin it for the longest time ago.\nIn our cache, the offer number is the key, while its description in the form of a string of characters is the\nvalue. This allows us to easily simulate almost any size of data in the cache (all we have to do is to make the\noffer description longer), and additionally, it makes the overhead related to the aforementioned serialisation\nrelatively small – serialisation of a text string is obviously faster than a complex DTO object.\nIn my project, I used the Caffeine cache to store the data in the on-heap area\nand OHC library to store it in the off-heap area.\nThe test scenario consists of querying for descriptions of different offers. During the test, I will\ncollect data on memory and GC parameters using jConsole. I will run the test scenario using jMeter,\nwhich additionally will allow me to measure response times.\nFrom my preliminary research I know that this method is only applicable to memory-intensive systems.\nHowever, for the sake of order, let’s first run an experiment on a small cache size with element set to 5 KB:\nmaximum number of cached elements: 10000\ncached element size: 5.000 bytes\n10 threads querying for random offers in a loop of 100000 iterations each\nTake a look at the screenshots from jConsole below. The results are in line with expectations: no benefit from the use\nof off-heap space. Both the number of garbage collection cycles (63 vs. 65) and GC run time (0.182s vs 0.235s)\nare nearly identical in both cases:\nThe GC profile of on-heap variant:\n\nThe GC profile of off-heap variant:\n\nNot much of an improvement for small to medium cache size. However, this result is not disappointing to me because\nI expected it. GC is designed to handle much more memory than 400 MB, it would therefore be strange if we obtained\nan improvement at such an early stage.\nNow let’s see how the comparison looks for a much larger cache element size, let’s increase it up to 100 KB.\nAt the same time, due to the fact that I am running the tests on a laptop with limited resources, I will reduce\nthreads configuration and cache maximum element size.\nThe configuration of the second test is as follows:\nmaximum number of cached elements: 5000\ncached element size: 100.000 bytes\n10 threads querying for random offers in a loop of 1000 iterations each\nLet’s take a look at the results.\nThe GC profile of on-heap variant:\n\nMemory usage increases throughout the test, there are 40 GC collection cycles that together last 0.212s.\nThe GC profile of off-heap variant:\n\nThis time heap memory usage chart definitely looks different, is shaped like a saw, and reaches half of the previous value.\nPlease note also, that this time there are only 13 GC cycles with total time of 0.108s.\nThe results of the GC profile comparison are therefore as expected, and what about the response times?\njMeter metrics of on-heap variant:\n\njMeter metrics of off-heap variant:\n\nRequest time metrics data is also in line with predictions, off-heap variant proved to be slightly slower than on-heap.\nNow let’s see what effect increasing the data size will have on the results. Let’s do tests for the following sizes:\n100.000 B, 200.000 B and 300.000 B, jMeter configuration stays unchanged: 10 threads with 1000 iterations each.\nThis time, for the sake of clarity, the results are summarized in a table:\nCached item size\n      Variant\n      GC cycles count\n      GC time\n      Request time (median)\n      Throughput\n    \n100.000 B\n      on-heap\n      40\n      0.212 s\n      171 ms\n      83.2 rps\n    \n100.000 B\n      off-heap\n      13\n      0.108 s\n      179 ms\n      78.1 rps\n    \n200.000 B\n      on-heap\n      84\n      0.453 s\n      396 ms\n      38.2 rps\n    \n200.000 B\n      off-heap\n      19\n      0.182 s\n      355 ms\n      40.2 rps\n    \n300.000 B\n      on-heap\n      114\n      0.6s\n      543 ms\n      27.3 rps\n    \n300.000 B\n      off-heap\n      27\n      0.185s\n      528 ms\n      27.9 rps\n    \nIt turns out that as the size of cache item increases, the benefits of using off-heap space grow – all metrics are improved.\nWhat about cache maximum elements? Let’s use 200.000B item size and check what happens when we increase the maximum cache\nelement size, we will test cache for 5000, 10.000 and 15.000 elements:\nCache max elements\n      Variant\n      GC cycles count\n      GC time\n      Request time (median)\n      Throughput\n    \n5000\n      on-heap\n      84\n      0.453 s\n      396 ms\n      38.2 rps\n    \n5000\n      off-heap\n      19\n      0.182 s\n      355 ms\n      40.2 rps\n    \n10000\n      on-heap\n      81\n      0.46 s\n      393 ms\n      38.8 rps\n    \n10000\n      off-heap\n      19\n      0.173 s\n      345 ms\n      42.6 rps\n    \n15000\n      on-heap\n      84\n      0.462 s\n      355 ms\n      41.8 rps\n    \n15000\n      off-heap\n      19\n      0.167 s\n      344 ms\n      42.6 rps\n    \nNo surprise here either, increasing cache size has a positive impact on both variants. Of course in case of on-heap cache,\nsome of the benefits are offset by the need for cleaning larger memory area.\nWith the experiments conducted, we can conclude that the more data we store in memory, the greater the benefit of using\nthe off-heap area may be. At the same time, it should be added that these benefits are not huge, just a few RPS more.\nIn the case of systems that store tremendous amounts of data, this method may bring some improvements in terms of resource utilization.\nHowever, for most of our apps and services, that’s probably not the way to go, a code audit is a better idea.\nThis is probably a good time to highlight how well implemented the current memory sweeper algorithms are. Well done GC!\nConclusions\nEveryone has probably come across a case when an application froze as a result of GC’s operation. As the above data\nshow, there is a relationship between the amount of data stored in memory and the time the GC requires to clean it up –\nthe more data we store on the heap, the longer it takes to free the memory. That is why the cases where we process large\namounts of data provide us with a potential benefit of using the off-heap area. There are some very specialised uses of\nthis technique, such as Spark, which can store large amounts of data for subsequent processing steps and can do so using\nthe off-heap space (you can read more about Spark memory model here).\nAnother example of the use of the off-heap approach is the Apache Cassandra database. The OHC used\nin this post was developed from this particular project.\nThere is a very narrow group of cases where storing data outside of GC control is justifiable. However, for the\nvast majority of applications, a much better approach is to take advantage of ever-improving GC\nimplementations. If you have experienced problems with the slow performance of the GC while developing your business\nservice, you should definitely audit your code first and experiment with different heap size settings and the GC\nalgorithm. When all other methods fail, you can give the off-heap area a try.\nHowever, if you are working on a server that processes massive amounts of data, it is worth considering off-heap\nstorage earlier, similar to Spark or Cassandra solutions.","guid":"https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html","categories":["tech","cache","performance","off-heap","garbage collectors"],"isoDate":"2022-06-29T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[],"events":[{"created":1657193453000,"duration":7200000,"id":"287035383","name":"Allegro Tech Labs #10 Online: Poskromić stan w React","date_in_series_pattern":false,"status":"past","time":1658934000000,"local_date":"2022-07-27","local_time":"17:00","updated":1658944632000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":26,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/287035383/","description":"❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: [https://app.evenea.pl/event/allegro-tech-labs-10/](https://app.evenea.pl/event/allegro-tech-labs-10/?fbclid=IwAR1Zj3sIcfx3WEWiFfS_hgiW6BJQD6stYouSGuSqfxDq9YVeom8fTFcrE1Q) ❗ **Allegro Tech Labs** to w 100% zdalna odsłona naszych stacjonarnych spotkań warsztatowych. Zazwyczaj spotykaliśmy się…","visibility":"public","member_pay_fee":false},{"created":1655131243000,"duration":5400000,"id":"286545395","name":"Allegro Tech Live #29 - Wyzwania Product Managera","date_in_series_pattern":false,"status":"past","time":1656604800000,"local_date":"2022-06-30","local_time":"18:00","updated":1656612323000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":88,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/286545395/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false},{"created":1650552918000,"duration":100800000,"id":"285416318","name":"UX Research Confetti - II edycja","date_in_series_pattern":false,"status":"past","time":1653562800000,"local_date":"2022-05-26","local_time":"13:00","updated":1653666063000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/285416318/","description":"REJESTRACJA NA WYDARZENIE -\u0026gt; https://app.evenea.pl/event/ux-research-confetti-2/ 🎉 Niech ponownie rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy……","visibility":"public","member_pay_fee":false},{"created":1651656994000,"duration":7200000,"id":"285691203","name":"Allegro Tech Live #28 - Mobile: Architektura softu i architektura sprzętu","date_in_series_pattern":false,"status":"past","time":1652976000000,"local_date":"2022-05-19","local_time":"18:00","updated":1652985850000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/285691203/","description":"**Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Kiedyś spotykaliśmy się w naszych biurach, a teraz to my gościmy…","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"S03E03 - Paweł Marcinkowski - O Data \u0026 AI w Allegro Pay","link":"https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/","pubDate":"Thu, 22 Sep 2022 00:00:00 GMT","content":"Jak zbudowany jest obszar Data \u0026 AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data \u0026 AI w Allegro Pay.","contentSnippet":"Jak zbudowany jest obszar Data \u0026 AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data \u0026 AI w Allegro Pay.","guid":"https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/","isoDate":"2022-09-22T00:00:00.000Z"},{"title":"S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box","link":"https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/","pubDate":"Thu, 08 Sep 2022 00:00:00 GMT","content":"Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.","contentSnippet":"Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.","guid":"https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/","isoDate":"2022-09-08T00:00:00.000Z"},{"title":"S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro","link":"https://podcast.allegro.tech/o-quality-assurance-w-allegro/","pubDate":"Thu, 25 Aug 2022 00:00:00 GMT","content":"Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro. ","contentSnippet":"Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro.","guid":"https://podcast.allegro.tech/o-quality-assurance-w-allegro/","isoDate":"2022-08-25T00:00:00.000Z"},{"title":"S02E12 - Piotr Betkier - Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro/","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro/","isoDate":"2021-06-16T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"pEqYuwKWlWmbs4B1d7Ex0","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>