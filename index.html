<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><link rel="author" href="humans.txt"/><script defer="" data-domain="allegro.tech" src="https://plausible.io/js/script.js"></script><script defer="" src="/clear-cookies.js"></script><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/c4277531f90028a4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c4277531f90028a4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/79db8b1e27b0a093.css" as="style"/><link rel="stylesheet" href="/_next/static/css/79db8b1e27b0a093.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-9d73513da824d371.js" defer=""></script><script src="/_next/static/chunks/206-8e417eb69d3aeb48.js" defer=""></script><script src="/_next/static/chunks/pages/index-7204a88054a30aad.js" defer=""></script><script src="/_next/static/iLWbxMd4nqRzBvv-AjXNq/_buildManifest.js" defer=""></script><script src="/_next/static/iLWbxMd4nqRzBvv-AjXNq/_ssgManifest.js" defer=""></script><script src="/_next/static/iLWbxMd4nqRzBvv-AjXNq/_middlewareManifest.js" defer=""></script></head><body class="m-color-bg_desk"><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://jobs.allegro.eu">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">About us</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text"><a href="https://about.allegro.eu/who-we-are/at-a-glance">Allegro</a> is one of the most technologically advanced companies in our part of Europe. Allegro is also over 2300 IT specialists of various specializations, developing our e-commerce platform. The unique scale and complexity of the problems that we solve on a daily basis give us the opportunity to work on a wide variety of projects. Allegro Tech is a place where our engineers share knowledge and case studies from selected projects in the company – in the form of articles, podcasts and events.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html" title="Unveiling bottlenecks of couchbase sub-documents operations"><img width="388" src="images/blogpost.png" alt="Unveiling bottlenecks of couchbase sub-documents operations" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html" title="Unveiling bottlenecks of couchbase sub-documents operations" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Unveiling bottlenecks of couchbase sub-documents operations</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">5 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/couchbase">#<!-- -->couchbase</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/sub-documents">#<!-- -->sub-documents</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bottlenecks">#<!-- -->bottlenecks</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">This story shows our journey in addressing a platform stability issue related to autoscaling, which, paradoxically, added some additional overhead instead
of reducing the load. A…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Tomasz Ziółkowski" src="https://blog.allegro.tech/img/authors/tomasz.ziolkowski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/tomasz.ziolkowski">Tomasz Ziółkowski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/04/ten-years-microservices.html" title="Ten Years and Counting: My Affair with Microservices"><img width="388" src="images/blogpost.png" alt="Ten Years and Counting: My Affair with Microservices" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/04/ten-years-microservices.html" title="Ten Years and Counting: My Affair with Microservices" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Ten Years and Counting: My Affair with Microservices</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około miesiąc temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/microservices">#<!-- -->microservices</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/architecture">#<!-- -->architecture</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">In early 2024, I hit ten years at Allegro, which also happens to be how long I’ve been working with microservices.
This timespan also roughly corresponds…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Michał Kosmulski" src="https://blog.allegro.tech/img/authors/michal.kosmulski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/michal.kosmulski">Michał Kosmulski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/04/ten-years-microservices.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/04/ten-years-microservices.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html" title="Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF"><img width="388" src="images/blogpost.png" alt="Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html" title="Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Unlocking Kafka&#x27;s Potential: Tackling Tail Latency with eBPF</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kafka">#<!-- -->kafka</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ebpf">#<!-- -->ebpf</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/bcc">#<!-- -->bcc</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/linux">#<!-- -->linux</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kernel">#<!-- -->kernel</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/ext4">#<!-- -->ext4</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/xfs">#<!-- -->xfs</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/performance">#<!-- -->performance</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tuning">#<!-- -->tuning</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/file system">#<!-- -->file system</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">At Allegro, we use Kafka as a backbone for asynchronous communication between microservices. With up to
300k messages published and 1M messages consumed every second, it…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:2"><img alt="Maciej Mościcki" src="https://blog.allegro.tech/img/authors/maciej.moscicki.jpg" class="MuiAvatar-img" width="32" height="32"/></div><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Piotr Rżysko" src="https://blog.allegro.tech/img/authors/piotr.rzysko.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/maciej.moscicki">Maciej Mościcki…</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/03/kafka-performance-analysis.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2024/02/rpa.html" title="Tired of repetitive tasks?! Go for RPA!"><img width="388" src="images/blogpost.png" alt="Tired of repetitive tasks?! Go for RPA!" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2024/02/rpa.html" title="Tired of repetitive tasks?! Go for RPA!" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Tired of repetitive tasks?! Go for RPA!</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/rpa">#<!-- -->rpa</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">Have you ever thought about ways of reducing repetitive, monotonous tasks? Maybe you would like to try to automate your own tasks? I will show…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Dominika Pleśniak" src="https://blog.allegro.tech/img/authors/dominika.plesniak.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/dominika.plesniak">Dominika Pleśniak</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2024/02/rpa.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2024/02/rpa.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/" title="O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali"><img src="images/podcast.png" alt="O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/" title="O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Na czym polega praca analityków w obszarze technologii w Allegro? Jakich narzędzi i technologii na co dzień używają osoby pracujące na tych stanowiskach? Jak efekty pracy analityków wpływają na naszą platformę, produkty i funkcjonalności? Czym zajmuje się Data Product Manager w Allegro Pay? Dlaczego monety są ważnym elementem ekosystemu Allegro? Posłuchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziałem Adrianny Napiórkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/" title="Programowanie - co liczy się w nim najbardziej?"><img src="images/podcast.png" alt="Programowanie - co liczy się w nim najbardziej?" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/" title="Programowanie - co liczy się w nim najbardziej?" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Programowanie - co liczy się w nim najbardziej?</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jaką ścieżkę trzeba przejść, aby dobrze programować? Gdzie zdobywać wiedzę, doświadczenie i szlifować swoje umiejętności? Ile czasu potrzeba aby nabrać doświadczenia i jak zadbać o swój dalszy rozwój? Na czym w praktyce polegają role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza się w naszych zespołach? Posłuchajcie nowego odcinka Allegro Tech Podcast z udziałem Rafała Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/" title="MBox: server-driven UI dla aplikacji mobilnych"><img src="images/podcast.png" alt="MBox: server-driven UI dla aplikacji mobilnych" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/" title="MBox: server-driven UI dla aplikacji mobilnych" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">MBox: server-driven UI dla aplikacji mobilnych</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">6 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym jest i jak powstał MBox: wewnętrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? Skąd wziął się pomysł na to rozwiązanie i na jakie bolączki odpowiada? Dlaczego zdecydowaliśmy się na budowanie tego typu rozwiązania in-house i z jakimi wyzwaniami mierzyliśmy się w procesie tworzenia? Co wyróżnia zespoły pracujące nad tym narzędziem i jak pracuje im się bez Product Ownera? Posłuchajcie siódmego odcinka Allegro Tech Podcast z udziałem Pauliny Sadowskiej i Tomasza Gębarowskiego - Managerów w obszarze Technical Platform Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/" title="O chatbotach i ich wpływie na Allegro"><img src="images/podcast.png" alt="O chatbotach i ich wpływie na Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/" title="O chatbotach i ich wpływie na Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O chatbotach i ich wpływie na Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">7 miesięcy temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieć w kontekście obszaru Customer Experience? W czym pomagają nam chatboty, jak je rozwijamy i dbamy o ich jakość? Kim są Allina oraz Albert i co mają wspólnego z automatyzacją? Za jakie rozwiązania otrzymaliśmy nagrodę hiperautomatyzacji? O tym wszystkim posłuchacie w odcinku z udziałem Rafała Gajewskiego - Managera w obszarze IT Services w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/301022703/" title="Allegro Tech Talks #43 - Wszystko o programie e-Xperience" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #43 - Wszystko o programie e-Xperience"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/301022703/" title="Allegro Tech Talks #43 - Wszystko o programie e-Xperience" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #43 - Wszystko o programie e-Xperience</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">za 1 dzień<!-- -->, Allegro Office - Poznań (Nowy Rynek)</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-43/](https://app.evenea.pl/event/allegro-tech-talk-43/) Zapraszamy Was na #43 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej kawie☕, napojach🥤…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/301022703/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/298027809/" title="UX Research Confetti - IV edycja" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - IV edycja"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/298027809/" title="UX Research Confetti - IV edycja" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - IV edycja</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">około 23 godziny temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**🎉 Przedstawiamy 4. edycję UX Research Confetti - bezpłatną, polską konferencję poświęconą badaniom UX, organizowaną przez zespół badaczy z Allegro.** ✨ Konferencja odbędzie się w…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/298027809/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/300288303/" title="DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/300288303/" title="DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">DDD &amp; EventStorming na luzie - unconference na 2 lata gildii w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">25 dni temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** **[https://app.evenea.pl/event/allegro-tech-talk-ddd/](https://app.evenea.pl/event/allegro-tech-talk-ddd/)** Dobrze Was widzieć! Allegro Tech to miejsce, w którym dzielimy się wiedzą, dobrymi praktykami i case study z różnych projektów prowadzonych w…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/300288303/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/300327359/" title="Allegro Tech Talks #42 - Kariera Product Managera" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #42 - Kariera Product Managera"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/300327359/" title="Allegro Tech Talks #42 - Kariera Product Managera" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #42 - Kariera Product Managera</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">28 dni temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-42/ ](https://app.evenea.pl/event/allegro-tech-talk-42/) Zapraszamy Was na #42 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/300327359/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Contractor Software Engineer (Java/Kotlin)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw, Poznań</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999988837033-contractor-software-engineer-javakotlin?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Salesforce Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999988151173-senior-salesforce-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (.NET) - Product Team - Allegro Pay</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999987145193-software-engineer-net-product-team-allegro-pay?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Software Engineer (Java/Kotlin)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań, Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999987078873-senior-software-engineer-javakotlin?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer (Java/Kotlin)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań, Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999987077949-software-engineer-javakotlin?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://jobs.allegro.eu">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Unveiling bottlenecks of couchbase sub-documents operations","link":"https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html","pubDate":"Thu, 16 May 2024 00:00:00 +0200","authors":{"author":[{"name":["Tomasz Ziółkowski"],"photo":["https://blog.allegro.tech/img/authors/tomasz.ziolkowski.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.ziolkowski"]}]},"content":"\u003cp\u003eThis story shows our journey in addressing a platform stability issue related to autoscaling, which, paradoxically, added some additional overhead instead\nof reducing the load. A pivotal part of this narrative is how we used \u003ca href=\"https://www.couchbase.com/\"\u003eCouchbase\u003c/a\u003e — a distributed NoSQL database. If you find\nyourself intrigued by another enigmatic story involving Couchbase, don’t miss my\n\u003ca href=\"/2024/02/couchbase-expired-docs-tuning.html\"\u003eblog post on tuning expired doc settings\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThis post unfolds our quest to discover the root cause of the bottleneck. Initially, I will outline the symptoms of the issue. Subsequently, you will be\nintroduced to how Couchbase is utilized by the aforementioned service. Equipped with this knowledge, I will recount our attempts to diagnose the problem and\nindicate which observations raised our suspicions. The following section is dedicated to conducting benchmarks to verify our predictions using\na custom benchmarking tool. Ultimately, we will explore the source code of Couchbase to uncover how the problematic operations are executed. This section\naims to provide a deep understanding of Couchbase’s inner workings. I firmly believe that the knowledge shared in that part is its most valuable asset and may\nenable you to swiftly identify and address some of the potential performance issues when using Couchbase.\u003c/p\u003e\n\n\u003ch2 id=\"set-the-scene\"\u003eSet the scene\u003c/h2\u003e\n\n\u003cp\u003eThe service at the heart of the stability issues handles external HTTP traffic; for the purpose of this discussion, we’ll refer to it as\n“the gateway service”. The traffic routed to the gateway service reflects a pattern similar to organic traffic on \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e,\ncharacterized by significant fluctuations in throughput between day and night hours. To efficiently utilize resources, the gateway service employs an autoscaler\nto dynamically adjust the number of instances based on current demands. It’s also important to note that spawning a new instance involves a warm-up phase,\nduring which the instance retrieves some data from Couchbase to populate its in-memory cache. The gateway service relies on a Couchbase cluster\ncomprised of \u003cstrong\u003ethree\u003c/strong\u003e nodes.\u003c/p\u003e\n\n\u003ch2 id=\"observations\"\u003eObservations\u003c/h2\u003e\n\n\u003cp\u003eThe team managing the service encountered a series of errors in communication with Couchbase. These errors indicated that 3-second timeouts occurred while\nfetching data from Couchbase:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ecom. couchbase.client.core.error.UnambiguousTimeoutException: SubdocGetRequest, Reason: TIMEOUT {\n    \"cancelled\":true,\n    \"completed\":true,\n    ... IRRELEVANT METADATA ...\n    \"timeoutMs\":3000,\n    \"timings\":{\"totalMicros\":3004052}\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eInterestingly, during these incidents, the Couchbase cluster did not exhibit high CPU or RAM usage. Furthermore, the traffic to Couchbase, measured in\noperations per second, was not exceptionally high. I mean that other Couchbase clients (different microservices) were generating an order of magnitude more\noperations per second without encountering stability issues.\u003c/p\u003e\n\n\u003cp\u003eAdditional key observations related to the issue include:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe instability primarily occurred during the service scaling-up process, initially triggered by the autoscaler.\u003c/li\u003e\n  \u003cli\u003eNewly spawned instances were predominantly affected.\u003c/li\u003e\n  \u003cli\u003eThe issues were reported solely for operations directed to a specific node within the cluster.\u003c/li\u003e\n  \u003cli\u003eA temporary mitigation of the problems involved repeatedly restarting the failing application instances.\u003c/li\u003e\n  \u003cli\u003eThere was a noticeable pattern on the driver side that preceded the widespread errors, including timeouts and the inability to send requests due to\na non-writable channel.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"temporary-solution\"\u003eTemporary solution\u003c/h3\u003e\n\n\u003cp\u003eAs a temporary measure, the team overseeing the gateway service implemented the following workarounds:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eDisabled certain types of requests to reduce the overall traffic volume directed to Couchbase.\u003c/li\u003e\n  \u003cli\u003eDeactivated the autoscaler, and manually scaled up the application to manage peak traffic loads.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThese actions successfully halted the problems, but they also had repercussions, including business impacts and decreased efficiency in resource utilization.\u003c/p\u003e\n\n\u003ch2 id=\"raising-suspicions\"\u003eRaising suspicions\u003c/h2\u003e\n\n\u003cp\u003eA pivotal aspect of this issue was the use of the \u003ca href=\"https://docs.couchbase.com/go-sdk/2.4/concept-docs/subdocument-operations.html\"\u003eCouchbase sub-document API\u003c/a\u003e\nwithin the gateway service, an approach not widely adopted across our internal microservice landscape, yet notable for its efficiency. According to\nthe documentation, this API significantly reduces traffic by allowing the fetching or mutating only specific parts of a Couchbase document.\nEssentially, it acts as a substitute for the concept of \u003ca href=\"https://en.wikipedia.org/wiki/Projection_(relational_algebra)\"\u003eprojection\u003c/a\u003e, familiar to SQL users.\u003c/p\u003e\n\n\u003cp\u003eIn our investigation we closely examined the data collected on the Couchbase node, the operational dynamics of the gateway service’s cache, and insights\nfrom scrutinizing both the Couchbase driver and server code. We hypothesized that the crux of the problem might be linked to the cache warm-up process for\nnewly launched instances.\u003c/p\u003e\n\n\u003cp\u003eOur investigation uncovered several indicators pointing toward the core of the issue:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eA disproportionately large number of requests targeted a single document, inevitably directing traffic to a specific node.\u003c/li\u003e\n  \u003cli\u003eThe node hosting this heavily queried document corresponded with the one mentioned in timeout-related logs.\u003c/li\u003e\n  \u003cli\u003eInstances that had been running for an extended period reported virtually no errors.\u003c/li\u003e\n  \u003cli\u003eThe volume of requests to Couchbase from the affected instances was extraordinarily high, not aligning with the number of requests registered on\nthe Couchbase side. This discrepancy suggested that if the cache warming process was at fault, the sheer magnitude of attempted requests was overwhelming\neven the local network buffers.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eHowever, these observations were merely pieces of a larger puzzle. We noticed a “snowball effect” where the system’s inability to process an initial set\nof requests for newly initiated instances triggered a cascade of failures. But the question remained: Why? What made these instances different,\nand why didn’t other clients on the same cluster experience similar issues? This was the crucial moment to take a closer examination of the sub-document\noperations to determine their efficiency and optimization.\u003c/p\u003e\n\n\u003ch2 id=\"lets-benchmark-it\"\u003eLet’s benchmark it\u003c/h2\u003e\n\n\u003cp\u003eDespite an extensive search, we were unable to locate any tools capable of reliably testing our hypothesis—that sub-document operations executed during\nthe warm-up phase could significantly challenge Couchbase’s handling capabilities. As a result, we developed a simple tool and made it\n\u003ca href=\"https://github.com/ziollek/cb-perf-tester\"\u003eavailable in on \u003cem\u003eGitHub\u003c/em\u003e\u003c/a\u003e.\nThis tool is designed to create a sample document and then execute parallel sub-document fetch operations concurrently.\nThe sample document is structured as follows:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e{\n    \"key\": \"test-subdoc\",\n    \"data\": {\n        \"subkey-000000\": \"value-000000\",\n        \"subkey-000001\": \"value-000001\",\n        . . .\n        \"subkey-0….N\": \"value-0…..N\",\n    }\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe tool allows manipulating several knobs, which includes:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eParallelism\u003c/strong\u003e: Determines the number of parallel \u003ca href=\"https://gobyexample.com/goroutines\"\u003egoroutines\u003c/a\u003e that will attempt to fetch the same sub-documents concurrently.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eDocument Size\u003c/strong\u003e: Defined by the number of sub-keys, this directly affects the document’s binary size.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eLevel of Search Difficulty\u003c/strong\u003e: This essentially refers to how deep or how far into the main document the target sub-document is located.\nThe concept is illustrated in the diagram below:\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-sub-difficulty.png\" alt=\"Difficulty of sub-document search\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"caveats\"\u003eCaveats\u003c/h3\u003e\n\n\u003cp\u003eThe primary objective of this exercise was to identify potential bottlenecks, not to conduct a highly accurate performance assessment of Couchbase clusters.\nTherefore, we opted to run our experiments using a local Docker container (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecouchbase:community-6.6.0\u003c/code\u003e), rather than on a dedicated, well-isolated cluster.\nWe acknowledge that hosting both the server and the benchmarking tool on the same machine may compromise the reliability and accuracy of the results.\nConsequently, we advise against using the findings from these tests for comprehensive assessments or comparisons with other technologies.\u003c/p\u003e\n\n\u003ch3 id=\"benchmark-steps\"\u003eBenchmark steps\u003c/h3\u003e\n\n\u003cp\u003eThe procedure for each experiment follows a similar framework, outlined in the steps below:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eDocument Preparation\u003c/strong\u003e: Initiate the document with the desired number of sub-documents, as dictated by one of the experimental variables.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eDocument Storage\u003c/strong\u003e: Save this document under a predetermined key.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eGoroutine Initialization\u003c/strong\u003e: Launch a specified number of goroutines, the quantity of which is determined by another experimental variable.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eFetch Operations\u003c/strong\u003e: Each goroutine executes a series of fetch operations, which can be either regular (retrieving the entire sample document) or\nsub-document (accessing a set of sub-documents). It’s important to note that these requests are executed in a blocking manner; a new fetch operation is\nperformed only after the completion of the preceding one. In sub-document mode, the difficulty of the fetch operation is controlled through\nan experiment variable.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCompletion Wait\u003c/strong\u003e: Await the termination of all goroutines.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eResults Reporting\u003c/strong\u003e: Calculate and display the estimated RPS (requests per second).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"estimate-baseline\"\u003eEstimate baseline\u003c/h3\u003e\n\n\u003cp\u003ePrior to delving into sub-document operations, we sought to establish the maximum number of regular get operations that our local Couchbase Server instance\ncould handle. Through testing at various levels of concurrency, we determined the maximum throughput for our specific setup.\nIt was approximately 6,000 to 7,000 RPS, regardless of whether the requests were for small documents (less than 200 bytes)\nor for non-existent documents. These findings were further validated by the statistics available through the Couchbase UI.\u003c/p\u003e\n\n\u003cp\u003eBenchmark Command: Attempting to fetch a non-existent document yielded a rate of \u003cem\u003e6388 RPS\u003c/em\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5 --search-non-existent\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=true, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: not-exists\n\nregular report: successes: 0, errors: 200000, duration: 31.306684977s, rps: 6388.411937, success rps: 0.000000\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-baseline-non-exitstent.png\" alt=\"Baseline - fetch a not-existent document\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBenchmark Command: Fetching an existing small (195 bytes) document yielded a rate of \u003cem\u003e6341 rps\u003c/em\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=false, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: test-regular\n\nregular report: successes: 200000, errors: 0, duration: 31.536538682s, rps: 6341.850068, success rps: 6341.850068\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-baseline-hits.png\" alt=\"Baseline - fetch an existing document\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"testing-scenarios\"\u003eTesting scenarios\u003c/h3\u003e\n\n\u003cp\u003eNow that we have a baseline for comparison, we’re set to evaluate it against the outcomes of various scenarios. To ensure the tests are comparable,\nwe’ll maintain constant parallelism across all tests, specifically using 200 goroutines. The variables that will differ across scenarios include:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eTotal Number of Sub-Documents\u003c/strong\u003e: This determines the overall size of the sample document, as the document’s size is directly related to the number\nof sub-documents it contains.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eNumber of Searched Sub-Documents\u003c/strong\u003e: This refers to how many sub-paths within the sample document will be targeted in a single fetch operation.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eSearch Difficulty\u003c/strong\u003e: This aspect dictates the difficulty of locating the searched sub-paths within the document.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIt’s important to highlight that in each scenario, we will manipulate only one variable at a time while keeping the other parameters constant.\u003c/p\u003e\n\n\u003ch4 id=\"scenario-a-the-impact-of-document-size-on-performance\"\u003eScenario A: The impact of document size on performance\u003c/h4\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-doc-size-vs-performance-all.png\" alt=\"Document size vs performance - aggregated\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eTo better visualize the impact, let’s look at the diagram for HARD scenario:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-doc-size-vs-performance-hard.png\" alt=\"Document size vs performance - hard\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIt is clearly visible that there is a strict correlation between document size and performance.\u003c/p\u003e\n\n\u003ch4 id=\"scenario-b-the-impact-of-the-number-of-searched-sub-documents-on-performance\"\u003eScenario B: The impact of the number of searched sub-documents on performance\u003c/h4\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-subdocs-num-vs-performance-all.png\" alt=\"Number of searched sub-documents vs performance - aggregated\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eTo better visualize the impact, let’s look at the diagram for HARD scenario:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-subdocs-num-vs-performance-hard.png\" alt=\"Number of searched sub-documents vs performance - hard\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAs observed in the previous scenario, there is a clear correlation between the number of sub-documents accessed and the system’s performance.\u003c/p\u003e\n\n\u003ch2 id=\"further-analysis\"\u003eFurther analysis\u003c/h2\u003e\n\n\u003cp\u003eGiven the evident correlation between the document size/number of queried sub-paths and performance degradation, we delve into the mechanics to understand\nthe root cause of these test results. A notable observation during the tests relates to CPU utilization within the Docker environment, where, despite having\nsix cores available, only a single core was actively utilized. Intriguingly, this usage was monopolized by a single thread (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emc:worker_X\u003c/code\u003e).\nThis phenomenon directly stems from Couchbase’s handling of Key-Value (KV) connections. By default, the Couchbase driver initiates only a single connection to\neach cluster node for KV operations. However, this configuration can be adjusted in certain Software Development Kits (SDKs)—the Java SDK,\nfor instance, allows modification through \u003ca href=\"https://docs.couchbase.com/java-sdk/current/ref/client-settings.html#io-options\"\u003eIoConfig.numKvConnections\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhen a connection is established, Couchbase assigns it to\na \u003ca href=\"https://github.com/couchbase/kv_engine/blob/master/docs/in-depth/C10k.md#current-approach-why-not-both\"\u003especific worker thread\u003c/a\u003e) using\na \u003ca href=\"https://github.com/couchbase/kv_engine/blob/master/daemon/front_end_thread.h#L84\"\u003eRound-Robin (RR)\u003c/a\u003e) algorithm. As a result, the Couchbase Server does not\nfully utilize available CPU power for a single connection, even when a lot of resources are free. This behavior can be seen as beneficial, serving to mitigate\nthe \u003ca href=\"https://en.wikipedia.org/wiki/Cloud_computing_issues#Performance_interference_and_noisy_neighbors\"\u003e“noisy neighbor” effect\u003c/a\u003e, provided there are sufficient\nspare cores available to manage new connections. This mechanism ensures balanced resource use across connections, as illustrated in the diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-dispatch-ok.png\" alt=\"Handling connections - free cores scenario\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eConversely, one may encounter fluctuating performance due to instances of misfortune, where if other clients significantly burden certain worker threads,\nand your connection is allocated to one of these overloaded threads, performance inconsistencies arise. This scenario, where a client experiences higher than\nusual response times due to an imbalanced distribution of workload across worker threads, is depicted in the diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-dispatch-bad.png\" alt=\"Handling connections - overloaded core scenario\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThis behavior explains the apparent paradox observed during the stability issues: the Couchbase node showed no clear signs of being overloaded, yet certain\nanomalous symptoms were present, such as a metric indicating the minimum \u003ca href=\"https://en.wikipedia.org/wiki/Idle_(CPU)\"\u003eidle\u003c/a\u003e percentage across all cores plummeting\nto 0% during the disturbances. This leaves no doubt that operations on sub-documents have the potential to overburden worker threads within\nthe Couchbase Server. With this understanding we are now ready to delve deeper into the root cause of such behavior.\u003c/p\u003e\n\n\u003ch3 id=\"what-documentation-says\"\u003eWhat documentation says\u003c/h3\u003e\n\n\u003cp\u003eThe documentation for Couchbase, housed alongside the server’s source code, is notably comprehensive, including a dedicated section on\n\u003ca href=\"https://github.com/couchbase/kv_engine/blob/master/docs/SubDocument.md\"\u003esub-documents\u003c/a\u003e. However, it falls short of providing detailed insights into\nthe internal workings of these operations. Additionally, there is a lack of discussion on the performance implications of sub-document operations,\nwith only a few remarks on data typing and float numbers that do not apply to the cases we tested. Attempts to find answers on the Couchbase forum were also\nunfruitful, yielding no substantial information on the performance issues we encountered. Despite this, there is confirmation from others in the community who\nhave observed\n\u003ca href=\"https://www.couchbase.com/forums/t/frequent-timeouts-and-requests-over-threshold-for-subdocgetrequests-via-reactive-java-sdk/30211\"\u003esimilar problems\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"what-source-code-says\"\u003eWhat source code says\u003c/h3\u003e\n\n\u003cp\u003eA thorough analysis of the codebase reveals a definitive cause for the performance degradation observed. It’s important to note that Couchbase requires\n\u003ca href=\"https://docs.couchbase.com/server/current/learn/buckets-memory-and-storage/compression.html\"\u003edecompression\u003c/a\u003e of a document for any lookup or manipulation\noperation, whether the document is retrieved from RAM or disk. Let’s start from a point where Couchbase starts doing\n\u003ca href=\"https://github.com/couchbase/kv_engine/blob/cf020888d2e09b132a02c90b99e160044ddabb11/daemon/subdocument.cc#L568\"\u003elookups\u003c/a\u003e on a decompressed object:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e// 2. Perform each of the operations on document.\nfor (auto\u0026amp; op : operations) {\n    switch (op.traits.scope) {\n    case CommandScope::SubJSON:\n        if (cb::mcbp::datatype::is_json(doc_datatype)) {\n            // Got JSON, perform the operation.\n            op.status = subdoc_operate_one_path(context, op, current.view);\n        }\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eA critical observation from our analysis is that each operation (lookup) is executed sequentially through the invocation of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esubdoc_operate_one_path\u003c/code\u003e.\nTo understand the performance implications, let’s examine\n\u003ca href=\"https://github.com/couchbase/kv_engine/blob/cf020888d2e09b132a02c90b99e160044ddabb11/daemon/subdocument.cc#L413C5-L414C76\"\u003ethe implementation\u003c/a\u003e of this lookup\nprocess:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e// ... and execute it.\nconst auto subdoc_res = op.op_exec(spec.path.data(), spec.path.size());\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe Investigation reveals that the lookup functionality is powered by a specialized library,\n\u003ca href=\"https://github.com/couchbase/subjson/blob/4b93d966f791209209a0825e46f7049df0673e8f/subdoc/operations.cc#L757\"\u003elibrary \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esubjson\u003c/code\u003e\u003c/a\u003e, which in turn\n\u003ca href=\"https://github.com/couchbase/subjson/blob/4b93d966f791209209a0825e46f7049df0673e8f/subdoc/match.cc#L371\"\u003euses\u003c/a\u003e\nthe \u003ca href=\"https://github.com/mnunberg/jsonsl\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ejsonsl\u003c/code\u003e library\u003c/a\u003e for parsing JSON in a streaming manner. An enlightening piece of information about performance can be found in\nthe README of the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esubjson\u003c/code\u003e library, which is integral to Couchbase’s solution. The direct quote from the README is as follows:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eBecause the library does not actually build a JSON tree, the memory usage and CPU consumption is constant, regardless of the size of the actual JSON object\nbeing operated upon, and thus the only variable performance factor is the amount of actual time the library can seek to the location in the document to\nbe modified.\u003c/p\u003e\n\n  \u003cp\u003eOn a single Xeon E5520 core, this library can process about 150MB/s-300MB/s of JSON. This processing includes the search logic as well as any\nreplacement logic.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis analysis clearly demonstrates that lookups targeting paths situated towards the end of a document are markedly slower compared to those aimed\nat the beginning. Moreover, each sequential lookup \u003cstrong\u003eneeds to repeat document parsing\u003c/strong\u003e. The impact of this implementation on performance is significant.\nFor a more intuitive understanding of these effects, please refer to the diagram below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-processing-sub-documents.png\" alt=\"Processing sub-documents in details\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe performance characteristics we’ve outlined align with the outcomes observed in our experiments. To illustrate, consider a detailed examination of\na single \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eHARD\u003c/code\u003e test scenario—specifically, a case where the sub-documents targeted for search were positioned towards the end of the JSON document:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e./cb-perf-tester subdoc  --parallel 200 --repeat 50 --search-keys 10 --difficulty hard\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=10000, level=Hard, search-keys=10, repeats=50, parallel=200\nGenerated doc with subkeys: 10000, byte size is: 310043\n\nsearch for subkeys [level=Hard]: [data.subkey-009999 data.subkey-009998 data.subkey-009997 data.subkey-009996 data.subkey-009995 data.subkey-009994 data.subkey-009993 data.subkey-009992 data.subkey-009991 data.subkey-009990]\n\nsubdoc report: successes: 10000, errors: 0, duration: 1m19.784865193s, rps: 125.337055, success rps: 125.337055\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eBy multiplying the size of the document by the number of sub-documents queried, we can determine the total stream size that the library must process, which,\nin this case, approximates to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e~3MB\u003c/code\u003e. Further multiplying this figure by the RPS gives us an insight into the overall throughput of\nthe stream processing:\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e3MB x 125 RPS ~= 375 MBps\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003eThe calculated throughput slightly exceeds the benchmarks outlined in the README. Moreover, the estimated throughput remains nearly constant across\nvarious tests. For a comprehensive view of these findings, please refer to the diagram below, which displays the estimated throughput for tests conducted under\nthe HARD level with a document size of approximately \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e300KB\u003c/code\u003e:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-05-16-couchbase-subdocuments-bottleneck/cb-estimated-subdocs-throughput.png\" alt=\"Estimated throughput\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eThe Couchbase sub-document API, while designed to optimize network throughput, introduces significant performance trade-offs. Notably, even\n\u003cem\u003eunder optimal conditions\u003c/em\u003e, operations on sub-documents are noticeably slower compared to regular get operations fetching\nsmall documents—evidenced by a comparison of the baseline performance; approximately 4-5k RPS for sub-document operations vs. 6-7k RPS for\nregular get operations.\u003c/p\u003e\n\n\u003cp\u003eThe method Couchbase employs for executing lookups directly influences performance, manifesting declines as either the document size increases or the number of\nlookups per request raises. This slowdown affects all requests over the same connection due to the high CPU demand of sub-document lookup operations.\nParticularly in environments utilizing reactive/asynchronous clients, this can overload the Couchbase worker thread, leading to a halt in request servicing.\nImportantly, an overloaded worker may manage connections from multiple clients, potentially exacerbating the “noisy neighbor” effect.\u003c/p\u003e\n\n\u003cp\u003eWhile there are strategies to mitigate these issues from the perspective of a client, such as the gateway service, these considerations warrant\na separate discussion, which I plan to address in a future blog post.\u003c/p\u003e\n","contentSnippet":"This story shows our journey in addressing a platform stability issue related to autoscaling, which, paradoxically, added some additional overhead instead\nof reducing the load. A pivotal part of this narrative is how we used Couchbase — a distributed NoSQL database. If you find\nyourself intrigued by another enigmatic story involving Couchbase, don’t miss my\nblog post on tuning expired doc settings.\nThis post unfolds our quest to discover the root cause of the bottleneck. Initially, I will outline the symptoms of the issue. Subsequently, you will be\nintroduced to how Couchbase is utilized by the aforementioned service. Equipped with this knowledge, I will recount our attempts to diagnose the problem and\nindicate which observations raised our suspicions. The following section is dedicated to conducting benchmarks to verify our predictions using\na custom benchmarking tool. Ultimately, we will explore the source code of Couchbase to uncover how the problematic operations are executed. This section\naims to provide a deep understanding of Couchbase’s inner workings. I firmly believe that the knowledge shared in that part is its most valuable asset and may\nenable you to swiftly identify and address some of the potential performance issues when using Couchbase.\nSet the scene\nThe service at the heart of the stability issues handles external HTTP traffic; for the purpose of this discussion, we’ll refer to it as\n“the gateway service”. The traffic routed to the gateway service reflects a pattern similar to organic traffic on Allegro,\ncharacterized by significant fluctuations in throughput between day and night hours. To efficiently utilize resources, the gateway service employs an autoscaler\nto dynamically adjust the number of instances based on current demands. It’s also important to note that spawning a new instance involves a warm-up phase,\nduring which the instance retrieves some data from Couchbase to populate its in-memory cache. The gateway service relies on a Couchbase cluster\ncomprised of three nodes.\nObservations\nThe team managing the service encountered a series of errors in communication with Couchbase. These errors indicated that 3-second timeouts occurred while\nfetching data from Couchbase:\n\ncom. couchbase.client.core.error.UnambiguousTimeoutException: SubdocGetRequest, Reason: TIMEOUT {\n    \"cancelled\":true,\n    \"completed\":true,\n    ... IRRELEVANT METADATA ...\n    \"timeoutMs\":3000,\n    \"timings\":{\"totalMicros\":3004052}\n}\n\n\nInterestingly, during these incidents, the Couchbase cluster did not exhibit high CPU or RAM usage. Furthermore, the traffic to Couchbase, measured in\noperations per second, was not exceptionally high. I mean that other Couchbase clients (different microservices) were generating an order of magnitude more\noperations per second without encountering stability issues.\nAdditional key observations related to the issue include:\nThe instability primarily occurred during the service scaling-up process, initially triggered by the autoscaler.\nNewly spawned instances were predominantly affected.\nThe issues were reported solely for operations directed to a specific node within the cluster.\nA temporary mitigation of the problems involved repeatedly restarting the failing application instances.\nThere was a noticeable pattern on the driver side that preceded the widespread errors, including timeouts and the inability to send requests due to\na non-writable channel.\nTemporary solution\nAs a temporary measure, the team overseeing the gateway service implemented the following workarounds:\nDisabled certain types of requests to reduce the overall traffic volume directed to Couchbase.\nDeactivated the autoscaler, and manually scaled up the application to manage peak traffic loads.\nThese actions successfully halted the problems, but they also had repercussions, including business impacts and decreased efficiency in resource utilization.\nRaising suspicions\nA pivotal aspect of this issue was the use of the Couchbase sub-document API\nwithin the gateway service, an approach not widely adopted across our internal microservice landscape, yet notable for its efficiency. According to\nthe documentation, this API significantly reduces traffic by allowing the fetching or mutating only specific parts of a Couchbase document.\nEssentially, it acts as a substitute for the concept of projection, familiar to SQL users.\nIn our investigation we closely examined the data collected on the Couchbase node, the operational dynamics of the gateway service’s cache, and insights\nfrom scrutinizing both the Couchbase driver and server code. We hypothesized that the crux of the problem might be linked to the cache warm-up process for\nnewly launched instances.\nOur investigation uncovered several indicators pointing toward the core of the issue:\nA disproportionately large number of requests targeted a single document, inevitably directing traffic to a specific node.\nThe node hosting this heavily queried document corresponded with the one mentioned in timeout-related logs.\nInstances that had been running for an extended period reported virtually no errors.\nThe volume of requests to Couchbase from the affected instances was extraordinarily high, not aligning with the number of requests registered on\nthe Couchbase side. This discrepancy suggested that if the cache warming process was at fault, the sheer magnitude of attempted requests was overwhelming\neven the local network buffers.\nHowever, these observations were merely pieces of a larger puzzle. We noticed a “snowball effect” where the system’s inability to process an initial set\nof requests for newly initiated instances triggered a cascade of failures. But the question remained: Why? What made these instances different,\nand why didn’t other clients on the same cluster experience similar issues? This was the crucial moment to take a closer examination of the sub-document\noperations to determine their efficiency and optimization.\nLet’s benchmark it\nDespite an extensive search, we were unable to locate any tools capable of reliably testing our hypothesis—that sub-document operations executed during\nthe warm-up phase could significantly challenge Couchbase’s handling capabilities. As a result, we developed a simple tool and made it\navailable in on GitHub.\nThis tool is designed to create a sample document and then execute parallel sub-document fetch operations concurrently.\nThe sample document is structured as follows:\n\n{\n    \"key\": \"test-subdoc\",\n    \"data\": {\n        \"subkey-000000\": \"value-000000\",\n        \"subkey-000001\": \"value-000001\",\n        . . .\n        \"subkey-0….N\": \"value-0…..N\",\n    }\n}\n\n\nThe tool allows manipulating several knobs, which includes:\nParallelism: Determines the number of parallel goroutines that will attempt to fetch the same sub-documents concurrently.\nDocument Size: Defined by the number of sub-keys, this directly affects the document’s binary size.\nLevel of Search Difficulty: This essentially refers to how deep or how far into the main document the target sub-document is located.\nThe concept is illustrated in the diagram below:\n\nCaveats\nThe primary objective of this exercise was to identify potential bottlenecks, not to conduct a highly accurate performance assessment of Couchbase clusters.\nTherefore, we opted to run our experiments using a local Docker container (couchbase:community-6.6.0), rather than on a dedicated, well-isolated cluster.\nWe acknowledge that hosting both the server and the benchmarking tool on the same machine may compromise the reliability and accuracy of the results.\nConsequently, we advise against using the findings from these tests for comprehensive assessments or comparisons with other technologies.\nBenchmark steps\nThe procedure for each experiment follows a similar framework, outlined in the steps below:\nDocument Preparation: Initiate the document with the desired number of sub-documents, as dictated by one of the experimental variables.\nDocument Storage: Save this document under a predetermined key.\nGoroutine Initialization: Launch a specified number of goroutines, the quantity of which is determined by another experimental variable.\nFetch Operations: Each goroutine executes a series of fetch operations, which can be either regular (retrieving the entire sample document) or\nsub-document (accessing a set of sub-documents). It’s important to note that these requests are executed in a blocking manner; a new fetch operation is\nperformed only after the completion of the preceding one. In sub-document mode, the difficulty of the fetch operation is controlled through\nan experiment variable.\nCompletion Wait: Await the termination of all goroutines.\nResults Reporting: Calculate and display the estimated RPS (requests per second).\nEstimate baseline\nPrior to delving into sub-document operations, we sought to establish the maximum number of regular get operations that our local Couchbase Server instance\ncould handle. Through testing at various levels of concurrency, we determined the maximum throughput for our specific setup.\nIt was approximately 6,000 to 7,000 RPS, regardless of whether the requests were for small documents (less than 200 bytes)\nor for non-existent documents. These findings were further validated by the statistics available through the Couchbase UI.\nBenchmark Command: Attempting to fetch a non-existent document yielded a rate of 6388 RPS.\n\n./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5 --search-non-existent\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=true, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: not-exists\n\nregular report: successes: 0, errors: 200000, duration: 31.306684977s, rps: 6388.411937, success rps: 0.000000\n\n\n\nBenchmark Command: Fetching an existing small (195 bytes) document yielded a rate of 6341 rps.\n\n./cb-perf-tester regular  --parallel 200 --repeat 1000 --keys 5\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=5, not-existent=false, repeats=1000, parallel=200\nGenerated doc with subkeys: 5, byte size is: 195\n\nsearch for key: test-regular\n\nregular report: successes: 200000, errors: 0, duration: 31.536538682s, rps: 6341.850068, success rps: 6341.850068\n\n\n\nTesting scenarios\nNow that we have a baseline for comparison, we’re set to evaluate it against the outcomes of various scenarios. To ensure the tests are comparable,\nwe’ll maintain constant parallelism across all tests, specifically using 200 goroutines. The variables that will differ across scenarios include:\nTotal Number of Sub-Documents: This determines the overall size of the sample document, as the document’s size is directly related to the number\nof sub-documents it contains.\nNumber of Searched Sub-Documents: This refers to how many sub-paths within the sample document will be targeted in a single fetch operation.\nSearch Difficulty: This aspect dictates the difficulty of locating the searched sub-paths within the document.\nIt’s important to highlight that in each scenario, we will manipulate only one variable at a time while keeping the other parameters constant.\nScenario A: The impact of document size on performance\n\nTo better visualize the impact, let’s look at the diagram for HARD scenario:\n\nIt is clearly visible that there is a strict correlation between document size and performance.\nScenario B: The impact of the number of searched sub-documents on performance\n\nTo better visualize the impact, let’s look at the diagram for HARD scenario:\n\nAs observed in the previous scenario, there is a clear correlation between the number of sub-documents accessed and the system’s performance.\nFurther analysis\nGiven the evident correlation between the document size/number of queried sub-paths and performance degradation, we delve into the mechanics to understand\nthe root cause of these test results. A notable observation during the tests relates to CPU utilization within the Docker environment, where, despite having\nsix cores available, only a single core was actively utilized. Intriguingly, this usage was monopolized by a single thread (mc:worker_X).\nThis phenomenon directly stems from Couchbase’s handling of Key-Value (KV) connections. By default, the Couchbase driver initiates only a single connection to\neach cluster node for KV operations. However, this configuration can be adjusted in certain Software Development Kits (SDKs)—the Java SDK,\nfor instance, allows modification through IoConfig.numKvConnections.\nWhen a connection is established, Couchbase assigns it to\na specific worker thread) using\na Round-Robin (RR)) algorithm. As a result, the Couchbase Server does not\nfully utilize available CPU power for a single connection, even when a lot of resources are free. This behavior can be seen as beneficial, serving to mitigate\nthe “noisy neighbor” effect, provided there are sufficient\nspare cores available to manage new connections. This mechanism ensures balanced resource use across connections, as illustrated in the diagram below:\n\nConversely, one may encounter fluctuating performance due to instances of misfortune, where if other clients significantly burden certain worker threads,\nand your connection is allocated to one of these overloaded threads, performance inconsistencies arise. This scenario, where a client experiences higher than\nusual response times due to an imbalanced distribution of workload across worker threads, is depicted in the diagram below:\n\nThis behavior explains the apparent paradox observed during the stability issues: the Couchbase node showed no clear signs of being overloaded, yet certain\nanomalous symptoms were present, such as a metric indicating the minimum idle percentage across all cores plummeting\nto 0% during the disturbances. This leaves no doubt that operations on sub-documents have the potential to overburden worker threads within\nthe Couchbase Server. With this understanding we are now ready to delve deeper into the root cause of such behavior.\nWhat documentation says\nThe documentation for Couchbase, housed alongside the server’s source code, is notably comprehensive, including a dedicated section on\nsub-documents. However, it falls short of providing detailed insights into\nthe internal workings of these operations. Additionally, there is a lack of discussion on the performance implications of sub-document operations,\nwith only a few remarks on data typing and float numbers that do not apply to the cases we tested. Attempts to find answers on the Couchbase forum were also\nunfruitful, yielding no substantial information on the performance issues we encountered. Despite this, there is confirmation from others in the community who\nhave observed\nsimilar problems.\nWhat source code says\nA thorough analysis of the codebase reveals a definitive cause for the performance degradation observed. It’s important to note that Couchbase requires\ndecompression of a document for any lookup or manipulation\noperation, whether the document is retrieved from RAM or disk. Let’s start from a point where Couchbase starts doing\nlookups on a decompressed object:\n\n// 2. Perform each of the operations on document.\nfor (auto\u0026 op : operations) {\n    switch (op.traits.scope) {\n    case CommandScope::SubJSON:\n        if (cb::mcbp::datatype::is_json(doc_datatype)) {\n            // Got JSON, perform the operation.\n            op.status = subdoc_operate_one_path(context, op, current.view);\n        }\n\n\nA critical observation from our analysis is that each operation (lookup) is executed sequentially through the invocation of subdoc_operate_one_path.\nTo understand the performance implications, let’s examine\nthe implementation of this lookup\nprocess:\n\n// ... and execute it.\nconst auto subdoc_res = op.op_exec(spec.path.data(), spec.path.size());\n\n\nThe Investigation reveals that the lookup functionality is powered by a specialized library,\nlibrary subjson, which in turn\nuses\nthe jsonsl library for parsing JSON in a streaming manner. An enlightening piece of information about performance can be found in\nthe README of the subjson library, which is integral to Couchbase’s solution. The direct quote from the README is as follows:\nBecause the library does not actually build a JSON tree, the memory usage and CPU consumption is constant, regardless of the size of the actual JSON object\nbeing operated upon, and thus the only variable performance factor is the amount of actual time the library can seek to the location in the document to\nbe modified.\nOn a single Xeon E5520 core, this library can process about 150MB/s-300MB/s of JSON. This processing includes the search logic as well as any\nreplacement logic.\nThis analysis clearly demonstrates that lookups targeting paths situated towards the end of a document are markedly slower compared to those aimed\nat the beginning. Moreover, each sequential lookup needs to repeat document parsing. The impact of this implementation on performance is significant.\nFor a more intuitive understanding of these effects, please refer to the diagram below:\n\nThe performance characteristics we’ve outlined align with the outcomes observed in our experiments. To illustrate, consider a detailed examination of\na single HARD test scenario—specifically, a case where the sub-documents targeted for search were positioned towards the end of the JSON document:\n\n./cb-perf-tester subdoc  --parallel 200 --repeat 50 --search-keys 10 --difficulty hard\nUsing config file: /Users/tomasz.ziolkowski/.cb-perf-tester.yaml\nbenchmark params: keys=10000, level=Hard, search-keys=10, repeats=50, parallel=200\nGenerated doc with subkeys: 10000, byte size is: 310043\n\nsearch for subkeys [level=Hard]: [data.subkey-009999 data.subkey-009998 data.subkey-009997 data.subkey-009996 data.subkey-009995 data.subkey-009994 data.subkey-009993 data.subkey-009992 data.subkey-009991 data.subkey-009990]\n\nsubdoc report: successes: 10000, errors: 0, duration: 1m19.784865193s, rps: 125.337055, success rps: 125.337055\n\n\nBy multiplying the size of the document by the number of sub-documents queried, we can determine the total stream size that the library must process, which,\nin this case, approximates to ~3MB. Further multiplying this figure by the RPS gives us an insight into the overall throughput of\nthe stream processing:\n3MB x 125 RPS ~= 375 MBps\nThe calculated throughput slightly exceeds the benchmarks outlined in the README. Moreover, the estimated throughput remains nearly constant across\nvarious tests. For a comprehensive view of these findings, please refer to the diagram below, which displays the estimated throughput for tests conducted under\nthe HARD level with a document size of approximately 300KB:\n\nConclusions\nThe Couchbase sub-document API, while designed to optimize network throughput, introduces significant performance trade-offs. Notably, even\nunder optimal conditions, operations on sub-documents are noticeably slower compared to regular get operations fetching\nsmall documents—evidenced by a comparison of the baseline performance; approximately 4-5k RPS for sub-document operations vs. 6-7k RPS for\nregular get operations.\nThe method Couchbase employs for executing lookups directly influences performance, manifesting declines as either the document size increases or the number of\nlookups per request raises. This slowdown affects all requests over the same connection due to the high CPU demand of sub-document lookup operations.\nParticularly in environments utilizing reactive/asynchronous clients, this can overload the Couchbase worker thread, leading to a halt in request servicing.\nImportantly, an overloaded worker may manage connections from multiple clients, potentially exacerbating the “noisy neighbor” effect.\nWhile there are strategies to mitigate these issues from the perspective of a client, such as the gateway service, these considerations warrant\na separate discussion, which I plan to address in a future blog post.","guid":"https://blog.allegro.tech/2024/05/couchbase-subdocuments-bottleneck.html","categories":["tech","couchbase","sub-documents","performance","bottlenecks"],"isoDate":"2024-05-15T22:00:00.000Z"},{"title":"Ten Years and Counting: My Affair with Microservices","link":"https://blog.allegro.tech/2024/04/ten-years-microservices.html","pubDate":"Fri, 12 Apr 2024 00:00:00 +0200","authors":{"author":[{"name":["Michał Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"\u003cp\u003eIn early 2024, I hit ten years at \u003ca href=\"https://allegro.tech/\"\u003eAllegro\u003c/a\u003e, which also happens to be how long I’ve been working with \u003ca href=\"https://martinfowler.com/microservices/\"\u003emicroservices\u003c/a\u003e.\nThis timespan also roughly corresponds to how long the company as a whole has been using them, so I think it’s a good time to outline the story of project\nRubicon: a very ambitious gamble which completely changed how we work and what our software is like. The idea probably seemed rather extreme at the time, yet I\nam certain that without this change, Allegro would not be where it is today, or perhaps would not be there at all.\u003c/p\u003e\n\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eAllegro is \u003ca href=\"https://about.allegro.eu/who-we-are/at-a-glance\"\u003eone of the largest e-commerce sites in Central Europe\u003c/a\u003e, with 20 million users and over 300 million\noffers. It was founded in 1999, originally with just the Polish market in mind. The story I want to tell you starts in 2013, a year before I joined.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/allegro-site.png\" alt=\"Allegro website showing some offers\" class=\"small-image\" style=\"box-shadow: 0 0 4px 0 #D7DBD6;\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eIn 2013, the site was already large and relevant, but its commercial success and further growth led to development bottlenecks emerging. The codebase was a\nmonolithic PHP application, with some auxiliary processes written in C. Checked out, the git monorepo weighed about 2 GB, and the number of pull requests\nproduced daily by a few hundred developers was so large that if you started a new branch in the morning, you were almost sure to get some conflicts if you\nwanted to merge in the afternoon. The system was centered around a single, huge database, with all the performance and architectural challenges you can imagine.\nTests were brittle and took ages to finish. Deployment was a mostly manual and thus time-consuming processes that required lots of attention and ran the risk of\ncausing a serious problem in production if something went wrong. It was so demanding and stressful I still remember my team having to run the deployment once\n(in place of the usual deployers) as a big event.\u003c/p\u003e\n\n\u003ch2 id=\"rubicon-rises\"\u003eRubicon Rises\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/Tabula_Peutingeriana_Rubicon.png\" alt=\"Rubicon river on an old map\" class=\"small-image-right\" /\u003e\nIt was becoming clear that we would hit a wall if we continued working this way. So, around 2012/2013, the idea for a complete overhaul of the architecture\nstarted to emerge. We began experimenting with \u003ca href=\"https://en.wikipedia.org/wiki/Service-oriented_architecture\"\u003eSOA (Service-Oriented Architecture)\u003c/a\u003e by\ncreating a small side project, the so-called New Platform, in PHP, as a proof-of-concept. We also decided we would start doing\n\u003ca href=\"https://en.wikipedia.org/wiki/Agile_software_development\"\u003eAgile\u003c/a\u003e, \u003ca href=\"https://en.wikipedia.org/wiki/Test-driven_development\"\u003eTDD\u003c/a\u003e, and\n\u003ca href=\"https://en.wikipedia.org/wiki/Cloud_computing\"\u003eCloud\u003c/a\u003e. After a short while, on top of this, we decided to switch to Java for backend development.\nIt was becoming clear that \u003ca href=\"/2015/02/Evolution-of-our-platform-architecture.html\"\u003eit would be a revolution indeed\u003c/a\u003e, requiring everyone to change\nthe way they worked, and to switch out the whole development ecosystem, starting with the core programming language. Once we got this going, there would be\nno turning back, so a matching name also appeared: Project \u003ca href=\"https://en.wikipedia.org/wiki/Crossing_the_Rubicon\"\u003eRubicon\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe project had such a broad scope that it even came with its own constitution, a set of high-level guidelines to be used in case of doubt. It focused mostly\non ways of working and highlighted the value of learning (on personal, team, and company level), testing, reuse, empirical approach to software development,\nand active participation in the open-source community both as users and as contributors. Specific technical assumptions included:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003efocus on quality\u003c/li\u003e\n  \u003cli\u003emicroservices\u003c/li\u003e\n  \u003cli\u003edistributed, multi-regional, active-active architecture\u003c/li\u003e\n  \u003cli\u003eJava\u003c/li\u003e\n  \u003cli\u003ecloud deployment\u003c/li\u003e\n  \u003cli\u003eusing open-source technologies\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThere was also a list of success criteria for the project:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003ethe monolith is gone\u003c/li\u003e\n  \u003cli\u003ewe have Java gurus on board\u003c/li\u003e\n  \u003cli\u003ewe have services\u003c/li\u003e\n  \u003cli\u003edevelopment is faster\u003c/li\u003e\n  \u003cli\u003ewe have continuous delivery\u003c/li\u003e\n  \u003cli\u003ewe don’t have another monolith\u003c/li\u003e\n  \u003cli\u003ewe still make money\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eFaster development was probably the most important goal, since slow delivery was the direct reason we embarked on this long journey.\u003c/p\u003e\n\n\u003cp\u003eOn top of these lists, more detailed plans were made as well, of course. For example, various parts of the system were prioritized for moving out of the\nmonolith as we were well aware we would not be able to work on everything at once. Being Agile does not mean planning is to be avoided, only that plans have\nto be flexible. So, armed with a plan, we got off to work.\u003c/p\u003e\n\n\u003ch2 id=\"execution\"\u003eExecution\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/tim-gouw-1K9T5YiZ2WU-unsplash.jpg\" alt=\"Man sitting at a laptop, overwhelmed by what he sees on the screen\" class=\"small-image-right\" /\u003e\nToo much has happened during the 10+ years to report here. The initial period was really frantic since we had to set up everything, and, first of all, teams had\nto switch to a new mindset. This was also a period of intense hiring, and the time I joined the recently opened office in Warsaw. Microservices were at that\ntime only starting to gain traction, so while we used the experiences of others as much has possible, we had to learn many things ourselves, sometimes learning\nthem the hard way.\u003c/p\u003e\n\n\u003cp\u003eTo give you an idea of the pace, here are just some of the things that happened in 2013:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eoutline of the common architecture (service discovery, logging) created\u003c/li\u003e\n  \u003cli\u003ea set of common libraries created (\u003ca href=\"https://youtu.be/PeioFobaq94\"\u003epresentation from 2016\u003c/a\u003e)\u003c/li\u003e\n  \u003cli\u003etraining in Java and JVM for PHP developers\u003c/li\u003e\n  \u003cli\u003erecruitment of Java developers started\u003c/li\u003e\n  \u003cli\u003efirst Java code got written\u003c/li\u003e\n  \u003cli\u003efierce discussions about technology choices (Guice vs Spring, Maven vs Gradle, Jetty vs Undertow)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWhat followed in 2014 (this is the part I could already experience in person):\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003evarious self-service tools allowing developers to handle common tasks such as creating databases themselves rather than by involving specialized support teams\u003c/li\u003e\n  \u003cli\u003eautomation tools\u003c/li\u003e\n  \u003cli\u003edevelopment of \u003ca href=\"https://hermes.allegro.tech/\"\u003eHermes\u003c/a\u003e, our open-source message broker built on top of Kafka, started\u003c/li\u003e\n  \u003cli\u003estrategic DDD training with Eric Evans\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/2014/12/How-to-migrate-to-Java-8.html\"\u003emigration to Java 8\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eglobal architecture improvements\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://allegro.tech/\"\u003eallegro.tech\u003c/a\u003e, the project to coordinate the visibility of our tech division online and offline, of which this blog is a part,\nstarted\u003c/li\u003e\n  \u003cli\u003eSRE team created\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/2016/09/CQK-TOP-10.html\"\u003eCQK (Code Quality Keepers) guild\u003c/a\u003e opened\u003c/li\u003e\n  \u003cli\u003efirst Java services deployed to production\u003c/li\u003e\n  \u003cli\u003eintense recruitment and learning\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe number of both production services and of tools supporting developers’ work that got created thereafter is staggering. It should be clear from just the\nlist above that this was a huge investment, and could only proceed due to full buy-in of both technology and business parts of the company. It was indeed a\ngamble, well-informed, but still a gamble that carried big risk should it fail, but an even greater risk if we were to stay with the old architecture.\u003c/p\u003e\n\n\u003cp\u003eAt this point you probably can see that actually building microservices seems like a minor part of the whole undertaking. There was a lot of work to\nwriting so many parts of this huge system anew, but indeed the amount of work that we had to invest into\n\u003ca href=\"/2016/02/datacenter-migration-great-adventure.html\"\u003einfrastructure\u003c/a\u003e,\n\u003ca href=\"/2018/10/turnilo-lets-change-the-way-people-explore-big-data.html\"\u003etooling\u003c/a\u003e, and\n\u003ca href=\"/2016/09/are-code-reviews-worth-your-time.html\"\u003elearning\u003c/a\u003e, was immense. It was also absolutely critical for the project’s success. A lot has been\nsaid about microservices, and it is true that for them to be beneficial, you need the right scale and the right kind of system. We had both, and so the decision\nto move to microservices proved to be worthwhile, but despite knowing the theory, I think no one expected the amount of auxiliary work to be so huge. Indeed,\nwhile microservices themselves may be simple, the glue that holds them together is not.\u003c/p\u003e\n\n\u003ch2 id=\"flashbacks\"\u003eFlashbacks\u003c/h2\u003e\n\n\u003cp\u003eSummarizing ten years of rapid development is tough, so instead of trying to tell you the full story, I decided to share a few flashbacks: moments which I\nremembered for one reason or another.\u003c/p\u003e\n\n\u003ch3 id=\"no-ing-sql\"\u003eNo-ing SQL\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/tobias-fischer-PkbZahEG2Ng-unsplash.jpg\" alt=\"Huge modern library filled with books\" class=\"small-image-right\" /\u003e\nWhen \u003ca href=\"/2015/01/working-with-legacy-architecture.html\"\u003erefactoring our huge monolith into smaller microservices\u003c/a\u003e, we needed to also choose the\ndatabase to use for each of them. Since horizontal scalability was our focus, we preferred \u003ca href=\"https://en.wikipedia.org/wiki/NoSQL\"\u003eNoSQL databases\u003c/a\u003e when possible.\nThis was a big change since the monolithic solution relied on a single, huge SQL database. On top of that, it was not modularized well, and in many places\nthere was little or no separation between domain and persistence layers. If the monolith was structured well, splitting it into separate services would have\nbeen much easier. Unfortunately, this was not the case, so we had to perform the transition to NoSQL together with other refactorings and cleanup. Usually,\nwe had to deeply remodel data and operations handling it, especially transactional, so that they could be executed in the new environment. This was often\na significant effort even if we could divide the code in such way that the transaction or set of related operations ended up within the same service. Things\nbecame even more complicated if an operation spanned multiple services (and databases) in the new architecture. This is one of the reasons why dividing a\nbig application into smaller chunks is much harder than it may seem at first.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://cassandra.apache.org/\"\u003eCassandra\u003c/a\u003e was initially our preferred NoSQL database for most tasks. Only after a while did we learn that each database is good\nfor some use cases and bad for others, and that we needed \u003ca href=\"https://en.wikipedia.org/wiki/Polyglot_persistence\"\u003epolyglot persistence\u003c/a\u003e to achieve high performance\nand get the required flexibility in all cases. The team I worked on was among the first Cassandra adopters at the company, and as is often the case when\nyou run something in production for the first time, we uncovered a number of issues in our Cassandra deployment which was “ready” but not tested in production\nyet. The team responsible for the DB was learning completely new stuff just as we were.\u003c/p\u003e\n\n\u003cp\u003eAn argument sometimes put up against the need to separate your application’s persistence layer from the domain logic is “you’re never going to switch out the\nDB for another one anyway”. Most of the time that’s true, but in one service we did have to switch from Cassandra to \u003ca href=\"https://www.mongodb.com/\"\u003eMongoDB\u003c/a\u003e\nafter we found out our access patterns were not very well aligned with Cassandra’s data model. We managed to do it inside a single two-week sprint, and\napart from the service becoming faster, its clients would not notice any difference as the external API stayed the same. While the (usually theoretical)\nprospect of switching databases is not the only reason for decoupling domain and persistence layers, it did help a lot in this case, and it is about this\ntime I started to understand why we were creating so many classes even though you could just cram all that code into one.\u003c/p\u003e\n\n\u003cp\u003eI also managed to kill our Cassandra instance once when I was learning about big data processing and created a job that was supposed to process some data from\nthe DB. The job was so massively parallel that the barrage of requests it generated overwhelmed even Cassandra. Fortunately, this situation also showed the\nadvantage of having separate databases for each service, as only that single service experienced an outage.\u003c/p\u003e\n\n\u003ch3 id=\"into-the-cloud\"\u003eInto the cloud\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/thisisengineering-raeng-zBLtU0zbJcU-unsplash.jpg\" alt=\"Engineer plugging in network cables into servers\" class=\"small-image-right\" /\u003e\nBefore joining Allegro, I had only deployed to physical servers, so moving to the cloud was a big change. At first, we deployed our services to virtual\nmachines configured in \u003ca href=\"https://www.openstack.org/\"\u003eOpenStack\u003c/a\u003e. What a convenience it was to be able to just set up a complete virtual server with a few\nclicks rather than wait days for a physical machine. We used \u003ca href=\"https://www.puppet.com/\"\u003ePuppet\u003c/a\u003e to fully configure the virtual machines for each service, so\nwhile you had to write some configuration once, you could spin up a new server configured for your service almost instantly afterwards.\u003c/p\u003e\n\n\u003cp\u003eThis \u003ca href=\"https://en.wikipedia.org/wiki/Infrastructure_as_a_service\"\u003eIaaS (Infrastructure as a Service)\u003c/a\u003e approach was very convenient, and quite a change, but in\nmany ways it still resembled what I had known before: you had a machine, even if virtual, and you could \u003ccode class=\"language-plaintext highlighter-rouge\"\u003essh\u003c/code\u003e and run any commands there if you wanted, even\nif it was rarely needed since Puppet set up everything for you.\u003c/p\u003e\n\n\u003cp\u003eThe real revolution came when we switched to \u003ca href=\"https://en.wikipedia.org/wiki/Platform_as_a_service\"\u003ePaaS (Platform as a Service)\u003c/a\u003e model, at that time based on\n\u003ca href=\"https://mesos.apache.org/\"\u003eMesos\u003c/a\u003e and \u003ca href=\"https://mesosphere.github.io/marathon/\"\u003eMarathon\u003c/a\u003e. Suddenly, there were no more virtual machines, and you could not \u003ccode class=\"language-plaintext highlighter-rouge\"\u003essh\u003c/code\u003e\nto the server where your software was running. For me, this was a real culture shock, and even though up to this point I was very enthusiastic about all the\ncool technology we were introducing, the thought of \u003cem\u003eno more \u003ccode class=\"language-plaintext highlighter-rouge\"\u003essh\u003c/code\u003e\u003c/em\u003e freaked me out. How would I know what was going on in the system if I couldn’t even access\nit? Despite my reservations, I gradually found out you could indeed deploy and monitor software despite not being able to access the machine via \u003ccode class=\"language-plaintext highlighter-rouge\"\u003essh\u003c/code\u003e. It\nsounds weird in retrospect, but this was one of the most difficult technological transitions in my career.\u003c/p\u003e\n\n\u003cp\u003eAfter a while, we built some abstraction layers on top of Mesos, including a custom \u003cem\u003eapp console\u003c/em\u003e that allows you to deploy a service and perform all\nmaintenance tasks. It isolates you from most details of the underlying system, and is so effective that when we migrated from Mesos to \u003ca href=\"https://kubernetes.io/\"\u003eKubernetes\u003c/a\u003e\nlater on, the impact on most teams was much smaller that you could imagine for such a big change. Our App Console is an internal project, but if you are\nfamiliar with \u003ca href=\"https://backstage.spotify.com/learn/backstage-for-all/backstage-for-all/1-introduction/\"\u003eBackstage\u003c/a\u003e, it should give you an idea of what kind\nof tool we’re talking about here.\u003c/p\u003e\n\n\u003ch3 id=\"monitoring\"\u003eMonitoring\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/carlos-muza-hpjSkU2UYSU-unsplash.jpg\" alt=\"Laptop displaying various charts\" class=\"small-image-right\" /\u003e\nBelieve it or not, initially all monitoring was centralized and handled by a single team. If you wanted to have any non-standard charts in\n\u003ca href=\"https://www.zabbix.com/\"\u003eZabbix\u003c/a\u003e or any custom alerts (and obviously, you wanted to), you had to create a ticket in JIRA, describe exactly what you wanted,\nand after a while, the monitoring team would set it up for you. The whole process took about a week, and quite often, right after seeing the new chart you\nknew you wanted to improve it, so you would file another ticket and wait another week. Needless to say, this was incredibly frustrating, and I consider it\none of my early big successes when I kept pushing the monitoring team until they finally gave in and allowed development teams to configure all of their\nobservability settings themselves.\u003c/p\u003e\n\n\u003ch3 id=\"going-polyglot\"\u003eGoing polyglot\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/azamat-e-FP_N_InBPdg-unsplash.jpg\" alt=\"Man working on two laptops at the same time\" class=\"small-image-right\" /\u003e\nWhile Rubicon started out with the premise of rewriting our software in Java, we quickly started experimenting with other JVM languages. The team I worked\non considered Scala for a while, but after some experimentation decided against using it as our main language. Some other teams, however, did choose it, and\neven though they are a minority at Allegro, we have some microservices written in Scala to this day. On the other hand, Scala is the dominant language at\nAllegro when it comes to writing \u003ca href=\"https://spark.apache.org/\"\u003eSpark\u003c/a\u003e jobs.\u003c/p\u003e\n\n\u003cp\u003eSome time around 2015, a teammate found out there was a relatively new, but promising, language called \u003ca href=\"https://kotlinlang.org/\"\u003eKotlin\u003c/a\u003e. It so happened that we\nwere just starting work on a new microservice which was still very simple and not quite critical. He decided to use it as a testbed, and within I think two days\nrewrote the whole thing in Kotlin. Thanks to the services being independent and this one not being very important yet, we could safely experiment in production\nand assess the stability of the rewritten service. Learning the language by writing actual production-ready code rather than just playing with throw-away code\nallowed us to check what advantages and disadvantages the language offered under realistic usage scenarios. Kotlin caught on, and gradually we started to use\nit for more and more new services and to use it for new features in existing Java services as mixing the two was easy. Many services already used\n\u003ca href=\"https://groovy-lang.org/\"\u003eGroovy\u003c/a\u003e and \u003ca href=\"https://spockframework.org/\"\u003eSpock\u003c/a\u003e for tests anyway. At this point, Kotlin is more popular than Java at Allegro, and\non our blog we published \u003ca href=\"/2016/06/kotlin-null-safety-part1.html\"\u003esome\u003c/a\u003e articles \u003ca href=\"/2021/04/kotlin-scripting.html\"\u003eabout Kotlin\u003c/a\u003e,\nincluding \u003ca href=\"/2018/05/From-Java-to-Kotlin-and-Back-Again.html\"\u003eone\u003c/a\u003e which unfortunately stirred a lot of controversy, and caused a (deserved IMO)\nshitstorm both inside and outside the company.\u003c/p\u003e\n\n\u003cp\u003eBesides JVM languages, we by now have also microservices written in \u003ca href=\"/2020/03/dotnet-new-templates.html\"\u003eC#\u003c/a\u003e,\n\u003ca href=\"/2016/03/writing-fast-cache-service-in-go.html\"\u003eGo\u003c/a\u003e, \u003ca href=\"/2022/01/how-do-coroutines-work-internally-in-python.html\"\u003ePython\u003c/a\u003e,\n\u003ca href=\"https://elixir-lang.org/\"\u003eElixir\u003c/a\u003e, and probably a few more languages I forgot to mention. This is just the backend, but our\n\u003ca href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\"\u003efrontend architecture\u003c/a\u003e also allows for components written in various\nlanguages. And besides customer-facing business code, there are also internal tools and utilities, sometimes written using yet other general-purpose languages\nand \u003ca href=\"https://en.wikipedia.org/wiki/Domain-specific_language\"\u003eDSL\u003c/a\u003es. Finally, there’s the whole world of AI, including prompting for generative AI that you\ncan also consider a programming language of sorts.\u003c/p\u003e\n\n\u003cp\u003eThe main point I want to make here is that using microservices has allowed us to safely experiment with various programming languages, to consciously limit\nthe blast radius of those experiments should anything go wrong, and to perform all transitions gradually. Of course, this all has a purpose: finding the\nbest tool for the job, and using all the different languages’ strengths where they can help us most. It is not about introducing new tools just for the sake\nof it, which would cause but chaos and introduce risks related to future maintenance. I think the autonomy teams get in making technical decisions, yet\ncombined with responsibility for the outcomes, is what allows us to learn and find new ways of doing things while at the same time it limits the risks\nassociated with experimenting. As in many other cases, things work well when the organization’s ways of working (team autonomy) are aligned with technical\nsolutions (microservices).\u003c/p\u003e\n\n\u003ch3 id=\"using-antipatterns-wisely\"\u003eUsing antipatterns wisely\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/alexander-schimmeck-kpihcevjT5w-unsplash.jpg\" alt=\"Two men performing a dangerous stunt on a bicycle\" class=\"small-image-right\" /\u003e\n\u003ca href=\"/2016/01/microservices-and-macro-mistakes.html\"\u003eGood practices\u003c/a\u003e are heuristics: most of the time, following them is a good idea. For example,\ntwo microservices should not share database tables since this introduces tight coupling: you can’t introduce a change to the schema and deploy just one service\nbut not the other. Your two services are not independent, but form a distributed monolith instead. Avoiding such situations is just common sense.\u003c/p\u003e\n\n\u003cp\u003eStill, you should always keep in mind the reasons why a good practice exists, what it protects you from and what costs it introduces. At one point we had a\ndiscussion within our team about how to best handle a peculiar performance issue. Our service connected to an Elasticsearch instance and performed two kinds\nof operations: reads and writes. Reads were much more numerous, but writes introduced heavy load (on the service itself — Elastic could handle it). Writes\ncame in bursts, so most of the time things worked well, but when a burst of writes arrived, performance of the whole service suffered and read times were\naffected. We tried various mechanisms for isolating the two kinds of operation, but couldn’t do it effectively.\u003c/p\u003e\n\n\u003cp\u003eA colleague suggested we split the service in two, one responsible for handling reads and the other for writes. We had a long discussion, in which I\npresented arguments for having a single service as the owner of the data, responsible for both reads and writes, and highlighted what issues could arise due to\nthe split. While keeping the service intact seemed to be the elegant thing to do, I didn’t have a good solution for the performance issue. My colleague’s\nidea to split the service, on the other hand, while somewhat messy, did offer a chance to solve it.\u003c/p\u003e\n\n\u003cp\u003eSo, we decided to just try it and see whether this approach would solve the performance issue and how bad the side effects would be. We did just that, and the\nantipattern-based solution worked great: performance hiccups went away, and despite sharing the common Elasticsearch cluster, the two services remained\nmaintainable. We were not able to fully assess this aspect right away, but time proved my colleague right as well: during the 3+ years we worked with that\ncodebase later on, we only ran into issues related to sharing Elasticsearch once, and we managed to fix that case quickly. It certainly did help, though, that\nboth services kept being developed by the same team, and that by the time we introduced the split, the schema was already quite stable and did not change often.\nNonetheless, had I insisted on keeping things clean, we would have probably spent much more time fighting performance issues than we lost during the single\nissue that resulted from sharing Elasticsearch between services. Know when to use patterns, know when to use antipatterns, and use both wisely.\u003c/p\u003e\n\n\u003ch3 id=\"one-size-does-not-fit-all\"\u003eOne size does not fit all\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/brianna-r-_-54wJzpH6Q-unsplash.jpg\" alt=\"Adult lion with cub\" class=\"small-image-right\" /\u003e\nI think we’ve always been quite pragmatic about sizing our microservices. It’s hard to define a set of specific rules for finding the right size, but going\ntoo far in one direction or the other causes considerable pain. Make a service huge, and it becomes too hard for a single team to maintain and develop, or\nscaling issues arise similar to those you could experience with a monolith. Make it very small, and you might get overwhelmed by the overhead of having your\nlogic split between too many places, issues with debugging, and the performance penalty of the system being distributed to the extreme.\u003c/p\u003e\n\n\u003cp\u003eMost services I got to work on at Allegro were not too tiny, and contained some non-trivial amount of logic. There were sometimes agitated discussions about\nwhere to implement a certain feature, in particular whether it should be in an existing service or in a new one. In hindsight, I think most decisions made\nsense, but there were certainly cases where a feature that we believed would grow ended up in a new service which then never took off and remained too small,\nand cases where something was bolted onto an existing service because it was easier to implement this way, but which caused some pain later on.\u003c/p\u003e\n\n\u003cp\u003eI think I only once saw a team fall into the nanoservice trap where services were designed so small the split caused more trouble than it was worth. On the\nother hand, there were certainly services which you could no longer call \u003cem\u003emicro\u003c/em\u003e by any stretch. This was not necessarily a bad thing. As long as a service\nfulfilled a well-defined role, a single team was enough to take care of it, and it was OK that you had to deploy and scale the whole thing together, things\nwere fine. In some cases of services which grew really much too big (indicators being that they contained pieces of logic only very loosely related to each\nother, and that at some point multiple teams were regularly interested in contributing), we did get back to them and split them up. It was not very easy,\nbut doable, and the second-hardest part was usually finding the right lines along which to divide. The only thing harder was finding the time to perform\nsuch operations, but with a bit of negotiation and persistence, after a while we usually succeeded.\u003c/p\u003e\n\n\u003cp\u003eThere is an ongoing discussion of whether we have too many microservices. It’s not an urgent thing, but there are reasons to not go too high, such as\ncertain technical limitations in the infrastructure and the cost of overprovisioning (each service allocates resources such as memory or CPUs with a margin,\nand those margins add up). Still, the fact that we are well above a thousand services and yet their number is only a minor nuisance, speaks well of our\ntooling and organization. Indeed, thanks to some custom tools, creating a new service is very easy (maybe too easy?), and managing those already there is\nalso quite pleasant. This is possible due to huge investments we made early on (and continue): we knew right from the start that while each microservice may\nbe relatively simple, a lot of complexity goes into the glue that holds the whole system together. Without it, things would not quite work so well. Another\nfactor is, obviously, that our system has an actual use case for microservices: we have hundreds of teams, a system that keeps growing in capacity and\ncomplexity, and scale that makes a truly distributed system necessary. I think much of the anti-microservice sentiment you see around the internet today\nstems from treating microservices as a silver bullet that you can apply to any problem regardless of whether they actually make sense in given situation, or\nfrom not being aware that they can bring huge payoffs but also require great investments. There is a good summary of the advantages and disadvantages of\nmicroservices \u003ca href=\"https://about.gitlab.com/blog/2022/09/29/what-are-the-benefits-of-a-microservices-architecture/\"\u003ein this Gitlab blog post\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"service-mesh-and-common-libraries\"\u003eService Mesh and common libraries\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/nasa-Q1p7bh3SHj8-unsplash.jpg\" alt=\"City lights visible from space\" class=\"small-image-right\" /\u003e\nProbably the most recent really significant change related to our microservice ecosystem was the \u003ca href=\"/2020/05/migrating-to-service-mesh.html\"\u003emigration to service mesh\u003c/a\u003e.\nFrom developers’ perspective it did not seem all that radical, but it required a lot of work from infrastructure teams. The most important gain is the\npossibility to control some aspects of services’ behavior in a single place. For example, originally if you wanted to have secure connections between\nservices, you had to support \u003ca href=\"https://en.wikipedia.org/wiki/Transport_Layer_Security\"\u003eTLS\u003c/a\u003e in code, using common libraries. With service mesh, you can just\nenable it globally without the developers even having to know. This makes maintaining the huge ecosystem that consists of more than a thousand services much\nmore bearable.\u003c/p\u003e\n\n\u003cp\u003eEach microservice needs certain behaviors in order to work well within our environment. For example, it needs a healthcheck endpoint which allows Kubernetes\nto tell if the service instance is working or not. We have a written Microservice Contract which defines those requirements. There are also features that are\nnot strictly necessary, but which many services will find useful, for example various metrics. Our initial approach was to have a set of common libraries\nthat provided both the required and many of the nice-to-have features. Of course, if you can’t or don’t want to use those libraries, you are free to do so, as\nlong as your service implements the Microservice Contract some other way.\u003c/p\u003e\n\n\u003cp\u003eOver time, the role of those libraries has changed, with the general direction being that of reducing their scope. There are several reasons.\u003c/p\u003e\n\n\u003cp\u003eReason number one is more and more features can be moved to infrastructure layer, of which Service Mesh is an important part. For example, originally\ncommunicating with another service required a service discovery client, implemented in a shared library.  Now, all this logic has been delegated to the\nService Mesh and requires no special support in shared libraries or service code.\u003c/p\u003e\n\n\u003cp\u003eAnother reason is that open source libraries have caught on and some features we used to need to implement ourselves, such as certain metrics, are now\navailable out of the box in Spring Boot or other frameworks. There is no point in reinventing the wheel and having more code to maintain.\u003c/p\u003e\n\n\u003cp\u003eFinally, the problem with libraries is that updating a library in 1000+ services is a slow and costly process. Meanwhile, a feature that the Service Mesh\nprovides can be switched on or reconfigured for all services almost instantly.\u003c/p\u003e\n\n\u003cp\u003eDespite common libraries falling out of favor with us, there are some features that are hard to implement in infrastructure alone. Even with a simple\nfeature such as logging, sometimes we need data that only code running within the services has access to. When we want to fill in certain standard fields in\norder to make searching logs easier, some fields, such as \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehost\u003c/code\u003e or \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edc\u003c/code\u003e, can easily be filled in by the infrastructure, but some, such as \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ethread_name\u003c/code\u003e are\nonly known inside the service and can’t be handled externally. Thus, the role of libraries is diminished but not completely eliminated. In order to make\nworking with shared libraries less cumbersome, we are working on ways to automate upgrades as much as possible so that we can keep all versions up to date\nwithout it costing too much developers’ time.\u003c/p\u003e\n\n\u003ch3 id=\"learning\"\u003eLearning\u003c/h3\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/thought-catalog-mmWqrsjZ4Lw-unsplash.jpg\" alt=\"Person reading\" class=\"small-image-right\" /\u003e\nDuring the transition, Allegro invested in learning and development heavily. Daily work was full of learning opportunities since everything we were doing\nwas quite new, and many approaches and technologies were not mature yet. We were really on the cutting edge of technology, so for many problems there were\nsimply no run-of-the-mill solutions yet. We were already several years into the microservice transition when microservices became a global hype.\u003c/p\u003e\n\n\u003cp\u003eSince everybody was well aware of what an ambitious plan we were pursuing, it was also well understood that some things took experimenting, and while of\ncourse we were expected to ship value, there was a company-wide understanding that time for learning, \u003ca href=\"/2019/09/team-tourism-at-allegro.html\"\u003eteam tourism\u003c/a\u003e,\ntrying out new things, and sometimes failing, were necessary for success. One of the things I really enjoyed was the focus on quality and doing things right.\nBusiness understood this as well, and actually at one point when Rubicon was quite advanced, developers were granted a 6-month grace period during which we\ncould focus on just technical changes without having to deliver any business value. As a matter of fact, many business logic changes were delivered anyway. For\nexample, the team I was on created a microservice-based approach to handling payments which was much more flexible than the old solution, so it was not just a\nrefactoring, but rather a rewrite that took new business requirements into account.\u003c/p\u003e\n\n\u003cp\u003eApart from learning by doing, we also invested in organized training and conferences. We bought a number of dedicated training sessions with\n\u003ca href=\"/2015/07/it-stars.html\"\u003eestablished experts\u003c/a\u003e from Silicon Valley on topics such as software architecture and JVM performance. Pretty much\neveryone could attend at least one good conference each year, and we also sponsored a number of developer-centric events in order to gain visibility and attract\ngood hires. About a year into my job, I got to attend JavaOne in San Francisco, whose scale and depth trumped even the biggest conferences I knew from Europe.\nAfter attending a few conferences, I decided to give speaking myself a try, and was able to take advantage of a number of useful trainings to help me with that.\nWe also started the \u003ca href=\"https://allegro.tech/\"\u003eallegro.tech\u003c/a\u003e initiative in order to organize all the activity used to promote our brand, and this blog is one\nof the projects that we run under the allegro.tech umbrella to this day.\u003c/p\u003e\n\n\u003ch3 id=\"the-cycle-of-life\"\u003eThe cycle of life\u003c/h3\u003e\n\n\u003cp\u003eIn 2022, a service I had worked on when I first started at Allegro was shut down due to being replaced with a newer solution. This way, I witnessed the\nfull lifecycle of a service: building it from scratch, adding more features to the mature solution, maintenance, and finally seeing it discontinued. It was\nreally a great experience to see that something I had built had run its course and I could be there to see the whole cycle.\u003c/p\u003e\n\n\u003ch2 id=\"takeaways\"\u003eTakeaways\u003c/h2\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-04-12-ten-years-microservices/pyramids.jpg\" alt=\"Pyramids of Egypt\" class=\"small-image-right\" /\u003e\nWhen we started out working with microservices, we were well aware of their benefits but also of their cost. The famous\n\u003ca href=\"https://martinfowler.com/bliki/MicroservicePrerequisites.html\"\u003eYou must be this tall to use microservices\u003c/a\u003e image adorned many of our presentations at that\ntime. By taking a realistic stance, we avoided many pitfalls. Our transition to the microservice world took several years, but was successful, and I am\ncertain we would be in a much worse place had the company not made that bold decision. Apart from being a huge technical challenge, it was also a great\ntransformation in our way of thinking and in the way we work together. \u003ca href=\"https://en.wikipedia.org/wiki/Conway%27s_law\"\u003eConway’s Law\u003c/a\u003e applies and the change\nin system architecture was possible only together with a change in company architecture. It was also possible thanks to many smart people with whom I had\nthe pleasure to work over these years.\u003c/p\u003e\n\n\u003cp\u003eWhen I look back, I see how far we have come. Creating a new service used to take a week or two at first, and now it takes minutes. Scaling a service required\na human operator, creating virtual machines, and manually adding them to the monitoring system. Today, an autoscaler handles most services and developers\ndo not even need to know that instances were added or removed. Our tooling is really convenient, even though there are things we could improve, and some\ncomponents are already showing signs of aging. Nonetheless, many things that used to be a challenge, are trivial today. New joiners at the company can\nbenefit from all these conveniences right from the start, and sometimes I think they might not fully appreciate them since they never had to perform all\nthat work manually.\u003c/p\u003e\n\n\u003cp\u003eThe world does not stand still, though. Technologies change, and some assumptions we made when planning our architecture ten years ago, have already had to\nbe updated. Our system has grown, and so has the company, so many issues we are dealing with now are different from those that troubled us in the beginning\nof Project Rubicon. Initially, everything was a greenfield project, but by now, some places have accumulated bit rot and need cleanup. The system is much\nbigger (which microservices enabled) so introducing changes gets harder (still, much easier than it would be within a monolith). And since ten\nyears is a lot of time, many people have moved through the company, so \u003ca href=\"/2023/10/battle-against-knowledge-loss.html\"\u003eknowledge transfer and continued learning\u003c/a\u003e\nare still essential. Only change is certain, and this has not changed a bit. I’m happy I could experience the heroic age of microservices myself, and I’m\nlooking forward to whatever comes next.\u003c/p\u003e\n","contentSnippet":"In early 2024, I hit ten years at Allegro, which also happens to be how long I’ve been working with microservices.\nThis timespan also roughly corresponds to how long the company as a whole has been using them, so I think it’s a good time to outline the story of project\nRubicon: a very ambitious gamble which completely changed how we work and what our software is like. The idea probably seemed rather extreme at the time, yet I\nam certain that without this change, Allegro would not be where it is today, or perhaps would not be there at all.\nBackground\nAllegro is one of the largest e-commerce sites in Central Europe, with 20 million users and over 300 million\noffers. It was founded in 1999, originally with just the Polish market in mind. The story I want to tell you starts in 2013, a year before I joined.\n\nIn 2013, the site was already large and relevant, but its commercial success and further growth led to development bottlenecks emerging. The codebase was a\nmonolithic PHP application, with some auxiliary processes written in C. Checked out, the git monorepo weighed about 2 GB, and the number of pull requests\nproduced daily by a few hundred developers was so large that if you started a new branch in the morning, you were almost sure to get some conflicts if you\nwanted to merge in the afternoon. The system was centered around a single, huge database, with all the performance and architectural challenges you can imagine.\nTests were brittle and took ages to finish. Deployment was a mostly manual and thus time-consuming processes that required lots of attention and ran the risk of\ncausing a serious problem in production if something went wrong. It was so demanding and stressful I still remember my team having to run the deployment once\n(in place of the usual deployers) as a big event.\nRubicon Rises\n\nIt was becoming clear that we would hit a wall if we continued working this way. So, around 2012/2013, the idea for a complete overhaul of the architecture\nstarted to emerge. We began experimenting with SOA (Service-Oriented Architecture) by\ncreating a small side project, the so-called New Platform, in PHP, as a proof-of-concept. We also decided we would start doing\nAgile, TDD, and\nCloud. After a short while, on top of this, we decided to switch to Java for backend development.\nIt was becoming clear that it would be a revolution indeed, requiring everyone to change\nthe way they worked, and to switch out the whole development ecosystem, starting with the core programming language. Once we got this going, there would be\nno turning back, so a matching name also appeared: Project Rubicon.\nThe project had such a broad scope that it even came with its own constitution, a set of high-level guidelines to be used in case of doubt. It focused mostly\non ways of working and highlighted the value of learning (on personal, team, and company level), testing, reuse, empirical approach to software development,\nand active participation in the open-source community both as users and as contributors. Specific technical assumptions included:\nfocus on quality\nmicroservices\ndistributed, multi-regional, active-active architecture\nJava\ncloud deployment\nusing open-source technologies\nThere was also a list of success criteria for the project:\nthe monolith is gone\nwe have Java gurus on board\nwe have services\ndevelopment is faster\nwe have continuous delivery\nwe don’t have another monolith\nwe still make money\nFaster development was probably the most important goal, since slow delivery was the direct reason we embarked on this long journey.\nOn top of these lists, more detailed plans were made as well, of course. For example, various parts of the system were prioritized for moving out of the\nmonolith as we were well aware we would not be able to work on everything at once. Being Agile does not mean planning is to be avoided, only that plans have\nto be flexible. So, armed with a plan, we got off to work.\nExecution\n\nToo much has happened during the 10+ years to report here. The initial period was really frantic since we had to set up everything, and, first of all, teams had\nto switch to a new mindset. This was also a period of intense hiring, and the time I joined the recently opened office in Warsaw. Microservices were at that\ntime only starting to gain traction, so while we used the experiences of others as much has possible, we had to learn many things ourselves, sometimes learning\nthem the hard way.\nTo give you an idea of the pace, here are just some of the things that happened in 2013:\noutline of the common architecture (service discovery, logging) created\na set of common libraries created (presentation from 2016)\ntraining in Java and JVM for PHP developers\nrecruitment of Java developers started\nfirst Java code got written\nfierce discussions about technology choices (Guice vs Spring, Maven vs Gradle, Jetty vs Undertow)\nWhat followed in 2014 (this is the part I could already experience in person):\nvarious self-service tools allowing developers to handle common tasks such as creating databases themselves rather than by involving specialized support teams\nautomation tools\ndevelopment of Hermes, our open-source message broker built on top of Kafka, started\nstrategic DDD training with Eric Evans\nmigration to Java 8\nglobal architecture improvements\nallegro.tech, the project to coordinate the visibility of our tech division online and offline, of which this blog is a part,\nstarted\nSRE team created\nCQK (Code Quality Keepers) guild opened\nfirst Java services deployed to production\nintense recruitment and learning\nThe number of both production services and of tools supporting developers’ work that got created thereafter is staggering. It should be clear from just the\nlist above that this was a huge investment, and could only proceed due to full buy-in of both technology and business parts of the company. It was indeed a\ngamble, well-informed, but still a gamble that carried big risk should it fail, but an even greater risk if we were to stay with the old architecture.\nAt this point you probably can see that actually building microservices seems like a minor part of the whole undertaking. There was a lot of work to\nwriting so many parts of this huge system anew, but indeed the amount of work that we had to invest into\ninfrastructure,\ntooling, and\nlearning, was immense. It was also absolutely critical for the project’s success. A lot has been\nsaid about microservices, and it is true that for them to be beneficial, you need the right scale and the right kind of system. We had both, and so the decision\nto move to microservices proved to be worthwhile, but despite knowing the theory, I think no one expected the amount of auxiliary work to be so huge. Indeed,\nwhile microservices themselves may be simple, the glue that holds them together is not.\nFlashbacks\nSummarizing ten years of rapid development is tough, so instead of trying to tell you the full story, I decided to share a few flashbacks: moments which I\nremembered for one reason or another.\nNo-ing SQL\n\nWhen refactoring our huge monolith into smaller microservices, we needed to also choose the\ndatabase to use for each of them. Since horizontal scalability was our focus, we preferred NoSQL databases when possible.\nThis was a big change since the monolithic solution relied on a single, huge SQL database. On top of that, it was not modularized well, and in many places\nthere was little or no separation between domain and persistence layers. If the monolith was structured well, splitting it into separate services would have\nbeen much easier. Unfortunately, this was not the case, so we had to perform the transition to NoSQL together with other refactorings and cleanup. Usually,\nwe had to deeply remodel data and operations handling it, especially transactional, so that they could be executed in the new environment. This was often\na significant effort even if we could divide the code in such way that the transaction or set of related operations ended up within the same service. Things\nbecame even more complicated if an operation spanned multiple services (and databases) in the new architecture. This is one of the reasons why dividing a\nbig application into smaller chunks is much harder than it may seem at first.\nCassandra was initially our preferred NoSQL database for most tasks. Only after a while did we learn that each database is good\nfor some use cases and bad for others, and that we needed polyglot persistence to achieve high performance\nand get the required flexibility in all cases. The team I worked on was among the first Cassandra adopters at the company, and as is often the case when\nyou run something in production for the first time, we uncovered a number of issues in our Cassandra deployment which was “ready” but not tested in production\nyet. The team responsible for the DB was learning completely new stuff just as we were.\nAn argument sometimes put up against the need to separate your application’s persistence layer from the domain logic is “you’re never going to switch out the\nDB for another one anyway”. Most of the time that’s true, but in one service we did have to switch from Cassandra to MongoDB\nafter we found out our access patterns were not very well aligned with Cassandra’s data model. We managed to do it inside a single two-week sprint, and\napart from the service becoming faster, its clients would not notice any difference as the external API stayed the same. While the (usually theoretical)\nprospect of switching databases is not the only reason for decoupling domain and persistence layers, it did help a lot in this case, and it is about this\ntime I started to understand why we were creating so many classes even though you could just cram all that code into one.\nI also managed to kill our Cassandra instance once when I was learning about big data processing and created a job that was supposed to process some data from\nthe DB. The job was so massively parallel that the barrage of requests it generated overwhelmed even Cassandra. Fortunately, this situation also showed the\nadvantage of having separate databases for each service, as only that single service experienced an outage.\nInto the cloud\n\nBefore joining Allegro, I had only deployed to physical servers, so moving to the cloud was a big change. At first, we deployed our services to virtual\nmachines configured in OpenStack. What a convenience it was to be able to just set up a complete virtual server with a few\nclicks rather than wait days for a physical machine. We used Puppet to fully configure the virtual machines for each service, so\nwhile you had to write some configuration once, you could spin up a new server configured for your service almost instantly afterwards.\nThis IaaS (Infrastructure as a Service) approach was very convenient, and quite a change, but in\nmany ways it still resembled what I had known before: you had a machine, even if virtual, and you could ssh and run any commands there if you wanted, even\nif it was rarely needed since Puppet set up everything for you.\nThe real revolution came when we switched to PaaS (Platform as a Service) model, at that time based on\nMesos and Marathon. Suddenly, there were no more virtual machines, and you could not ssh\nto the server where your software was running. For me, this was a real culture shock, and even though up to this point I was very enthusiastic about all the\ncool technology we were introducing, the thought of no more ssh freaked me out. How would I know what was going on in the system if I couldn’t even access\nit? Despite my reservations, I gradually found out you could indeed deploy and monitor software despite not being able to access the machine via ssh. It\nsounds weird in retrospect, but this was one of the most difficult technological transitions in my career.\nAfter a while, we built some abstraction layers on top of Mesos, including a custom app console that allows you to deploy a service and perform all\nmaintenance tasks. It isolates you from most details of the underlying system, and is so effective that when we migrated from Mesos to Kubernetes\nlater on, the impact on most teams was much smaller that you could imagine for such a big change. Our App Console is an internal project, but if you are\nfamiliar with Backstage, it should give you an idea of what kind\nof tool we’re talking about here.\nMonitoring\n\nBelieve it or not, initially all monitoring was centralized and handled by a single team. If you wanted to have any non-standard charts in\nZabbix or any custom alerts (and obviously, you wanted to), you had to create a ticket in JIRA, describe exactly what you wanted,\nand after a while, the monitoring team would set it up for you. The whole process took about a week, and quite often, right after seeing the new chart you\nknew you wanted to improve it, so you would file another ticket and wait another week. Needless to say, this was incredibly frustrating, and I consider it\none of my early big successes when I kept pushing the monitoring team until they finally gave in and allowed development teams to configure all of their\nobservability settings themselves.\nGoing polyglot\n\nWhile Rubicon started out with the premise of rewriting our software in Java, we quickly started experimenting with other JVM languages. The team I worked\non considered Scala for a while, but after some experimentation decided against using it as our main language. Some other teams, however, did choose it, and\neven though they are a minority at Allegro, we have some microservices written in Scala to this day. On the other hand, Scala is the dominant language at\nAllegro when it comes to writing Spark jobs.\nSome time around 2015, a teammate found out there was a relatively new, but promising, language called Kotlin. It so happened that we\nwere just starting work on a new microservice which was still very simple and not quite critical. He decided to use it as a testbed, and within I think two days\nrewrote the whole thing in Kotlin. Thanks to the services being independent and this one not being very important yet, we could safely experiment in production\nand assess the stability of the rewritten service. Learning the language by writing actual production-ready code rather than just playing with throw-away code\nallowed us to check what advantages and disadvantages the language offered under realistic usage scenarios. Kotlin caught on, and gradually we started to use\nit for more and more new services and to use it for new features in existing Java services as mixing the two was easy. Many services already used\nGroovy and Spock for tests anyway. At this point, Kotlin is more popular than Java at Allegro, and\non our blog we published some articles about Kotlin,\nincluding one which unfortunately stirred a lot of controversy, and caused a (deserved IMO)\nshitstorm both inside and outside the company.\nBesides JVM languages, we by now have also microservices written in C#,\nGo, Python,\nElixir, and probably a few more languages I forgot to mention. This is just the backend, but our\nfrontend architecture also allows for components written in various\nlanguages. And besides customer-facing business code, there are also internal tools and utilities, sometimes written using yet other general-purpose languages\nand DSLs. Finally, there’s the whole world of AI, including prompting for generative AI that you\ncan also consider a programming language of sorts.\nThe main point I want to make here is that using microservices has allowed us to safely experiment with various programming languages, to consciously limit\nthe blast radius of those experiments should anything go wrong, and to perform all transitions gradually. Of course, this all has a purpose: finding the\nbest tool for the job, and using all the different languages’ strengths where they can help us most. It is not about introducing new tools just for the sake\nof it, which would cause but chaos and introduce risks related to future maintenance. I think the autonomy teams get in making technical decisions, yet\ncombined with responsibility for the outcomes, is what allows us to learn and find new ways of doing things while at the same time it limits the risks\nassociated with experimenting. As in many other cases, things work well when the organization’s ways of working (team autonomy) are aligned with technical\nsolutions (microservices).\nUsing antipatterns wisely\n\nGood practices are heuristics: most of the time, following them is a good idea. For example,\ntwo microservices should not share database tables since this introduces tight coupling: you can’t introduce a change to the schema and deploy just one service\nbut not the other. Your two services are not independent, but form a distributed monolith instead. Avoiding such situations is just common sense.\nStill, you should always keep in mind the reasons why a good practice exists, what it protects you from and what costs it introduces. At one point we had a\ndiscussion within our team about how to best handle a peculiar performance issue. Our service connected to an Elasticsearch instance and performed two kinds\nof operations: reads and writes. Reads were much more numerous, but writes introduced heavy load (on the service itself — Elastic could handle it). Writes\ncame in bursts, so most of the time things worked well, but when a burst of writes arrived, performance of the whole service suffered and read times were\naffected. We tried various mechanisms for isolating the two kinds of operation, but couldn’t do it effectively.\nA colleague suggested we split the service in two, one responsible for handling reads and the other for writes. We had a long discussion, in which I\npresented arguments for having a single service as the owner of the data, responsible for both reads and writes, and highlighted what issues could arise due to\nthe split. While keeping the service intact seemed to be the elegant thing to do, I didn’t have a good solution for the performance issue. My colleague’s\nidea to split the service, on the other hand, while somewhat messy, did offer a chance to solve it.\nSo, we decided to just try it and see whether this approach would solve the performance issue and how bad the side effects would be. We did just that, and the\nantipattern-based solution worked great: performance hiccups went away, and despite sharing the common Elasticsearch cluster, the two services remained\nmaintainable. We were not able to fully assess this aspect right away, but time proved my colleague right as well: during the 3+ years we worked with that\ncodebase later on, we only ran into issues related to sharing Elasticsearch once, and we managed to fix that case quickly. It certainly did help, though, that\nboth services kept being developed by the same team, and that by the time we introduced the split, the schema was already quite stable and did not change often.\nNonetheless, had I insisted on keeping things clean, we would have probably spent much more time fighting performance issues than we lost during the single\nissue that resulted from sharing Elasticsearch between services. Know when to use patterns, know when to use antipatterns, and use both wisely.\nOne size does not fit all\n\nI think we’ve always been quite pragmatic about sizing our microservices. It’s hard to define a set of specific rules for finding the right size, but going\ntoo far in one direction or the other causes considerable pain. Make a service huge, and it becomes too hard for a single team to maintain and develop, or\nscaling issues arise similar to those you could experience with a monolith. Make it very small, and you might get overwhelmed by the overhead of having your\nlogic split between too many places, issues with debugging, and the performance penalty of the system being distributed to the extreme.\nMost services I got to work on at Allegro were not too tiny, and contained some non-trivial amount of logic. There were sometimes agitated discussions about\nwhere to implement a certain feature, in particular whether it should be in an existing service or in a new one. In hindsight, I think most decisions made\nsense, but there were certainly cases where a feature that we believed would grow ended up in a new service which then never took off and remained too small,\nand cases where something was bolted onto an existing service because it was easier to implement this way, but which caused some pain later on.\nI think I only once saw a team fall into the nanoservice trap where services were designed so small the split caused more trouble than it was worth. On the\nother hand, there were certainly services which you could no longer call micro by any stretch. This was not necessarily a bad thing. As long as a service\nfulfilled a well-defined role, a single team was enough to take care of it, and it was OK that you had to deploy and scale the whole thing together, things\nwere fine. In some cases of services which grew really much too big (indicators being that they contained pieces of logic only very loosely related to each\nother, and that at some point multiple teams were regularly interested in contributing), we did get back to them and split them up. It was not very easy,\nbut doable, and the second-hardest part was usually finding the right lines along which to divide. The only thing harder was finding the time to perform\nsuch operations, but with a bit of negotiation and persistence, after a while we usually succeeded.\nThere is an ongoing discussion of whether we have too many microservices. It’s not an urgent thing, but there are reasons to not go too high, such as\ncertain technical limitations in the infrastructure and the cost of overprovisioning (each service allocates resources such as memory or CPUs with a margin,\nand those margins add up). Still, the fact that we are well above a thousand services and yet their number is only a minor nuisance, speaks well of our\ntooling and organization. Indeed, thanks to some custom tools, creating a new service is very easy (maybe too easy?), and managing those already there is\nalso quite pleasant. This is possible due to huge investments we made early on (and continue): we knew right from the start that while each microservice may\nbe relatively simple, a lot of complexity goes into the glue that holds the whole system together. Without it, things would not quite work so well. Another\nfactor is, obviously, that our system has an actual use case for microservices: we have hundreds of teams, a system that keeps growing in capacity and\ncomplexity, and scale that makes a truly distributed system necessary. I think much of the anti-microservice sentiment you see around the internet today\nstems from treating microservices as a silver bullet that you can apply to any problem regardless of whether they actually make sense in given situation, or\nfrom not being aware that they can bring huge payoffs but also require great investments. There is a good summary of the advantages and disadvantages of\nmicroservices in this Gitlab blog post.\nService Mesh and common libraries\n\nProbably the most recent really significant change related to our microservice ecosystem was the migration to service mesh.\nFrom developers’ perspective it did not seem all that radical, but it required a lot of work from infrastructure teams. The most important gain is the\npossibility to control some aspects of services’ behavior in a single place. For example, originally if you wanted to have secure connections between\nservices, you had to support TLS in code, using common libraries. With service mesh, you can just\nenable it globally without the developers even having to know. This makes maintaining the huge ecosystem that consists of more than a thousand services much\nmore bearable.\nEach microservice needs certain behaviors in order to work well within our environment. For example, it needs a healthcheck endpoint which allows Kubernetes\nto tell if the service instance is working or not. We have a written Microservice Contract which defines those requirements. There are also features that are\nnot strictly necessary, but which many services will find useful, for example various metrics. Our initial approach was to have a set of common libraries\nthat provided both the required and many of the nice-to-have features. Of course, if you can’t or don’t want to use those libraries, you are free to do so, as\nlong as your service implements the Microservice Contract some other way.\nOver time, the role of those libraries has changed, with the general direction being that of reducing their scope. There are several reasons.\nReason number one is more and more features can be moved to infrastructure layer, of which Service Mesh is an important part. For example, originally\ncommunicating with another service required a service discovery client, implemented in a shared library.  Now, all this logic has been delegated to the\nService Mesh and requires no special support in shared libraries or service code.\nAnother reason is that open source libraries have caught on and some features we used to need to implement ourselves, such as certain metrics, are now\navailable out of the box in Spring Boot or other frameworks. There is no point in reinventing the wheel and having more code to maintain.\nFinally, the problem with libraries is that updating a library in 1000+ services is a slow and costly process. Meanwhile, a feature that the Service Mesh\nprovides can be switched on or reconfigured for all services almost instantly.\nDespite common libraries falling out of favor with us, there are some features that are hard to implement in infrastructure alone. Even with a simple\nfeature such as logging, sometimes we need data that only code running within the services has access to. When we want to fill in certain standard fields in\norder to make searching logs easier, some fields, such as host or dc, can easily be filled in by the infrastructure, but some, such as thread_name are\nonly known inside the service and can’t be handled externally. Thus, the role of libraries is diminished but not completely eliminated. In order to make\nworking with shared libraries less cumbersome, we are working on ways to automate upgrades as much as possible so that we can keep all versions up to date\nwithout it costing too much developers’ time.\nLearning\n\nDuring the transition, Allegro invested in learning and development heavily. Daily work was full of learning opportunities since everything we were doing\nwas quite new, and many approaches and technologies were not mature yet. We were really on the cutting edge of technology, so for many problems there were\nsimply no run-of-the-mill solutions yet. We were already several years into the microservice transition when microservices became a global hype.\nSince everybody was well aware of what an ambitious plan we were pursuing, it was also well understood that some things took experimenting, and while of\ncourse we were expected to ship value, there was a company-wide understanding that time for learning, team tourism,\ntrying out new things, and sometimes failing, were necessary for success. One of the things I really enjoyed was the focus on quality and doing things right.\nBusiness understood this as well, and actually at one point when Rubicon was quite advanced, developers were granted a 6-month grace period during which we\ncould focus on just technical changes without having to deliver any business value. As a matter of fact, many business logic changes were delivered anyway. For\nexample, the team I was on created a microservice-based approach to handling payments which was much more flexible than the old solution, so it was not just a\nrefactoring, but rather a rewrite that took new business requirements into account.\nApart from learning by doing, we also invested in organized training and conferences. We bought a number of dedicated training sessions with\nestablished experts from Silicon Valley on topics such as software architecture and JVM performance. Pretty much\neveryone could attend at least one good conference each year, and we also sponsored a number of developer-centric events in order to gain visibility and attract\ngood hires. About a year into my job, I got to attend JavaOne in San Francisco, whose scale and depth trumped even the biggest conferences I knew from Europe.\nAfter attending a few conferences, I decided to give speaking myself a try, and was able to take advantage of a number of useful trainings to help me with that.\nWe also started the allegro.tech initiative in order to organize all the activity used to promote our brand, and this blog is one\nof the projects that we run under the allegro.tech umbrella to this day.\nThe cycle of life\nIn 2022, a service I had worked on when I first started at Allegro was shut down due to being replaced with a newer solution. This way, I witnessed the\nfull lifecycle of a service: building it from scratch, adding more features to the mature solution, maintenance, and finally seeing it discontinued. It was\nreally a great experience to see that something I had built had run its course and I could be there to see the whole cycle.\nTakeaways\n\nWhen we started out working with microservices, we were well aware of their benefits but also of their cost. The famous\nYou must be this tall to use microservices image adorned many of our presentations at that\ntime. By taking a realistic stance, we avoided many pitfalls. Our transition to the microservice world took several years, but was successful, and I am\ncertain we would be in a much worse place had the company not made that bold decision. Apart from being a huge technical challenge, it was also a great\ntransformation in our way of thinking and in the way we work together. Conway’s Law applies and the change\nin system architecture was possible only together with a change in company architecture. It was also possible thanks to many smart people with whom I had\nthe pleasure to work over these years.\nWhen I look back, I see how far we have come. Creating a new service used to take a week or two at first, and now it takes minutes. Scaling a service required\na human operator, creating virtual machines, and manually adding them to the monitoring system. Today, an autoscaler handles most services and developers\ndo not even need to know that instances were added or removed. Our tooling is really convenient, even though there are things we could improve, and some\ncomponents are already showing signs of aging. Nonetheless, many things that used to be a challenge, are trivial today. New joiners at the company can\nbenefit from all these conveniences right from the start, and sometimes I think they might not fully appreciate them since they never had to perform all\nthat work manually.\nThe world does not stand still, though. Technologies change, and some assumptions we made when planning our architecture ten years ago, have already had to\nbe updated. Our system has grown, and so has the company, so many issues we are dealing with now are different from those that troubled us in the beginning\nof Project Rubicon. Initially, everything was a greenfield project, but by now, some places have accumulated bit rot and need cleanup. The system is much\nbigger (which microservices enabled) so introducing changes gets harder (still, much easier than it would be within a monolith). And since ten\nyears is a lot of time, many people have moved through the company, so knowledge transfer and continued learning\nare still essential. Only change is certain, and this has not changed a bit. I’m happy I could experience the heroic age of microservices myself, and I’m\nlooking forward to whatever comes next.","guid":"https://blog.allegro.tech/2024/04/ten-years-microservices.html","categories":["tech","microservices","architecture"],"isoDate":"2024-04-11T22:00:00.000Z"},{"title":"Unlocking Kafka's Potential: Tackling Tail Latency with eBPF","link":"https://blog.allegro.tech/2024/03/kafka-performance-analysis.html","pubDate":"Wed, 06 Mar 2024 00:00:00 +0100","authors":{"author":[{"name":["Maciej Mościcki"],"photo":["https://blog.allegro.tech/img/authors/maciej.moscicki.jpg"],"url":["https://blog.allegro.tech/authors/maciej.moscicki"]},{"name":["Piotr Rżysko"],"photo":["https://blog.allegro.tech/img/authors/piotr.rzysko.jpg"],"url":["https://blog.allegro.tech/authors/piotr.rzysko"]}]},"content":"\u003cp\u003eAt \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e, we use \u003ca href=\"https://kafka.apache.org/\"\u003eKafka\u003c/a\u003e as a backbone for asynchronous communication between microservices. With up to\n300k messages published and 1M messages consumed every second, it is a key part of our infrastructure. A few months ago, in our main Kafka cluster, we noticed\nthe following discrepancy: while median response times for \u003ca href=\"https://developer.confluent.io/courses/architecture/broker/#inside-the-apache-kafka-broker:~:text=Client%20requests%20fall%20into%20two%20categories%3A%20produce%20requests%20and%20fetch%20requests.%20A%20produce%20request%20is%20requesting%20that%20a%20batch%20of%20data%20be%20written%20to%20a%20specified%20topic.%20A%20fetch%20request%20is%20requesting%20data%20from%20Kafka%20topics.\"\u003eproduce requests\u003c/a\u003e\nwere in single-digit milliseconds, the tail latency was much worse. Namely, the\np99 latency was up to 1 second, and the p999 latency was up to 3 seconds. This was unacceptable for a new project that we were about to start, so we\ndecided to look into this issue. In this blog post, we would like to describe our journey — how we used Kafka protocol sniffing and eBPF to identify and remove\nthe performance bottleneck.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/kafka-performance-analysis.png\" alt=\"Kafka Produce Latency\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"the-need-for-tracing\"\u003eThe Need for Tracing\u003c/h2\u003e\n\u003cp\u003eKafka brokers \u003ca href=\"https://docs.confluent.io/platform/current/kafka/monitoring.html#localtimems\"\u003eexpose various metrics\u003c/a\u003e. From them, we were able to tell that\nproduce requests were slow for high percentiles, but we couldn’t identify the cause. System metrics were also not showing anything alarming.\u003c/p\u003e\n\n\u003cp\u003eTo pinpoint the underlying problem, we decided to trace individual requests. By analyzing components of Kafka involved in handling produce requests,\nwe aimed to uncover the source of the latency spikes. One way of doing that would be to fork Kafka, implement instrumentation, and deploy our custom version\nto the cluster. However, this would be very time-consuming and invasive. We decided to try an alternative approach.\u003c/p\u003e\n\n\u003cp\u003eThe first thing we did was finding \u003cem\u003earrival\u003c/em\u003e and \u003cem\u003eend\u003c/em\u003e times for every Kafka produce request.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/request_timeline1.png\" alt=\"Timeline of Kafka produce request\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eTimeline of a produce request. Arrival and end times define the boundaries of the request. The components of Kafka involved in handling the request and their latencies are unknown.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eKafka uses a binary protocol over TCP to send requests from producers (and consumers) to brokers. We started by capturing the network traffic on a selected\nbroker using \u003ca href=\"https://www.tcpdump.org/\"\u003etcpdump\u003c/a\u003e. Then we wrote a tool for analyzing the captured packets, which enabled us to list all the request and response\ntimes. In the output, we saw a confirmation of what we already knew — there were many slow produce requests taking over a second to complete. What’s more we\nwere able to see request metadata — \u003cem\u003etopic name\u003c/em\u003e, \u003cem\u003epartition ID\u003c/em\u003e and \u003cem\u003emessage ID\u003c/em\u003e (our internal identifier included in Kafka headers):\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eARRIVAL TIME  END TIME      LATENCY(ms)  MESSAGE_ID  TOPIC   PARTITION\n12:11:36.521  12:11:37.060  538          371409548   topicA  2\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.555  12:11:37.061  505          371409578   topicC  7\n12:11:36.587  12:11:37.061  473          375783728   topicD  16\n12:11:36.690  12:11:37.061  370          375783907   topicB  18\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWith that extra knowledge in hand, we were ready to dig deeper.\u003c/p\u003e\n\n\u003ch2 id=\"dynamic-tracing\"\u003eDynamic Tracing\u003c/h2\u003e\n\n\u003cp\u003eThanks to network traffic analysis we had arrival time, end time and metadata for each request. We then wanted to gain insights into\nwhich Kafka components were the source of latency. Since produce requests are mostly concerned with saving data,\nwe decided to instrument writes to the underlying storage.\u003c/p\u003e\n\n\u003cp\u003eOn Linux, Kafka uses regular files for storing data. Writes are done using ordinary \u003ca href=\"https://man7.org/linux/man-pages/man2/write.2.html\"\u003ewrite system calls\u003c/a\u003e — data is first stored in the page cache\nand then asynchronously flushed to disk. How can we trace individual file writes without modifying the source code? We can make use of \u003cem\u003edynamic tracing\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhat is \u003cem\u003edynamic tracing\u003c/em\u003e? In Brendan Gregg’s \u003cem\u003eSystem Performance\u003c/em\u003e, he uses the following analogy that we really like:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eConsider an operating system kernel: analyzing kernel internals can be like venturing into a dark room, with candles […] placed where the kernel engineers\nthought they were needed. Dynamic instrumentation is like having a flashlight that you can point anywhere.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis basically means that it is possible to instrument arbitrary kernel code without the need to modify a user space application or the kernel itself. For\nexample, we can use dynamic tracing to instrument file system calls to check whether they are the source of latency. To do that we can make use of a technology\ncalled BPF.\u003c/p\u003e\n\n\u003cp\u003eBPF (or eBPF) which stands for \u003cem\u003e(extended) Berkeley Packet Filter\u003c/em\u003e is a technology with a rich history, but today it is a generic in-kernel execution\nenvironment [\u003cem\u003eGregg Brendan (2020). Systems Performance: Enterprise and the Cloud, 2nd Edition\u003c/em\u003e]. It has a wide range of applications, including networking,\nsecurity and tracing tools. eBPF programs are compiled to bytecode which is then interpreted by the Linux Kernel.\u003c/p\u003e\n\n\u003cp\u003eThere are a couple of well-established front-ends for eBPF, including \u003ca href=\"https://github.com/iovisor/bcc/tree/master\"\u003eBCC\u003c/a\u003e,\n\u003ca href=\"https://github.com/bpftrace/bpftrace\"\u003ebpftrace\u003c/a\u003e and \u003ca href=\"https://github.com/libbpf/libbpf\"\u003elibbpf\u003c/a\u003e. They can be used to write custom tracing programs, but they\nalso ship with many useful tools already implemented. One such tool is \u003ca href=\"https://github.com/iovisor/bcc/blob/master/tools/ext4slower.py\"\u003eext4slower\u003c/a\u003e.\nIt allows tracing file system operations in the ext4 file system, which is the default file system for Linux.\u003c/p\u003e\n\n\u003ch2 id=\"tracing-kafka\"\u003eTracing Kafka\u003c/h2\u003e\n\u003cp\u003eIn Kafka, every partition has its own directory, named according to the pattern: \u003cem\u003etopicName\u003c/em\u003e-\u003cem\u003epartitionID\u003c/em\u003e. Within each of these directories, there are segment\nfiles where messages are stored. In the figure below, we can see an example of this structure. In this scenario, the broker hosts two partitions (0 and 7)\nfor \u003cem\u003etopicA\u003c/em\u003e and one partition (1) for \u003cem\u003etopicB\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/kafka_directories.png\" alt=\"Kafka Partition Directories\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eBy slightly altering the ext4slower program to include parent directories, we were able to trace Kafka file system writes. For every write with a duration\nexceeding a specified threshold, we observed the following:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eStart time and end time\u003c/li\u003e\n  \u003cli\u003eDuration\u003c/li\u003e\n  \u003cli\u003eThread ID (TID)\u003c/li\u003e\n  \u003cli\u003eNumber of bytes written\u003c/li\u003e\n  \u003cli\u003eFile offset\u003c/li\u003e\n  \u003cli\u003eTopic name\u003c/li\u003e\n  \u003cli\u003ePartition ID\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBelow is an example output from the program:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n15:37:00.627  15:37:00.785  158 ms   4478  2009   88847331   topicA-0/00000000002938697123.log\n15:37:00.629  15:37:00.785  156 ms   4492  531    289315894  topicB-7/00000000001119733846.log\n15:37:00.629  15:37:00.785  156 ms   4495  815    167398027  topicC-7/00000000015588371822.log\n15:37:00.631  15:37:00.785  154 ms   4488  778    502626221  topicD-7/00000000004472160265.log\n15:37:00.644  15:37:00.785  141 ms   4486  341    340818418  topicE-7/00000000002661443174.log\n15:37:00.650  15:37:00.785  135 ms   4470  374    230883174  topicF-7/00000000006102922534.log\n15:37:00.653  15:37:00.785  132 ms   4461  374    375758631  topicF-19/00000000001555977358.log\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThis was already very helpful since we could, based on timestamp, topic and partition, correlate produce requests from the tcpdump output with writes to\nthe file system:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA  0\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB  7\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC  7\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD  7\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE  7\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF  7\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF  19\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eTo gain extra confidence, we wrote a tool that parses a Kafka log file, reads the records written to it (using file offset and number of bytes written),\nparses them, and returns their \u003cem\u003emessage IDs\u003c/em\u003e. With that, we were able to perfectly correlate incoming requests with their respective writes:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  MESSAGE_ID  FILE                                TOPIC   PARTITION  BYTES  OFF_KB\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA-0/00000000002938697123.log   topicA  0          2009   88847331\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB-7/00000000001119733846.log   topicB  7          531    289315894\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC-7/00000000015588371822.log   topicC  7          815    167398027\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD-7/00000000004472160265.log   topicD  7          778    502626221\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE-7/00000000002661443174.log   topicE  7          341    340818418\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF-7/00000000006102922534.log   topicF  7          374    230883174\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF-19/00000000001555977358.log  topicF  19         374    375758631\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFrom the analysis, we were able to tell that \u003cstrong\u003ethere were many slow produce requests that spent all of their time waiting for the file system write to\ncomplete.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/timeline_slow_write.png\" alt=\"Request Timeline with Slow Write\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThere were however requests that didn’t have corresponding slow writes.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/timeline_fast_write.png\" alt=\"Request Timeline with Fast Write\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"kafka-lock-contention\"\u003eKafka Lock Contention\u003c/h2\u003e\n\u003cp\u003eSlow produce requests without corresponding slow writes were always occurring around the time of some other slow write. We started wondering whether those\nrequests were perhaps queuing and waiting for something to finish. By analyzing Kafka source code, we identified a couple of places that use \u003cem\u003esynchronized\u003c/em\u003e\nblocks, including those guarding log file writes.\u003c/p\u003e\n\n\u003cp\u003eWe set out to measure how much time Kafka’s threads, processing produce requests, spend on the aforementioned locks. Our goal was to correlate periods when\nthey were waiting on locks with writes to the file system. We considered two approaches to do that.\u003c/p\u003e\n\n\u003cp\u003eThe first one was to use tracing again, and perhaps combine its results with the tool we already had for tracing the ext4 file system.\nLooking at the JDK source code we were not able to identify a connection between \u003cem\u003esynchronized\u003c/em\u003e blocks and traceable kernel routines. Instead, we learned that\nJVM ships with predefined DTrace tracepoints (DTrace can be thought of as a predecessor of eBPF). These tracepoints include \u003cem\u003ehotspot:monitor__contended__enter\u003c/em\u003e\nand \u003cem\u003ehotspot:monitor__contended__entered\u003c/em\u003e, which monitor when a thread begins waiting on a contended lock and when it finally enters it. By running Kafka\nwith the \u003cem\u003e-XX:+DTraceMonitorProbes\u003c/em\u003e VM option and attaching to these tracepoints we were able to see monitor wait times for a given thread.\u003c/p\u003e\n\n\u003cp\u003eAnother approach we came up with was to capture states of Kafka’s threads by running \u003ca href=\"https://github.com/async-profiler/async-profiler\"\u003easync-profiler\u003c/a\u003e\nalongside the ext4 tracing script. We would then analyze results from both tools and correlate their outputs.\u003c/p\u003e\n\n\u003cp\u003eAfter experimenting with both ideas, we ultimately chose to stick with async-profiler. It provided a clean visualization of thread states and offered more\ninsights into JVM-specific properties of threads.\u003c/p\u003e\n\n\u003cp\u003eNow, let’s delve into how we analyzed a situation when a latency spike occurred, based on an example async-profiler recording, eBPF traces, and parsed\ntcpdump output. For brevity, we’ll focus on one Kafka topic.\u003c/p\u003e\n\n\u003cp\u003eBy capturing network traffic on a broker, we were able to see that there were four slow produce requests to the selected topic:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n17:58:00.644  17:58:00.770  126 ms   75567596    topicF  6\n17:58:00.651  17:58:00.770  119 ms   33561917    topicF  6\n17:58:00.655  17:58:00.775  119 ms   20422312    topicF  6\n17:58:00.661  17:58:00.776  114 ms   18658935    topicF  6\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHowever, there was only one slow file system write for that topic:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.643  17:58:00.769  126 ms   4462  498    167428091  topicF-6/00000000000966764382.log\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAll other writes to that topic were fast at that time:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.770  17:58:00.770  0 ms     4484  798    167451825  topicF-6/00000000000966764382.log\n17:58:00.775  17:58:00.775  0 ms     4499  14410  167437415  topicF-6/00000000000966764382.log\n17:58:00.776  17:58:00.776  0 ms     4467  1138   167436277  topicF-6/00000000000966764382.log\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWe knew that one of the fast writes was performed from a thread with ID 4484. From a thread dump, we extracted thread names and Native IDs (NIDs).\nKnowing that NIDs translate directly to Linux TIDs (thread IDs), we found a thread with NID 0x1184 (decimal: 4484). We determined that the name of\nthis thread was \u003cem\u003edata-plane-kafka-request-handler-24\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eWe searched for this thread’s activity in the async-profiler output:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/locks.png\" alt=\"Async profiler output visualized in Java Mission Control\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eAsync profiler output visualized in Java Mission Control. Thread with TID 4484 is blocked on a monitor.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eIn the output, we saw what we suspected — a thread was waiting on a lock for approximately the same duration as the slow write occurring on another thread.\nThis confirmed our initial hypothesis.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/timeline_lock.png\" alt=\"For a slow request with fast file system writes, waiting to obtain a lock turned out to be the source of latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eFor a slow request with fast file system writes, waiting to acquire a lock turned out to be the source of latency.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eApplying this technique, we analyzed numerous cases, and the results were consistent: \u003cstrong\u003efor a slow produce request there was either a matching slow write or a\nthread was waiting to acquire a lock guarding access to a log file\u003c/strong\u003e. We confirmed that file system writes were the root cause of slow produce requests.\u003c/p\u003e\n\n\u003ch2 id=\"tracing-the-file-system\"\u003eTracing the File System\u003c/h2\u003e\n\u003cp\u003eOur original eBPF script traced only calls to the \u003ca href=\"https://elixir.bootlin.com/linux/v5.15.91/source/fs/ext4/file.c#L673\"\u003eext4_file_write_iter\u003c/a\u003e function.\nWhile this was sufficient to roughly determine that slow writes to the file system were causing the latency spikes, it was not enough to pinpoint which\nparameters of the file system needed tuning. To address this, we captured both \u003ca href=\"https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html\"\u003eon-CPU\u003c/a\u003e\nand \u003ca href=\"https://www.brendangregg.com/offcpuanalysis.html\"\u003eoff-CPU\u003c/a\u003e profiles of \u003cem\u003eext4_file_write_iter\u003c/em\u003e, using\n\u003ca href=\"https://github.com/iovisor/bcc/blob/master/tools/profile.py\"\u003eprofile\u003c/a\u003e and \u003ca href=\"https://github.com/iovisor/bcc/blob/master/tools/offcputime.py\"\u003eoffcputime\u003c/a\u003e,\nrespectively. Our goal was to identify the activated paths in the kernel and then measure the latency of functions associated with them.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/on_cpu.png\" alt=\"on-CPU profile of ext4_file_write_iter\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eon-CPU profile of ext4_file_write_iter\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/off_cpu.png\" alt=\"off-CPU profile of ext4_file_write_iter\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eoff-CPU profile of ext4_file_write_iter\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eWe noticed that the function \u003ca href=\"https://elixir.bootlin.com/linux/v5.15.91/source/fs/ext4/inode.c#L5971\"\u003eext4_dirty_inode\u003c/a\u003e [1] was present in both flamegraphs.\nIn the Linux kernel, the \u003cem\u003eext4_dirty_inode\u003c/em\u003e function is responsible for marking an inode (file or directory data structure) as being in a dirty state. A \u003cem\u003edirty\u003c/em\u003e\ninode indicates that the corresponding file’s data or metadata has been modified and needs to be synchronized with the underlying storage device, typically a\ndisk, to ensure data consistency.\u003c/p\u003e\n\n\u003cp\u003eWhat caught our attention in the off-CPU profile was the \u003ca href=\"https://elixir.bootlin.com/linux/v5.15.91/source/fs/jbd2/transaction.c#L490\"\u003ejbd2__journal_start\u003c/a\u003e\n[2] function which is part of a journaling mechanism employed in ext4 that ensures data integrity and reliability. Journaling in ext4 involves maintaining a\ndetailed log that records the changes before they are committed to the file system. This log, often referred to as the \u003cem\u003ejournal\u003c/em\u003e, serves as a safety net in the\nevent of an unexpected system crash or power failure. When a file system operation occurs, such as creating, modifying, or deleting a file, ext4 first records\nthis change in the journal. Subsequently, the actual file system structures are updated. The process of updating the file system is known as \u003cem\u003ecommitting\u003c/em\u003e the\njournal. During a commit, the changes recorded in the journal are applied to the file system structures in a controlled and atomic manner. In the event of an\ninterruption, the file system can recover quickly by replaying the journal, ensuring that it reflects the consistent state of the file system.\u003c/p\u003e\n\n\u003cp\u003eAs seen in the figure with the off-CPU profile, \u003ca href=\"https://elixir.bootlin.com/linux/v5.15/source/fs/jbd2/transaction.c#L169\"\u003ewait_transaction_locked\u003c/a\u003e [3] is the\nfunction executed before voluntarily yielding the processor, allowing the scheduler to select and switch to a different process or thread ready to run\n(\u003ca href=\"https://elixir.bootlin.com/linux/v5.15/source/kernel/sched/core.c#L6359\"\u003eschedule()\u003c/a\u003e). Guided by the comment above the \u003cem\u003ewait_transaction_locked\u003c/em\u003e function:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eWait until running transaction passes to T_FLUSH state and new transaction can thus be started. Also starts the commit if needed. The function expects running\ntransaction to exist and releases j_state_lock.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe searched the kernel code to identify what sets the \u003cem\u003eT_FLUSH\u003c/em\u003e flag. The only place that we discovered was within the\n\u003ca href=\"https://elixir.bootlin.com/linux/v5.15/source/fs/jbd2/commit.c#L381\"\u003ejbd2_journal_commit_transaction\u003c/a\u003e function executed periodically by a kernel journal\nthread. Consequently, we decided to trace this function to explore any correlation between its latency and the latency of \u003cem\u003eext4_dirty_inode\u003c/em\u003e. The obtained\nresults aligned precisely with our expectations – namely, \u003cstrong\u003ea high latency in  \u003cem\u003ejbd2_journal_commit_transaction\u003c/em\u003e translates to a high latency in\n\u003cem\u003eext4_dirty_inode\u003c/em\u003e.\u003c/strong\u003e The details of our findings are presented below:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eSTART TIME    END TIME      LATENCY  FUNCTION\n19:35:24.503  19:35:24.680  176 ms   jbd2_journal_commit_transaction\n19:35:24.507  19:35:24.648  141 ms   ext4_dirty_inode\n19:35:24.508  19:35:24.648  139 ms   ext4_dirty_inode\n19:35:24.514  19:35:24.648  134 ms   ext4_dirty_inode\n...\n19:38:14.508  19:38:14.929  420 ms   jbd2_journal_commit_transaction\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.512  19:38:14.868  356 ms   ext4_dirty_inode\n...\n19:48:39.475  19:48:40.808  1332 ms  jbd2_journal_commit_transaction\n19:48:39.477  19:48:40.757  1280 ms  ext4_dirty_inode\n19:48:39.487  19:48:40.757  1270 ms  ext4_dirty_inode\n19:48:39.543  19:48:40.757  1213 ms  ext4_dirty_inode\n...\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2 id=\"ext4-improvements-monitoring\"\u003eext4 Improvements Monitoring\u003c/h2\u003e\n\u003cp\u003eHaving identified journal commits as the cause of slow writes, we started thinking how to alleviate the problem. We had a few ideas, but we were wondering how\nwe would be able to observe improvements.  Up until that point, we relied on command-line tools and analyzing their output for short time ranges. We wanted\nto be able to observe the impact of our optimizations over longer periods.\u003c/p\u003e\n\n\u003cp\u003eTo report traced functions latency over long periods, we used \u003ca href=\"https://github.com/cloudflare/ebpf_exporter\"\u003eebpf_exporter\u003c/a\u003e, a tool that exposes eBPF-based\nmetrics in Prometheus format. We were then able to visualize traces in Grafana. For example, maximum ext4 write latency for a given broker:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_max_write_iter.png\" alt=\"Base ext4 Latency\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWith that, we were able to run brokers with different configurations and observe their write latency over time.\u003c/p\u003e\n\n\u003ch2 id=\"ext4-improvements\"\u003eext4 Improvements\u003c/h2\u003e\n\u003cp\u003eLet’s go back to ext4. We knew that journal commits were the source of latency. By studying ext4 documentation, we identified a few possible solutions for\nimproving the performance:\u003c/p\u003e\n\u003col\u003e\n  \u003cli\u003eDisabling journaling\u003c/li\u003e\n  \u003cli\u003eDecreasing the commit interval\u003c/li\u003e\n  \u003cli\u003eChanging the journaling mode from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=ordered\u003c/code\u003e to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eEnabling fast commits\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eLet’s discuss each of them.\u003c/p\u003e\n\n\u003ch3 id=\"disabling-journaling\"\u003eDisabling Journaling\u003c/h3\u003e\n\u003cp\u003eIf journaling is the source of high latency, why not disable it completely? Well, it turns out that journaling is there for a reason. Without journaling, we\nwould risk long recovery in case of a crash. Thus, we quickly ruled out this solution.\u003c/p\u003e\n\n\u003ch3 id=\"decreasing-the-commit-interval\"\u003eDecreasing the Commit Interval\u003c/h3\u003e\n\u003cp\u003eext4 has the \u003cem\u003ecommit\u003c/em\u003e mount parameter which tells how often to perform commits. It has the default value of 5 seconds. According to the ext4 documentation:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThis default value (or any low value) will hurt performance, but it’s good for data-safety. […] Setting it to very large values will improve performance.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eHowever, instead of increasing the value we decided to decrease it. Why? Our intuition was that by performing commits more frequently we would make them\n“lighter” which would make them faster. We would trade throughput for lower latency. We experimented with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecommit=1\u003c/code\u003e, and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecommit=3\u003c/code\u003e but observed no\nsignificant differences.\u003c/p\u003e\n\n\u003ch3 id=\"changing-the-journaling-mode-from-dataordered-to-datawriteback\"\u003eChanging the Journaling Mode from data=ordered to data=writeback\u003c/h3\u003e\n\u003cp\u003eext4 offers three journaling modes: \u003cem\u003ejournal\u003c/em\u003e, \u003cem\u003eordered\u003c/em\u003e and \u003cem\u003ewriteback\u003c/em\u003e. The default mode is \u003cem\u003eordered\u003c/em\u003e and compared to the most performant mode, \u003cem\u003ewriteback\u003c/em\u003e,\nit guarantees that the data is written to the main file system prior to the metadata being committed to the journal. As mentioned in\n\u003ca href=\"https://kafka.apache.org/documentation/#ext4\"\u003edocs\u003c/a\u003e, Kafka does not rely on this property, so switching the mode to \u003cem\u003ewriteback\u003c/em\u003e should reduce latency.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eWe switched the journaling mode on one of the brokers, and indeed, we observed latency improvements:\u003c/strong\u003e\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_p999_2.png\" alt=\"Base Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/writeback_p999_2.png\" alt=\"Writeback Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eWith data=writeback, p999 decreased from 3 seconds to 800 milliseconds.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3 id=\"enabling-fast-commit\"\u003eEnabling Fast Commit\u003c/h3\u003e\n\u003cp\u003eWhen reading about ext4 journaling, we stumbled upon an \u003ca href=\"https://lwn.net/Articles/842385/\"\u003earticle\u003c/a\u003e describing a new feature introduced in Linux 5.10 called\n\u003cem\u003efast commits\u003c/em\u003e. As explained in the article, \u003cem\u003efast commit\u003c/em\u003e is a lighter-weight journaling method that could result in performance boost for certain workloads.\u003c/p\u003e\n\n\u003cp\u003eWe enabled \u003cem\u003efast commit\u003c/em\u003e on one of the brokers. \u003cstrong\u003eWe noticed that max write latency decreased significantly.\u003c/strong\u003e Diving deeper we found out that on a broker with\n\u003cem\u003efast commit\u003c/em\u003e enabled:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe latency of \u003cem\u003ejdb2_journal_commit_transaction\u003c/em\u003e decreased by an order of magnitude. This meant that periodic journal commits were indeed much faster\nthanks to enabling \u003cem\u003efast commits\u003c/em\u003e.\u003c/li\u003e\n  \u003cli\u003eSlow ext4 writes occurred at the same time when there was a spike in latency of \u003cem\u003ejbd2_fc_begin_commit\u003c/em\u003e. This method is part of the \u003cem\u003efast commit\u003c/em\u003e flow. It\nbecame the new source of latency but its maximum latency was lower than that of \u003cem\u003ejdb2_journal_commit_transaction\u003c/em\u003e without fast commits.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/write_iter_heatmap.png\" alt=\"Comparison of maximum latency [s] of ext4 writes for brokers without and with fast commit.\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eComparison of maximum latency [s] of ext4 writes for brokers without and with fast commit.\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eLower file system write latency, in turn, resulted in reduced produce latency:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: center\"\u003e \u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_p999_2.png\" alt=\"Base Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/fc_p999_2.png\" alt=\"Fast Commit Produce Latency\" /\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: center\"\u003e\u003cem\u003eWith fast commit enabled, produce P999 latency went down from 3 seconds to 500 milliseconds\u003c/em\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\u003cp\u003eTo summarize, we’ve tested the following ext4 optimizations:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eDecreasing the commit interval\u003c/li\u003e\n  \u003cli\u003eChanging the journaling mode to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eEnabling \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commit\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe observed that both \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commit\u003c/code\u003e significantly reduced latency, with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commit\u003c/code\u003e having slightly lower latency. The results were\npromising, but we had higher hopes. Thankfully, we had one more idea left.\u003c/p\u003e\n\n\u003ch2 id=\"xfs\"\u003eXFS\u003c/h2\u003e\n\u003cp\u003eWhile researching the topic of journaling in ext4, we stumbled upon a few sources suggesting that the XFS file system, with its more advanced journaling,\nis well-suited for handling large files and high-throughput workloads, often outperforming ext4 in such scenarios. Kafka documentation also mentions that XFS\nhas a lot of tuning already in place and should be a better fit than the default ext4.\u003c/p\u003e\n\n\u003cp\u003eWe migrated one of the brokers to the XFS file system. The results were impressive. The thing that was very distinctive compared to the aforementioned ext4\noptimizations was the consistency of XFS performance. While other broker configurations experienced p999 latency spikes throughout the day, XFS – with its default configuration – had only a\nfew hiccups.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/base_p999_2.png\" alt=\"Base Produce Latency\" /\u003e\n\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/xfs_p999_2.png\" alt=\"Produce Latency XFS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAfter a couple of weeks of testing, we were confident that XFS was the best choice. Consequently, we migrated all our brokers from ext4 to XFS.\u003c/p\u003e\n\n\u003ch2 id=\"summary-1\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eUsing a combination of packet sniffing, eBPF, and async-profiler we managed to identify the root cause of slow produce requests in our Kafka cluster. We\nthen tested a couple of solutions to the problem: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edata=writeback\u003c/code\u003e journaling mode, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efast commits\u003c/code\u003e, and changing the file system to XFS. The results of these\noptimizations are visualized in the heatmap below:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-03-06-kafka-performance-analysis/heatmap_p999.png\" alt=\"Produce Latency Heatmap\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eUltimately, we found XFS to be the most performant and rolled it out to all of our brokers. \u003cstrong\u003eWith XFS, the number of produce requests exceeding 65ms (our SLO)\nwas lowered by 82%.\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003eHere are some of the lessons we learned along the way:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eeBPF was extremely useful during the analysis. It was straightforward to utilize one of the pre-existing tools from bcc or bpftrace. We were also able to\neasily modify them for our custom use cases.\u003c/li\u003e\n  \u003cli\u003eebpf_exporter is a great tool for observing trace results over longer periods of time. It allows to expose Prometheus metrics based on libbpf programs.\u003c/li\u003e\n  \u003cli\u003ep99 and p999 analysis is sometimes not enough. In our case, the p999 latency of file system writes was less than 1ms. It turned out that a single slow write\ncould cause lock contention and a cascade of slow requests. Without tracing individual requests, the root cause would have been very hard to catch.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe hope that you found this blog post useful, and we wish you good luck in your future performance analysis endeavors!\u003c/p\u003e\n\n\u003chr /\u003e\n\u003ch3 id=\"acknowledgments\"\u003eAcknowledgments\u003c/h3\u003e\n\n\u003cp\u003e\u003cem\u003eWe would like to thank our colleague Dominik Kowalski for performing the XFS migration and applying the ext4 configuration changes to the Kafka cluster.\u003c/em\u003e\u003c/p\u003e\n\n\u003cstyle\u003e\n  .post-content table, .post-content td, .post-content th {\n    border: none;\n    background-color: transparent;\n}\n\n.post-content th {\n    display: none;\n}\n\n.post-content td {\n    padding: 0;\n}\n\n\u003c/style\u003e\n\n","contentSnippet":"At Allegro, we use Kafka as a backbone for asynchronous communication between microservices. With up to\n300k messages published and 1M messages consumed every second, it is a key part of our infrastructure. A few months ago, in our main Kafka cluster, we noticed\nthe following discrepancy: while median response times for produce requests\nwere in single-digit milliseconds, the tail latency was much worse. Namely, the\np99 latency was up to 1 second, and the p999 latency was up to 3 seconds. This was unacceptable for a new project that we were about to start, so we\ndecided to look into this issue. In this blog post, we would like to describe our journey — how we used Kafka protocol sniffing and eBPF to identify and remove\nthe performance bottleneck.\n\nThe Need for Tracing\nKafka brokers expose various metrics. From them, we were able to tell that\nproduce requests were slow for high percentiles, but we couldn’t identify the cause. System metrics were also not showing anything alarming.\nTo pinpoint the underlying problem, we decided to trace individual requests. By analyzing components of Kafka involved in handling produce requests,\nwe aimed to uncover the source of the latency spikes. One way of doing that would be to fork Kafka, implement instrumentation, and deploy our custom version\nto the cluster. However, this would be very time-consuming and invasive. We decided to try an alternative approach.\nThe first thing we did was finding arrival and end times for every Kafka produce request.\n \n    \n\n    \nTimeline of a produce request. Arrival and end times define the boundaries of the request. The components of Kafka involved in handling the request and their latencies are unknown.\n    \nKafka uses a binary protocol over TCP to send requests from producers (and consumers) to brokers. We started by capturing the network traffic on a selected\nbroker using tcpdump. Then we wrote a tool for analyzing the captured packets, which enabled us to list all the request and response\ntimes. In the output, we saw a confirmation of what we already knew — there were many slow produce requests taking over a second to complete. What’s more we\nwere able to see request metadata — topic name, partition ID and message ID (our internal identifier included in Kafka headers):\n\nARRIVAL TIME  END TIME      LATENCY(ms)  MESSAGE_ID  TOPIC   PARTITION\n12:11:36.521  12:11:37.060  538          371409548   topicA  2\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.519  12:11:37.060  540          375783615   topicB  18\n12:11:36.555  12:11:37.061  505          371409578   topicC  7\n12:11:36.587  12:11:37.061  473          375783728   topicD  16\n12:11:36.690  12:11:37.061  370          375783907   topicB  18\n\n\nWith that extra knowledge in hand, we were ready to dig deeper.\nDynamic Tracing\nThanks to network traffic analysis we had arrival time, end time and metadata for each request. We then wanted to gain insights into\nwhich Kafka components were the source of latency. Since produce requests are mostly concerned with saving data,\nwe decided to instrument writes to the underlying storage.\nOn Linux, Kafka uses regular files for storing data. Writes are done using ordinary write system calls — data is first stored in the page cache\nand then asynchronously flushed to disk. How can we trace individual file writes without modifying the source code? We can make use of dynamic tracing.\nWhat is dynamic tracing? In Brendan Gregg’s System Performance, he uses the following analogy that we really like:\nConsider an operating system kernel: analyzing kernel internals can be like venturing into a dark room, with candles […] placed where the kernel engineers\nthought they were needed. Dynamic instrumentation is like having a flashlight that you can point anywhere.\nThis basically means that it is possible to instrument arbitrary kernel code without the need to modify a user space application or the kernel itself. For\nexample, we can use dynamic tracing to instrument file system calls to check whether they are the source of latency. To do that we can make use of a technology\ncalled BPF.\nBPF (or eBPF) which stands for (extended) Berkeley Packet Filter is a technology with a rich history, but today it is a generic in-kernel execution\nenvironment [Gregg Brendan (2020). Systems Performance: Enterprise and the Cloud, 2nd Edition]. It has a wide range of applications, including networking,\nsecurity and tracing tools. eBPF programs are compiled to bytecode which is then interpreted by the Linux Kernel.\nThere are a couple of well-established front-ends for eBPF, including BCC,\nbpftrace and libbpf. They can be used to write custom tracing programs, but they\nalso ship with many useful tools already implemented. One such tool is ext4slower.\nIt allows tracing file system operations in the ext4 file system, which is the default file system for Linux.\nTracing Kafka\nIn Kafka, every partition has its own directory, named according to the pattern: topicName-partitionID. Within each of these directories, there are segment\nfiles where messages are stored. In the figure below, we can see an example of this structure. In this scenario, the broker hosts two partitions (0 and 7)\nfor topicA and one partition (1) for topicB.\n\nBy slightly altering the ext4slower program to include parent directories, we were able to trace Kafka file system writes. For every write with a duration\nexceeding a specified threshold, we observed the following:\nStart time and end time\nDuration\nThread ID (TID)\nNumber of bytes written\nFile offset\nTopic name\nPartition ID\nBelow is an example output from the program:\n\nSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n15:37:00.627  15:37:00.785  158 ms   4478  2009   88847331   topicA-0/00000000002938697123.log\n15:37:00.629  15:37:00.785  156 ms   4492  531    289315894  topicB-7/00000000001119733846.log\n15:37:00.629  15:37:00.785  156 ms   4495  815    167398027  topicC-7/00000000015588371822.log\n15:37:00.631  15:37:00.785  154 ms   4488  778    502626221  topicD-7/00000000004472160265.log\n15:37:00.644  15:37:00.785  141 ms   4486  341    340818418  topicE-7/00000000002661443174.log\n15:37:00.650  15:37:00.785  135 ms   4470  374    230883174  topicF-7/00000000006102922534.log\n15:37:00.653  15:37:00.785  132 ms   4461  374    375758631  topicF-19/00000000001555977358.log\n\n\nThis was already very helpful since we could, based on timestamp, topic and partition, correlate produce requests from the tcpdump output with writes to\nthe file system:\n\nARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA  0\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB  7\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC  7\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD  7\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE  7\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF  7\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF  19\n\n\nTo gain extra confidence, we wrote a tool that parses a Kafka log file, reads the records written to it (using file offset and number of bytes written),\nparses them, and returns their message IDs. With that, we were able to perfectly correlate incoming requests with their respective writes:\n\nSTART TIME    END TIME      LATENCY  MESSAGE_ID  FILE                                TOPIC   PARTITION  BYTES  OFF_KB\n15:37:00.627  15:37:00.785  158 ms   839584818   topicA-0/00000000002938697123.log   topicA  0          2009   88847331\n15:37:00.629  15:37:00.785  156 ms   982282008   topicB-7/00000000001119733846.log   topicB  7          531    289315894\n15:37:00.629  15:37:00.785  156 ms   398037998   topicC-7/00000000015588371822.log   topicC  7          815    167398027\n15:37:00.631  15:37:00.785  154 ms   793357083   topicD-7/00000000004472160265.log   topicD  7          778    502626221\n15:37:00.644  15:37:00.786  141 ms   605597592   topicE-7/00000000002661443174.log   topicE  7          341    340818418\n15:37:00.649  15:37:00.785  136 ms   471986034   topicF-7/00000000006102922534.log   topicF  7          374    230883174\n15:37:00.653  15:37:00.786  132 ms   190735697   topicF-19/00000000001555977358.log  topicF  19         374    375758631\n\n\nFrom the analysis, we were able to tell that there were many slow produce requests that spent all of their time waiting for the file system write to\ncomplete.\n\nThere were however requests that didn’t have corresponding slow writes.\n\nKafka Lock Contention\nSlow produce requests without corresponding slow writes were always occurring around the time of some other slow write. We started wondering whether those\nrequests were perhaps queuing and waiting for something to finish. By analyzing Kafka source code, we identified a couple of places that use synchronized\nblocks, including those guarding log file writes.\nWe set out to measure how much time Kafka’s threads, processing produce requests, spend on the aforementioned locks. Our goal was to correlate periods when\nthey were waiting on locks with writes to the file system. We considered two approaches to do that.\nThe first one was to use tracing again, and perhaps combine its results with the tool we already had for tracing the ext4 file system.\nLooking at the JDK source code we were not able to identify a connection between synchronized blocks and traceable kernel routines. Instead, we learned that\nJVM ships with predefined DTrace tracepoints (DTrace can be thought of as a predecessor of eBPF). These tracepoints include hotspot:monitor__contended__enter\nand hotspot:monitor__contended__entered, which monitor when a thread begins waiting on a contended lock and when it finally enters it. By running Kafka\nwith the -XX:+DTraceMonitorProbes VM option and attaching to these tracepoints we were able to see monitor wait times for a given thread.\nAnother approach we came up with was to capture states of Kafka’s threads by running async-profiler\nalongside the ext4 tracing script. We would then analyze results from both tools and correlate their outputs.\nAfter experimenting with both ideas, we ultimately chose to stick with async-profiler. It provided a clean visualization of thread states and offered more\ninsights into JVM-specific properties of threads.\nNow, let’s delve into how we analyzed a situation when a latency spike occurred, based on an example async-profiler recording, eBPF traces, and parsed\ntcpdump output. For brevity, we’ll focus on one Kafka topic.\nBy capturing network traffic on a broker, we were able to see that there were four slow produce requests to the selected topic:\n\nARRIVAL TIME  END TIME      LATENCY  MESSAGE_ID  TOPIC   PARTITION\n17:58:00.644  17:58:00.770  126 ms   75567596    topicF  6\n17:58:00.651  17:58:00.770  119 ms   33561917    topicF  6\n17:58:00.655  17:58:00.775  119 ms   20422312    topicF  6\n17:58:00.661  17:58:00.776  114 ms   18658935    topicF  6\n\n\nHowever, there was only one slow file system write for that topic:\n\nSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.643  17:58:00.769  126 ms   4462  498    167428091  topicF-6/00000000000966764382.log\n\n\nAll other writes to that topic were fast at that time:\n\nSTART TIME    END TIME      LATENCY  TID   BYTES  OFF_KB     FILE\n17:58:00.770  17:58:00.770  0 ms     4484  798    167451825  topicF-6/00000000000966764382.log\n17:58:00.775  17:58:00.775  0 ms     4499  14410  167437415  topicF-6/00000000000966764382.log\n17:58:00.776  17:58:00.776  0 ms     4467  1138   167436277  topicF-6/00000000000966764382.log\n\n\nWe knew that one of the fast writes was performed from a thread with ID 4484. From a thread dump, we extracted thread names and Native IDs (NIDs).\nKnowing that NIDs translate directly to Linux TIDs (thread IDs), we found a thread with NID 0x1184 (decimal: 4484). We determined that the name of\nthis thread was data-plane-kafka-request-handler-24.\nWe searched for this thread’s activity in the async-profiler output:\n \n    \n\n    \nAsync profiler output visualized in Java Mission Control. Thread with TID 4484 is blocked on a monitor.\n    \nIn the output, we saw what we suspected — a thread was waiting on a lock for approximately the same duration as the slow write occurring on another thread.\nThis confirmed our initial hypothesis.\n \n    \n\n    \nFor a slow request with fast file system writes, waiting to acquire a lock turned out to be the source of latency.\n    \nApplying this technique, we analyzed numerous cases, and the results were consistent: for a slow produce request there was either a matching slow write or a\nthread was waiting to acquire a lock guarding access to a log file. We confirmed that file system writes were the root cause of slow produce requests.\nTracing the File System\nOur original eBPF script traced only calls to the ext4_file_write_iter function.\nWhile this was sufficient to roughly determine that slow writes to the file system were causing the latency spikes, it was not enough to pinpoint which\nparameters of the file system needed tuning. To address this, we captured both on-CPU\nand off-CPU profiles of ext4_file_write_iter, using\nprofile and offcputime,\nrespectively. Our goal was to identify the activated paths in the kernel and then measure the latency of functions associated with them.\n \n    \n\n    \non-CPU profile of ext4_file_write_iter\n    \n \n    \n\n    \noff-CPU profile of ext4_file_write_iter\n    \nWe noticed that the function ext4_dirty_inode [1] was present in both flamegraphs.\nIn the Linux kernel, the ext4_dirty_inode function is responsible for marking an inode (file or directory data structure) as being in a dirty state. A dirty\ninode indicates that the corresponding file’s data or metadata has been modified and needs to be synchronized with the underlying storage device, typically a\ndisk, to ensure data consistency.\nWhat caught our attention in the off-CPU profile was the jbd2__journal_start\n[2] function which is part of a journaling mechanism employed in ext4 that ensures data integrity and reliability. Journaling in ext4 involves maintaining a\ndetailed log that records the changes before they are committed to the file system. This log, often referred to as the journal, serves as a safety net in the\nevent of an unexpected system crash or power failure. When a file system operation occurs, such as creating, modifying, or deleting a file, ext4 first records\nthis change in the journal. Subsequently, the actual file system structures are updated. The process of updating the file system is known as committing the\njournal. During a commit, the changes recorded in the journal are applied to the file system structures in a controlled and atomic manner. In the event of an\ninterruption, the file system can recover quickly by replaying the journal, ensuring that it reflects the consistent state of the file system.\nAs seen in the figure with the off-CPU profile, wait_transaction_locked [3] is the\nfunction executed before voluntarily yielding the processor, allowing the scheduler to select and switch to a different process or thread ready to run\n(schedule()). Guided by the comment above the wait_transaction_locked function:\nWait until running transaction passes to T_FLUSH state and new transaction can thus be started. Also starts the commit if needed. The function expects running\ntransaction to exist and releases j_state_lock.\nWe searched the kernel code to identify what sets the T_FLUSH flag. The only place that we discovered was within the\njbd2_journal_commit_transaction function executed periodically by a kernel journal\nthread. Consequently, we decided to trace this function to explore any correlation between its latency and the latency of ext4_dirty_inode. The obtained\nresults aligned precisely with our expectations – namely, a high latency in  jbd2_journal_commit_transaction translates to a high latency in\next4_dirty_inode. The details of our findings are presented below:\n\nSTART TIME    END TIME      LATENCY  FUNCTION\n19:35:24.503  19:35:24.680  176 ms   jbd2_journal_commit_transaction\n19:35:24.507  19:35:24.648  141 ms   ext4_dirty_inode\n19:35:24.508  19:35:24.648  139 ms   ext4_dirty_inode\n19:35:24.514  19:35:24.648  134 ms   ext4_dirty_inode\n...\n19:38:14.508  19:38:14.929  420 ms   jbd2_journal_commit_transaction\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.511  19:38:14.868  357 ms   ext4_dirty_inode\n19:38:14.512  19:38:14.868  356 ms   ext4_dirty_inode\n...\n19:48:39.475  19:48:40.808  1332 ms  jbd2_journal_commit_transaction\n19:48:39.477  19:48:40.757  1280 ms  ext4_dirty_inode\n19:48:39.487  19:48:40.757  1270 ms  ext4_dirty_inode\n19:48:39.543  19:48:40.757  1213 ms  ext4_dirty_inode\n...\n\n\next4 Improvements Monitoring\nHaving identified journal commits as the cause of slow writes, we started thinking how to alleviate the problem. We had a few ideas, but we were wondering how\nwe would be able to observe improvements.  Up until that point, we relied on command-line tools and analyzing their output for short time ranges. We wanted\nto be able to observe the impact of our optimizations over longer periods.\nTo report traced functions latency over long periods, we used ebpf_exporter, a tool that exposes eBPF-based\nmetrics in Prometheus format. We were then able to visualize traces in Grafana. For example, maximum ext4 write latency for a given broker:\n\nWith that, we were able to run brokers with different configurations and observe their write latency over time.\next4 Improvements\nLet’s go back to ext4. We knew that journal commits were the source of latency. By studying ext4 documentation, we identified a few possible solutions for\nimproving the performance:\nDisabling journaling\nDecreasing the commit interval\nChanging the journaling mode from data=ordered to data=writeback\nEnabling fast commits\nLet’s discuss each of them.\nDisabling Journaling\nIf journaling is the source of high latency, why not disable it completely? Well, it turns out that journaling is there for a reason. Without journaling, we\nwould risk long recovery in case of a crash. Thus, we quickly ruled out this solution.\nDecreasing the Commit Interval\next4 has the commit mount parameter which tells how often to perform commits. It has the default value of 5 seconds. According to the ext4 documentation:\nThis default value (or any low value) will hurt performance, but it’s good for data-safety. […] Setting it to very large values will improve performance.\nHowever, instead of increasing the value we decided to decrease it. Why? Our intuition was that by performing commits more frequently we would make them\n“lighter” which would make them faster. We would trade throughput for lower latency. We experimented with commit=1, and commit=3 but observed no\nsignificant differences.\nChanging the Journaling Mode from data=ordered to data=writeback\next4 offers three journaling modes: journal, ordered and writeback. The default mode is ordered and compared to the most performant mode, writeback,\nit guarantees that the data is written to the main file system prior to the metadata being committed to the journal. As mentioned in\ndocs, Kafka does not rely on this property, so switching the mode to writeback should reduce latency.\nWe switched the journaling mode on one of the brokers, and indeed, we observed latency improvements:\n \n    \n\n    \n\n    \nWith data=writeback, p999 decreased from 3 seconds to 800 milliseconds.\n    \nEnabling Fast Commit\nWhen reading about ext4 journaling, we stumbled upon an article describing a new feature introduced in Linux 5.10 called\nfast commits. As explained in the article, fast commit is a lighter-weight journaling method that could result in performance boost for certain workloads.\nWe enabled fast commit on one of the brokers. We noticed that max write latency decreased significantly. Diving deeper we found out that on a broker with\nfast commit enabled:\nThe latency of jdb2_journal_commit_transaction decreased by an order of magnitude. This meant that periodic journal commits were indeed much faster\nthanks to enabling fast commits.\nSlow ext4 writes occurred at the same time when there was a spike in latency of jbd2_fc_begin_commit. This method is part of the fast commit flow. It\nbecame the new source of latency but its maximum latency was lower than that of jdb2_journal_commit_transaction without fast commits.\n \n    \n\n    \nComparison of maximum latency [s] of ext4 writes for brokers without and with fast commit.\n    \nLower file system write latency, in turn, resulted in reduced produce latency:\n \n    \n\n    \n\n    \nWith fast commit enabled, produce P999 latency went down from 3 seconds to 500 milliseconds\n    \nSummary\nTo summarize, we’ve tested the following ext4 optimizations:\nDecreasing the commit interval\nChanging the journaling mode to data=writeback\nEnabling fast commit\nWe observed that both data=writeback and fast commit significantly reduced latency, with fast commit having slightly lower latency. The results were\npromising, but we had higher hopes. Thankfully, we had one more idea left.\nXFS\nWhile researching the topic of journaling in ext4, we stumbled upon a few sources suggesting that the XFS file system, with its more advanced journaling,\nis well-suited for handling large files and high-throughput workloads, often outperforming ext4 in such scenarios. Kafka documentation also mentions that XFS\nhas a lot of tuning already in place and should be a better fit than the default ext4.\nWe migrated one of the brokers to the XFS file system. The results were impressive. The thing that was very distinctive compared to the aforementioned ext4\noptimizations was the consistency of XFS performance. While other broker configurations experienced p999 latency spikes throughout the day, XFS – with its default configuration – had only a\nfew hiccups.\n\n\nAfter a couple of weeks of testing, we were confident that XFS was the best choice. Consequently, we migrated all our brokers from ext4 to XFS.\nSummary\nUsing a combination of packet sniffing, eBPF, and async-profiler we managed to identify the root cause of slow produce requests in our Kafka cluster. We\nthen tested a couple of solutions to the problem: data=writeback journaling mode, fast commits, and changing the file system to XFS. The results of these\noptimizations are visualized in the heatmap below:\n\nUltimately, we found XFS to be the most performant and rolled it out to all of our brokers. With XFS, the number of produce requests exceeding 65ms (our SLO)\nwas lowered by 82%.\nHere are some of the lessons we learned along the way:\neBPF was extremely useful during the analysis. It was straightforward to utilize one of the pre-existing tools from bcc or bpftrace. We were also able to\neasily modify them for our custom use cases.\nebpf_exporter is a great tool for observing trace results over longer periods of time. It allows to expose Prometheus metrics based on libbpf programs.\np99 and p999 analysis is sometimes not enough. In our case, the p999 latency of file system writes was less than 1ms. It turned out that a single slow write\ncould cause lock contention and a cascade of slow requests. Without tracing individual requests, the root cause would have been very hard to catch.\nWe hope that you found this blog post useful, and we wish you good luck in your future performance analysis endeavors!\nAcknowledgments\nWe would like to thank our colleague Dominik Kowalski for performing the XFS migration and applying the ext4 configuration changes to the Kafka cluster.\n\n\n  .post-content table, .post-content td, .post-content th {\n    border: none;\n    background-color: transparent;\n}\n\n.post-content th {\n    display: none;\n}\n\n.post-content td {\n    padding: 0;\n}","guid":"https://blog.allegro.tech/2024/03/kafka-performance-analysis.html","categories":["tech","kafka","ebpf","bcc","linux","kernel","ext4","xfs","performance","tuning","file system"],"isoDate":"2024-03-05T23:00:00.000Z"},{"title":"Tired of repetitive tasks?! Go for RPA!","link":"https://blog.allegro.tech/2024/02/rpa.html","pubDate":"Tue, 20 Feb 2024 00:00:00 +0100","authors":{"author":[{"name":["Dominika Pleśniak"],"photo":["https://blog.allegro.tech/img/authors/dominika.plesniak.jpg"],"url":["https://blog.allegro.tech/authors/dominika.plesniak"]}]},"content":"\u003cp\u003eHave you ever thought about ways of reducing repetitive, monotonous tasks? Maybe you would like to try to automate your own tasks? I will show you what\ntechnology we use at Allegro, what processes we have automated, and how to do it on your own.\u003c/p\u003e\n\n\u003ch2 id=\"what-is-rpa-and-how-do-we-use-it-at-allegro\"\u003eWhat is RPA and how do we use it at Allegro?\u003c/h2\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cem\u003e“Robotic process automation (RPA) is a software technology that makes it easy to build, deploy, and manage software robots that emulate humans actions\ninteracting with digital systems and software. Just like people, software robots can do things like understand what’s on a screen, complete the right \nkeystrokes, navigate systems, identify and extract data, and perform a wide range of defined actions.”\u003c/em\u003e \n\u003cbr /\u003e \nSource: \u003ca href=\"https://www.uipath.com/rpa/robotic-process-automation\"\u003eUiPath Robotic Process Automation\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAt Allegro, our Process Automation Team primarily relies on UiPath as our key RPA tool. Processes suitable for automation through RPA are standardized, repetitive, manual, with high volume, stable and has data in digital form. If possible, we try to combine UiPath with different integrations such as scripts, databases, chatbots.\u003c/p\u003e\n\n\u003cp\u003eUiPath provides the ability to automate all kinds of applications (web, desktop, java, etc.). Automations can be created through the user interface of an \napplication, meaning that the created robot imitates an employee’s clicks. Furthermore, when an application’s API is available, it is easy to integrate \nUiPath with API, and in that case, the robot’s steps are performed in the backend-side. UiPath also allows the use of\nOCR (optical character recognition) and machine learning modules.\u003c/p\u003e\n\n\u003cp\u003eThanks to the various roles within Process Automation Team, such as analysts and developers, we are able to approach processes holistically. When we receive an idea for \nautomatization, we first perform an assessment to establish if the process is suitable for robotization, and we calculate the Return On Investment (ROI) and the potential of\ntime savings from automation in terms of \u003ca href=\"https://en.wikipedia.org/wiki/Full-time_equivalent\"\u003eFull-Time Equivalent (FTE)\u003c/a\u003e. Once the assessment is done and costs of investment return are approved, we analyze and optimize the process. \nAs a result, the analyst prepares a Process Definition Document (PDD) which serves as an instruction/description of the process.\nIn the next phase, based on PDD, a developer takes over the process and designs a solution. After that, the development part begins while the robot is created.\nLast but not least, there is the testing phase, where we check the results of the robotization together with the analyst and the business process owner.\nIf the solution is successful and performs as intended, we run the robot in production. Then we enter the hypercare period, during which we monitor and make necessary adjustments\nin tandem with the business process owner. After about two weeks of this phase, if both sides are satisfied with the results, we “go live”.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2024-02-20-rpa/robotic_process_automation_workflow.png\" alt=\"Robotic Process Automation Workflow\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"what-kind-of-processes-do-we-automate\"\u003eWhat kind of processes do we automate?\u003c/h2\u003e\n\n\u003ch3 id=\"jira-automations\"\u003eJira automations\u003c/h3\u003e\n\n\u003cp\u003eIn our organization we have a lot of processes based on Jira “tickets”. Many employees had to manage and operate Jira’s queues. Our team implemented several \nrobots to relieve administrators from repetitive tasks. Jira has an API available which we used in combination with the UiPath platform. \nFor example, when an employee is leaving Allegro, several Jira tasks are automatically created to retract authorization in various systems. Previously, these tasks were \nperformed manually by administrators. Now, the process is fully automated. The robot manages tickets via API and checks accounts in systems by GUI.\u003c/p\u003e\n\n\u003ch3 id=\"sap-enterprise-resource-planning-system-processes\"\u003eSAP (Enterprise Resource Planning system) processes\u003c/h3\u003e\n\n\u003cp\u003eAll repetitive, rule-based processes in SAP can be automated. For instance, let’s consider the processes in the Finance team. They handle massive amounts of \ninvoices. For some suppliers, with the largest quantity of invoices and unchangeable invoice layout, we were able to automate the accounting process in SAP. \nThe robot accesses an appropriate transaction in SAP and lists invoices. Based on business rules, the bot selects a specific invoice, opens it, and \nreads selected values. Then, it compares these values with business conditions which were established in the Process Definition Document. Depending on the \nsituation, the robot fills or corrects adequate fields and either accepts or rejects the invoice.\u003c/p\u003e\n\n\u003ch3 id=\"automation-by-api\"\u003eAutomation by API\u003c/h3\u003e\n\n\u003cp\u003eOne of the excellent examples of API automations is the process of changing product categories on the Allegro platform. Allegro hosts a vast number of products. Initially, \nnot all of them are assigned to the proper category. We were able to create a robot that uses Allegro’s REST API to move these products to the target category.\nBefore automation, this task was time-consuming and monotonous. Recently, the robot completed a huge task, moving almost 3 million products in two days!\u003c/p\u003e\n\n\u003ch3 id=\"processes-across-multiple-applications-and-integrations\"\u003eProcesses across multiple applications and integrations\u003c/h3\u003e\n\n\u003cp\u003eIt is possible to combine tasks from different applications into one robot. This approach allows us to automate more complex processes. \nThe most interesting ones include:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eThe process of user’s data change in Allegro platform is carried out at the request of users. To perform all actions, the bot uses Salesforce and \nInternal Admin tools. The robot downloads a report with requests, then checks the pre-set business rules. Based on the results, the bot changes user’s data or\n rejects the request.\nThe robot works 24 hours a day, handling 80% of applications. The number of tasks it performs can be compared to the work of four full-time employees.\u003c/li\u003e\n  \u003cli\u003eThe anti-fraud process. The robot verifies hundreds of thousands of messages and blocks suspected accounts. Using suspicious messages from the Spoof \napplication (Message Center), the robot determines if a message is spam or not. To make a proper decision, it checks various business conditions to decide \nif an account should be blocked. After making the decision, the bot blocks the account’s message sending capabilities.\u003c/li\u003e\n  \u003cli\u003eThe process for the HR team where the robot works on two applications. The robot interacts with the interface of an application and also uses its API. \nIn the recruitment process, specialists from different fields participate and help recruiters to find the best candidates. These specialists are known as \nthe Hiring Squad. A significant number of people are involved in this process, and the robot is responsible for keeping the Hiring Squad updated. Based \non a report with job offers the robot checks if a candidate has active status for specific skills required for the interview process. If the status is active, \nthe bot selects a particular job offer from the platform and assigns the interviewer from Hiring Squad.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"human-in-the-loop\"\u003e“Human in the Loop”\u003c/h3\u003e\n\n\u003cp\u003eThose processes which are rule-based, repetitive, but require human decision-making or the robot does not know what to do on the basis of the collected data, \nare referred to as “human in the loop”. A great example could be the process of damage complaints regarding packages that we have automated. The robot gathers a report \nfrom Salesforce and filters all jobs referring to damaged packages. Then, in the Internal Admin tool, the bot checks and collects various pieces of information based \non specific business rules. If necessary, it also checks the status of packages on carrier websites. Finally, the robot creates a form with all the gathered \ndetails, information, and attachments. This form is sent to a human for verification. With all this collected information, an employee can quickly decide \nwhether the complaint should be accepted. Then, the decision is sent back to the robot, which is able to close the case. It sends appropriate emails and \nrecords the results in the system.\u003c/p\u003e\n\n\u003ch2 id=\"robotics-workshops-for-employees\"\u003eRobotics workshops for employees\u003c/h2\u003e\n\n\u003cp\u003eAdditionally, twice a year, we organize an educational program for employees called “Allegro Robot School”. Employees get a chance to learn how to build \nbasic robots in UiPath and build one to support their daily tasks. To sign up for the program there is no need to have coding experience. It is enough if \nan employee can think analytically and has motivation to learn new things.\nThe program is intensive, consisting of five days of workshops. After the workshops, there is a three-weeks period where, with our support, employees choose \ntheir own processes and build robots.\nFor each edition we have around ten participants. The robots created during one edition account for about 3 FTEs! We have many examples of graduate \nemployees who created more robots to support their daily work in a team. Moreover, we created a Slack community for graduates to stay in touch, share \nknowledge, and support the development of new robots.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eToday, the number of processes and different applications used in companies is enormous. Moreover, it can sometimes be challenging to integrate one application\nwith another, and employees are burdened with many manual, repetitive tasks. It is important to know that there is a solution to automate these processes. \nThe RPA technology can quickly help with that, freeing up employees for more creative tasks. The above examples visualize the possibilities of using UiPath.\nThe most important thing to remember is that the process to automate has to be manual, rule-based, repetitive, with data in digital form. What is more, \nit is possible to learn for those who don’t have coding experience. Thanks to that, the automation of processes can be expanded across the company beyond the Process Automation Team.\u003c/p\u003e\n","contentSnippet":"Have you ever thought about ways of reducing repetitive, monotonous tasks? Maybe you would like to try to automate your own tasks? I will show you what\ntechnology we use at Allegro, what processes we have automated, and how to do it on your own.\nWhat is RPA and how do we use it at Allegro?\n“Robotic process automation (RPA) is a software technology that makes it easy to build, deploy, and manage software robots that emulate humans actions\ninteracting with digital systems and software. Just like people, software robots can do things like understand what’s on a screen, complete the right \nkeystrokes, navigate systems, identify and extract data, and perform a wide range of defined actions.” \n \nSource: UiPath Robotic Process Automation\nAt Allegro, our Process Automation Team primarily relies on UiPath as our key RPA tool. Processes suitable for automation through RPA are standardized, repetitive, manual, with high volume, stable and has data in digital form. If possible, we try to combine UiPath with different integrations such as scripts, databases, chatbots.\nUiPath provides the ability to automate all kinds of applications (web, desktop, java, etc.). Automations can be created through the user interface of an \napplication, meaning that the created robot imitates an employee’s clicks. Furthermore, when an application’s API is available, it is easy to integrate \nUiPath with API, and in that case, the robot’s steps are performed in the backend-side. UiPath also allows the use of\nOCR (optical character recognition) and machine learning modules.\nThanks to the various roles within Process Automation Team, such as analysts and developers, we are able to approach processes holistically. When we receive an idea for \nautomatization, we first perform an assessment to establish if the process is suitable for robotization, and we calculate the Return On Investment (ROI) and the potential of\ntime savings from automation in terms of Full-Time Equivalent (FTE). Once the assessment is done and costs of investment return are approved, we analyze and optimize the process. \nAs a result, the analyst prepares a Process Definition Document (PDD) which serves as an instruction/description of the process.\nIn the next phase, based on PDD, a developer takes over the process and designs a solution. After that, the development part begins while the robot is created.\nLast but not least, there is the testing phase, where we check the results of the robotization together with the analyst and the business process owner.\nIf the solution is successful and performs as intended, we run the robot in production. Then we enter the hypercare period, during which we monitor and make necessary adjustments\nin tandem with the business process owner. After about two weeks of this phase, if both sides are satisfied with the results, we “go live”.\n\nWhat kind of processes do we automate?\nJira automations\nIn our organization we have a lot of processes based on Jira “tickets”. Many employees had to manage and operate Jira’s queues. Our team implemented several \nrobots to relieve administrators from repetitive tasks. Jira has an API available which we used in combination with the UiPath platform. \nFor example, when an employee is leaving Allegro, several Jira tasks are automatically created to retract authorization in various systems. Previously, these tasks were \nperformed manually by administrators. Now, the process is fully automated. The robot manages tickets via API and checks accounts in systems by GUI.\nSAP (Enterprise Resource Planning system) processes\nAll repetitive, rule-based processes in SAP can be automated. For instance, let’s consider the processes in the Finance team. They handle massive amounts of \ninvoices. For some suppliers, with the largest quantity of invoices and unchangeable invoice layout, we were able to automate the accounting process in SAP. \nThe robot accesses an appropriate transaction in SAP and lists invoices. Based on business rules, the bot selects a specific invoice, opens it, and \nreads selected values. Then, it compares these values with business conditions which were established in the Process Definition Document. Depending on the \nsituation, the robot fills or corrects adequate fields and either accepts or rejects the invoice.\nAutomation by API\nOne of the excellent examples of API automations is the process of changing product categories on the Allegro platform. Allegro hosts a vast number of products. Initially, \nnot all of them are assigned to the proper category. We were able to create a robot that uses Allegro’s REST API to move these products to the target category.\nBefore automation, this task was time-consuming and monotonous. Recently, the robot completed a huge task, moving almost 3 million products in two days!\nProcesses across multiple applications and integrations\nIt is possible to combine tasks from different applications into one robot. This approach allows us to automate more complex processes. \nThe most interesting ones include:\nThe process of user’s data change in Allegro platform is carried out at the request of users. To perform all actions, the bot uses Salesforce and \nInternal Admin tools. The robot downloads a report with requests, then checks the pre-set business rules. Based on the results, the bot changes user’s data or\n rejects the request.\nThe robot works 24 hours a day, handling 80% of applications. The number of tasks it performs can be compared to the work of four full-time employees.\nThe anti-fraud process. The robot verifies hundreds of thousands of messages and blocks suspected accounts. Using suspicious messages from the Spoof \napplication (Message Center), the robot determines if a message is spam or not. To make a proper decision, it checks various business conditions to decide \nif an account should be blocked. After making the decision, the bot blocks the account’s message sending capabilities.\nThe process for the HR team where the robot works on two applications. The robot interacts with the interface of an application and also uses its API. \nIn the recruitment process, specialists from different fields participate and help recruiters to find the best candidates. These specialists are known as \nthe Hiring Squad. A significant number of people are involved in this process, and the robot is responsible for keeping the Hiring Squad updated. Based \non a report with job offers the robot checks if a candidate has active status for specific skills required for the interview process. If the status is active, \nthe bot selects a particular job offer from the platform and assigns the interviewer from Hiring Squad.\n“Human in the Loop”\nThose processes which are rule-based, repetitive, but require human decision-making or the robot does not know what to do on the basis of the collected data, \nare referred to as “human in the loop”. A great example could be the process of damage complaints regarding packages that we have automated. The robot gathers a report \nfrom Salesforce and filters all jobs referring to damaged packages. Then, in the Internal Admin tool, the bot checks and collects various pieces of information based \non specific business rules. If necessary, it also checks the status of packages on carrier websites. Finally, the robot creates a form with all the gathered \ndetails, information, and attachments. This form is sent to a human for verification. With all this collected information, an employee can quickly decide \nwhether the complaint should be accepted. Then, the decision is sent back to the robot, which is able to close the case. It sends appropriate emails and \nrecords the results in the system.\nRobotics workshops for employees\nAdditionally, twice a year, we organize an educational program for employees called “Allegro Robot School”. Employees get a chance to learn how to build \nbasic robots in UiPath and build one to support their daily tasks. To sign up for the program there is no need to have coding experience. It is enough if \nan employee can think analytically and has motivation to learn new things.\nThe program is intensive, consisting of five days of workshops. After the workshops, there is a three-weeks period where, with our support, employees choose \ntheir own processes and build robots.\nFor each edition we have around ten participants. The robots created during one edition account for about 3 FTEs! We have many examples of graduate \nemployees who created more robots to support their daily work in a team. Moreover, we created a Slack community for graduates to stay in touch, share \nknowledge, and support the development of new robots.\nSummary\nToday, the number of processes and different applications used in companies is enormous. Moreover, it can sometimes be challenging to integrate one application\nwith another, and employees are burdened with many manual, repetitive tasks. It is important to know that there is a solution to automate these processes. \nThe RPA technology can quickly help with that, freeing up employees for more creative tasks. The above examples visualize the possibilities of using UiPath.\nThe most important thing to remember is that the process to automate has to be manual, rule-based, repetitive, with data in digital form. What is more, \nit is possible to learn for those who don’t have coding experience. Thanks to that, the automation of processes can be expanded across the company beyond the Process Automation Team.","guid":"https://blog.allegro.tech/2024/02/rpa.html","categories":["tech","rpa"],"isoDate":"2024-02-19T23:00:00.000Z"}],"jobs":[{"id":"743999988837033","name":"Contractor Software Engineer (Java/Kotlin)","uuid":"bc22f235-405b-42c5-a377-08b9431c739f","jobAdId":"eba60e1f-3a45-4af9-a9e7-2b36917ce3eb","defaultJobAd":true,"refNumber":"REF4956T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-05-20T16:44:43.844Z","location":{"city":"Warsaw, Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"contract","label":"Contract"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"e543c79f-6df9-499f-8ba8-237c3c331cc1","valueLabel":"NEW B2B Technology CL 1-6"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"f7af19b5-5d6a-43a0-9a2b-1e99277515c7","valueLabel":"Opennet.pl Sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999988837033","language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999988151173","name":"Senior Salesforce Software Engineer","uuid":"6b29728d-b2c3-4cea-a2cf-91b593d4c313","jobAdId":"d0b08f39-f02c-4e32-ad36-475f900ddfc1","defaultJobAd":true,"refNumber":"REF4747J","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-05-17T07:49:11.356Z","location":{"city":"Poznań","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"26b58095-3c5f-4596-937f-27547fb80b07","valueLabel":"5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999988151173","creator":{"name":"Agnieszka Adamus"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999987145193","name":"Software Engineer (.NET) - Product Team - Allegro Pay","uuid":"295c893e-3c03-431b-9da7-eb18a45f98ed","jobAdId":"57040109-b330-495b-aa0d-247de3ea2066","defaultJobAd":false,"refNumber":"REF4748E","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-05-13T13:02:53.832Z","location":{"city":"Warsaw","region":"Masovian Voivodeship","country":"pl","remote":false,"latitude":"52.2296756","longitude":"21.0122287"},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"b3da614a-1ddb-441b-a557-5acfdb6fcb90","valueLabel":"NEW Technology CL 1-6"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999987145193","creator":{"name":"Martyna Stafa"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999987078873","name":"Senior Software Engineer (Java/Kotlin)","uuid":"4f9132b2-a419-4b95-8901-351571919dcd","jobAdId":"b1d96cd5-7296-471b-9a31-ab8b188c1afb","defaultJobAd":false,"refNumber":"REF4072X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-05-13T08:30:16.627Z","location":{"city":"Poznań, Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999987078873","language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999987077949","name":"Software Engineer (Java/Kotlin)","uuid":"f0ab337d-e325-4e3c-8de5-99011078cc54","jobAdId":"a4412007-4766-449a-a16d-19acb5cee072","defaultJobAd":true,"refNumber":"REF4072X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2024-05-13T08:28:13.931Z","location":{"city":"Poznań, Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"visibility":"PUBLIC","ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999987077949","language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}}],"events":[{"created":1715716710000,"duration":7200000,"id":"301022703","name":"Allegro Tech Talks #43 - Wszystko o programie e-Xperience","date_in_series_pattern":false,"status":"upcoming","time":1716393600000,"local_date":"2024-05-22","local_time":"18:00","updated":1715716710000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":12,"venue":{"id":27570147,"name":"Allegro Office - Poznań (Nowy Rynek)","lat":52.40021514892578,"lon":16.92083168029785,"repinned":false,"address_1":"Wierzbięcice 1B - budynek D","city":"Poznań","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/301022703/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-43/](https://app.evenea.pl/event/allegro-tech-talk-43/) Zapraszamy Was na #43 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej kawie☕, napojach🥤…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Nowy Rynek. Szukaj budynku D i kieruj się do wejścia od ul. Wierzbięcice. Komunikacja miejska: najbliższy przystanek to Wierzbięcice i kursują tu linie tramwajowe numer 2, 5, 6, 10, 12, 18 Spacerem - z dworca Poznań Główny przejście zajmie Ci około 5 minut.","visibility":"public","member_pay_fee":false},{"created":1702979844000,"duration":187200000,"id":"298027809","name":"UX Research Confetti - IV edycja","date_in_series_pattern":false,"status":"upcoming","time":1716202800000,"local_date":"2024-05-20","local_time":"13:00","updated":1702985612000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":82,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/298027809/","description":"**🎉 Przedstawiamy 4. edycję UX Research Confetti - bezpłatną, polską konferencję poświęconą badaniom UX, organizowaną przez zespół badaczy z Allegro.** ✨ Konferencja odbędzie się w…","visibility":"public","member_pay_fee":false},{"created":1712583756000,"duration":14400000,"id":"300288303","name":"DDD \u0026 EventStorming na luzie - unconference na 2 lata gildii w Allegro","date_in_series_pattern":false,"status":"past","time":1714129200000,"local_date":"2024-04-26","local_time":"13:00","updated":1714146607000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":103,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/300288303/","description":"**➡ Rejestracja:** **[https://app.evenea.pl/event/allegro-tech-talk-ddd/](https://app.evenea.pl/event/allegro-tech-talk-ddd/)** Dobrze Was widzieć! Allegro Tech to miejsce, w którym dzielimy się wiedzą, dobrymi praktykami i case study z różnych projektów prowadzonych w…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Fabryki Norblina. Szukaj wejścia Plater 3, od ul. Żelaznej. \n\nKomunikacja miejska: najbliższe przystanki to Norblin 05 i 06 z liniami: 109, 178, 157. Dojedziecie do nas również tramwajami numer 10 i 11 oraz metrem (Rondo ONZ lub Rondo Daszyńskiego).","visibility":"public","member_pay_fee":false},{"created":1712756447000,"duration":7200000,"id":"300327359","name":"Allegro Tech Talks #42 - Kariera Product Managera","date_in_series_pattern":false,"status":"past","time":1713888000000,"local_date":"2024-04-23","local_time":"18:00","updated":1713900030000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":37,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/300327359/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-42/ ](https://app.evenea.pl/event/allegro-tech-talk-42/) Zapraszamy Was na #42 wydarzenie z serii Allegro Tech Talk, podczas których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy przy dobrej…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Fabryki Norblina. Szukaj wejścia Plater 3, od ul. Żelaznej. \n\nKomunikacja miejska: najbliższe przystanki to Norblin 05 i 06 z liniami: 109, 178, 157. Dojedziesz do nas również tramwajami numer 10 i 11 oraz metrem (Rondo ONZ lub Rondo Daszyńskiego).","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"O pracy analityków w obszarze technologii i przetwarzaniu danych w dużej skali","link":"https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/","pubDate":"Thu, 29 Feb 2024 00:00:00 GMT","content":"Na czym polega praca analityków w obszarze technologii w Allegro? Jakich narzędzi i technologii na co dzień używają osoby pracujące na tych stanowiskach? Jak efekty pracy analityków wpływają na naszą platformę, produkty i funkcjonalności? Czym zajmuje się Data Product Manager w Allegro Pay? Dlaczego monety są ważnym elementem ekosystemu Allegro? Posłuchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziałem Adrianny Napiórkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.","contentSnippet":"Na czym polega praca analityków w obszarze technologii w Allegro? Jakich narzędzi i technologii na co dzień używają osoby pracujące na tych stanowiskach? Jak efekty pracy analityków wpływają na naszą platformę, produkty i funkcjonalności? Czym zajmuje się Data Product Manager w Allegro Pay? Dlaczego monety są ważnym elementem ekosystemu Allegro? Posłuchajcie kolejnego odcinka Allegro Tech Podcast tym razem z udziałem Adrianny Napiórkowskiej - Data Product Managerki w Allegro Pay oraz Kaya Akcelikli - Senior Managera w obszarze Data w Allegro.","guid":"https://podcast.allegro.tech/o-pracy-analitykow-w-obszarze-technologii-i-przetwarzaniu-danych-w-duzej-skali/","isoDate":"2024-02-29T00:00:00.000Z"},{"title":"Programowanie - co liczy się w nim najbardziej?","link":"https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/","pubDate":"Thu, 01 Feb 2024 00:00:00 GMT","content":"Jaką ścieżkę trzeba przejść, aby dobrze programować? Gdzie zdobywać wiedzę, doświadczenie i szlifować swoje umiejętności? Ile czasu potrzeba aby nabrać doświadczenia i jak zadbać o swój dalszy rozwój? Na czym w praktyce polegają role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza się w naszych zespołach? Posłuchajcie nowego odcinka Allegro Tech Podcast z udziałem Rafała Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.","contentSnippet":"Jaką ścieżkę trzeba przejść, aby dobrze programować? Gdzie zdobywać wiedzę, doświadczenie i szlifować swoje umiejętności? Ile czasu potrzeba aby nabrać doświadczenia i jak zadbać o swój dalszy rozwój? Na czym w praktyce polegają role (Senior) Software Engineer oraz Engineering Manager w Allegro i kto najlepiej sprawdza się w naszych zespołach? Posłuchajcie nowego odcinka Allegro Tech Podcast z udziałem Rafała Schmidta (Senior Software Engineer) i Waldemara Panasa (Manager, Engineering) z Allegro.","guid":"https://podcast.allegro.tech/programowanie-co-liczy-sie-w-nim-najbardziej/","isoDate":"2024-02-01T00:00:00.000Z"},{"title":"MBox: server-driven UI dla aplikacji mobilnych","link":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","pubDate":"Thu, 16 Nov 2023 00:00:00 GMT","content":"Czym jest i jak powstał MBox: wewnętrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? Skąd wziął się pomysł na to rozwiązanie i na jakie bolączki odpowiada? Dlaczego zdecydowaliśmy się na budowanie tego typu rozwiązania in-house i z jakimi wyzwaniami mierzyliśmy się w procesie tworzenia? Co wyróżnia zespoły pracujące nad tym narzędziem i jak pracuje im się bez Product Ownera? Posłuchajcie siódmego odcinka Allegro Tech Podcast z udziałem Pauliny Sadowskiej i Tomasza Gębarowskiego - Managerów w obszarze Technical Platform Services w Allegro.","contentSnippet":"Czym jest i jak powstał MBox: wewnętrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? Skąd wziął się pomysł na to rozwiązanie i na jakie bolączki odpowiada? Dlaczego zdecydowaliśmy się na budowanie tego typu rozwiązania in-house i z jakimi wyzwaniami mierzyliśmy się w procesie tworzenia? Co wyróżnia zespoły pracujące nad tym narzędziem i jak pracuje im się bez Product Ownera? Posłuchajcie siódmego odcinka Allegro Tech Podcast z udziałem Pauliny Sadowskiej i Tomasza Gębarowskiego - Managerów w obszarze Technical Platform Services w Allegro.","guid":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","isoDate":"2023-11-16T00:00:00.000Z"},{"title":"O chatbotach i ich wpływie na Allegro","link":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","pubDate":"Wed, 11 Oct 2023 00:00:00 GMT","content":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieć w kontekście obszaru Customer Experience? W czym pomagają nam chatboty, jak je rozwijamy i dbamy o ich jakość? Kim są Allina oraz Albert i co mają wspólnego z automatyzacją? Za jakie rozwiązania otrzymaliśmy nagrodę hiperautomatyzacji? O tym wszystkim posłuchacie w odcinku z udziałem Rafała Gajewskiego - Managera w obszarze IT Services w Allegro.","contentSnippet":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieć w kontekście obszaru Customer Experience? W czym pomagają nam chatboty, jak je rozwijamy i dbamy o ich jakość? Kim są Allina oraz Albert i co mają wspólnego z automatyzacją? Za jakie rozwiązania otrzymaliśmy nagrodę hiperautomatyzacji? O tym wszystkim posłuchacie w odcinku z udziałem Rafała Gajewskiego - Managera w obszarze IT Services w Allegro.","guid":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","isoDate":"2023-10-11T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"iLWbxMd4nqRzBvv-AjXNq","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>