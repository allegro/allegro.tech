<!DOCTYPE html><html lang="pl"><head><meta charSet="utf-8"/><link rel="prefetch" href="https://allegrotechio.disqus.com/count.js"/><meta name="viewport" content="initial-scale=1.0, width=device-width"/><meta name="description" content="Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie artykułów, podcastów oraz eventów."/><title>Allegro Tech</title><meta property="og:site_name" content="allegro.tech"/><meta property="og:title" content="allegro.tech"/><meta property="og:url" content="https://allegro.tech"/><meta property="og:type" content="site"/><meta property="og:image" content="https://allegro.tech/images/allegro-tech.png"/><link rel="shortcut icon" href="favicon.ico"/><link rel="canonical" href="https://allegro.tech" itemProp="url"/><link rel="preload" href="images/splash.jpg" as="image"/><link rel="author" href="humans.txt"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-1M1FJ5PXWW"></script><script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){dataLayer.push(arguments);}
                    gtag('js', new Date());
                    gtag('config', 'G-1M1FJ5PXWW');
                </script><meta name="next-head-count" content="16"/><link rel="preload" href="/_next/static/css/c4277531f90028a4.css" as="style"/><link rel="stylesheet" href="/_next/static/css/c4277531f90028a4.css" data-n-g=""/><link rel="preload" href="/_next/static/css/79db8b1e27b0a093.css" as="style"/><link rel="stylesheet" href="/_next/static/css/79db8b1e27b0a093.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/_next/static/chunks/webpack-69bfa6990bb9e155.js" defer=""></script><script src="/_next/static/chunks/framework-e70c6273bfe3f237.js" defer=""></script><script src="/_next/static/chunks/main-f635b472c367d1c7.js" defer=""></script><script src="/_next/static/chunks/pages/_app-179adf437ae674f2.js" defer=""></script><script src="/_next/static/chunks/206-3a56e5ded293e83e.js" defer=""></script><script src="/_next/static/chunks/pages/index-9e9857288e1eab25.js" defer=""></script><script src="/_next/static/e0cwA16mXqsL06qi5gBqu/_buildManifest.js" defer=""></script><script src="/_next/static/e0cwA16mXqsL06qi5gBqu/_ssgManifest.js" defer=""></script><script src="/_next/static/e0cwA16mXqsL06qi5gBqu/_middlewareManifest.js" defer=""></script></head><body class="m-color-bg_desk"><div id="__next" data-reactroot=""><header class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card Header_navbar__Zc5aN m-color-bg_card"><nav class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-justify-between m-flex-items-center"><a href="/"><img src="images/logo.svg" alt="Allegro Tech" width="205" height="45"/></a><div><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex@lg m-display-none"><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://blog.allegro.tech">Blog</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://ml.allegro.tech">Machine Learning</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://podcast.allegro.tech">Podcast</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://github.com/Allegro">Open Source</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://www.meetup.com/allegrotech/events">Wydarzenia</a></li><li class="m-margin-left-16@lg"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display-block m-display-inline@lg m-padding-left-16 m-padding-right-16 m-padding-top-16 m-padding-bottom-16 m-padding-left-0@lg m-padding-top-0@lg m-padding-right-0@lg m-padding-bottom-0@lg m-link--signal m-line-height_21" href="https://praca.allegro.pl">Praca</a></li></ul><button class="m-display-none@lg m-height_40 m-line-height_40 m-border-style-top_none m-border-style-right_none m-border-style-bottom_none m-border-style-left_none m-border-radius-top-left_2 m-border-radius-top-right_2 m-border-radius-bottom-left_2 m-border-radius-bottom-right_2 m-cursor_pointer m-overflow_hidden m-appearance_none m-padding-left_4 m-padding-right_4 m-padding-top_4 m-padding-bottom_4 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button" style="background:transparent" aria-label="Otwórz menu"><img src="https://assets.allegrostatic.com/metrum/icon/menu-23e046bf68.svg" alt="" class="m-icon" width="32" height="32"/></button></div></nav></header><div class="Header_hero__PYE0B"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-display-flex m-flex-column m-flex-justify-end Header_image__Cj6ZF"><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-color-bg_desk"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text  m-font-weight_100 m-font-size_32 m-font-size_43_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125">About us</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">Allegro is one of the most technologically advanced companies in our part of Europe. Allegro is also over 1700 IT specialists of various specializations, developing our website. The unique scale and complexity of the problems that we solve on a daily basis give us the opportunity to develop on a wide variety of projects. Allegro Tech is a place where our engineers share knowledge and case studies from selected projects in the company – in the form of articles, podcasts and events.</p></div></div></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Blog</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/09/online-mongodb-migration.html" title="Online MongoDB migration"><img width="388" src="images/post-headers/mongodb.png" alt="Online MongoDB migration" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/09/online-mongodb-migration.html" title="Online MongoDB migration" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Online MongoDB migration</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mongodb">#<!-- -->mongodb</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/nosql">#<!-- -->nosql</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/kotlin">#<!-- -->kotlin</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/open source">#<!-- -->open source</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/mongo change streams">#<!-- -->mongo change streams</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">MongoDB is the most popular database used at Allegro. We have hundreds of MongoDB databases running on our on—premise servers.
In 2022 we decided that we…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Szymon Marcinkiewicz" src="https://blog.allegro.tech/img/authors/szymon.marcinkiewicz.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/szymon.marcinkiewicz">Szymon Marcinkiewicz</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/09/online-mongodb-migration.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/09/online-mongodb-migration.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html" title="The Acrobatics of Switching Between Management and Engineering"><img width="388" src="images/post-headers/default.jpg" alt="The Acrobatics of Switching Between Management and Engineering" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html" title="The Acrobatics of Switching Between Management and Engineering" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">The Acrobatics of Switching Between Management and Engineering</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">27 dni temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/coding">#<!-- -->coding</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/management">#<!-- -->management</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/developer">#<!-- -->developer</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/team leader">#<!-- -->team leader</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/career path">#<!-- -->career path</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">After six years as a Team Leader, I went back to hands-on engineering work, and I’m very happy about taking
this step. While it may appear…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Michał Kosmulski" src="https://blog.allegro.tech/img/authors/michal.kosmulski.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/michal.kosmulski">Michał Kosmulski</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html" title="From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance"><img width="388" src="images/post-headers/default.jpg" alt="From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html" title="From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">2 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/azure">#<!-- -->azure</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/sql">#<!-- -->sql</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/saving">#<!-- -->saving</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/cloud">#<!-- -->cloud</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">In the era of ubiquitous cloud services and an increasingly growing PaaS and serverless-oriented approach, performance
and resources seem to be becoming less and less important.
After…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Mateusz Stolecki" src="https://blog.allegro.tech/img/authors/mateusz.stolecki.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/mateusz.stolecki">Mateusz Stolecki</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/07/save-money-on-large-database.html">przejdź do wpisu</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://blog.allegro.tech/2023/05/debugging-hangs.html" title="Debugging hangs - piecing together why nothing happens"><img width="388" src="images/post-headers/java.png" alt="Debugging hangs - piecing together why nothing happens" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://blog.allegro.tech/2023/05/debugging-hangs.html" title="Debugging hangs - piecing together why nothing happens" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Debugging hangs - piecing together why nothing happens</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">4 miesiące temu</time><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0"><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/tech">#<!-- -->tech</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/java">#<!-- -->java</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/jvm">#<!-- -->jvm</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/debugging">#<!-- -->debugging</a></li><li class="m-margin-right-8 m-display-inline-block"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/hashtag/dependency hell">#<!-- -->dependency hell</a></li></ul><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1 m-padding-top-16">As a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.
The task…</p><div class="m-display-flex m-flex-justify-between m-padding-top-16 m-flex-items_center"><div class="m-display-flex m-flex-items_center"><div class="MuiAvatarGroup-root m-padding-right_16 Post_avatars__FHMgb"><div class="MuiAvatar-root MuiAvatar-circle MuiAvatarGroup-avatar" style="z-index:1"><img alt="Łukasz Rokita" src="https://blog.allegro.tech/img/authors/lukasz.rokita.jpg" class="MuiAvatar-img" width="32" height="32"/></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/authors/lukasz.rokita">Łukasz Rokita</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://blog.allegro.tech/2023/05/debugging-hangs.html#disqus_thread">0 Comments</a></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech/2023/05/debugging-hangs.html">przejdź do wpisu</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://blog.allegro.tech">Zobacz więcej wpisów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Podcasty</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/" title="O roli analityków biznesowych w Allegro"><img src="images/podcast.png" alt="O roli analityków biznesowych w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/" title="O roli analityków biznesowych w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O roli analityków biznesowych w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">25 dni temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Czym zajmują się analitycy danych w Allegro i za jakie projekty odpowiadają? Z jakich rodzajów danych i narzędzi korzystają w codziennej pracy? Jakie (przykładowe) obszary tematyczne pokrywamy danymi, które analizujemy w Allegro? Jakich umiejętności szukamy u analityków biznesowych w Allegro i jak można do nas dołączyć? O roli analityków biznesowych i pracy w skali Allegro opowiadają Jakub Król i Mateusz Falkowski - Senior Data Analysts w Allegro.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/" title="O społeczności Allegro Tech i rozwoju inżynierów w Allegro"><img src="images/podcast.png" alt="O społeczności Allegro Tech i rozwoju inżynierów w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/" title="O społeczności Allegro Tech i rozwoju inżynierów w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O społeczności Allegro Tech i rozwoju inżynierów w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">około 2 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Na czym polega rola Principal Software Engineera w Allegro oraz co ma wspólnego z rozwijaniem siebie i dzieleniem się wiedzą? Co warto wiedzieć o turystyce, która pojawia się niemal w każdym odcinku naszych podcastów? Na czym polega, kto, kiedy i jak może z niej skorzystać? Jak pracujemy z talentami Gallupa (także w zespołach technicznych)?  Co dają nam wewnętrzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy się doświadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przyświeca? O społeczności Allegro Tech i możliwościach rozwoju w Allegro z perspektywy inżynierów rozmawialiśmy z Marcinem Turkiem i Michałem Kosmulskim.</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-data-science-hub-w-allegro/" title="O Data Science Hub w Allegro"><img src="images/podcast.png" alt="O Data Science Hub w Allegro" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-data-science-hub-w-allegro/" title="O Data Science Hub w Allegro" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O Data Science Hub w Allegro</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">2 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Co kryje się pod pojęciem Data Science Hub w Allegro? Jakie działania rozwijamy w tym obszarze i jak oceniamy ich potencjał? O czym jest projekt Wilson i na czym skupiamy się w projekcie przewidywania zakupów cyklicznych? Jak wykorzystujemy sztuczną inteligencję i gdzie jest dla niej miejsce wśród naszych kierunków rozwoju? O AI Transformation, poczuciu sprawczości, mieszance kompetencji i talentów zamkniętej w rolach Data Scientist, Data Engineer i Data Analyst rozmawialiśmy z Karoliną Nieradką i Kamilem Konikiewiczem.,</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-data-science-hub-w-allegro/">Posłuchaj odcinka</a></div></article></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--3@xl m-display-flex m-flex-direction_column"><article class="m-margin-bottom_16 m-display-flex m-flex-column m-flex-grow_1"><a href="https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/" title="O technologiach i projektach w Allegro Pay"><img src="images/podcast.png" alt="O technologiach i projektach w Allegro Pay" class="m-display-block m-width-fluid"/></a><div class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-column m-flex-grow_1 m-padding-bottom-0 m-color-bg_card"><a href="https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/" title="O technologiach i projektach w Allegro Pay" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">O technologiach i projektach w Allegro Pay</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-bottom-16">3 miesiące temu</time><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-flex-grow-1">Jak powstała usługa Allegro Pay i co ma wspólnego z ratatouille? Jakie projekty i technologie stoją za tym rozwiązaniem? Jak to jest pracować w Azure i obsługiwać ruch, który generuje Allegro? Czym inżynierów może zaskoczyć praca w Allegro Pay i co czeka na nich (na przykład) w programie All4Customer? O migrowaniu baz CosmosDB, wymaganiach skali i dostępności, a także o rozwijaniu ludzi i technologii rozmawialiśmy z Mariuszem Budzynem i Tomaszem Szczerbą. Zapraszamy do słuchania! na różnych płaszczyznach?</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/">Posłuchaj odcinka</a></div></article></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://podcast.allegro.tech">Zobacz więcej podcastów</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Wydarzenia</h2><div class="m-display_flex m-flex-wrap_1 m-flex-grow_1 m-grid"><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #38 - Mobile: o iOS bez spinki"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293929321/" title="Allegro Tech Talks #38 - Mobile: o iOS bez spinki" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #38 - Mobile: o iOS bez spinki</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">3 miesiące temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwą wakacyjną, stacjonarne spotkanie z cyklu Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293929321/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/292278882/" title="UX Research Confetti - III edycja " class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="UX Research Confetti - III edycja "/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/292278882/" title="UX Research Confetti - III edycja " class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">UX Research Confetti - III edycja </h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">4 miesiące temu</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**Rejestracja na wydarzenie ➡ [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **🎉 Przedstawiamy 3. edycję UX Research Confetti organizowaną przez Allegro - bezpłatną, polską konferencję poświęconą badaniom…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/292278882/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293341234/" title="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293341234/" title="Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">4 miesiące temu<!-- -->, Allegro Office - Poznań (Nowy Rynek)</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-37/](https://app.evenea.pl/event/allegro-tech-talk-37/) Ciąg dalszy naszych stacjonarnych spotkań Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów w kuluarach. 📌…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293341234/">Szczegóły</a></article></div></div><div class="m-grid__col m-grid__col--12 m-grid__col--6@sm m-grid__col--6@xl m-display-flex m-flex-direction_column"><div class="m-margin-bottom_16 m-display-flex m-flex-direction_column m-flex-direction_row_sm m-padding-bottom_0"><a href="https://www.meetup.com/allegrotech/events/293215214/" title="AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć" class="m-display_none m-display_block_lg" style="background-color:#fd4a02"><img width="218" src="images/event.png" alt="AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć"/></a><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-display-flex m-flex-direction_column m-padding-bottom_0 m-flex_1 m-flex-justify_between m-color-bg_card"><a href="https://www.meetup.com/allegrotech/events/293215214/" title="AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć" class="m-text-decoration_none"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm" style="text-overflow:ellipsis;display:-webkit-box;-webkit-box-orient:vertical;-webkit-line-clamp:2;min-height:2em;overflow:hidden">AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć</h2></a><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text">4 miesiące temu<!-- -->, Allegro Warsaw Office</time><time class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-padding-top-8">**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-kwanty/](https://app.evenea.pl/event/allegro-tech-kwanty/) Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie…</time><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-margin-top-16 m-display_block m-border-width_1 m-border-color_gray m-border-style-top_solid m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/293215214/">Szczegóły</a></article></div></div></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.meetup.com/allegrotech/events/">Zobacz więcej wydarzeń</a></div><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_300 m-font-size_27 m-font-size_36_sm m-margin-bottom_16 m-margin-bottom_24_sm m-line-height_125 m-padding-left-24 m-padding-right-24">Oferty pracy</h2><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto"><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Front-End Software Engineer - Technical Platform &amp; Operations</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznan, Warsaw</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999930016493-front-end-software-engineer-technical-platform-operations?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Front-End Software Engineer</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Warsaw, Poznan</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999930016183-front-end-software-engineer?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Senior Software Engineer (Java/Kotlin)</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Poznań, Warsaw, Cracow, Wrocław, Gdańsk</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999929962756-senior-software-engineer-javakotlin?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer Java - Mall.cz</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Prague, Remote</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999929309124-software-engineer-java-mallcz?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article><article class="m-padding-top_16 m-padding-top_24_lg m-padding-right_16 m-padding-right_24_lg m-padding-bottom_16 m-padding-bottom_24_lg m-padding-left_16 m-padding-left_24_lg m-card m-margin-bottom_16 m-display-flex m-flex-justify_between m-flex-items-center m-flex-direction_column m-flex-direction_row_md m-color-bg_card"><h2 class="m-font-family_roboto m-margin-left_0 m-margin-right_0 m-margin-top_0 m-color_text m-font-weight_500 m-line-height_130 m-margin-bottom_8 m-margin-bottom_16_sm m-font-size_21 m-font-size_25_sm m-margin-bottom_0_sm m-flex-grow_1">Software Engineer Java - Mall.cz</h2><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-font-size_21 m-font-size_25_sm m-font-weight_300 m-line-height_normal m-margin-top_16 m-text-align_left m-padding-right_16">Prague, Remote</p><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://www.smartrecruiters.com/Allegro/743999929308615-software-engineer-java-mallcz?trid=de9dfdf3-7f9e-4ebf-8a64-49f6c69ad640">Sprawdź</a></article></div><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-display_block m-margin-bottom_8 m-width_100 m-height_40 m-line-height_40 m-text-transform_uppercase m-letter-spacing_2 m-white-space_nowrap m-cursor_pointer m-overflow_hidden m-padding-left_16 m-padding-right_16 m-padding-top_0 m-padding-bottom_0 m-outline-style_dotted--focus m-outline-width_2 m-outline-color_teal m-outline-offset_n2 m-button m-box_border m-text-align_center m-display_inline-block m-color_secondary m-background-position_50p m-background-size_5000p m-transition-property_background-color m-transition-duration_fast m-button--secondary" href="https://allegro.pl/praca">Zobacz więcej ofert</a></div><footer class="m-color-bg_navy m-margin-top-32"><div class="m-width-max_1600 m-margin-left_auto m-margin-right_auto m-padding-top-24 m-padding-bottom-24 m-display-flex@sm m-flex-justify-between m-flex-items-center m-text-align_center"><p class="m-font-family_sans m-line-height_21 m-font-size_14 m-text-size-adjust_100p m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-color_text m-color_white m-padding-left-24@sm">Proudly built by Allegro Tech engineers</p><ul class="m-font-family_sans m-font-size_14 m-line-height_21 m-color_text m-text-size-adjust_100p m-list-style-type_none m-margin-top_0 m-margin-right_0 m-margin-bottom_0 m-margin-left_0 m-padding-top_0 m-padding-right_0 m-padding-bottom_0 m-padding-left_0 m-display-flex m-flex-justify-center"><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://github.com/allegro"><img src="https://assets.allegrostatic.com/metrum/icon/github-6a18df1729.svg" alt="Github" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://www.facebook.com/allegro.tech/"><img src="https://assets.allegrostatic.com/metrum/icon/facebook-a2b92f9dcb.svg" alt="Facebook" class="m-icon"/></a></li><li style="filter:brightness(0) invert(1);opacity:0.5" class="m-margin-right-8 m-margin-top-16 m-margin-top-0@sm m-color_white"><a class="m-font-size_14 m-font-family_sans m-color_text m-text-decoration_none m-text-size-adjust_100p m-cursor_pointer m-link m-line-height_21" href="https://twitter.com/allegrotech"><img src="https://assets.allegrostatic.com/metrum/icon/twitter-25164a58aa.svg" alt="Twitter" class="m-icon"/></a></li></ul></div></footer><div style="visibility:hidden;height:0;overflow:hidden;position:relative"><img alt="doubleclick" width="1" height="1" style="position:absolute" src="https://pubads.g.doubleclick.net/activity;dc_iu=/21612525419/DFPAudiencePixel;ord=9508333052805.76;dc_seg=507368552?"/><img alt="fb" height="1" width="1" style="position:absolute" src="https://www.facebook.com/tr?id=1650870088530325&amp;ev=PageView&amp;noscript=1"/></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"title":"Online MongoDB migration","link":"https://blog.allegro.tech/2023/09/online-mongodb-migration.html","pubDate":"Thu, 14 Sep 2023 00:00:00 +0200","authors":{"author":[{"name":["Szymon Marcinkiewicz"],"photo":["https://blog.allegro.tech/img/authors/szymon.marcinkiewicz.jpg"],"url":["https://blog.allegro.tech/authors/szymon.marcinkiewicz"]}]},"content":"\u003cp\u003eMongoDB is the most popular database used at \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e. We have hundreds of MongoDB databases running on our on—premise servers.\nIn 2022 we decided that we need to migrate all our MongoDB databases\nfrom existing shared clusters to new MongoDB clusters hosted on Kubernetes pods with separated resources.\nTo perform the migration of all databases we needed a tool for transfering all the data and keeping consistency between old and new databases.\nThat’s how \u003cem\u003emongo-migration-stream\u003c/em\u003e project was born.\u003c/p\u003e\n\n\u003ch2 id=\"why-do-we-needed-to-migrate-mongodb-databases-at-all\"\u003eWhy do we needed to migrate MongoDB databases at all?\u003c/h2\u003e\n\n\u003cp\u003eAt Allegro we are managing tens of MongoDB clusters, with hundreds of MongoDB databases running on them.\nThis kind of approach, where one MongoDB cluster runs multiple MongoDB databases, allowed us to utilize resources\nmore effectively, while at the same time easing maintenance of clusters.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/one_cluster_multiple_databases.png\" alt=\"Old approach\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWe’ve been living with this approach for years, but over time, more and more databases were\ncreated on shared clusters, increasing the frequency of the noisy neighbour problem.\u003c/p\u003e\n\n\u003ch3 id=\"noisy-neighbour-problem\"\u003eNoisy neighbour problem\u003c/h3\u003e\n\n\u003cp\u003eGenerally speaking, a noisy neighbour situation appears while multiple applications run on shared infrastructure,\nand one of those applications starts to consume so many resources (like CPU, RAM or Storage),\nthat it causes starvation of other applications.\u003c/p\u003e\n\n\u003cp\u003eAt Allegro this problem started to be visible because over the years we’ve created more and more new MongoDB databases\nwhich were hosted on a fixed number of clusters.\u003c/p\u003e\n\n\u003cp\u003eThe most common cause of the noisy neighbour problem in the Allegro infrastructure was long time high CPU usage caused by one of MongoDB databases on a given cluster.\nOn various occasions it occurred that a non-optimal query performed on a large collection was consuming too much CPU,\nnegatively affecting all the other databases on that cluster, making them slower or completely unresponsive.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/cluster_cpu.png\" alt=\"Cluster CPU usage\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"mongodb-on-kubernetes-as-a-solution-to-the-noisy-neighbour-problem\"\u003eMongoDB on Kubernetes as a solution to the noisy neighbour problem\u003c/h3\u003e\n\n\u003cp\u003eTo solve the noisy neighbour problem a separate team implemented a solution allowing Allegro engineers to create independent MongoDB clusters on Kubernetes.\nFrom now on, each MongoDB cluster is formed of multiple replicas and an arbiter spread among datacenters, serving only a single MongoDB database.\nRunning each database on a separate cluster with isolated resources managed by Kubernetes was our solution to the noisy neighbour problem.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/k8s_cpu.png\" alt=\"Kubernetes CPU usage\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAt this point we knew what we needed to do to solve our problems — we had to migrate all MongoDB databases from old shared clusters\nto new independent clusters on Kubernetes. Now came the time to answer the question: \u003cem\u003eHow should we do it?\u003c/em\u003e\u003c/p\u003e\n\n\u003ch2 id=\"available-options\"\u003eAvailable options\u003c/h2\u003e\n\n\u003cp\u003eFirstly, we prepared a list of requirements which a tool for migrating databases (referred to as \u003cem\u003emigrator\u003c/em\u003e) had to meet in order to perform\nsuccessful migrations.\u003c/p\u003e\n\n\u003ch3 id=\"requirements\"\u003eRequirements\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003eMigrator must be able to migrate databases from older MongoDB versions to newer ones,\u003c/li\u003e\n  \u003cli\u003eMigrator must be able to migrate \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eReplicaSets\u003c/code\u003e and sharded clusters,\u003c/li\u003e\n  \u003cli\u003eMigrator must copy indexes from source database to destination database,\u003c/li\u003e\n  \u003cli\u003eMigrator must be able to handle more than 10k write operations per second,\u003c/li\u003e\n  \u003cli\u003eMigration must be performed without any downtime,\u003c/li\u003e\n  \u003cli\u003eMigration cannot affect database clients,\u003c/li\u003e\n  \u003cli\u003eDatabase owners (software engineers) need to be able to perform migrations on their own.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"existing-solutions\"\u003eExisting solutions\u003c/h3\u003e\n\n\u003cp\u003eHaving defined a list of requirements, we checked what tools were available on the market at the time.\u003c/p\u003e\n\n\u003ch4 id=\"py-mongo-sync\"\u003e\u003ca href=\"https://github.com/caosiyang/py-mongo-sync\"\u003epy-mongo-sync\u003c/a\u003e\u003c/h4\u003e\n\n\u003cp\u003eAccording to documentation \u003cem\u003epy-mongo-sync\u003c/em\u003e is:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e“Oplog—based data sync tool that synchronizes data from a replica set to another deployment,\ne.g.: standalone, replica set, and sharded cluster.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eAs you can see, \u003cem\u003epy-mongo-sync\u003c/em\u003e is not a tool that would suit our needs from end to end.\n\u003cem\u003epy-mongo-sync\u003c/em\u003e focuses on the synchronization of the data stored on the \u003cem\u003esource database\u003c/em\u003e after starting the tool.\nIt doesn’t copy already existing data from the \u003cem\u003esource\u003c/em\u003e to the \u003cem\u003edestination database\u003c/em\u003e.\nWhat’s more, at the time \u003cem\u003epy-mongo-sync\u003c/em\u003e supported MongoDB versions between 2.4 to 3.4, which were older than those used at Allegro.\u003c/p\u003e\n\n\u003ch4 id=\"mongodb-cluster-to-cluster-sync\"\u003e\u003ca href=\"https://www.mongodb.com/docs/cluster-to-cluster-sync/current/\"\u003eMongoDB Cluster-to-Cluster Sync\u003c/a\u003e\u003c/h4\u003e\n\n\u003cp\u003eOn July 22, 2022 MongoDB released \u003cem\u003emongosync\u003c/em\u003e v1.0 — a tool for migrating and synchronizing data between MongoDB clusters.\nAs described in the \u003cem\u003emongosync\u003c/em\u003e documentation:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e“The mongosync binary is the primary process used in Cluster—to—Cluster Sync. mongosync migrates data from one cluster\nto another and can keep the clusters in continuous sync.”\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis description sounded like a perfect fit for us! Unfortunately, after initial excitement\n(and hours spent on reading \u003ca href=\"https://www.mongodb.com/docs/cluster-to-cluster-sync/current/reference/mongosync/\"\u003e\u003cem\u003emongosync\u003c/em\u003e documentation\u003c/a\u003e)\nwe realized we couldn’t use \u003cem\u003emongosync\u003c/em\u003e as it was able to perform migration and synchronization process only if source database and destination database\nwere both in the exact same version.\nIt meant that there was no option to migrate databases from older MongoDB versions to the newest one, which was a no-go for us.\u003c/p\u003e\n\n\u003cp\u003eWhen we realised that there wasn’t a tool which met all our requirements, we made a tough decision to implement our own online MongoDB migration tool\nnamed \u003cem\u003emongo-migration-stream\u003c/em\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"mongo-migration-stream\"\u003emongo-migration-stream\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/allegro/mongo-migration-stream\"\u003e\u003cem\u003emongo-migration-stream\u003c/em\u003e\u003c/a\u003e is an open source tool from Allegro, that performs online migrations of MongoDB databases.\nIt’s a \u003ca href=\"https://kotlinlang.org/\"\u003eKotlin\u003c/a\u003e application utilising\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e \u003ca href=\"https://www.mongodb.com/docs/database-tools/\"\u003eMongoDB Command Line Database Tools\u003c/a\u003e\nalong with \u003ca href=\"https://www.mongodb.com/docs/manual/changeStreams/\"\u003eMongo Change Streams\u003c/a\u003e mechanism.\nIn this section I will explain how \u003cem\u003emongo-migration-stream\u003c/em\u003e works under the hood, by covering its functionalities from a high—level overview and\nproviding details about its low—level implementation.\u003c/p\u003e\n\n\u003ch3 id=\"mongo-migration-stream-terminology\"\u003emongo-migration-stream terminology\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cem\u003eSource database\u003c/em\u003e - MongoDB database which is a data source for migration,\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eDestination database\u003c/em\u003e - MongoDB database which is a target for the data from \u003cem\u003esource database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eTransfer\u003c/em\u003e - a process of dumping data from \u003cem\u003esource database\u003c/em\u003e, and restoring it on \u003cem\u003edestination database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eSynchronization\u003c/em\u003e - a process of keeping eventual consistency between \u003cem\u003esource database\u003c/em\u003e and \u003cem\u003edestination database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eMigration\u003c/em\u003e - an end-to-end migration process combining both \u003cem\u003etransfer\u003c/em\u003e and \u003cem\u003esynchronization\u003c/em\u003e processes,\u003c/li\u003e\n  \u003cli\u003e\u003cem\u003eMigrator\u003c/em\u003e - a tool for performing \u003cem\u003emigrations\u003c/em\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"building-blocks\"\u003eBuilding blocks\u003c/h3\u003e\n\n\u003cp\u003eAs I’ve mentioned at the beginning of this section, \u003cem\u003emongo-migration-stream\u003c/em\u003e utilises \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e, Mongo Change Streams\nand a custom Kotlin application to perform migrations.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e is used to dump \u003cem\u003esource database\u003c/em\u003e in form of a binary file,\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e is used to restore previously created dump on \u003cem\u003edestination database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003eMongo Change Streams are used to keep eventual consistency between \u003cem\u003esource database\u003c/em\u003e and \u003cem\u003edestination database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003eKotlin application orchestrates, manages, and monitors all above processes.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e are responsible for the \u003cem\u003etransfer\u003c/em\u003e part of migration,\nwhile Mongo Change Streams play the main role in the \u003cem\u003esynchronization\u003c/em\u003e process.\u003c/p\u003e\n\n\u003ch3 id=\"birds-eye-view\"\u003eBird’s eye view\u003c/h3\u003e\n\n\u003cp\u003eTo implement a \u003cem\u003emigrator\u003c/em\u003e, we needed a robust procedure for \u003cem\u003emigrations\u003c/em\u003e which ensures that no data is lost during a \u003cem\u003emigration\u003c/em\u003e.\nWe have formulated a procedure consisting of six consecutive steps:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eStart listening for Mongo Change Events on \u003cem\u003esource database\u003c/em\u003e and save them in the queue,\u003c/li\u003e\n  \u003cli\u003eDump all the data from \u003cem\u003esource database\u003c/em\u003e using \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e,\u003c/li\u003e\n  \u003cli\u003eRestore all the data on \u003cem\u003edestination database\u003c/em\u003e using \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e,\u003c/li\u003e\n  \u003cli\u003eCopy indexes definitions from \u003cem\u003esource database\u003c/em\u003e and start creating them on \u003cem\u003edestination database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003eStart to push all the events stored in the queue (changes on \u003cem\u003esource database\u003c/em\u003e) to the \u003cem\u003edestination database\u003c/em\u003e,\u003c/li\u003e\n  \u003cli\u003eWait for the queue to empty to establish eventual consistency.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/migration_process.png\" alt=\"Migration process\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eOur migration procedure works flawlessly because processing Mongo Change Events in a sequence guarantees migration idempotency.\nWithout this characteristic, we would have to change the order of steps 1 and 2 in the procedure, creating a possibility of losing data during migration.\u003c/p\u003e\n\n\u003cp\u003eTo explain this problem in more detail, let’s assume that the \u003cem\u003esource database\u003c/em\u003e deals with a continuous high volume of writes.\nIf we had started the migration by performing dump in the first place and then started to listen for events,\nwe would have lost the events stored on the \u003cem\u003esource database\u003c/em\u003e in the meantime.\nHowever, as we start the migration by listening for events on the \u003cem\u003esource database\u003c/em\u003e, and then proceeding with the dump,\nwe do not lose any of the events stored on the \u003cem\u003esource database\u003c/em\u003e during that time.\u003c/p\u003e\n\n\u003cp\u003eThe diagram below presents how such a kind of \u003cem\u003ewrite anomaly\u003c/em\u003e could happen if we started dumping data before listening for Mongo Change Events.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/avoiding_event_loss.png\" alt=\"Avoiding event loss\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"implementation-details\"\u003eImplementation details\u003c/h3\u003e\n\n\u003ch4 id=\"concurrency\"\u003eConcurrency\u003c/h4\u003e\n\n\u003cp\u003eFrom the beginning we wanted to make \u003cem\u003emongo-migration-stream\u003c/em\u003e fast — we knew that it would need to cope with databases having more than 10k writes per second.\nAs a result \u003cem\u003emongo-migration-stream\u003c/em\u003e parallelizes migration of one MongoDB database into independent migrations of collections.\nEach database migration consists of multiple little \u003cem\u003emigrators\u003c/em\u003e — one \u003cem\u003emigrator\u003c/em\u003e per collection in the database.\u003c/p\u003e\n\n\u003cp\u003eThe \u003cem\u003etransfer\u003c/em\u003e process is performed in parallel for each collection, in separate \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e processes.\n\u003cem\u003eSynchronization\u003c/em\u003e process was also implemented concurrently — at the beginning of migration, each collection on \u003cem\u003esource database\u003c/em\u003e is watched individually\nusing \u003ca href=\"https://www.mongodb.com/docs/manual/changeStreams/#watch-a-collection--database--or-deployment\"\u003eMongo Change Streams with collection target\u003c/a\u003e.\nAll collections have their own separate queues in which Mongo Change Events are stored.\nAt the final phase of migration, each of these queues is processed independently.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/concurrent_migrations.png\" alt=\"Concurrent migrations\" /\u003e\u003c/p\u003e\n\n\u003ch4 id=\"initial-data-transfer\"\u003eInitial data transfer\u003c/h4\u003e\n\n\u003cp\u003eTo perform transfer of the database, we’re executing \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e commands for each collection.\nFor that reason, machines on which \u003cem\u003emongo-migration-stream\u003c/em\u003e is running are required to have MongoDB Command Line Database Tools installed.\u003c/p\u003e\n\n\u003cp\u003eDumping data from collection \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecollectionName\u003c/code\u003e in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esource\u003c/code\u003e database can be achieved by running a command:\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003emongodump \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--uri\u003c/span\u003e \u003cspan class=\"s2\"\u003e\"mongodb://mongo_rs36_1:36301,mongo_rs36_2:36302,mongo_rs36_3:36303/?replicaSet=replicaSet36\"\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--db\u003c/span\u003e \u003cspan class=\"nb\"\u003esource\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--collection\u003c/span\u003e collectionName \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--out\u003c/span\u003e /home/user/mongomigrationstream/dumps \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--readPreference\u003c/span\u003e secondary\n \u003cspan class=\"nt\"\u003e--username\u003c/span\u003e username \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--config\u003c/span\u003e /home/user/mongomigrationstream/password_config/dump.config \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n \u003cspan class=\"nt\"\u003e--authenticationDatabase\u003c/span\u003e admin\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eStarting a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e process from Kotlin code is done with Java’s \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eProcessBuilder\u003c/code\u003e feature.\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eProcessBuilder\u003c/code\u003e requires us to provide a process program and arguments in the form of a list of Strings.\nWe construct this list using \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eprepareCommand\u003c/code\u003e function:\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003eprepareCommand\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eList\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003elistOf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003emongoToolsPath\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e\"mongodump\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"--uri\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edbProperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003euri\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"--db\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edbCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edbName\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"--collection\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edbCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecollectionName\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"--out\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edumpPath\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"s\"\u003e\"--readPreference\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ereadPreference\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"nf\"\u003ecredentialsIfNotNull\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edbProperties\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eauthenticationProperties\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003epasswordConfigPath\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHaving \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eProcessBuilder\u003c/code\u003e with properly configured list of process program and arguments, we’re ready to start a new process\nusing the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003estart()\u003c/code\u003e function.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003erunCommand\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecommand\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eCommand\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eCommandResult\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eprocessBuilder\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eProcessBuilder\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003ecommand\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ecommand\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eprepareCommand\u003c/span\u003e\u003cspan class=\"p\"\u003e())\u003c/span\u003e \u003cspan class=\"c1\"\u003e// Configure ProcessBuilder with mongodump command in form of List\u0026lt;String\u0026gt;\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ecurrentProcess\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eprocessBuilder\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003estart\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"c1\"\u003e// Start a new process\u003c/span\u003e\n    \u003cspan class=\"c1\"\u003e// ...\u003c/span\u003e\n    \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003eexitCode\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ecurrentProcess\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003ewaitFor\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"nf\"\u003estopRunningCommand\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nc\"\u003eCommandResult\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eexitCode\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAn analogous approach is implemented in \u003cem\u003emongo-migration-stream\u003c/em\u003e to execute the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e command.\u003c/p\u003e\n\n\u003ch4 id=\"event-queue\"\u003eEvent queue\u003c/h4\u003e\n\n\u003cp\u003eDuring the process of migration \u003cem\u003esource database\u003c/em\u003e can constantly receive changes, which \u003cem\u003emongo-migration-stream\u003c/em\u003e is listening to with Mongo Change Streams.\nEvents from the stream are saved in the queue for sending to the \u003cem\u003edestination database\u003c/em\u003e at a later time.\nCurrently \u003cem\u003emongo-migration-stream\u003c/em\u003e provides two implementations of the queue,\nwhere one implementation stores the data in RAM, while the other one persists the data to disk.\u003c/p\u003e\n\n\u003cp\u003eIn-memory implementation can be used for databases with low traffic, or for testing purposes,\nor on machines with a sufficient amount of RAM (as events are stored as objects on the JVM heap).\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e// In-memory queue implementation\u003c/span\u003e\n\u003cspan class=\"k\"\u003einternal\u003c/span\u003e \u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eInMemoryEventQueue\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eEventQueue\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003equeue\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eConcurrentLinkedQueue\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;()\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003eoffer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eelement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eBoolean\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eoffer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eelement\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003epoll\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epoll\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003epeek\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epeek\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eInt\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003eremoveAll\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eremoveAll\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"k\"\u003etrue\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIn our production setup we decided to use a persistent event queue, which is implemented on top of \u003ca href=\"https://github.com/bulldog2011/bigqueue\"\u003eBigQueue project\u003c/a\u003e.\nAs BigQueue only allows enqueuing and dequeuing byte arrays, we had to implement serialization and deserialization of the data from the events.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e// Persistent queue implementation\u003c/span\u003e\n\u003cspan class=\"k\"\u003einternal\u003c/span\u003e \u003cspan class=\"kd\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eBigQueueEventQueue\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eE\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eSerializable\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;(\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003equeueName\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eString\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eEventQueue\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kd\"\u003eval\u003c/span\u003e \u003cspan class=\"py\"\u003equeue\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eBigQueueImpl\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003equeueName\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003eoffer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eelement\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eBoolean\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eenqueue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eelement\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoByteArray\u003c/span\u003e\u003cspan class=\"p\"\u003e()).\u003c/span\u003e\u003cspan class=\"nf\"\u003elet\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"k\"\u003etrue\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003epoll\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edequeue\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003etoE\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003epeek\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003epeek\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003etoE\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eInt\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003etoInt\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eoverride\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003eremoveAll\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eremoveAll\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"n\"\u003equeue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egc\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoByteArray\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eByteArray\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eSerializationUtils\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eserialize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nc\"\u003eByteArray\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoE\u003c/span\u003e\u003cspan class=\"p\"\u003e():\u003c/span\u003e \u003cspan class=\"nc\"\u003eE\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nc\"\u003eSerializationUtils\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003edeserialize\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003ethis\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch4 id=\"migrating-indexes\"\u003eMigrating indexes\u003c/h4\u003e\n\n\u003cp\u003eIn early versions of \u003cem\u003emongo-migration-stream\u003c/em\u003e, to copy indexes from \u003cem\u003esource collection\u003c/em\u003e to \u003cem\u003edestination collection\u003c/em\u003e, we used\nan \u003ca href=\"https://www.mongodb.com/docs/database-tools/mongorestore/#rebuild-indexes\"\u003eindex rebuilding feature\u003c/a\u003e from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e tools.\nThis feature works on the principle that the result of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e consists of both documents from the collection and definitions of indexes.\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e can use those definitions to rebuild indexes on \u003cem\u003edestination collection\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eUnfortunately it occurred that rebuilding indexes on \u003cem\u003edestination collection\u003c/em\u003e after \u003cem\u003etransfer\u003c/em\u003e phase (before starting \u003cem\u003esynchronization\u003c/em\u003e process)\nwith the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e tool lengthened the entire \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e process, preventing us from emptying the queue in the meantime.\nIt resulted in a growing queue of events to synchronize, ending up with overall longer migration times and higher resources utilisation.\nWe’ve come to the conclusion, that we must rebuild indexes, while at the same time, keep sending events from the queue to \u003cem\u003edestination collection\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eTo migrate indexes without blocking \u003cem\u003emigration\u003c/em\u003e process, we implemented a solution which for each collection,\nfetches all its indexes, and rebuilds them on \u003cem\u003edestination collection\u003c/em\u003e.\nLooking from the application perspective, we use \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetRawSourceIndexes\u003c/code\u003e function to fetch a list of Documents\n(representing indexes definitions) from \u003cem\u003esource collection\u003c/em\u003e,\nand then recreate them on \u003cem\u003edestination collection\u003c/em\u003e using \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecreateIndexOnDestinationCollection\u003c/code\u003e.\u003c/p\u003e\n\n\u003cdiv class=\"language-kotlin highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003egetRawSourceIndexes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esourceToDestination\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eSourceToDestination\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e \u003cspan class=\"nc\"\u003eList\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"nc\"\u003eDocument\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esourceDb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003egetCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esourceToDestination\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esource\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecollectionName\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"nf\"\u003elistIndexes\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003etoList\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003efilterNot\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"k\"\u003eget\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"key\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nc\"\u003eDocument\u003c/span\u003e\u003cspan class=\"o\"\u003e::\u003c/span\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejava\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e==\u003c/span\u003e \u003cspan class=\"nc\"\u003eDocument\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"_id\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003emap\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eremove\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"ns\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eremove\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"v\"\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eit\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e\"background\"\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003etrue\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eit\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"k\"\u003efun\u003c/span\u003e \u003cspan class=\"nf\"\u003ecreateIndexOnDestinationCollection\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esourceToDestination\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eSourceToDestination\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eindexDefinition\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"nc\"\u003eDocument\u003c/span\u003e\n\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"n\"\u003edestinationDb\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003erunCommand\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n        \u003cspan class=\"nc\"\u003eDocument\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"nf\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"createIndexes\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003esourceToDestination\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edestination\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecollectionName\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n            \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\"indexes\"\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"nf\"\u003elistOf\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eindexDefinition\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eOur solution can rebuild indexes in both older and newer versions of MongoDB.\nTo support older MongoDB versions we specify \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e{ background: true }\u003c/code\u003e option, which does not block all operations on a given database during index creation.\nIn case where \u003cem\u003edestination database\u003c/em\u003e is newer than or equal to MongoDB 4.2, the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e{ background: true }\u003c/code\u003e option is ignored, and\n\u003ca href=\"https://www.mongodb.com/docs/manual/core/index-creation/#comparison-to-foreground-and-background-builds\"\u003eoptimized index build is used\u003c/a\u003e.\nIn both scenarios rebuilding indexes does not block \u003cem\u003esynchronization\u003c/em\u003e process, improving overall \u003cem\u003emigration\u003c/em\u003e times.\u003c/p\u003e\n\n\u003ch4 id=\"verification-of-migration-state\"\u003eVerification of migration state\u003c/h4\u003e\n\n\u003cp\u003eThrought \u003cem\u003emongo-migration-stream\u003c/em\u003e implementation we kept in mind that \u003cem\u003emigrator\u003c/em\u003e user should be aware what’s happening within his/her migration.\nFor that purpose \u003cem\u003emongo-migration-stream\u003c/em\u003e exposes data about migration in multiple different ways:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eLogs — \u003cem\u003emigrator\u003c/em\u003e logs all important information, so user can verify what’s going on,\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003ePeriodical checks — when all migrated collections are in \u003cem\u003esynchronization\u003c/em\u003e process, \u003cem\u003emigrator\u003c/em\u003e starts a periodical check for each collection, verifying if all the data has been migrated, making collection on \u003cem\u003edestination database\u003c/em\u003e ready to use,\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eMetrics — various metrics about migration state are exposed through \u003ca href=\"https://micrometer.io/\"\u003eMicrometer\u003c/a\u003e.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOn top of all that, each migrator internal state change emits an event to in—memory event bus. There are multiple types of events which \u003cem\u003emongo-migration-stream\u003c/em\u003e produces:\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth\u003eEvent type\u003c/th\u003e\n      \u003cth\u003eWhen the event is emitted\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eStartEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStart of the migration\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSourceToLocalStartEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStart watching for a collection specific Mongo Change Stream\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDumpStartEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStart \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e for a collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDumpUpdateEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eEach \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e print to stdout\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDumpFinishEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eFinish \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongodump\u003c/code\u003e for a collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eRestoreStartEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStart \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e for a collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eRestoreUpdateEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eEach \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e print to stdout\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eRestoreFinishEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eFinish \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongorestore\u003c/code\u003e for a collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eIndexRebuildStartEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStart rebuilding indexes for a collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eIndexRebuildFinishEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eFinish rebuilding indexes for a collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eLocalToDestinationStartEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStart sending events from queue to destination collection\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eStopEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eStop of the migration\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ePauseEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003ePause of the migration\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eResumeEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eResume of paused migration\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFailedEvent\u003c/code\u003e\u003c/td\u003e\n      \u003ctd\u003eFail of collection migration\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch4 id=\"application-modules\"\u003eApplication modules\u003c/h4\u003e\n\n\u003cp\u003e\u003cem\u003emongo-migration-stream\u003c/em\u003e code was split into two separate modules:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongo-migration-stream-core\u003c/code\u003e module which can be used as a library in JVM application,\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emongo-migration-stream-cli\u003c/code\u003e module which can be run as a standalone JAR.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"mongo-migration-stream-in-production-at-allegro\"\u003emongo-migration-stream in production at Allegro\u003c/h2\u003e\n\n\u003cp\u003eSince internal launch in January 2023, we have migrated more than a hundred production databases using \u003cem\u003emongo-migration-stream\u003c/em\u003e.\nThe largest migrated collection stored more than one and a half billion documents.\nAt its peak moments, \u003cem\u003emigrator\u003c/em\u003e was synchronizing a collection which emitted about 4 thousand Mongo Change Events per second.\nDuring one of our migrations, one collection queue size reached almost one hundred million events.\nAll of those events were later successfully synchronized into \u003cem\u003edestination collection\u003c/em\u003e.\u003c/p\u003e\n\n\u003cp\u003eAt Allegro we use \u003cem\u003emongo-migration-stream\u003c/em\u003e as a library in a Web application with graphical user interface.\nThis approach allows Allegro engineers to manage database migrations on their own, without involving database team members.\nOn the screenshot below you can see our Web application GUI during a migration.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-09-14-online-mongodb-migration/mongo_migration_stream_ui.png\" alt=\"mongo-migration-stream Web Application at Allegro\" /\u003e\u003c/p\u003e\n","contentSnippet":"MongoDB is the most popular database used at Allegro. We have hundreds of MongoDB databases running on our on—premise servers.\nIn 2022 we decided that we need to migrate all our MongoDB databases\nfrom existing shared clusters to new MongoDB clusters hosted on Kubernetes pods with separated resources.\nTo perform the migration of all databases we needed a tool for transfering all the data and keeping consistency between old and new databases.\nThat’s how mongo-migration-stream project was born.\nWhy do we needed to migrate MongoDB databases at all?\nAt Allegro we are managing tens of MongoDB clusters, with hundreds of MongoDB databases running on them.\nThis kind of approach, where one MongoDB cluster runs multiple MongoDB databases, allowed us to utilize resources\nmore effectively, while at the same time easing maintenance of clusters.\n\nWe’ve been living with this approach for years, but over time, more and more databases were\ncreated on shared clusters, increasing the frequency of the noisy neighbour problem.\nNoisy neighbour problem\nGenerally speaking, a noisy neighbour situation appears while multiple applications run on shared infrastructure,\nand one of those applications starts to consume so many resources (like CPU, RAM or Storage),\nthat it causes starvation of other applications.\nAt Allegro this problem started to be visible because over the years we’ve created more and more new MongoDB databases\nwhich were hosted on a fixed number of clusters.\nThe most common cause of the noisy neighbour problem in the Allegro infrastructure was long time high CPU usage caused by one of MongoDB databases on a given cluster.\nOn various occasions it occurred that a non-optimal query performed on a large collection was consuming too much CPU,\nnegatively affecting all the other databases on that cluster, making them slower or completely unresponsive.\n\nMongoDB on Kubernetes as a solution to the noisy neighbour problem\nTo solve the noisy neighbour problem a separate team implemented a solution allowing Allegro engineers to create independent MongoDB clusters on Kubernetes.\nFrom now on, each MongoDB cluster is formed of multiple replicas and an arbiter spread among datacenters, serving only a single MongoDB database.\nRunning each database on a separate cluster with isolated resources managed by Kubernetes was our solution to the noisy neighbour problem.\n\nAt this point we knew what we needed to do to solve our problems — we had to migrate all MongoDB databases from old shared clusters\nto new independent clusters on Kubernetes. Now came the time to answer the question: How should we do it?\nAvailable options\nFirstly, we prepared a list of requirements which a tool for migrating databases (referred to as migrator) had to meet in order to perform\nsuccessful migrations.\nRequirements\nMigrator must be able to migrate databases from older MongoDB versions to newer ones,\nMigrator must be able to migrate ReplicaSets and sharded clusters,\nMigrator must copy indexes from source database to destination database,\nMigrator must be able to handle more than 10k write operations per second,\nMigration must be performed without any downtime,\nMigration cannot affect database clients,\nDatabase owners (software engineers) need to be able to perform migrations on their own.\nExisting solutions\nHaving defined a list of requirements, we checked what tools were available on the market at the time.\npy-mongo-sync\nAccording to documentation py-mongo-sync is:\n“Oplog—based data sync tool that synchronizes data from a replica set to another deployment,\ne.g.: standalone, replica set, and sharded cluster.”\nAs you can see, py-mongo-sync is not a tool that would suit our needs from end to end.\npy-mongo-sync focuses on the synchronization of the data stored on the source database after starting the tool.\nIt doesn’t copy already existing data from the source to the destination database.\nWhat’s more, at the time py-mongo-sync supported MongoDB versions between 2.4 to 3.4, which were older than those used at Allegro.\nMongoDB Cluster-to-Cluster Sync\nOn July 22, 2022 MongoDB released mongosync v1.0 — a tool for migrating and synchronizing data between MongoDB clusters.\nAs described in the mongosync documentation:\n“The mongosync binary is the primary process used in Cluster—to—Cluster Sync. mongosync migrates data from one cluster\nto another and can keep the clusters in continuous sync.”\nThis description sounded like a perfect fit for us! Unfortunately, after initial excitement\n(and hours spent on reading mongosync documentation)\nwe realized we couldn’t use mongosync as it was able to perform migration and synchronization process only if source database and destination database\nwere both in the exact same version.\nIt meant that there was no option to migrate databases from older MongoDB versions to the newest one, which was a no-go for us.\nWhen we realised that there wasn’t a tool which met all our requirements, we made a tough decision to implement our own online MongoDB migration tool\nnamed mongo-migration-stream.\nmongo-migration-stream\nmongo-migration-stream is an open source tool from Allegro, that performs online migrations of MongoDB databases.\nIt’s a Kotlin application utilising\nmongodump and mongorestore MongoDB Command Line Database Tools\nalong with Mongo Change Streams mechanism.\nIn this section I will explain how mongo-migration-stream works under the hood, by covering its functionalities from a high—level overview and\nproviding details about its low—level implementation.\nmongo-migration-stream terminology\nSource database - MongoDB database which is a data source for migration,\nDestination database - MongoDB database which is a target for the data from source database,\nTransfer - a process of dumping data from source database, and restoring it on destination database,\nSynchronization - a process of keeping eventual consistency between source database and destination database,\nMigration - an end-to-end migration process combining both transfer and synchronization processes,\nMigrator - a tool for performing migrations.\nBuilding blocks\nAs I’ve mentioned at the beginning of this section, mongo-migration-stream utilises mongodump, mongorestore, Mongo Change Streams\nand a custom Kotlin application to perform migrations.\nmongodump is used to dump source database in form of a binary file,\nmongorestore is used to restore previously created dump on destination database,\nMongo Change Streams are used to keep eventual consistency between source database and destination database,\nKotlin application orchestrates, manages, and monitors all above processes.\nmongodump and mongorestore are responsible for the transfer part of migration,\nwhile Mongo Change Streams play the main role in the synchronization process.\nBird’s eye view\nTo implement a migrator, we needed a robust procedure for migrations which ensures that no data is lost during a migration.\nWe have formulated a procedure consisting of six consecutive steps:\nStart listening for Mongo Change Events on source database and save them in the queue,\nDump all the data from source database using mongodump,\nRestore all the data on destination database using mongorestore,\nCopy indexes definitions from source database and start creating them on destination database,\nStart to push all the events stored in the queue (changes on source database) to the destination database,\nWait for the queue to empty to establish eventual consistency.\n\nOur migration procedure works flawlessly because processing Mongo Change Events in a sequence guarantees migration idempotency.\nWithout this characteristic, we would have to change the order of steps 1 and 2 in the procedure, creating a possibility of losing data during migration.\nTo explain this problem in more detail, let’s assume that the source database deals with a continuous high volume of writes.\nIf we had started the migration by performing dump in the first place and then started to listen for events,\nwe would have lost the events stored on the source database in the meantime.\nHowever, as we start the migration by listening for events on the source database, and then proceeding with the dump,\nwe do not lose any of the events stored on the source database during that time.\nThe diagram below presents how such a kind of write anomaly could happen if we started dumping data before listening for Mongo Change Events.\n\nImplementation details\nConcurrency\nFrom the beginning we wanted to make mongo-migration-stream fast — we knew that it would need to cope with databases having more than 10k writes per second.\nAs a result mongo-migration-stream parallelizes migration of one MongoDB database into independent migrations of collections.\nEach database migration consists of multiple little migrators — one migrator per collection in the database.\nThe transfer process is performed in parallel for each collection, in separate mongodump and mongorestore processes.\nSynchronization process was also implemented concurrently — at the beginning of migration, each collection on source database is watched individually\nusing Mongo Change Streams with collection target.\nAll collections have their own separate queues in which Mongo Change Events are stored.\nAt the final phase of migration, each of these queues is processed independently.\n\nInitial data transfer\nTo perform transfer of the database, we’re executing mongodump and mongorestore commands for each collection.\nFor that reason, machines on which mongo-migration-stream is running are required to have MongoDB Command Line Database Tools installed.\nDumping data from collection collectionName in source database can be achieved by running a command:\n\nmongodump \\\n --uri \"mongodb://mongo_rs36_1:36301,mongo_rs36_2:36302,mongo_rs36_3:36303/?replicaSet=replicaSet36\" \\\n --db source \\\n --collection collectionName \\\n --out /home/user/mongomigrationstream/dumps \\\n --readPreference secondary\n --username username \\\n --config /home/user/mongomigrationstream/password_config/dump.config \\\n --authenticationDatabase admin\n\n\nStarting a mongodump process from Kotlin code is done with Java’s ProcessBuilder feature.\nProcessBuilder requires us to provide a process program and arguments in the form of a list of Strings.\nWe construct this list using prepareCommand function:\n\noverride fun prepareCommand(): List\u003cString\u003e = listOf(\n    mongoToolsPath + \"mongodump\",\n    \"--uri\", dbProperties.uri,\n    \"--db\", dbCollection.dbName,\n    \"--collection\", dbCollection.collectionName,\n    \"--out\", dumpPath,\n    \"--readPreference\", readPreference\n) + credentialsIfNotNull(dbProperties.authenticationProperties, passwordConfigPath)\n\n\nHaving ProcessBuilder with properly configured list of process program and arguments, we’re ready to start a new process\nusing the start() function.\n\nfun runCommand(command: Command): CommandResult {\n    val processBuilder = ProcessBuilder().command(command.prepareCommand()) // Configure ProcessBuilder with mongodump command in form of List\u003cString\u003e\n    currentProcess = processBuilder.start() // Start a new process\n    // ...\n    val exitCode = currentProcess.waitFor()\n    stopRunningCommand()\n    return CommandResult(exitCode)\n}\n\n\nAn analogous approach is implemented in mongo-migration-stream to execute the mongorestore command.\nEvent queue\nDuring the process of migration source database can constantly receive changes, which mongo-migration-stream is listening to with Mongo Change Streams.\nEvents from the stream are saved in the queue for sending to the destination database at a later time.\nCurrently mongo-migration-stream provides two implementations of the queue,\nwhere one implementation stores the data in RAM, while the other one persists the data to disk.\nIn-memory implementation can be used for databases with low traffic, or for testing purposes,\nor on machines with a sufficient amount of RAM (as events are stored as objects on the JVM heap).\n\n// In-memory queue implementation\ninternal class InMemoryEventQueue\u003cE\u003e : EventQueue\u003cE\u003e {\n    private val queue = ConcurrentLinkedQueue\u003cE\u003e()\n\n    override fun offer(element: E): Boolean = queue.offer(element)\n    override fun poll(): E = queue.poll()\n    override fun peek(): E = queue.peek()\n    override fun size(): Int = queue.size\n    override fun removeAll() {\n        queue.removeAll { true }\n    }\n}\n\n\nIn our production setup we decided to use a persistent event queue, which is implemented on top of BigQueue project.\nAs BigQueue only allows enqueuing and dequeuing byte arrays, we had to implement serialization and deserialization of the data from the events.\n\n// Persistent queue implementation\ninternal class BigQueueEventQueue\u003cE : Serializable\u003e(path: String, queueName: String) : EventQueue\u003cE\u003e {\n    private val queue = BigQueueImpl(path, queueName)\n\n    override fun offer(element: E): Boolean = queue.enqueue(element.toByteArray()).let { true }\n    override fun poll(): E = queue.dequeue().toE()\n    override fun peek(): E = queue.peek().toE()\n    override fun size(): Int = queue.size().toInt()\n    override fun removeAll() {\n        queue.removeAll()\n        queue.gc()\n    }\n\n    private fun E.toByteArray(): ByteArray = SerializationUtils.serialize(this)\n    private fun ByteArray.toE(): E = SerializationUtils.deserialize(this)\n}\n\n\nMigrating indexes\nIn early versions of mongo-migration-stream, to copy indexes from source collection to destination collection, we used\nan index rebuilding feature from mongodump and mongorestore tools.\nThis feature works on the principle that the result of mongodump consists of both documents from the collection and definitions of indexes.\nmongorestore can use those definitions to rebuild indexes on destination collection.\nUnfortunately it occurred that rebuilding indexes on destination collection after transfer phase (before starting synchronization process)\nwith the mongorestore tool lengthened the entire mongorestore process, preventing us from emptying the queue in the meantime.\nIt resulted in a growing queue of events to synchronize, ending up with overall longer migration times and higher resources utilisation.\nWe’ve come to the conclusion, that we must rebuild indexes, while at the same time, keep sending events from the queue to destination collection.\nTo migrate indexes without blocking migration process, we implemented a solution which for each collection,\nfetches all its indexes, and rebuilds them on destination collection.\nLooking from the application perspective, we use getRawSourceIndexes function to fetch a list of Documents\n(representing indexes definitions) from source collection,\nand then recreate them on destination collection using createIndexOnDestinationCollection.\n\nprivate fun getRawSourceIndexes(sourceToDestination: SourceToDestination): List\u003cDocument\u003e =\n    sourceDb.getCollection(sourceToDestination.source.collectionName).listIndexes()\n        .toList()\n        .filterNot { it.get(\"key\", Document::class.java) == Document().append(\"_id\", 1) }\n        .map {\n            it.remove(\"ns\")\n            it.remove(\"v\")\n            it[\"background\"] = true\n            it\n        }\n\nprivate fun createIndexOnDestinationCollection(\n    sourceToDestination: SourceToDestination,\n    indexDefinition: Document\n) {\n    destinationDb.runCommand(\n        Document().append(\"createIndexes\", sourceToDestination.destination.collectionName)\n            .append(\"indexes\", listOf(indexDefinition))\n    )\n}\n\n\nOur solution can rebuild indexes in both older and newer versions of MongoDB.\nTo support older MongoDB versions we specify { background: true } option, which does not block all operations on a given database during index creation.\nIn case where destination database is newer than or equal to MongoDB 4.2, the { background: true } option is ignored, and\noptimized index build is used.\nIn both scenarios rebuilding indexes does not block synchronization process, improving overall migration times.\nVerification of migration state\nThrought mongo-migration-stream implementation we kept in mind that migrator user should be aware what’s happening within his/her migration.\nFor that purpose mongo-migration-stream exposes data about migration in multiple different ways:\nLogs — migrator logs all important information, so user can verify what’s going on,\nPeriodical checks — when all migrated collections are in synchronization process, migrator starts a periodical check for each collection, verifying if all the data has been migrated, making collection on destination database ready to use,\nMetrics — various metrics about migration state are exposed through Micrometer.\nOn top of all that, each migrator internal state change emits an event to in—memory event bus. There are multiple types of events which mongo-migration-stream produces:\nEvent type\n      When the event is emitted\n    \nStartEvent\n      Start of the migration\n    \nSourceToLocalStartEvent\n      Start watching for a collection specific Mongo Change Stream\n    \nDumpStartEvent\n      Start mongodump for a collection\n    \nDumpUpdateEvent\n      Each mongodump print to stdout\n    \nDumpFinishEvent\n      Finish mongodump for a collection\n    \nRestoreStartEvent\n      Start mongorestore for a collection\n    \nRestoreUpdateEvent\n      Each mongorestore print to stdout\n    \nRestoreFinishEvent\n      Finish mongorestore for a collection\n    \nIndexRebuildStartEvent\n      Start rebuilding indexes for a collection\n    \nIndexRebuildFinishEvent\n      Finish rebuilding indexes for a collection\n    \nLocalToDestinationStartEvent\n      Start sending events from queue to destination collection\n    \nStopEvent\n      Stop of the migration\n    \nPauseEvent\n      Pause of the migration\n    \nResumeEvent\n      Resume of paused migration\n    \nFailedEvent\n      Fail of collection migration\n    \nApplication modules\nmongo-migration-stream code was split into two separate modules:\nmongo-migration-stream-core module which can be used as a library in JVM application,\nmongo-migration-stream-cli module which can be run as a standalone JAR.\nmongo-migration-stream in production at Allegro\nSince internal launch in January 2023, we have migrated more than a hundred production databases using mongo-migration-stream.\nThe largest migrated collection stored more than one and a half billion documents.\nAt its peak moments, migrator was synchronizing a collection which emitted about 4 thousand Mongo Change Events per second.\nDuring one of our migrations, one collection queue size reached almost one hundred million events.\nAll of those events were later successfully synchronized into destination collection.\nAt Allegro we use mongo-migration-stream as a library in a Web application with graphical user interface.\nThis approach allows Allegro engineers to manage database migrations on their own, without involving database team members.\nOn the screenshot below you can see our Web application GUI during a migration.","guid":"https://blog.allegro.tech/2023/09/online-mongodb-migration.html","categories":["tech","mongodb","nosql","kotlin","open source","mongo change streams"],"isoDate":"2023-09-13T22:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"The Acrobatics of Switching Between Management and Engineering","link":"https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html","pubDate":"Tue, 22 Aug 2023 00:00:00 +0200","authors":{"author":[{"name":["Michał Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"\u003cp\u003eAfter six years as a Team Leader, I went back to hands-on engineering work, and I’m very happy about taking\nthis step. While it may appear surprising at first, it was a well-thought-out decision, and actually I’ve already\nperformed such a maneuver once before.\u003c/p\u003e\n\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\n\u003cp\u003eA few years ago I stumbled upon \u003ca href=\"https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/\"\u003eThe Engineer/Manager Pendulum\u003c/a\u003e\nby Charity Majors, and the follow-up post \u003ca href=\"https://charity.wtf/2019/01/04/engineering-management-the-pendulum-or-the-ladder/\"\u003eEngineering Management: The Pendulum Or The Ladder\u003c/a\u003e.\nI found both pieces interesting, and quite in line with my own experiences: in my previous job some time earlier I\nhad been a team leader, but consciously joined \u003ca href=\"https://allegro.tech\"\u003eAllegro\u003c/a\u003e as a software engineer. After a while, I\nbecame a team leader again, and recently, another few years down the line, I went back to engineering yet again.\u003c/p\u003e\n\n\u003cp\u003eBoth above-mentioned posts make very good points, so I recommend you read them first. What I want to add on top of\nthem are my personal experiences and some tips on organizing a transition between a management and an individual\ncontributor role. The journey in the other direction (from developer to team leader) has been discussed in depth\nelsewhere, so I won’t delve into that.\u003c/p\u003e\n\n\u003ch2 id=\"why\"\u003eWhy\u003c/h2\u003e\n\n\u003cp\u003eWhy would I want to switch between developer and team leader roles? The problem is they are both interesting and have\n\u003ca href=\"/2019/06/allegro-culture-tech-leaders-meeting.html\"\u003etheir own highlights\u003c/a\u003e, but you can’t do both at the\nsame time. When you become a manager, not only do you have less time for technical tasks, but you also pretty much\nlose the ability to focus on these tasks even if you do find a time slot. This is because, like it or not, you end\nup with a \u003ca href=\"http://www.paulgraham.com/makersschedule.html\"\u003emanager’s schedule\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-08-22-management-engineering-acrobatics/flying-trapeze-performers.jpg\" alt=\"Circus performers on the flying trapeze, Public Domain image from https://commons.wikimedia.org/wiki/File:Programma_van_Circus_Krone_in_Rotterdam_drie_Alizes_,_vliegende_trapeze_met_o.a.,_Bestanddeelnr_910-4372.jpg\" class=\"small-image\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eOver time, your technical skills start deteriorating, and if you miss the right moment, you may find yourself at\na point of no return. Like a circus artist on a flying trapeze, you have to time your actions right to avoid\ndisaster. I think this metaphor fits the situation better than that of the pendulum, which moves in its own rhythm,\nindependent of external influences.\u003c/p\u003e\n\n\u003cp\u003eObviously, you need to ask yourself whether you actually want to prevent your technical skills from deteriorating. Some\npeople move on to management, wave goodbye to getting their hands dirty, and are completely fine with that. Some tinker\nwith technology in their spare time. As for me, I like technology and would not only prefer to not lose what I have\nlearned so far, but actually want to learn something new. However, due to a number of other hobbies and already\nspending more time in front of the screen than I would like, doing tech in the afternoons was not a viable option for\nme. Hence, the decision to make technology an important part of my job again.\u003c/p\u003e\n\n\u003ch2 id=\"how\"\u003eHow\u003c/h2\u003e\n\n\u003cp\u003eA crucial factor to take into account when planning such a change is that it will take time. If you take your\nteam seriously, you can’t just disappear overnight. You need to think ahead. Do not ask yourself “have I already\nlost my tech skills beyond repair?”. Ask yourself “how will my tech skills be a year from now?”.\u003c/p\u003e\n\n\u003cp\u003eAlso, keep a cool head. Just as when moving in the opposite direction, from engineer to manager, consider all\nconsequences, both positive and negative. It has been repeated many times that becoming a manager is not a promotion\n(being a lateral move to another career path), but in practice sometimes it is. In particular, it may come with a\nhigher level or salary. Make sure you check all details of your target role in order to avoid unpleasant surprises when\ngoing back. In my case, I was aiming for the Principal Software Engineer role, which is the same level as the Team\nLeader role at Allegro, so there were no issues in this regard.\u003c/p\u003e\n\n\u003cp\u003eWhen I started thinking about making the switch, once I had a rough idea of what I wanted, I talked to my superior.\nThis was an important step: it allowed him to plan ahead, and also to look for opportunities for making the\nreorganization easier. Some elements of the process would depend only on our actions, but some, such as finding a good\nreplacement team leader, would also depend on a number of factors outside our control. Knowing that my boss understood\nmy need, and supported it, mattered a lot, and made the wait and preparations easier.\u003c/p\u003e\n\n\u003cp\u003eChance favors the prepared mind, as Louis Pasteur supposedly said. There happened to be a team leader in\nanother part of the company who was thinking about moving on to a different area. Thanks to being aware of my plan,\nmy boss was able to grab the chance, and we had a perfect match. We discussed with the potential new leader the team\nand the project, and he found them interesting. We planned a transition period, as short as possible, but long enough\nfor me to transfer to him a reasonable part of my knowledge about the team and its work.\u003c/p\u003e\n\n\u003cp\u003eNow that we had a specific plan, we could tell the team. It was important to let everyone know as soon as possible, but\nnot before we had a specific plan. Without it, this information would only stir uncertainty. Apart\nfrom telling the team as a whole, I also talked to each person individually, in order to resolve any questions or doubts\nand to try to reduce any problems resulting from the transition as much as possible.\u003c/p\u003e\n\n\u003cp\u003eWaiting for the switch date, we kept meeting online with the new leader, transferring knowledge and preparing him for\nworking with the team. There’s actually quite a lot of stuff a leader needs to know: not only how the project\nworks on technical and business levels, but also current plans, who the stakeholders are and how to work with them,\nand each team member’s individual strengths and development plan. The new leader himself also started meeting\npeople he would now work with, both team members and our product’s stakeholders, and attending team meetings such as\nthe daily stand-up. Despite gradually moving on to other tasks after the switch date, I was still available to clarify\nany doubts, and our boss would also help out when necessary, so the new leader knew he would not be left on his own.\nWhile it required quite a bit of work, the switch went smoothly, and we didn’t notice any serious disturbance to the\nteam’s functioning.\u003c/p\u003e\n\n\u003ch2 id=\"the-aftermath\"\u003eThe Aftermath\u003c/h2\u003e\n\n\u003cp\u003eIt’s been several months since the switch now. Me changing back to a technical role has certainly required extra work,\nfor me, my boss, and the new team leader. Despite our best efforts, it probably put a little extra strain on the team as\nwell. Nonetheless, I think it was a win-win, even more so thanks to us being able to spot and exploit a happy\ncoincidence. I am glad to be closer to technology again, and the new leader also got to try something new, just as he\nwanted.\u003c/p\u003e\n\n\u003cp\u003eThere is one more subtle advantage to the whole process. When people leave the team, some knowledge inevitably gets\nlost. One of the reasons is \u003cem\u003etacit knowledge\u003c/em\u003e: there are always things you know, but are not aware of knowing. You can\nuse this knowledge when it’s needed, but you will probably not transfer it to others because you are not even aware of\nits existence in the first place. Removing someone from the team in a controlled manner as happened here (and being\nstill able to reach out to them if needed) causes such latent knowledge to be discovered, and once discovered, to be\npropagated. This causes a little disruption short-term, but in the long run it reduces\n\u003ca href=\"https://en.wikipedia.org/wiki/Information_silo\"\u003eknowledge silos\u003c/a\u003e and increases the\n\u003ca href=\"https://en.wikipedia.org/wiki/Bus_factor\"\u003ebus factor\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eWhen I first started thinking about going back to hands-on technical work for the second time, I had some doubts about\nhow much my technical skills had already deteriorated and how difficult it would be to go back. It seems I made it, but\nnot by much. Had I delayed by one more year, I might have really struggled. It’s not a matter of knowledge: theory,\nespecially generic things that do not change that fast with technology, is not lost so quickly. Also, while a team\nleader, I tried to stay in touch with technology by taking part in task refinement, architecture discussions, on-call\nrotation, etc.\u003c/p\u003e\n\n\u003cp\u003eHowever, I really felt, and to some degree still feel, a difference in practical, hands-on work, such as actually\nwriting code. There are many small quirks that you need to be aware of in order to accomplish things quickly that you\ndon’t even notice if you use them every day and know inside-out. Knowing all the little useful tools, the less often\nused features of your IDE, or what to do when something breaks unexpectedly, make a world of a difference, but this\npractical knowledge gets lost when not used and I had to rebuild it almost from scratch. Another thing that I still\nexperience is the difficulty in focusing on a single topic. Working on a manager’s schedule for several years has taken\nits toll, and now that I often have large contiguous blocks of time, I find myself not using them as effectively as I\ncould, because I have become accustomed to always doing multiple things at once and without a chance to stay focused\nanyway. It’s gradually getting better, but I still feel the impact, and this is probably my biggest surprise of the\nwhole process.\u003c/p\u003e\n\n\u003ch2 id=\"about-the-principal-software-engineer-role\"\u003eAbout the Principal Software Engineer role\u003c/h2\u003e\n\n\u003cp\u003eMy current role is that of Principal Software Engineer (PSE). It is a relatively new addition to the junior, mid,\nand senior roles we’ve had so far. It has gone through a number of revisions, and is still evolving. Most people in this\nrole come from a Senior Software Engineer background, so my case of getting there after being a Team Leader is a bit\nuntypical. At many companies, roles like this are called Staff Software Engineer or similar. While still an individual\ncontributor role, a PSE differs from a senior in several ways.\u003c/p\u003e\n\n\u003cp\u003eFirst of all, a PSE is expected to spend significant time on topics whose scope is much larger than a single team can\nhandle. Seniors can also do this, but it’s not a requirement for them. Such topics may be area-wide, such as planning a\nmajor change to a single subsystem’s architecture, or have a company-wide scope. Much work on this level consists of\ncoming up with ideas and discussing them while implementation is often left to others. So, while still technical, this\nrole encompasses less coding than that of a senior. Not very surprising given that generally moving up the career ladder\nmeans more coming up with ideas, teaching others, and planning work, while coding less yourself.\u003c/p\u003e\n\n\u003cp\u003eSecondly, a PSE should be very autonomous. Most PSEs are not members of regular development teams since they move from\ntask to task depending on where they can help most. This means you cooperate with more people from different parts of\nthe company, but you don’t have the few peers you work with every day that most people have. You don’t get a backlog\nof tasks to work on, but have to plan your work yourself. People do come to you, asking for support or doing something\nfor their project, but that’s just one of many inputs.\u003c/p\u003e\n\n\u003cp\u003eThirdly, since there are few PSEs compared to other positions, for each person the role is a little different. On one\nhand this means you can’t fully know what to expect when you start. On the other, you get to shape the role yourself,\nand personally I enjoy this flexibility.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\n\u003cp\u003eI think the idea of swinging back and forth between engineering and management described in\n\u003ca href=\"https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/\"\u003eThe Engineer/Manager Pendulum\u003c/a\u003e,\nis valid, and my own experience backs it up fully. One element which I want to additionally stress, however, is that\nthe switch is more like a flying trapeze than a pendulum: timing is crucial, and missing the right moment can have\nserious consequences. Becoming a Principal Software Engineer was a unique experience, both on a technical\nlevel and as my de facto last task as a team leader. Who knows what the future holds? Perhaps some time from now I’ll\ntake another swing on the flying trapeze and go back to management?\u003c/p\u003e\n","contentSnippet":"After six years as a Team Leader, I went back to hands-on engineering work, and I’m very happy about taking\nthis step. While it may appear surprising at first, it was a well-thought-out decision, and actually I’ve already\nperformed such a maneuver once before.\nBackground\nA few years ago I stumbled upon The Engineer/Manager Pendulum\nby Charity Majors, and the follow-up post Engineering Management: The Pendulum Or The Ladder.\nI found both pieces interesting, and quite in line with my own experiences: in my previous job some time earlier I\nhad been a team leader, but consciously joined Allegro as a software engineer. After a while, I\nbecame a team leader again, and recently, another few years down the line, I went back to engineering yet again.\nBoth above-mentioned posts make very good points, so I recommend you read them first. What I want to add on top of\nthem are my personal experiences and some tips on organizing a transition between a management and an individual\ncontributor role. The journey in the other direction (from developer to team leader) has been discussed in depth\nelsewhere, so I won’t delve into that.\nWhy\nWhy would I want to switch between developer and team leader roles? The problem is they are both interesting and have\ntheir own highlights, but you can’t do both at the\nsame time. When you become a manager, not only do you have less time for technical tasks, but you also pretty much\nlose the ability to focus on these tasks even if you do find a time slot. This is because, like it or not, you end\nup with a manager’s schedule.\n\nOver time, your technical skills start deteriorating, and if you miss the right moment, you may find yourself at\na point of no return. Like a circus artist on a flying trapeze, you have to time your actions right to avoid\ndisaster. I think this metaphor fits the situation better than that of the pendulum, which moves in its own rhythm,\nindependent of external influences.\nObviously, you need to ask yourself whether you actually want to prevent your technical skills from deteriorating. Some\npeople move on to management, wave goodbye to getting their hands dirty, and are completely fine with that. Some tinker\nwith technology in their spare time. As for me, I like technology and would not only prefer to not lose what I have\nlearned so far, but actually want to learn something new. However, due to a number of other hobbies and already\nspending more time in front of the screen than I would like, doing tech in the afternoons was not a viable option for\nme. Hence, the decision to make technology an important part of my job again.\nHow\nA crucial factor to take into account when planning such a change is that it will take time. If you take your\nteam seriously, you can’t just disappear overnight. You need to think ahead. Do not ask yourself “have I already\nlost my tech skills beyond repair?”. Ask yourself “how will my tech skills be a year from now?”.\nAlso, keep a cool head. Just as when moving in the opposite direction, from engineer to manager, consider all\nconsequences, both positive and negative. It has been repeated many times that becoming a manager is not a promotion\n(being a lateral move to another career path), but in practice sometimes it is. In particular, it may come with a\nhigher level or salary. Make sure you check all details of your target role in order to avoid unpleasant surprises when\ngoing back. In my case, I was aiming for the Principal Software Engineer role, which is the same level as the Team\nLeader role at Allegro, so there were no issues in this regard.\nWhen I started thinking about making the switch, once I had a rough idea of what I wanted, I talked to my superior.\nThis was an important step: it allowed him to plan ahead, and also to look for opportunities for making the\nreorganization easier. Some elements of the process would depend only on our actions, but some, such as finding a good\nreplacement team leader, would also depend on a number of factors outside our control. Knowing that my boss understood\nmy need, and supported it, mattered a lot, and made the wait and preparations easier.\nChance favors the prepared mind, as Louis Pasteur supposedly said. There happened to be a team leader in\nanother part of the company who was thinking about moving on to a different area. Thanks to being aware of my plan,\nmy boss was able to grab the chance, and we had a perfect match. We discussed with the potential new leader the team\nand the project, and he found them interesting. We planned a transition period, as short as possible, but long enough\nfor me to transfer to him a reasonable part of my knowledge about the team and its work.\nNow that we had a specific plan, we could tell the team. It was important to let everyone know as soon as possible, but\nnot before we had a specific plan. Without it, this information would only stir uncertainty. Apart\nfrom telling the team as a whole, I also talked to each person individually, in order to resolve any questions or doubts\nand to try to reduce any problems resulting from the transition as much as possible.\nWaiting for the switch date, we kept meeting online with the new leader, transferring knowledge and preparing him for\nworking with the team. There’s actually quite a lot of stuff a leader needs to know: not only how the project\nworks on technical and business levels, but also current plans, who the stakeholders are and how to work with them,\nand each team member’s individual strengths and development plan. The new leader himself also started meeting\npeople he would now work with, both team members and our product’s stakeholders, and attending team meetings such as\nthe daily stand-up. Despite gradually moving on to other tasks after the switch date, I was still available to clarify\nany doubts, and our boss would also help out when necessary, so the new leader knew he would not be left on his own.\nWhile it required quite a bit of work, the switch went smoothly, and we didn’t notice any serious disturbance to the\nteam’s functioning.\nThe Aftermath\nIt’s been several months since the switch now. Me changing back to a technical role has certainly required extra work,\nfor me, my boss, and the new team leader. Despite our best efforts, it probably put a little extra strain on the team as\nwell. Nonetheless, I think it was a win-win, even more so thanks to us being able to spot and exploit a happy\ncoincidence. I am glad to be closer to technology again, and the new leader also got to try something new, just as he\nwanted.\nThere is one more subtle advantage to the whole process. When people leave the team, some knowledge inevitably gets\nlost. One of the reasons is tacit knowledge: there are always things you know, but are not aware of knowing. You can\nuse this knowledge when it’s needed, but you will probably not transfer it to others because you are not even aware of\nits existence in the first place. Removing someone from the team in a controlled manner as happened here (and being\nstill able to reach out to them if needed) causes such latent knowledge to be discovered, and once discovered, to be\npropagated. This causes a little disruption short-term, but in the long run it reduces\nknowledge silos and increases the\nbus factor.\nWhen I first started thinking about going back to hands-on technical work for the second time, I had some doubts about\nhow much my technical skills had already deteriorated and how difficult it would be to go back. It seems I made it, but\nnot by much. Had I delayed by one more year, I might have really struggled. It’s not a matter of knowledge: theory,\nespecially generic things that do not change that fast with technology, is not lost so quickly. Also, while a team\nleader, I tried to stay in touch with technology by taking part in task refinement, architecture discussions, on-call\nrotation, etc.\nHowever, I really felt, and to some degree still feel, a difference in practical, hands-on work, such as actually\nwriting code. There are many small quirks that you need to be aware of in order to accomplish things quickly that you\ndon’t even notice if you use them every day and know inside-out. Knowing all the little useful tools, the less often\nused features of your IDE, or what to do when something breaks unexpectedly, make a world of a difference, but this\npractical knowledge gets lost when not used and I had to rebuild it almost from scratch. Another thing that I still\nexperience is the difficulty in focusing on a single topic. Working on a manager’s schedule for several years has taken\nits toll, and now that I often have large contiguous blocks of time, I find myself not using them as effectively as I\ncould, because I have become accustomed to always doing multiple things at once and without a chance to stay focused\nanyway. It’s gradually getting better, but I still feel the impact, and this is probably my biggest surprise of the\nwhole process.\nAbout the Principal Software Engineer role\nMy current role is that of Principal Software Engineer (PSE). It is a relatively new addition to the junior, mid,\nand senior roles we’ve had so far. It has gone through a number of revisions, and is still evolving. Most people in this\nrole come from a Senior Software Engineer background, so my case of getting there after being a Team Leader is a bit\nuntypical. At many companies, roles like this are called Staff Software Engineer or similar. While still an individual\ncontributor role, a PSE differs from a senior in several ways.\nFirst of all, a PSE is expected to spend significant time on topics whose scope is much larger than a single team can\nhandle. Seniors can also do this, but it’s not a requirement for them. Such topics may be area-wide, such as planning a\nmajor change to a single subsystem’s architecture, or have a company-wide scope. Much work on this level consists of\ncoming up with ideas and discussing them while implementation is often left to others. So, while still technical, this\nrole encompasses less coding than that of a senior. Not very surprising given that generally moving up the career ladder\nmeans more coming up with ideas, teaching others, and planning work, while coding less yourself.\nSecondly, a PSE should be very autonomous. Most PSEs are not members of regular development teams since they move from\ntask to task depending on where they can help most. This means you cooperate with more people from different parts of\nthe company, but you don’t have the few peers you work with every day that most people have. You don’t get a backlog\nof tasks to work on, but have to plan your work yourself. People do come to you, asking for support or doing something\nfor their project, but that’s just one of many inputs.\nThirdly, since there are few PSEs compared to other positions, for each person the role is a little different. On one\nhand this means you can’t fully know what to expect when you start. On the other, you get to shape the role yourself,\nand personally I enjoy this flexibility.\nSummary\nI think the idea of swinging back and forth between engineering and management described in\nThe Engineer/Manager Pendulum,\nis valid, and my own experience backs it up fully. One element which I want to additionally stress, however, is that\nthe switch is more like a flying trapeze than a pendulum: timing is crucial, and missing the right moment can have\nserious consequences. Becoming a Principal Software Engineer was a unique experience, both on a technical\nlevel and as my de facto last task as a team leader. Who knows what the future holds? Perhaps some time from now I’ll\ntake another swing on the flying trapeze and go back to management?","guid":"https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html","categories":["tech","coding","management","developer","team leader","career path"],"isoDate":"2023-08-21T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"From 3TB to 100GB: A Cost-Saving Journey in Database Maintenance","link":"https://blog.allegro.tech/2023/07/save-money-on-large-database.html","pubDate":"Mon, 10 Jul 2023 00:00:00 +0200","authors":{"author":[{"name":["Mateusz Stolecki"],"photo":["https://blog.allegro.tech/img/authors/mateusz.stolecki.jpg"],"url":["https://blog.allegro.tech/authors/mateusz.stolecki"]}]},"content":"\u003cp\u003eIn the era of ubiquitous cloud services and an increasingly growing PaaS and serverless-oriented approach, performance\nand resources seem to be becoming less and less important.\nAfter all, we can scale horizontally and vertically at any time, without worrying about potential performance challenges\nthat the business may introduce.\u003c/p\u003e\n\n\u003cp\u003eHowever, there is also another side to the coin – rising costs. While it can be argued that in many situations it is simply\ncheaper to add another instance of the service than to engage a developer who will work tirelessly to diagnose\nand optimize performance problems, the problem will persist and intensify as the business and its requirements grow.\u003c/p\u003e\n\n\u003cp\u003eA similar situation arises with databases. We often store huge amounts of data for auditing or historical purposes.\nWhile the cost of maintaining such databases is negligible at a small scale,\nover time it can become a notable burden on our budget.\u003c/p\u003e\n\n\u003cp\u003eI wanted to talk about such a case and how we managed to reduce the cost of maintaining a database nearly 30-fold.\u003c/p\u003e\n\n\u003ch2 id=\"the-problem\"\u003eThe problem\u003c/h2\u003e\n\u003cp\u003eAs the amount of data grows, the need for scaling arises. In the case of \u003cstrong\u003eAzure\u003c/strong\u003e services, scaling also has its \u003ca href=\"https://learn.microsoft.com/en-us/azure/azure-sql/database/purchasing-models?view=azuresql\"\u003elimitations\u003c/a\u003e.\nIt is not always possible to infinitely increase the available disk space without scaling other resources (CPU, RAM, I/O).\nIn our case, this limit became apparent when we exceeded 1TB of data. Our database was based on the vCore model,\nwhere we used \u003cstrong\u003e4 vCores\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eUnfortunately, this number of vCores limited the available disk space to \u003cstrong\u003e1TB\u003c/strong\u003e. Due to the increase in the number of users\nand the demand for disk space, we needed more resources. We continued to scale up, adding not only more disk resources\nbut also computational resources (I will mention that at this point we reached a scale of \u003cstrong\u003e3TB\u003c/strong\u003e of data, which requires\nat least \u003cstrong\u003e12 vCores\u003c/strong\u003e). At some point, the cost of maintaining the database amounted to several thousand euros.\nThis prompted us to look for solutions.\u003c/p\u003e\n\n\u003cp\u003eComparing the cost of storing substantial amounts of data within \u003cstrong\u003eAzure SQL\u003c/strong\u003e and \u003cstrong\u003eStorage Account\u003c/strong\u003e\n(especially blobs in the \u003cstrong\u003earchive\u003c/strong\u003e tier), we concluded that we could achieve significant cost reduction\nby archiving old/unused data and placing it in a cost-optimized container.\u003c/p\u003e\n\n\u003ch3 id=\"monthly-cost-of-storing-3tb-of-data\"\u003eMonthly cost of storing 3TB of data\u003c/h3\u003e\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eAzure SQL 12vCore 3TB\u003c/th\u003e\n    \u003cth\u003eStorage Account Archive tier\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e$2,876.18\u003c/td\u003e\n    \u003ctd\u003e$31.12\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\u003ch2 id=\"analysis\"\u003eAnalysis\u003c/h2\u003e\n\u003cp\u003eAfter some investigation, It turned out that significant part of data could be safely archived,\nwhich would certainly provide\npotential savings and eliminate the problem of an overgrown database. Most of this data was actually historical.\u003c/p\u003e\n\n\u003cp\u003eWe implemented a solution that allows for much more scalable data archiving\nby asynchronously loading data into the warehouse.\nHowever, data from before the implementation of aforementioned solution were still generating considerable storage costs.\u003c/p\u003e\n\n\u003cp\u003eThe idea seemed simple both in concept and execution. However, we immediately encountered several problems.\nExporting such massive amounts of data is a time-consuming process and puts a heavy load on the database\ncausing responsiveness issues.\u003c/p\u003e\n\n\u003cp\u003eDealing with a production system, we could not reduce the reliability and availability of services.\nIn addition, the export functionality offered by the Azure portal is limited to databases up to \u003cstrong\u003e200GB\u003c/strong\u003e in size,\nwhich meant that we had to look for another solution.\u003c/p\u003e\n\n\u003ch2 id=\"action-plan\"\u003eAction plan\u003c/h2\u003e\n\u003ch3 id=\"concept\"\u003eConcept\u003c/h3\u003e\n\u003cp\u003eAs it turned out, there are ways to export even huge databases. After some investigation,\nwe found the \u003cstrong\u003eSQL Package\u003c/strong\u003e tool.\nIt provides \u003cstrong\u003eexport\u003c/strong\u003e option and is great for solving aforementioned problem. It is able to produce a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebacpac\u003c/code\u003e\nfile that contains highly compressed content of the database.\nThe tool also allows you to restore data at any time using the \u003cstrong\u003eimport\u003c/strong\u003e operation,\nif there is ever a need to review it, for example for audit purposes.\u003c/p\u003e\n\n\u003cp\u003eThe next step is to copy the file to the container in the Storage Account using the \u003cstrong\u003eAzCopy\u003c/strong\u003e tool and ensure\nthat it is stored in the \u003cstrong\u003eARCHIVE\u003c/strong\u003e tier, what will massively reduce the costs of maintaining it.\u003c/p\u003e\n\n\u003cp\u003eThe final stage is to delete unnecessary data from the database, then \u003cstrong\u003eSHRINK\u003c/strong\u003e it, what will reduce database resources.\u003c/p\u003e\n\u003ch3 id=\"script-and-tools\"\u003eScript and tools\u003c/h3\u003e\n\u003cp\u003eTo export and archive the database, we used two tools provided by Microsoft: \u003ca href=\"https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage?view=sql-server-ver16\"\u003eSQL Package\u003c/a\u003e\nand \u003ca href=\"https://learn.microsoft.com/en-us/azure/storage/common/storage-use-azcopy-v10\"\u003eAzCopy\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAfter analyzing their documentation, we prepared the appropriate procedure taking\ninto account performance and operation duration.\u003c/p\u003e\n\u003ch3 id=\"infrastructure\"\u003eInfrastructure\u003c/h3\u003e\n\u003cp\u003eDue to the fact that the export and upload process to the Storage Account container with this amount of data may take\na long time, we decided to set up a temporary \u003cstrong\u003eVM\u003c/strong\u003e with the accelerated networking option, which served us\nto execute all required scripts. It should be mentioned that the need to set up a dedicated virtual machine also arises\nfrom the fact that it must be located in an internal network, where it is also possible to connect to the machine that\nhandles the database. Thanks to meeting this condition,\nit was possible to successfully connect to the database and perform the export operation.\u003c/p\u003e\n\n\u003cp\u003eThe virtual machine turned out to be moderately priced, as all performed operations were not computationally demanding\n(both CPU and RAM usage were low), what allowed us to use a very resource-efficient machine. The only notable extension\nof its functionality is \u003cstrong\u003eaccelerated networking\u003c/strong\u003e, as it must work with data transfer over the network\nand we needed good performance.\u003c/p\u003e\n\n\u003ch2 id=\"testing\"\u003eTesting\u003c/h2\u003e\n\u003ch3 id=\"optimization\"\u003eOptimization\u003c/h3\u003e\n\u003cp\u003eBefore we proceeded with the implementation in the production environment, we conducted a series of\ntests using test environments. They mainly involved running all the steps of the process using\ndata packages of approximately \u003cstrong\u003e50GB\u003c/strong\u003e and \u003cstrong\u003e200GB\u003c/strong\u003e in size.\nWe spent the majority of time testing and optimizing the use of the SQL Package tool.\u003c/p\u003e\n\n\u003cp\u003eOur goal was to shorten the export time and obtain an optimal size for the resulting file,\nso it would not generate excessive costs due to the need to store it. We tested several scenarios\n(mostly by manipulating the \u003cstrong\u003ecompression level\u003c/strong\u003e parameter).\u003c/p\u003e\n\n\u003cp\u003eCompression in \u003cstrong\u003eFAST\u003c/strong\u003e mode showed an average of 10-20% faster export time than \u003cstrong\u003eMAXIMUM\u003c/strong\u003e, with the resulting file\nsize varying within \u0026lt;10%.\u003c/p\u003e\n\n\u003ch3 id=\"performance-testing\"\u003ePerformance testing\u003c/h3\u003e\n\u003cp\u003eWe also tested the load on the databases in each environment.\n\u003cstrong\u003eData IO\u003c/strong\u003e and \u003cstrong\u003eCPU\u003c/strong\u003e load were tested using the test environment relying on DTU-based infrastructure utilising \u003cstrong\u003e100 DTU\u003c/strong\u003e\nunits.\u003c/p\u003e\n\n\u003cp\u003eData IO\n\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-test-dev-iops.png\" alt=\"Data IO\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eCPU\n\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-test-dev-cpu.png\" alt=\"CPU\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eNotice, that the export operation primarily consumes IO resources.\u003c/p\u003e\n\u003ch3 id=\"data-import\"\u003eData Import\u003c/h3\u003e\n\u003cp\u003eDue to the possible need to reuse archived data, we had to make sure that the data we imported was suitable for re-import.\u003c/p\u003e\n\n\u003cp\u003eInitially, we attempted to import the data using the \u003cstrong\u003eSQL Server Management Studio\u003c/strong\u003e tool provided by Microsoft.\nUnfortunately, this attempt failed due to errors related to file reading during the import operation.\nWe made an additional attempt to import the archive using the SQL Package tool, which, in addition to the export option,\nalso provides import options.\u003c/p\u003e\n\n\u003cp\u003eCommand\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003esqlpackage /Action:Import `\n        /tsn:$ServerName `\n        /tdn:$DatabaseName `\n        /tu:$SqlAdminName `\n        /tp:$SqlAdminPassword `\n        /tec:true `\n        /ttsc:false `\n        /d:true `\n        /sf:$SourceFile `\n        /p:CommandTimeout=999 `\n        /p:LongRunningCommandTimeout=0 `\n        /p:DatabaseLockTimeout=-1 `\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003esolved the problem.\u003c/p\u003e\n\n\u003ch2 id=\"deployment\"\u003eDeployment\u003c/h2\u003e\n\u003ch3 id=\"exporting-the-database-using-sql-package-tool\"\u003eExporting the database using SQL Package tool\u003c/h3\u003e\n\u003cp\u003eThe following script was executed, successfully extracting data from the database and creating the appropriate \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebacpac\u003c/code\u003e file.\nAs a result, we received a compressed file of around 100GB.\nIt is worth pointing out that data in the database occupied about 3TB, so compression was very efficient.\nThe whole process took several hours.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003esqlpackage\n    /Action:Export `\n    /ssn:$ServerName `\n    /sdn:$DatabaseName `\n    /su:$SqlAdminName `\n    /sp:$SqlAdminPassword `\n    /sec:true `\n    /stsc:false `\n    /tf:$TargetFile `\n    /p:CompressionOption=Fast `\n    /p:CommandTimeout=999 `\n    /p:LongRunningCommandTimeout=0 `\n    /p:DatabaseLockTimeout=-1 `\n    /p:TempDirectoryForTableData=$TempDirectory `\n    /d:true `\n    /df:$SqlLogs `\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eMany parameters of this operation were evaluated during trials on test environments.\nThe particularly important ones are:\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eCommandTimeout, LongRunningCommandTimeout, DatabaseLockTimeout\u003c/strong\u003e - This set of\nparameters ensures that the connection\nis maintained throughout the entire duration of the export operation (assuming that it will be long-running).\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCompressionOption\u003c/strong\u003e - The degree of data compression in the output file.\nTwo variants were tested:\n\u003cstrong\u003eFAST\u003c/strong\u003e and \u003cstrong\u003eMAXIMUM\u003c/strong\u003e.\n\u003cstrong\u003eFAST\u003c/strong\u003e allowed us to shorten the export time by about 2 hours while showing only slightly lower\ndata compression (in our case, the difference was around 10%).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cdiv class=\"language-powershell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003e/p:TableData\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\"dbo.TestTable\"\u003c/span\u003e\u003cspan class=\"w\"\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe parameter allows us to limit the data export only to the tables selected by us, what significantly shortens\nthe overall operation time. It is also worth mentioning that it is possible to set the parameter multiple times.\u003c/p\u003e\n\n\u003cp\u003eSince the export was launched at night, the procedure had no negative impact on users. The impact of the\nexport operation on the database load (Data I/O percentage) is presented in the graph below. It can be observed that\nthe resource load increased during this operation.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-export-iops.png\" alt=\"Data IO\" /\u003e\u003c/p\u003e\n\n\u003ch3 id=\"copying-the-archived-database-using-azcopy\"\u003eCopying the archived database using AzCopy\u003c/h3\u003e\n\u003cp\u003eThe following script was executed to copy the exported file to the Storage Account:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e.\\azcopy `\n    copy `\n    $TargetFile `\n\"https://$StorageAccountName.blob.core.windows.net/$StorageContainerName/$StorageBlobName$SAS\" `\n    --recursive `\n    --overwrite=true `\n    --blob-type=BlockBlob `\n    --put-md5 `\n    --log-level=info `\n    --block-blob-tier=archive `\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe process went quickly. Copying the 100GB file took only a few minutes, thanks to the high network throughput.\nIt is worth noting that the archive tier is set immediately.\u003c/p\u003e\n\n\u003ch3 id=\"conducting-a-shrink-operation\"\u003eConducting a SHRINK operation\u003c/h3\u003e\n\u003cp\u003eThe SHRINK operation is, unfortunately, required to downscale the Azure SQL database. It took several hours to complete.\n\u003cstrong\u003eWAIT_AT_LOW_PRIORITY\u003c/strong\u003e was used to reduce the impact of this operation on the database users.\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eDBCC\u003c/span\u003e \u003cspan class=\"n\"\u003eSHRINKDATABASE\u003c/span\u003e \u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"n\"\u003eDB_NAME\u003c/span\u003e\u003cspan class=\"p\"\u003e])\u003c/span\u003e \u003cspan class=\"k\"\u003eWITH\u003c/span\u003e \u003cspan class=\"n\"\u003eWAIT_AT_LOW_PRIORITY\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe performance chart (Data IO) during the above operation looked as follows:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-shrink.png\" alt=\"Data IO\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eWe observed a slight increase in Data IO operations during the SHRINK operation.\u003c/p\u003e\n\u003ch3 id=\"performance-analysis-and-index-rebuild\"\u003ePerformance analysis and index rebuild\u003c/h3\u003e\n\u003cp\u003eThis step appeared quite unexpectedly in our procedure. After performing the SHRINK operation and successfully\nlowering the parameters of the machine responsible for the database, we began to observe\nthe impact of our operations on performance.\u003c/p\u003e\n\n\u003cp\u003eTo our concern, we observed a noticeable performance regression.\nEndpoints that use the database on which we performed \u003cstrong\u003eSHRINK\u003c/strong\u003e operation showed abnormally increased response times.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-rps-before-index.png\" alt=\"RPS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe database load chart also did not look encouraging, with frequent peaks during query execution.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-iops-before-index.png\" alt=\"IOPS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eAttempts to scale the machine did not bring spectacular results and only increased costs (considering that our goal was\nto lower them, it was not an optimal solution).\u003c/p\u003e\n\n\u003cp\u003eAs it turned out, the culprit was extraordinarily high index fragmentation. The result of the SHRINK operation was an increase\nin the mentioned fragmentation to almost \u0026gt;90% for practically all existing indexes.\nThis forced us to consider rebuilding all of them.\u003c/p\u003e\n\n\u003cp\u003eEven Microsoft recommends rebuilding indexes in their documentation \u003ca href=\"https://learn.microsoft.com/en-us/sql/relational-databases/databases/shrink-a-database?view=sql-server-ver16\"\u003ehere\u003c/a\u003e:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eData that is moved to shrink a file can be scattered to any available location in the file.\nThis causes index fragmentation and can slow the performance of queries that search a range of the index.\nTo eliminate the fragmentation, consider rebuilding the indexes on the file after shrinking.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eWe decided to proceed with the above-mentioned index rebuild process. Here, we also applied possible optimizations\nto avoid negative consequences related to the availability of our services. The \u003cstrong\u003eONLINE\u003c/strong\u003e option is particularly noteworthy,\nas it ensures that existing indexes and tables will not be blocked, what is an important issue in the case\nof continuous operation of our services.\u003c/p\u003e\n\n\u003cdiv class=\"language-sql highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eALTER\u003c/span\u003e \u003cspan class=\"k\"\u003eINDEX\u003c/span\u003e \u003cspan class=\"k\"\u003eALL\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e \u003cspan class=\"n\"\u003edbo\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eTableName\u003c/span\u003e \u003cspan class=\"n\"\u003eREBUILD\u003c/span\u003e \u003cspan class=\"k\"\u003eWITH\u003c/span\u003e\n\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eFILLFACTOR\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e80\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eSORT_IN_TEMPDB\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eSTATISTICS_NORECOMPUTE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eONLINE\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003eON\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eIt should also be noted that this can be a time-consuming operation, but as a result of its execution,\nthe indexes returned to the required consistency level, reaching a level of fragmentation close to 0%.\nThe response time and resource consumption charts of the database also returned to the values closer to the initial ones.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-rps-after-rebuild.png\" alt=\"RPS\" /\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/perf-xyz-iops-after-index.png\" alt=\"IOPS\" /\u003e\u003c/p\u003e\n\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eAfter performing all of the described actions, we achieved a reduction\nin the size of the database from over 3TB to slightly below 100GB.\nBy lowering the required disk space, we could also significantly reduce the computational resources of the database,\ngenerating further serious savings.\u003c/p\u003e\n\n\u003cp\u003eBefore performing all the operations,\nthe monthly cost of maintaining the database was close to €3000.\nBy switching from a database based on a 12 vCore and 3TB model to a Standard DTU with 100 units and 150GB\nwe managed to cut our monthly spendings to mere €125.\nAfter all, our effort paid off.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-07-10-save-money-on-large-database/montly-cost-reduction.png\" alt=\"Cost reduction\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThe above example demonstrates how to greatly reduce infrastructure costs. Of course,\nthe described procedure will apply to specific cases and data characteristics.\nHowever, if you have a similar problem, I think it is worth considering this approach.\u003c/p\u003e\n","contentSnippet":"In the era of ubiquitous cloud services and an increasingly growing PaaS and serverless-oriented approach, performance\nand resources seem to be becoming less and less important.\nAfter all, we can scale horizontally and vertically at any time, without worrying about potential performance challenges\nthat the business may introduce.\nHowever, there is also another side to the coin – rising costs. While it can be argued that in many situations it is simply\ncheaper to add another instance of the service than to engage a developer who will work tirelessly to diagnose\nand optimize performance problems, the problem will persist and intensify as the business and its requirements grow.\nA similar situation arises with databases. We often store huge amounts of data for auditing or historical purposes.\nWhile the cost of maintaining such databases is negligible at a small scale,\nover time it can become a notable burden on our budget.\nI wanted to talk about such a case and how we managed to reduce the cost of maintaining a database nearly 30-fold.\nThe problem\nAs the amount of data grows, the need for scaling arises. In the case of Azure services, scaling also has its limitations.\nIt is not always possible to infinitely increase the available disk space without scaling other resources (CPU, RAM, I/O).\nIn our case, this limit became apparent when we exceeded 1TB of data. Our database was based on the vCore model,\nwhere we used 4 vCores.\nUnfortunately, this number of vCores limited the available disk space to 1TB. Due to the increase in the number of users\nand the demand for disk space, we needed more resources. We continued to scale up, adding not only more disk resources\nbut also computational resources (I will mention that at this point we reached a scale of 3TB of data, which requires\nat least 12 vCores). At some point, the cost of maintaining the database amounted to several thousand euros.\nThis prompted us to look for solutions.\nComparing the cost of storing substantial amounts of data within Azure SQL and Storage Account\n(especially blobs in the archive tier), we concluded that we could achieve significant cost reduction\nby archiving old/unused data and placing it in a cost-optimized container.\nMonthly cost of storing 3TB of data\nAzure SQL 12vCore 3TB\n    Storage Account Archive tier\n  \n$2,876.18\n    $31.12\n  \nAnalysis\nAfter some investigation, It turned out that significant part of data could be safely archived,\nwhich would certainly provide\npotential savings and eliminate the problem of an overgrown database. Most of this data was actually historical.\nWe implemented a solution that allows for much more scalable data archiving\nby asynchronously loading data into the warehouse.\nHowever, data from before the implementation of aforementioned solution were still generating considerable storage costs.\nThe idea seemed simple both in concept and execution. However, we immediately encountered several problems.\nExporting such massive amounts of data is a time-consuming process and puts a heavy load on the database\ncausing responsiveness issues.\nDealing with a production system, we could not reduce the reliability and availability of services.\nIn addition, the export functionality offered by the Azure portal is limited to databases up to 200GB in size,\nwhich meant that we had to look for another solution.\nAction plan\nConcept\nAs it turned out, there are ways to export even huge databases. After some investigation,\nwe found the SQL Package tool.\nIt provides export option and is great for solving aforementioned problem. It is able to produce a bacpac\nfile that contains highly compressed content of the database.\nThe tool also allows you to restore data at any time using the import operation,\nif there is ever a need to review it, for example for audit purposes.\nThe next step is to copy the file to the container in the Storage Account using the AzCopy tool and ensure\nthat it is stored in the ARCHIVE tier, what will massively reduce the costs of maintaining it.\nThe final stage is to delete unnecessary data from the database, then SHRINK it, what will reduce database resources.\nScript and tools\nTo export and archive the database, we used two tools provided by Microsoft: SQL Package\nand AzCopy.\nAfter analyzing their documentation, we prepared the appropriate procedure taking\ninto account performance and operation duration.\nInfrastructure\nDue to the fact that the export and upload process to the Storage Account container with this amount of data may take\na long time, we decided to set up a temporary VM with the accelerated networking option, which served us\nto execute all required scripts. It should be mentioned that the need to set up a dedicated virtual machine also arises\nfrom the fact that it must be located in an internal network, where it is also possible to connect to the machine that\nhandles the database. Thanks to meeting this condition,\nit was possible to successfully connect to the database and perform the export operation.\nThe virtual machine turned out to be moderately priced, as all performed operations were not computationally demanding\n(both CPU and RAM usage were low), what allowed us to use a very resource-efficient machine. The only notable extension\nof its functionality is accelerated networking, as it must work with data transfer over the network\nand we needed good performance.\nTesting\nOptimization\nBefore we proceeded with the implementation in the production environment, we conducted a series of\ntests using test environments. They mainly involved running all the steps of the process using\ndata packages of approximately 50GB and 200GB in size.\nWe spent the majority of time testing and optimizing the use of the SQL Package tool.\nOur goal was to shorten the export time and obtain an optimal size for the resulting file,\nso it would not generate excessive costs due to the need to store it. We tested several scenarios\n(mostly by manipulating the compression level parameter).\nCompression in FAST mode showed an average of 10-20% faster export time than MAXIMUM, with the resulting file\nsize varying within \u003c10%.\nPerformance testing\nWe also tested the load on the databases in each environment.\nData IO and CPU load were tested using the test environment relying on DTU-based infrastructure utilising 100 DTU\nunits.\nData IO\n\nCPU\n\nNotice, that the export operation primarily consumes IO resources.\nData Import\nDue to the possible need to reuse archived data, we had to make sure that the data we imported was suitable for re-import.\nInitially, we attempted to import the data using the SQL Server Management Studio tool provided by Microsoft.\nUnfortunately, this attempt failed due to errors related to file reading during the import operation.\nWe made an additional attempt to import the archive using the SQL Package tool, which, in addition to the export option,\nalso provides import options.\nCommand\n\nsqlpackage /Action:Import `\n        /tsn:$ServerName `\n        /tdn:$DatabaseName `\n        /tu:$SqlAdminName `\n        /tp:$SqlAdminPassword `\n        /tec:true `\n        /ttsc:false `\n        /d:true `\n        /sf:$SourceFile `\n        /p:CommandTimeout=999 `\n        /p:LongRunningCommandTimeout=0 `\n        /p:DatabaseLockTimeout=-1 `\n\n\nsolved the problem.\nDeployment\nExporting the database using SQL Package tool\nThe following script was executed, successfully extracting data from the database and creating the appropriate bacpac file.\nAs a result, we received a compressed file of around 100GB.\nIt is worth pointing out that data in the database occupied about 3TB, so compression was very efficient.\nThe whole process took several hours.\n\nsqlpackage\n    /Action:Export `\n    /ssn:$ServerName `\n    /sdn:$DatabaseName `\n    /su:$SqlAdminName `\n    /sp:$SqlAdminPassword `\n    /sec:true `\n    /stsc:false `\n    /tf:$TargetFile `\n    /p:CompressionOption=Fast `\n    /p:CommandTimeout=999 `\n    /p:LongRunningCommandTimeout=0 `\n    /p:DatabaseLockTimeout=-1 `\n    /p:TempDirectoryForTableData=$TempDirectory `\n    /d:true `\n    /df:$SqlLogs `\n\n\nMany parameters of this operation were evaluated during trials on test environments.\nThe particularly important ones are:\nCommandTimeout, LongRunningCommandTimeout, DatabaseLockTimeout - This set of\nparameters ensures that the connection\nis maintained throughout the entire duration of the export operation (assuming that it will be long-running).\nCompressionOption - The degree of data compression in the output file.\nTwo variants were tested:\nFAST and MAXIMUM.\nFAST allowed us to shorten the export time by about 2 hours while showing only slightly lower\ndata compression (in our case, the difference was around 10%).\n\n/p:TableData=\"dbo.TestTable\"\n\n\nThe parameter allows us to limit the data export only to the tables selected by us, what significantly shortens\nthe overall operation time. It is also worth mentioning that it is possible to set the parameter multiple times.\nSince the export was launched at night, the procedure had no negative impact on users. The impact of the\nexport operation on the database load (Data I/O percentage) is presented in the graph below. It can be observed that\nthe resource load increased during this operation.\n\nCopying the archived database using AzCopy\nThe following script was executed to copy the exported file to the Storage Account:\n\n.\\azcopy `\n    copy `\n    $TargetFile `\n\"https://$StorageAccountName.blob.core.windows.net/$StorageContainerName/$StorageBlobName$SAS\" `\n    --recursive `\n    --overwrite=true `\n    --blob-type=BlockBlob `\n    --put-md5 `\n    --log-level=info `\n    --block-blob-tier=archive `\n\n\nThe process went quickly. Copying the 100GB file took only a few minutes, thanks to the high network throughput.\nIt is worth noting that the archive tier is set immediately.\nConducting a SHRINK operation\nThe SHRINK operation is, unfortunately, required to downscale the Azure SQL database. It took several hours to complete.\nWAIT_AT_LOW_PRIORITY was used to reduce the impact of this operation on the database users.\n\nDBCC SHRINKDATABASE ([DB_NAME]) WITH WAIT_AT_LOW_PRIORITY\n\n\nThe performance chart (Data IO) during the above operation looked as follows:\n\nWe observed a slight increase in Data IO operations during the SHRINK operation.\nPerformance analysis and index rebuild\nThis step appeared quite unexpectedly in our procedure. After performing the SHRINK operation and successfully\nlowering the parameters of the machine responsible for the database, we began to observe\nthe impact of our operations on performance.\nTo our concern, we observed a noticeable performance regression.\nEndpoints that use the database on which we performed SHRINK operation showed abnormally increased response times.\n\nThe database load chart also did not look encouraging, with frequent peaks during query execution.\n\nAttempts to scale the machine did not bring spectacular results and only increased costs (considering that our goal was\nto lower them, it was not an optimal solution).\nAs it turned out, the culprit was extraordinarily high index fragmentation. The result of the SHRINK operation was an increase\nin the mentioned fragmentation to almost \u003e90% for practically all existing indexes.\nThis forced us to consider rebuilding all of them.\nEven Microsoft recommends rebuilding indexes in their documentation here:\nData that is moved to shrink a file can be scattered to any available location in the file.\nThis causes index fragmentation and can slow the performance of queries that search a range of the index.\nTo eliminate the fragmentation, consider rebuilding the indexes on the file after shrinking.\nWe decided to proceed with the above-mentioned index rebuild process. Here, we also applied possible optimizations\nto avoid negative consequences related to the availability of our services. The ONLINE option is particularly noteworthy,\nas it ensures that existing indexes and tables will not be blocked, what is an important issue in the case\nof continuous operation of our services.\n\nALTER INDEX ALL ON dbo.TableName REBUILD WITH\n(FILLFACTOR = 80, SORT_IN_TEMPDB = ON, STATISTICS_NORECOMPUTE = ON, ONLINE = ON);\n\n\nIt should also be noted that this can be a time-consuming operation, but as a result of its execution,\nthe indexes returned to the required consistency level, reaching a level of fragmentation close to 0%.\nThe response time and resource consumption charts of the database also returned to the values closer to the initial ones.\n\n\nConclusion\nAfter performing all of the described actions, we achieved a reduction\nin the size of the database from over 3TB to slightly below 100GB.\nBy lowering the required disk space, we could also significantly reduce the computational resources of the database,\ngenerating further serious savings.\nBefore performing all the operations,\nthe monthly cost of maintaining the database was close to €3000.\nBy switching from a database based on a 12 vCore and 3TB model to a Standard DTU with 100 units and 150GB\nwe managed to cut our monthly spendings to mere €125.\nAfter all, our effort paid off.\n\nThe above example demonstrates how to greatly reduce infrastructure costs. Of course,\nthe described procedure will apply to specific cases and data characteristics.\nHowever, if you have a similar problem, I think it is worth considering this approach.","guid":"https://blog.allegro.tech/2023/07/save-money-on-large-database.html","categories":["tech","azure","sql","saving","cloud"],"isoDate":"2023-07-09T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Debugging hangs - piecing together why nothing happens","link":"https://blog.allegro.tech/2023/05/debugging-hangs.html","pubDate":"Wed, 31 May 2023 00:00:00 +0200","authors":{"author":[{"name":["Łukasz Rokita"],"photo":["https://blog.allegro.tech/img/authors/lukasz.rokita.jpg"],"url":["https://blog.allegro.tech/authors/lukasz.rokita"]}]},"content":"\u003cp\u003eAs a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.\nThe task is daunting and filled with challenges,\nhowever overall progress is steady and thanks to the modular nature of our code it should end in finite time.\nEveryone who has performed such an upgrade knows that you need to expect the unexpected and at the end of the day prepare for lots of debugging.\nNo amount of migration guide would prepare you for what’s coming in the field.\nIn the words of Donald Rumsfeld there are unknown unknowns and we need to be equipped with the tools to uncover these unknowns and patch them up.\nIn this blog post I’d like to walk you through a process that should show where the application hangs,\nalthough there seems to be nothing wrong with it. I will also show that you don’t always know what code you have – problem known as dependecy hell,\nplace we got quite cosy in during this upgrade.\u003c/p\u003e\n\n\u003ch2 id=\"the-change\"\u003eThe change\u003c/h2\u003e\n\u003cp\u003eNote that we keep versions as separate key–value pairs in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebuild.gradle\u003c/code\u003e files and reference them in dependencies by key.\nUpdating often means a single line change. The upgrade is trivial and git diff looks like this.\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eext.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n]\n\next.versions = [\n+        spring         : '6.0.5',\n+        spock          : '2.4-M1-groovy-4.0',\n+        groovy         : '4.0.9',\n]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eNothing much happens. We upgrade Spring and since there are some problems with Spock not working well with the newest Spring\nwe need to upgrade it as well, along with Groovy. This is the easy part.\nNow we run the tests and expect to be either elated with the sight of a successful build or greeted with descriptive error messages\nthat help us quickly patch the issue. Nobody expects anything and in this case this is an unknown unknown.\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e97% EXECUTING [15m 55s]\n\u0026gt; :platform-libraries-webclient:integrationTest \u0026gt; 1 test completed, 1 failed\n\u0026gt; :platform-libraries-webclient:integrationTest \u0026gt; Executing test pl.allegro....WebClientContextContainerInterceptorSpec\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eAfter 15 minutes we expect the process to end. A quick cross-check with the master branch confirms that tests run and execute in less than a minute.\nSomething is wrong and it’s on us. However, no error is presented. Adding logging does not help, nothing streams to standard output.\nSomething hangs and refuses to budge. When that happenes there is only one way to inspect what is going on and\nthat is to pop the hood open and look into JVM to see what the threads are doing or where they are slacking.\u003c/p\u003e\n\n\u003ch2 id=\"thread-theory\"\u003eThread theory\u003c/h2\u003e\n\n\u003cp\u003eLet’s interrupt this story with a short summary of threading in JVM. You can skip this chapter if you are familiar with the topic.\nAs the priceless book Java Concurrency in Practice by Brian Goetz et al. teaches us:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e“Threads may block, or pause, for several reasons: waiting for I/O completion, waiting to acquire a lock,\nwaiting to wake up from Thread.sleep, or waiting for the result of a computation in another thread.\nWhen a thread blocks, it is usually suspended and placed in one of the blocked thread states\n(BLOCKED, WAITING, or TIMED_WAITING). (…) blocked thread must wait for an event beyond its control before it can proceed”.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis sounds exactly like the situation we are in. So there is hope. Let’s educate ourselves further.\nAnother excerpt that would prove insightful reads as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e“(…) tasks can block for exteded periods of time, even if deadlock is not a possibility.\n(…) One technique that can mitigate the ill effects of long–running tasks is for tasks to use timed resource waits instead of\nunbound waits.”\nThis seems like an answer to our woes. However, two mysteries remain.\nWhere to put the timeout? What the thread is waiting for? To answer these questions we need to inspect the threads in the JVM itself.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"the-investigation\"\u003eThe investigation\u003c/h2\u003e\n\u003cp\u003eAt this point we did two things. First we pushed our code to a branch.\nAfter all at any moment our laptops could burst into flames and all the work would go to waste.\nThe remote CI confirmed our suspicion since it also hung. The problem was real and not only confined to the local machine.\nThe second thing is to scout for the offending thread. This is easy with the help of some JDK binaries:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ejps -lv | grep platform-libraries\n38983 worker.org.gradle.process.internal.worker.GradleWorkerMain -Dorg.gradle.internal.worker.tmpdir=/path/to/code/platform-libraries/platform-libraries-webclient/build/tmp/integrationTest/work -Dorg.gradle.native=false -Xmx512m -Dfile.encoding=UTF-8\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eSo we have the a lvmid – local JVM identifier, which will help us locate the offending thread in jconsole.\nIn the screen below we can see that the thread waits on \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMono.block()\u003c/code\u003e which is left unbounded in a happy path scenario.\nWell, we are in the worst case so first of all we add a simple timeout \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMono.block(Duration.ofSeconds(10))\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/img/articles/2023-05-31-debugging-hangs/jconsole.png\" alt=\"jconsole\" /\u003e\u003c/p\u003e\n\n\u003cp\u003eThis fails our tests and for the first time the error appears:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\t08:13:39.556 [Test worker] WARN reactor.core.Exceptions - throwIfFatal detected a jvm fatal exception, which is thrown and logged below:\njava.lang.NoSuchMethodError: 'reactor.core.publisher.Mono reactor.core.publisher.Mono.subscriberContext(reactor.util.context.Context)'\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\tat reactor.netty.internal.shaded.reactor.pool.AbstractPool$Borrower.request(AbstractPool.java:427)\n\tat reactor.netty.resources.PooledConnectionProvider$DisposableAcquire.onSubscribe(PooledConnectionProvider.java:533)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool$QueueBorrowerMono.subscribe(SimpleDequePool.java:676)\n\tat reactor.netty.resources.PooledConnectionProvider.disposableAcquire(PooledConnectionProvider.java:219)\n\tat reactor.netty.resources.PooledConnectionProvider.lambda$acquire$3(PooledConnectionProvider.java:183)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:326)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.core.publisher.FluxRetryWhen.subscribe(FluxRetryWhen.java:77)\n\tat reactor.core.publisher.MonoRetryWhen.subscribeOrReturn(MonoRetryWhen.java:46)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:329)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)\n\tat reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2545)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.request(MonoFlatMap.java:194)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.request(FluxOnAssembly.java:649)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2341)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:2215)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onSubscribe(FluxOnAssembly.java:633)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onSubscribe(MonoFlatMap.java:117)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)\n\tat reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoDeferContextual.subscribe(MonoDeferContextual.java:55)\n\tat reactor.core.publisher.Mono.subscribe(Mono.java:4485)\n\tat reactor.core.publisher.Mono.block(Mono.java:1733)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$Trait$Helper.makeRequest(WebClientContextContainerAdapterConfiguration.groovy:22)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$makeRequest.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)\n\tat pl.allegro....AdapterConfiguration$Trait$Helper.makeRequest(AdapterConfiguration.groovy:11)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....SharedInterceptorSpec$makeRequest.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:203)\n\tat pl.allegro....SharedInterceptorSpec.$spock_feature_0_0(SharedInterceptorSpec.groovy:44)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.spockframework.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:196)\n\tat org.spockframework.runtime.model.MethodInfo.lambda$new$0(MethodInfo.java:49)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeatureMethod(PlatformSpecRunner.java:324)\n\tat org.spockframework.runtime.IterationNode.execute(IterationNode.java:50)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:58)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.IterationNode.lambda$around$0(IterationNode.java:67)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunIteration$5(PlatformSpecRunner.java:236)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.junit4.AbstractRuleInterceptor$1.evaluate(AbstractRuleInterceptor.java:46)\n\tat com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:79)\n\tat org.spockframework.junit4.MethodRuleInterceptor.intercept(MethodRuleInterceptor.java:40)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runIteration(PlatformSpecRunner.java:218)\n\tat org.spockframework.runtime.IterationNode.around(IterationNode.java:67)\n\tat org.spockframework.runtime.SimpleFeatureNode.lambda$around$0(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)\n\tat org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:63)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:11)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eFor the first time we force the entire reactive code to finally execute itself and present us with the result,\neven if it is an error this moves us in the right direction.\u003c/p\u003e\n\n\u003ch2 id=\"result\"\u003eResult\u003c/h2\u003e\n\n\u003cp\u003eLike in any good crime story uncovering one mystery presents another.\nA quick \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egrep\u003c/code\u003e shows that there are no calls to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereactor.core.publisher.Mono.subscriberContext\u003c/code\u003e.\nWhere could this call be hiding, if it’s not present in our code?\u003c/p\u003e\n\n\u003cp\u003eThe answer is simple but I assure you that it took us some time to come up with it.\nIf it isn’t in our code and it runs inside our JVM then this must be dependency code.\nThe observant reader is able to spot it from afar. The stack trace confirms where the error lies:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    at reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWe need to patch \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereactor–netty\u003c/code\u003e which in this version still used deprecated code. Referring back to our diff:\u003c/p\u003e\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eext.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n-        reactorNetty   : '0.9.25.RELEASE',\n]\n\next.versions = [\n+        spring        : '6.0.5',\n+        spock         : '2.4-M1-groovy-4.0',\n+        groovy        : '4.0.9',\n+        reactorNetty  : '1.1.3',\n]\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eWe escape the dependency hell and are delighted to see the green letters \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBUILD SUCCESSFUL in 24s\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003eWell this was quite a thrilling journey one doesn’t often embark on.\nThe odd peculiarity of the problem combined with peculiarity of the task provided us with a great challange and satisfaction.\nDependency hell is no joke, but armed with the JDK tools and thinking the problem through, there is no obstacle that could not be overcome.\nNext time your code hangs with no apparent reason this is a perfect opportunity to dust off the swiss army knife of JDK binaries and dig in.\u003c/p\u003e\n","contentSnippet":"As a part of a broader initiative of refreshing Allegro platform, we are upgrading our internal libraries to Spring Boot 3.0 and Java 17.\nThe task is daunting and filled with challenges,\nhowever overall progress is steady and thanks to the modular nature of our code it should end in finite time.\nEveryone who has performed such an upgrade knows that you need to expect the unexpected and at the end of the day prepare for lots of debugging.\nNo amount of migration guide would prepare you for what’s coming in the field.\nIn the words of Donald Rumsfeld there are unknown unknowns and we need to be equipped with the tools to uncover these unknowns and patch them up.\nIn this blog post I’d like to walk you through a process that should show where the application hangs,\nalthough there seems to be nothing wrong with it. I will also show that you don’t always know what code you have – problem known as dependecy hell,\nplace we got quite cosy in during this upgrade.\nThe change\nNote that we keep versions as separate key–value pairs in build.gradle files and reference them in dependencies by key.\nUpdating often means a single line change. The upgrade is trivial and git diff looks like this.\n\next.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n]\n\next.versions = [\n+        spring         : '6.0.5',\n+        spock          : '2.4-M1-groovy-4.0',\n+        groovy         : '4.0.9',\n]\n\n\nNothing much happens. We upgrade Spring and since there are some problems with Spock not working well with the newest Spring\nwe need to upgrade it as well, along with Groovy. This is the easy part.\nNow we run the tests and expect to be either elated with the sight of a successful build or greeted with descriptive error messages\nthat help us quickly patch the issue. Nobody expects anything and in this case this is an unknown unknown.\n\n97% EXECUTING [15m 55s]\n\u003e :platform-libraries-webclient:integrationTest \u003e 1 test completed, 1 failed\n\u003e :platform-libraries-webclient:integrationTest \u003e Executing test pl.allegro....WebClientContextContainerInterceptorSpec\n\n\nAfter 15 minutes we expect the process to end. A quick cross-check with the master branch confirms that tests run and execute in less than a minute.\nSomething is wrong and it’s on us. However, no error is presented. Adding logging does not help, nothing streams to standard output.\nSomething hangs and refuses to budge. When that happenes there is only one way to inspect what is going on and\nthat is to pop the hood open and look into JVM to see what the threads are doing or where they are slacking.\nThread theory\nLet’s interrupt this story with a short summary of threading in JVM. You can skip this chapter if you are familiar with the topic.\nAs the priceless book Java Concurrency in Practice by Brian Goetz et al. teaches us:\n“Threads may block, or pause, for several reasons: waiting for I/O completion, waiting to acquire a lock,\nwaiting to wake up from Thread.sleep, or waiting for the result of a computation in another thread.\nWhen a thread blocks, it is usually suspended and placed in one of the blocked thread states\n(BLOCKED, WAITING, or TIMED_WAITING). (…) blocked thread must wait for an event beyond its control before it can proceed”.\nThis sounds exactly like the situation we are in. So there is hope. Let’s educate ourselves further.\nAnother excerpt that would prove insightful reads as follows:\n“(…) tasks can block for exteded periods of time, even if deadlock is not a possibility.\n(…) One technique that can mitigate the ill effects of long–running tasks is for tasks to use timed resource waits instead of\nunbound waits.”\nThis seems like an answer to our woes. However, two mysteries remain.\nWhere to put the timeout? What the thread is waiting for? To answer these questions we need to inspect the threads in the JVM itself.\nThe investigation\nAt this point we did two things. First we pushed our code to a branch.\nAfter all at any moment our laptops could burst into flames and all the work would go to waste.\nThe remote CI confirmed our suspicion since it also hung. The problem was real and not only confined to the local machine.\nThe second thing is to scout for the offending thread. This is easy with the help of some JDK binaries:\n\njps -lv | grep platform-libraries\n38983 worker.org.gradle.process.internal.worker.GradleWorkerMain -Dorg.gradle.internal.worker.tmpdir=/path/to/code/platform-libraries/platform-libraries-webclient/build/tmp/integrationTest/work -Dorg.gradle.native=false -Xmx512m -Dfile.encoding=UTF-8\n\n\nSo we have the a lvmid – local JVM identifier, which will help us locate the offending thread in jconsole.\nIn the screen below we can see that the thread waits on Mono.block() which is left unbounded in a happy path scenario.\nWell, we are in the worst case so first of all we add a simple timeout Mono.block(Duration.ofSeconds(10)).\n\nThis fails our tests and for the first time the error appears:\n\n\t08:13:39.556 [Test worker] WARN reactor.core.Exceptions - throwIfFatal detected a jvm fatal exception, which is thrown and logged below:\njava.lang.NoSuchMethodError: 'reactor.core.publisher.Mono reactor.core.publisher.Mono.subscriberContext(reactor.util.context.Context)'\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\tat reactor.netty.internal.shaded.reactor.pool.AbstractPool$Borrower.request(AbstractPool.java:427)\n\tat reactor.netty.resources.PooledConnectionProvider$DisposableAcquire.onSubscribe(PooledConnectionProvider.java:533)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool$QueueBorrowerMono.subscribe(SimpleDequePool.java:676)\n\tat reactor.netty.resources.PooledConnectionProvider.disposableAcquire(PooledConnectionProvider.java:219)\n\tat reactor.netty.resources.PooledConnectionProvider.lambda$acquire$3(PooledConnectionProvider.java:183)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.lambda$subscribe$0(HttpClientConnect.java:326)\n\tat reactor.core.publisher.MonoCreate.subscribe(MonoCreate.java:58)\n\tat reactor.core.publisher.FluxRetryWhen.subscribe(FluxRetryWhen.java:77)\n\tat reactor.core.publisher.MonoRetryWhen.subscribeOrReturn(MonoRetryWhen.java:46)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:57)\n\tat reactor.netty.http.client.HttpClientConnect$MonoHttpConnect.subscribe(HttpClientConnect.java:329)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)\n\tat reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2545)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:171)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.request(MonoFlatMap.java:194)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.request(FluxOnAssembly.java:649)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2341)\n\tat reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:2215)\n\tat reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onSubscribe(FluxOnAssembly.java:633)\n\tat reactor.core.publisher.MonoFlatMap$FlatMapMain.onSubscribe(MonoFlatMap.java:117)\n\tat reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:96)\n\tat reactor.core.publisher.MonoJust.subscribe(MonoJust.java:55)\n\tat reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:64)\n\tat reactor.core.publisher.MonoDeferContextual.subscribe(MonoDeferContextual.java:55)\n\tat reactor.core.publisher.Mono.subscribe(Mono.java:4485)\n\tat reactor.core.publisher.Mono.block(Mono.java:1733)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$Trait$Helper.makeRequest(WebClientContextContainerAdapterConfiguration.groovy:22)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....WebClientContextContainerAdapterConfiguration$makeRequest.call(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:45)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:125)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:166)\n\tat pl.allegro....AdapterConfiguration$Trait$Helper.makeRequest(AdapterConfiguration.groovy:11)\n\tat org.codehaus.groovy.vmplugin.v8.IndyInterface.fromCache(IndyInterface.java:321)\n\tat pl.allegro....WebClientContextContainerInterceptorSpec.makeRequest(WebClientContextContainerInterceptorSpec.groovy)\n\tat pl.allegro....SharedInterceptorSpec$makeRequest.callCurrent(Unknown Source)\n\tat org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:171)\n\tat org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:203)\n\tat pl.allegro....SharedInterceptorSpec.$spock_feature_0_0(SharedInterceptorSpec.groovy:44)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.spockframework.util.ReflectionUtil.invokeMethod(ReflectionUtil.java:196)\n\tat org.spockframework.runtime.model.MethodInfo.lambda$new$0(MethodInfo.java:49)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeatureMethod(PlatformSpecRunner.java:324)\n\tat org.spockframework.runtime.IterationNode.execute(IterationNode.java:50)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:58)\n\tat org.spockframework.runtime.SimpleFeatureNode.execute(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.IterationNode.lambda$around$0(IterationNode.java:67)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunIteration$5(PlatformSpecRunner.java:236)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:102)\n\tat org.spockframework.junit4.ExceptionAdapterInterceptor.intercept(ExceptionAdapterInterceptor.java:13)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.junit4.AbstractRuleInterceptor$1.evaluate(AbstractRuleInterceptor.java:46)\n\tat com.github.tomakehurst.wiremock.junit.WireMockRule$1.evaluate(WireMockRule.java:79)\n\tat org.spockframework.junit4.MethodRuleInterceptor.intercept(MethodRuleInterceptor.java:40)\n\tat org.spockframework.runtime.extension.MethodInvocation.proceed(MethodInvocation.java:101)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:398)\n\tat org.spockframework.runtime.PlatformSpecRunner.runIteration(PlatformSpecRunner.java:218)\n\tat org.spockframework.runtime.IterationNode.around(IterationNode.java:67)\n\tat org.spockframework.runtime.SimpleFeatureNode.lambda$around$0(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.FeatureNode.lambda$around$0(FeatureNode.java:41)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunFeature$4(PlatformSpecRunner.java:199)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runFeature(PlatformSpecRunner.java:192)\n\tat org.spockframework.runtime.FeatureNode.around(FeatureNode.java:41)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:52)\n\tat org.spockframework.runtime.SimpleFeatureNode.around(SimpleFeatureNode.java:15)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.spockframework.runtime.SpockNode.sneakyInvoke(SpockNode.java:40)\n\tat org.spockframework.runtime.SpecNode.lambda$around$0(SpecNode.java:63)\n\tat org.spockframework.runtime.PlatformSpecRunner.lambda$createMethodInfoForDoRunSpec$0(PlatformSpecRunner.java:61)\n\tat org.spockframework.runtime.model.MethodInfo.invoke(MethodInfo.java:156)\n\tat org.spockframework.runtime.PlatformSpecRunner.invokeRaw(PlatformSpecRunner.java:407)\n\tat org.spockframework.runtime.PlatformSpecRunner.invoke(PlatformSpecRunner.java:390)\n\tat org.spockframework.runtime.PlatformSpecRunner.runSpec(PlatformSpecRunner.java:55)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:63)\n\tat org.spockframework.runtime.SpecNode.around(SpecNode.java:11)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)\n\tat org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)\n\tat org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)\n\tat org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)\n\tat org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n\tat org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)\n\tat org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n\tat org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n\tat org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n\tat org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:99)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:79)\n\tat org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:75)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)\n\tat jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)\n\tat org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113)\n\tat org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n\tat worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\n\nFor the first time we force the entire reactive code to finally execute itself and present us with the result,\neven if it is an error this moves us in the right direction.\nResult\nLike in any good crime story uncovering one mystery presents another.\nA quick grep shows that there are no calls to reactor.core.publisher.Mono.subscriberContext.\nWhere could this call be hiding, if it’s not present in our code?\nThe answer is simple but I assure you that it took us some time to come up with it.\nIf it isn’t in our code and it runs inside our JVM then this must be dependency code.\nThe observant reader is able to spot it from afar. The stack trace confirms where the error lies:\n\n    at reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.drainLoop(SimpleDequePool.java:403)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.pendingOffer(SimpleDequePool.java:558)\n\tat reactor.netty.internal.shaded.reactor.pool.SimpleDequePool.doAcquire(SimpleDequePool.java:268)\n\n\nWe need to patch reactor–netty which in this version still used deprecated code. Referring back to our diff:\n\next.versions = [\n-        spring         : '5.3.24',\n-        spock          : '2.3-groovy-3.0',\n-        groovy         : '3.0.14',\n-        reactorNetty   : '0.9.25.RELEASE',\n]\n\next.versions = [\n+        spring        : '6.0.5',\n+        spock         : '2.4-M1-groovy-4.0',\n+        groovy        : '4.0.9',\n+        reactorNetty  : '1.1.3',\n]\n\n\nWe escape the dependency hell and are delighted to see the green letters BUILD SUCCESSFUL in 24s.\nSummary\nWell this was quite a thrilling journey one doesn’t often embark on.\nThe odd peculiarity of the problem combined with peculiarity of the task provided us with a great challange and satisfaction.\nDependency hell is no joke, but armed with the JDK tools and thinking the problem through, there is no obstacle that could not be overcome.\nNext time your code hangs with no apparent reason this is a perfect opportunity to dust off the swiss army knife of JDK binaries and dig in.","guid":"https://blog.allegro.tech/2023/05/debugging-hangs.html","categories":["tech","java","jvm","debugging","dependency hell"],"isoDate":"2023-05-30T22:00:00.000Z","thumbnail":"images/post-headers/java.png"}],"jobs":[{"id":"743999930016493","name":"Front-End Software Engineer - Technical Platform \u0026 Operations","uuid":"4644c5e0-f675-40d0-a446-e2bcbae71020","jobAdId":"505b825e-d64d-4f46-b4c8-5e2c86e88336","defaultJobAd":false,"refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-09-12T15:11:42.111Z","location":{"city":"Poznan, Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999930016493","creator":{"name":"Dominika Fujarowicz"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999930016183","name":"Front-End Software Engineer","uuid":"34a735f8-d691-4e05-b42e-c7e4b94c0943","jobAdId":"05b6643a-43dd-46a2-b998-bfbaa0a31c68","defaultJobAd":true,"refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-09-12T15:10:49.174Z","location":{"city":"Warsaw, Poznan","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999930016183","creator":{"name":"Dominika Fujarowicz"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999929962756","name":"Senior Software Engineer (Java/Kotlin)","uuid":"636e15e0-318b-444f-a5f3-f86c7e92bc3f","jobAdId":"b1d96cd5-7296-471b-9a31-ab8b188c1afb","defaultJobAd":false,"refNumber":"REF4072X","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-09-12T12:18:48.703Z","location":{"city":"Poznań, Warsaw, Cracow, Wrocław, Gdańsk","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999929962756","creator":{"name":"Monika Walaszek"},"language":{"code":"en-GB","label":"English (UK)","labelNative":"English (UK)"}},{"id":"743999929309124","name":"Software Engineer Java - Mall.cz","uuid":"33d37816-c3ca-4f55-a020-ded6ad76f7c2","jobAdId":"7236e848-3276-4740-a59d-de2209ea6536","defaultJobAd":false,"refNumber":"REF4327W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-09-08T10:41:30.160Z","location":{"city":"Prague, Remote","country":"cz","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"f2bb5bc2-3fb0-4d5a-96d2-59e7d59ab3d7","valueLabel":"Tech Engineer/Non-Engineer - IC (MG)"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"cz","valueLabel":"Czech republic"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"b61e1897-7104-4a9d-b1cf-04fc2c537081","valueLabel":"N/A"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"0bca93e9-bc16-4156-902e-50465671c8fa","valueLabel":"Mall.cz"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999929309124","creator":{"name":"Natalia Glińska"},"language":{"code":"cs","label":"Czech","labelNative":"čeština"}},{"id":"743999929308615","name":"Software Engineer Java - Mall.cz","uuid":"994dcf33-072a-46a2-a0f0-080f9572aa85","jobAdId":"2b53fed0-4b15-488e-a1be-516a97bf5866","defaultJobAd":true,"refNumber":"REF4327W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-09-08T10:37:31.279Z","location":{"city":"Prague, Remote","country":"cz","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"f2bb5bc2-3fb0-4d5a-96d2-59e7d59ab3d7","valueLabel":"Tech Engineer/Non-Engineer - IC (MG)"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"cz","valueLabel":"Czech republic"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"b61e1897-7104-4a9d-b1cf-04fc2c537081","valueLabel":"N/A"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"0bca93e9-bc16-4156-902e-50465671c8fa","valueLabel":"Mall.cz"},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999929308615","creator":{"name":"Natalia Glińska"},"language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1685697967000,"duration":7200000,"id":"293929321","name":"Allegro Tech Talks #38 - Mobile: o iOS bez spinki","date_in_series_pattern":false,"status":"past","time":1686760200000,"local_date":"2023-06-14","local_time":"18:30","updated":1686773845000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":17,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":0,"lon":0,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293929321/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwą wakacyjną, stacjonarne spotkanie z cyklu Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Fabryki Norblina (wejście Plater 3 od ul. Żelaznej). W niedalekiej odległości znajdują się dwie stacje metra linii M2, Rondo Daszyńskiego i Rondo ONZ. Autobusy, tramwaje i inne środki transportu sprawdzisz też na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false},{"created":1678978572000,"duration":111600000,"id":"292278882","name":"UX Research Confetti - III edycja ","date_in_series_pattern":false,"status":"past","time":1684915200000,"local_date":"2023-05-24","local_time":"10:00","updated":1685029049000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":33,"is_online_event":true,"eventType":"ONLINE","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/292278882/","description":"**Rejestracja na wydarzenie ➡ [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **🎉 Przedstawiamy 3. edycję UX Research Confetti organizowaną przez Allegro - bezpłatną, polską konferencję poświęconą badaniom…","visibility":"public","member_pay_fee":false},{"created":1683275557000,"duration":7200000,"id":"293341234","name":"Allegro Tech Talks #37 - Kotlin Native i niebezpieczeństwa współdzielonego stanu","date_in_series_pattern":false,"status":"past","time":1684425600000,"local_date":"2023-05-18","local_time":"18:00","updated":1684437308000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":19,"venue":{"id":27570147,"name":"Allegro Office - Poznań (Nowy Rynek)","lat":52.40021514892578,"lon":16.92083168029785,"repinned":true,"address_1":"Wierzbięcice 1B - budynek D","city":"Poznań","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293341234/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-37/](https://app.evenea.pl/event/allegro-tech-talk-37/) Ciąg dalszy naszych stacjonarnych spotkań Allegro Tech Talks, na których dzielimy się wiedzą, wzajemnie inspirujemy oraz integrujemy podczas rozmów w kuluarach. 📌…","how_to_find_us":"Biuro Allegro znajduje się w kompleksie Nowy Rynek w budynku D. Najbliższy przystanek to Wierzbięcice i kursują tu linie tramwajowe numer 2, 5, 6, 10, 12, 18. ","visibility":"public","member_pay_fee":false},{"created":1682779438000,"duration":9000000,"id":"293215214","name":"AlleKwanty: o komputerach przyszłości, które na Allegro dopiero będą mieć","date_in_series_pattern":false,"status":"past","time":1684252800000,"local_date":"2023-05-16","local_time":"18:00","updated":1684266490000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":41,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. Żelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293215214/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-kwanty/](https://app.evenea.pl/event/allegro-tech-kwanty/) Allegro Tech to miejsce, w którym nasi inżynierowie dzielą się wiedzą oraz case study z wybranych projektów w firmie - w formie…","how_to_find_us":"The Allegro office is located in Norblin Factory (entrance Plater 3, from Żelazna Street). You can check the details of the journey (buses, trams, metro) at: https://fabrykanorblina.pl/dojazd/","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"O roli analityków biznesowych w Allegro","link":"https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/","pubDate":"Thu, 24 Aug 2023 00:00:00 GMT","content":"Czym zajmują się analitycy danych w Allegro i za jakie projekty odpowiadają? Z jakich rodzajów danych i narzędzi korzystają w codziennej pracy? Jakie (przykładowe) obszary tematyczne pokrywamy danymi, które analizujemy w Allegro? Jakich umiejętności szukamy u analityków biznesowych w Allegro i jak można do nas dołączyć? O roli analityków biznesowych i pracy w skali Allegro opowiadają Jakub Król i Mateusz Falkowski - Senior Data Analysts w Allegro.","contentSnippet":"Czym zajmują się analitycy danych w Allegro i za jakie projekty odpowiadają? Z jakich rodzajów danych i narzędzi korzystają w codziennej pracy? Jakie (przykładowe) obszary tematyczne pokrywamy danymi, które analizujemy w Allegro? Jakich umiejętności szukamy u analityków biznesowych w Allegro i jak można do nas dołączyć? O roli analityków biznesowych i pracy w skali Allegro opowiadają Jakub Król i Mateusz Falkowski - Senior Data Analysts w Allegro.","guid":"https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/","isoDate":"2023-08-24T00:00:00.000Z"},{"title":"O społeczności Allegro Tech i rozwoju inżynierów w Allegro","link":"https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/","pubDate":"Thu, 27 Jul 2023 00:00:00 GMT","content":"Na czym polega rola Principal Software Engineera w Allegro oraz co ma wspólnego z rozwijaniem siebie i dzieleniem się wiedzą? Co warto wiedzieć o turystyce, która pojawia się niemal w każdym odcinku naszych podcastów? Na czym polega, kto, kiedy i jak może z niej skorzystać? Jak pracujemy z talentami Gallupa (także w zespołach technicznych)?  Co dają nam wewnętrzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy się doświadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przyświeca? O społeczności Allegro Tech i możliwościach rozwoju w Allegro z perspektywy inżynierów rozmawialiśmy z Marcinem Turkiem i Michałem Kosmulskim.","contentSnippet":"Na czym polega rola Principal Software Engineera w Allegro oraz co ma wspólnego z rozwijaniem siebie i dzieleniem się wiedzą? Co warto wiedzieć o turystyce, która pojawia się niemal w każdym odcinku naszych podcastów? Na czym polega, kto, kiedy i jak może z niej skorzystać? Jak pracujemy z talentami Gallupa (także w zespołach technicznych)?  Co dają nam wewnętrzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy się doświadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przyświeca? O społeczności Allegro Tech i możliwościach rozwoju w Allegro z perspektywy inżynierów rozmawialiśmy z Marcinem Turkiem i Michałem Kosmulskim.","guid":"https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/","isoDate":"2023-07-27T00:00:00.000Z"},{"title":"O Data Science Hub w Allegro","link":"https://podcast.allegro.tech/o-data-science-hub-w-allegro/","pubDate":"Fri, 14 Jul 2023 00:00:00 GMT","content":"Co kryje się pod pojęciem Data Science Hub w Allegro? Jakie działania rozwijamy w tym obszarze i jak oceniamy ich potencjał? O czym jest projekt Wilson i na czym skupiamy się w projekcie przewidywania zakupów cyklicznych? Jak wykorzystujemy sztuczną inteligencję i gdzie jest dla niej miejsce wśród naszych kierunków rozwoju? O AI Transformation, poczuciu sprawczości, mieszance kompetencji i talentów zamkniętej w rolach Data Scientist, Data Engineer i Data Analyst rozmawialiśmy z Karoliną Nieradką i Kamilem Konikiewiczem.,","contentSnippet":"Co kryje się pod pojęciem Data Science Hub w Allegro? Jakie działania rozwijamy w tym obszarze i jak oceniamy ich potencjał? O czym jest projekt Wilson i na czym skupiamy się w projekcie przewidywania zakupów cyklicznych? Jak wykorzystujemy sztuczną inteligencję i gdzie jest dla niej miejsce wśród naszych kierunków rozwoju? O AI Transformation, poczuciu sprawczości, mieszance kompetencji i talentów zamkniętej w rolach Data Scientist, Data Engineer i Data Analyst rozmawialiśmy z Karoliną Nieradką i Kamilem Konikiewiczem.,","guid":"https://podcast.allegro.tech/o-data-science-hub-w-allegro/","isoDate":"2023-07-14T00:00:00.000Z"},{"title":"O technologiach i projektach w Allegro Pay","link":"https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/","pubDate":"Thu, 29 Jun 2023 00:00:00 GMT","content":"Jak powstała usługa Allegro Pay i co ma wspólnego z ratatouille? Jakie projekty i technologie stoją za tym rozwiązaniem? Jak to jest pracować w Azure i obsługiwać ruch, który generuje Allegro? Czym inżynierów może zaskoczyć praca w Allegro Pay i co czeka na nich (na przykład) w programie All4Customer? O migrowaniu baz CosmosDB, wymaganiach skali i dostępności, a także o rozwijaniu ludzi i technologii rozmawialiśmy z Mariuszem Budzynem i Tomaszem Szczerbą. Zapraszamy do słuchania! na różnych płaszczyznach?","contentSnippet":"Jak powstała usługa Allegro Pay i co ma wspólnego z ratatouille? Jakie projekty i technologie stoją za tym rozwiązaniem? Jak to jest pracować w Azure i obsługiwać ruch, który generuje Allegro? Czym inżynierów może zaskoczyć praca w Allegro Pay i co czeka na nich (na przykład) w programie All4Customer? O migrowaniu baz CosmosDB, wymaganiach skali i dostępności, a także o rozwijaniu ludzi i technologii rozmawialiśmy z Mariuszem Budzynem i Tomaszem Szczerbą. Zapraszamy do słuchania! na różnych płaszczyznach?","guid":"https://podcast.allegro.tech/o-technologiach-i-projektach-w-allegro-pay/","isoDate":"2023-06-29T00:00:00.000Z"}]},"__N_SSG":true},"page":"/","query":{},"buildId":"e0cwA16mXqsL06qi5gBqu","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>