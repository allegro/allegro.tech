{"pageProps":{"posts":[{"title":"Clean Architecture Story","link":"https://blog.allegro.tech/2021/12/clean-architecture-story.html","pubDate":"Mon, 13 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Michał Kowalcze"],"photo":["https://blog.allegro.tech/img/authors/michal.kowalcze.jpg"],"url":["https://blog.allegro.tech/authors/michal.kowalcze"]}]},"content":"<p><a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">The Clean Architecture</a> concept has been\naround for some time and keeps surfacing in one place or another, yet it is not widely adopted. In this post I would\nlike to introduce this topic in a less conventional way: starting with customer’s needs and going through various\nstages to present a solution that is clean enough to satisfy concepts from the aforementioned blog (or\n<a href=\"https://www.goodreads.com/book/show/18043011-clean-architecture\">the book</a> with the same name).</p>\n\n<h2 id=\"the-perspective\">The perspective</h2>\n<p>Why do we need software architecture? What is it anyway? An extensive definition can be found in a place a bit unexpected\nfor an agile world — an enterprise-architecture definition from <a href=\"https://en.wikipedia.org/wiki/The_Open_Group_Architecture_Framework\">TOGAF</a>:</p>\n\n<ul>\n  <li>The fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and\nin the principles of its design and evolution. (Source: ISO/IEC/IEEE 42010:2011)</li>\n  <li>The structure of components, their inter-relationships, and the principles and guidelines governing their design and\nevolution over time.</li>\n</ul>\n\n<p>And what do we need such a governing structure or shape for? Basically it allows us to make cost/time-efficient choices\nwhen it comes to development. And deployment. And operation. And maintenance.</p>\n\n<p>It also allows us to keep as many options open as possible, so our future choices are not limited by an overcommitment\nfrom the past.</p>\n\n<p>So — we have our perspective defined. Let’s dive into a real-world problem!</p>\n\n<h2 id=\"the-challenge\">The challenge</h2>\n<p>You are a young, promising programmer sitting in a dorm and one afternoon a stranger appears. “I run a small company\nthat delivers packages from furniture shops to customers. I need a database that will allow reservation of slots. Is it\nsomething you are able to deliver?” “Of course!” — what else could a young, promising programmer answer?</p>\n\n<h2 id=\"the-false-start\">The false start</h2>\n<p>The customer needs a database, so what can we start with? The database schema, of course! We can identify entities with\nease: a transport slot, a schedule, a user (we need some authentication, right?), a … something? Okay, perhaps it is\nnot the easiest way. So why don’t we start with something else?</p>\n\n<p>Let’s choose the technology to use! Let’s go with React frontend, Java+Spring backend, some SQL as persistence. To\npresent a clickable version to our customer we need some warm-up work to set up an environment, create a deployable\nservice version or GUI mockups, configure persistence and so on. In general: to pay attention to technical details —\ncode necessary to set up something working, of which non-devs are usually not aware. It simply has to be done before we\nstart talking about nitty-gritty for business logic.</p>\n\n<h2 id=\"the-use-case-driven-approach\">The use-case-driven approach</h2>\n<p>What if instead of starting with what we already know — how to visualize relationships, how to build a web-system — we\nstarted with what we didn’t know? Simply — by asking questions such as: How is the system going to be used? By whom?</p>\n\n<h2 id=\"use-cases\">Use cases</h2>\n<p>In other words — what are the use cases for the system? Let’s define the challenge once more using high-level actors\nand interactions: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_cases.png\" alt=\"Use cases\" /> and pick the first\nrequired interaction: shop makes a reservation. What is required to make a reservation? Hmm, I think that it would be\ngood to get the current schedule in the first place. Why am I using “get” instead of “display”? “Display” already\nsuggests a way of delivering output, when we hear “display” a computer screen comes to our minds, with a web\napplication. Single page web app, of course. “Get” is more neutral, it does not constrain our vision by a specific\npresentation method. Frankly — is there anything wrong with delivering the current schedule over the phone, for\nexample?</p>\n\n<h3 id=\"getting-the-schedule\">Getting the schedule</h3>\n<p>So, we can start thinking about our schedule model — let it be a single instance representing a day with slots inside.\nGreat, we have our entities! How to get one? Well, we need to check if there is already a stored schedule and if so\n— retrieve it from the repository. If the schedule is not available we have to create one. Based on…? Exactly — we do\nnot know yet, all we can say is that it will probably be something flexible. Something to discuss with our customer\n— but this does not prevent us from going forward with our first use case. Logic is indeed simple:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">:</span> <span class=\"nc\">LocalDate</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">{</span>\n  <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">daySchedulerRepository</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n  <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">daySchedule</span> <span class=\"p\">!=</span> <span class=\"k\">null</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">daySchedule</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">val</span> <span class=\"py\">newSchedule</span> <span class=\"p\">=</span> <span class=\"n\">dayScheduleCreator</span><span class=\"p\">.</span><span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">daySchedulerRepository</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">newSchedule</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/6dfeee53554a4ccf37e81aa50a2bd24af7e02cce\">GitHub</a>)</p>\n\n<p>And even with this simple logic we identified a hidden assumption regarding the schedule definition: that there is a\nrecipe for creating a daily schedule. What is more we can test retrieval of a schedule — with definition of schedule\ncreator if required — without any irrelevant details, like database, UI, framework and so on. Test only business rules,\nwithout unnecessary details.</p>\n\n<h2 id=\"reserving-the-slot\">Reserving the slot</h2>\n<p>To finish the reservation we have to add at least one more use case — one for reservation of a free slot. Provided that\nwe re-use existing logic, the interaction is still simple:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">reserve</span><span class=\"p\">(</span><span class=\"n\">slotId</span><span class=\"p\">:</span> <span class=\"nc\">SlotId</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">{</span>\n  <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">getScheduleUseCase</span><span class=\"p\">.</span><span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span> <span class=\"p\">=</span> <span class=\"n\">slotId</span><span class=\"p\">.</span><span class=\"n\">day</span><span class=\"p\">)</span>\n\n  <span class=\"kd\">val</span> <span class=\"py\">modifiedSchedule</span> <span class=\"p\">=</span> <span class=\"n\">daySchedule</span><span class=\"p\">.</span><span class=\"nf\">reserveSlot</span><span class=\"p\">(</span><span class=\"n\">slotId</span><span class=\"p\">.</span><span class=\"n\">index</span><span class=\"p\">)</span>\n\n  <span class=\"k\">return</span> <span class=\"n\">dayScheduleRepository</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">modifiedSchedule</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/7b7961b28107c3c89d40ce69a8383bf9f32337b0\">GitHub</a>)</p>\n\n<p>And, as we can see — the slot reservation business rule (and constraint) is implemented at the domain model itself — so\nwe are safe, that any other interaction, any other use case, is not going to break these rules. This approach also\nsimplifies testing, as business rules can be verified in separation from the use case interaction logic.</p>\n\n<h2 id=\"where-is-the-clean-architecture\">Where is the “Clean Architecture”?</h2>\n<p>Let‘s stop with business logic for a moment. We created quite thoughtful, extensible code for sure, but why are we\ntalking about “Clean” architecture? We already used Domain-Driven Design and Hexagonal architecture concepts. Is there\nanything more? Imagine that another person is going to help us with implementation. She is not aware of the source code\nyet and simply would like to take a look at the codebase. And she sees: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_case_classes.png\" alt=\"Use case classes\" />\nIt looks like something to her, doesn‘t it? A kind of reservation system! It is not yet another domain service with\nsome methods that have no clear connection with possible uses — the list of classes itself describes what the system\ncan do.</p>\n\n<h2 id=\"the-first-assumption\">The first assumption</h2>\n<p>We have a mocked implementation as the schedule creator. It is OK to test logic at the unit test level, but not enough\nto run a prototype.</p>\n\n<p>After a short call with our customer we know more about the daily schedule — there are six slots, two hours each,\nstarting at 8:oo a.m. We also know that this recipe for the daily schedule is very, very simple and it is going to be\nchanged soon (e.g. to accommodate for holidays, etc.). All these issues will be solved later, now we are at the\nprototype stage and our desired outcome is to have a working demo for our stranger.</p>\n\n<p>Where to put this simple implementation of the schedule creator? So far, the domain used an interface for that. Are we\ngoing to put an implementation of this interface to the infrastructure package and treat it as something outside the\ndomain? Certainly not! It is not complicated and this is part of the domain itself, we simply replace the mocked\nimplementation of the schedule creator with class specification.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">package</span> <span class=\"nn\">eu.kowalcze.michal.arch.clean.example.domain.model</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">DayScheduleCreator</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">:</span> <span class=\"nc\">LocalDate</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">=</span> <span class=\"nc\">DaySchedule</span><span class=\"p\">(</span>\n        <span class=\"n\">scheduleDay</span><span class=\"p\">,</span>\n        <span class=\"nf\">createStandardSlots</span><span class=\"p\">()</span>\n    <span class=\"p\">)</span>\n<span class=\"c1\">//...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/2792fc31e98d76a610561636f03073dee73fbb47\">GitHub</a>)</p>\n\n<h2 id=\"the-prototype\">The prototype</h2>\n<p>I will not be original here — for the first prototype version the REST API sounds like something reasonable. Do we care\nabout other infrastructure at the moment? Persistence? No! In the previous commits a map-based persistence layer is\nused for unit tests and this solution is good enough to start with. As long as the system is not restarted, of course.</p>\n\n<p>What is important at this stage? We are introducing an <strong>API</strong> — this is a separate layer, so it is crucial to ensure\nthat domain classes are not exposed to the outside world — and that we do not introduce a dependency on the API into\nthe domain.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">package</span> <span class=\"nn\">eu.kowalcze.michal.arch.clean.example.api</span>\n\n<span class=\"nd\">@Controller</span>\n<span class=\"kd\">class</span> <span class=\"nc\">GetScheduleEndpoint</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">getScheduleUseCase</span><span class=\"p\">:</span> <span class=\"nc\">GetScheduleUseCase</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@GetMapping</span><span class=\"p\">(</span><span class=\"s\">\"/schedules/{localDate}\"</span><span class=\"p\">)</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">getSchedules</span><span class=\"p\">(</span><span class=\"nd\">@PathVariable</span> <span class=\"n\">localDate</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">DayScheduleDto</span> <span class=\"p\">{</span>\n        <span class=\"kd\">val</span> <span class=\"py\">scheduleDay</span> <span class=\"p\">=</span> <span class=\"nc\">LocalDate</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">localDate</span><span class=\"p\">)</span>\n        <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">getScheduleUseCase</span><span class=\"p\">.</span><span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">daySchedule</span><span class=\"p\">.</span><span class=\"nf\">toApi</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/b1d1c3fe3901d9328bdfaf560331d35131f8224b\">GitHub</a>)</p>\n\n<h2 id=\"the-abstractions\">The abstractions</h2>\n<h3 id=\"use-case\">Use Case</h3>\n<p>Checking the implementation of endpoints (see comments in the code) we can see that conceptually each endpoint executes\nlogic according to the same structure: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_case_flow.png\" alt=\"Use case flow\" />\nWell, why don’t we make some abstraction for this? Sounds like a crazy idea? Let‘s check! Based on our code and the\ndiagram above we can identify the <code class=\"language-plaintext highlighter-rouge\">UseCase</code> abstraction — something that takes some input (domain input, to be precise)\nand converts it to a (domain) output.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">interface</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">INPUT</span><span class=\"p\">):</span> <span class=\"nc\">OUTPUT</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/006811b49ae4531b96b300c964d3a66d725183bf\">GitHub</a>)</p>\n\n<h3 id=\"use-case-executor\">Use Case Executor</h3>\n<p>Great! We have use cases and I just realized that I would like to have an email in my inbox each time an exception is\nthrown — and I do not want to depend on a spring-specific mechanism to do this. A common <code class=\"language-plaintext highlighter-rouge\">UseCaseExecutor</code> will be a\ngreat help to address this non-functional requirement.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">notificationGateway</span><span class=\"p\">:</span> <span class=\"nc\">NotificationGateway</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;,</span> <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">INPUT</span><span class=\"p\">):</span> <span class=\"nc\">OUTPUT</span> <span class=\"p\">{</span>\n        <span class=\"k\">try</span> <span class=\"p\">{</span>\n            <span class=\"k\">return</span> <span class=\"n\">useCase</span><span class=\"p\">.</span><span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">Exception</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"n\">notificationGateway</span><span class=\"p\">.</span><span class=\"nf\">notify</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">)</span>\n            <span class=\"k\">throw</span> <span class=\"n\">e</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/54d3187aed94427bb60af9781d0eec573c8c8db0\">GitHub</a>)</p>\n\n<h3 id=\"framework-independent-response\">Framework-independent response</h3>\n<p>In order to handle the next requirements in our plan we have to change the logic a bit — add the possibility of returning\nspring-specific response entities from the executor itself. To make our code reusable in a non-spring world (ktor,\nanyone?) we separated the plain executor from spring specific decorator, so that it is possible to use this code easily\nin other frameworks.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">data class</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;(</span>\n    <span class=\"kd\">val</span> <span class=\"py\">responseCode</span><span class=\"p\">:</span> <span class=\"nc\">Int</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">output</span><span class=\"p\">:</span> <span class=\"nc\">API_OUTPUT</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">SpringUseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">useCaseExecutor</span><span class=\"p\">:</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">,</span> <span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n        <span class=\"n\">toApiConversion</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">)</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span>\n    <span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"n\">useCaseExecutor</span><span class=\"p\">.</span><span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">,</span> <span class=\"n\">input</span><span class=\"p\">,</span> <span class=\"n\">toApiConversion</span><span class=\"p\">).</span><span class=\"nf\">toSpringResponse</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;.</span><span class=\"nf\">toSpringResponse</span><span class=\"p\">():</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"nc\">ResponseEntity</span><span class=\"p\">.</span><span class=\"nf\">status</span><span class=\"p\">(</span><span class=\"n\">responseCode</span><span class=\"p\">).</span><span class=\"nf\">body</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/d44f7f993fab2e749e3048561b3ac4d3cff6fd88\">GitHub</a>)</p>\n\n<h3 id=\"handle-domain-exceptions\">Handle domain exceptions</h3>\n<p>Ooops. Our prototype is running and we observe exceptions resulting in HTTP 500 errors. It would be nice to convert\nthese to dedicated response codes in a reasonable way yet without using much of spring infrastructure, for simplified\nmaintenance (and possible future changes). This can be easily achieved by adding another parameter to use case\nexecution, like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">notificationGateway</span><span class=\"p\">:</span> <span class=\"nc\">NotificationGateway</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n        <span class=\"n\">toApiConversion</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">)</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">handledExceptions</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"nc\">ExceptionHandler</span><span class=\"p\">.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">Any</span><span class=\"p\">)?</span> <span class=\"p\">=</span> <span class=\"k\">null</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n\n        <span class=\"k\">try</span> <span class=\"p\">{</span>\n            <span class=\"kd\">val</span> <span class=\"py\">domainOutput</span> <span class=\"p\">=</span> <span class=\"n\">useCase</span><span class=\"p\">.</span><span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"nf\">toApiConversion</span><span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">Exception</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"c1\">// conceptual logic</span>\n            <span class=\"kd\">val</span> <span class=\"py\">exceptionHandler</span> <span class=\"p\">=</span> <span class=\"nc\">ExceptionHandler</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span>\n            <span class=\"n\">handledExceptions</span><span class=\"o\">?.</span><span class=\"nf\">let</span> <span class=\"p\">{</span> <span class=\"n\">exceptionHandler</span><span class=\"p\">.</span><span class=\"nf\">handledExceptions</span><span class=\"p\">()</span> <span class=\"p\">}</span>\n            <span class=\"k\">return</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">(</span><span class=\"n\">responseCodeIfExceptionIsHandled</span><span class=\"p\">,</span> <span class=\"n\">exceptionHandler</span><span class=\"p\">.</span><span class=\"n\">message</span> <span class=\"o\">?:</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/ac6763f19e2f3f61adc1f8b02bab6cb1e1a65c11\">GitHub</a>)</p>\n\n<h3 id=\"handle-dto-conversion-exceptions\">Handle DTO conversion exceptions</h3>\n<p>By simply replacing input with:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">inputProvider</span><span class=\"p\">:</span> <span class=\"nc\">Any</span><span class=\"p\">.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/a9ef4bb835977a4bd4a62eb754d8563340bd3d4e\">GitHub</a>)</p>\n\n<p>we are able to handle exceptions raised during creation of input domain objects in a uniform way, without any\nadditional try/catches at the endpoint level.</p>\n\n<h2 id=\"the-outcome\">The outcome</h2>\n\n<p>What is the result of our journey across some functional requirements and a bit more non-functional requirements? By\nlooking at the definition of an endpoint we have full documentation of its behaviour, including exceptions. Our code is\neasily portable to some different API (e.g. EJB), we have fully-auditable modifications, and we can exchange layers\nquite freely. Also analysis of whole service is simplified, as possible use cases are explicitely stated.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@PutMapping</span><span class=\"p\">(</span><span class=\"s\">\"/schedules/{localDate}/{index}\"</span><span class=\"p\">,</span> <span class=\"n\">produces</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s\">\"application/json\"</span><span class=\"p\">],</span> <span class=\"n\">consumes</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s\">\"application/json\"</span><span class=\"p\">])</span>\n<span class=\"k\">fun</span> <span class=\"nf\">getSchedules</span><span class=\"p\">(</span><span class=\"nd\">@PathVariable</span> <span class=\"n\">localDate</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"nd\">@PathVariable</span> <span class=\"n\">index</span><span class=\"p\">:</span> <span class=\"nc\">Int</span><span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"n\">useCaseExecutor</span><span class=\"p\">.</span><span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span> <span class=\"p\">=</span> <span class=\"n\">reserveSlotUseCase</span><span class=\"p\">,</span>\n        <span class=\"n\">inputProvider</span> <span class=\"p\">=</span> <span class=\"p\">{</span> <span class=\"nc\">SlotId</span><span class=\"p\">(</span><span class=\"nc\">LocalDate</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">localDate</span><span class=\"p\">),</span> <span class=\"n\">index</span><span class=\"p\">)</span> <span class=\"p\">},</span>\n        <span class=\"n\">toApiConversion</span> <span class=\"p\">=</span> <span class=\"p\">{</span>\n            <span class=\"kd\">val</span> <span class=\"py\">dayScheduleDto</span> <span class=\"p\">=</span> <span class=\"n\">it</span><span class=\"p\">.</span><span class=\"nf\">toApi</span><span class=\"p\">()</span>\n            <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">(</span><span class=\"nc\">HttpServletResponse</span><span class=\"p\">.</span><span class=\"nc\">SC_ACCEPTED</span><span class=\"p\">,</span> <span class=\"n\">dayScheduleDto</span><span class=\"p\">)</span>\n        <span class=\"p\">},</span>\n        <span class=\"n\">handledExceptions</span> <span class=\"p\">=</span> <span class=\"p\">{</span>\n            <span class=\"nf\">exception</span><span class=\"p\">(</span><span class=\"nc\">InvalidSlotIndexException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">,</span> <span class=\"nc\">UNPROCESSABLE_ENTITY</span><span class=\"p\">,</span> <span class=\"s\">\"INVALID-SLOT-ID\"</span><span class=\"p\">)</span>\n            <span class=\"nf\">exception</span><span class=\"p\">(</span><span class=\"nc\">SlotAlreadyReservedException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">,</span> <span class=\"nc\">CONFLICT</span><span class=\"p\">,</span> <span class=\"s\">\"SLOT-ALREADY-RESERVED\"</span><span class=\"p\">)</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>(repository: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example\">GitHub</a>)</p>\n\n<p>A simple evaluation of our solution with measures mentioned at the beginning:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left\">Aspect</th>\n      <th style=\"text-align: left\">Evaluation</th>\n      <th style=\"text-align: center\">Has advantage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left\">Development</td>\n      <td style=\"text-align: left\"><code class=\"language-plaintext highlighter-rouge\">UseCase</code> abstraction forces unification of approach across different teams in a more significant way than standard service approach.</td>\n      <td style=\"text-align: center\">✓</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Deployment</td>\n      <td style=\"text-align: left\">We did not consider deployment in our example. It certainly is not going to be different/harder than in case of hexagonal architecture.</td>\n      <td style=\"text-align: center\"> </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Operation</td>\n      <td style=\"text-align: left\">Use case-based approach reveals operation of the system, which reduces learning curve for both development and maintenance.</td>\n      <td style=\"text-align: center\">✓</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Maintenance</td>\n      <td style=\"text-align: left\">Entry threshold might be lower compared to hexagonal approach, as service is separated horizontally (into layers) and vertically (into use cases with common domain model).</td>\n      <td style=\"text-align: center\">✓</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Keeping options open</td>\n      <td style=\"text-align: left\">Similar to hexagonal architecture approach.</td>\n      <td style=\"text-align: center\"> </td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"tldr\">TL;DR</h3>\n<p>It is like hexagonal architecture with one additional dimension, composed of use cases, giving better insight into\noperations of a system and streamlining development and maintenance. Solution that was created during this narrative\nallows for creation of a self-documenting API endpoint.</p>\n\n<h2 id=\"high-level-overview\">High-level overview</h2>\n<p>With all this read we can switch our view to the high-level perspective:</p>\n\n<p><img src=\"/img/articles/2021-12-13-clean-architecture-story/clean_architecture_diagram.png\" alt=\"The Clean Architecture Diagram\" /></p>\n\n<p>and describe abstractions. Starting from the inside we have:</p>\n<ul>\n  <li><strong>Domain Model</strong>, <strong>Services</strong> and <strong>Gateways</strong>, which are responsible for defining\nbusiness rules for the domain.</li>\n  <li><strong>UseCase</strong>, which orchestrates execution of business rules.</li>\n  <li><strong>UseCaseExecutor</strong> providing common behavior for all use cases.</li>\n  <li><strong>API</strong> connecting service with the outside world.</li>\n  <li><strong>Implementation of gateways</strong>, which connects with other services or persistence providers.</li>\n  <li><strong>Configuration</strong>, responsible for gluing all elements together.</li>\n</ul>\n\n<p>I hope that you enjoy this simple story and find the concept of\n<a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">the Clean Architecture</a> useful.\nThank you for reading!</p>\n","contentSnippet":"The Clean Architecture concept has been\naround for some time and keeps surfacing in one place or another, yet it is not widely adopted. In this post I would\nlike to introduce this topic in a less conventional way: starting with customer’s needs and going through various\nstages to present a solution that is clean enough to satisfy concepts from the aforementioned blog (or\nthe book with the same name).\nThe perspective\nWhy do we need software architecture? What is it anyway? An extensive definition can be found in a place a bit unexpected\nfor an agile world — an enterprise-architecture definition from TOGAF:\nThe fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and\nin the principles of its design and evolution. (Source: ISO/IEC/IEEE 42010:2011)\nThe structure of components, their inter-relationships, and the principles and guidelines governing their design and\nevolution over time.\nAnd what do we need such a governing structure or shape for? Basically it allows us to make cost/time-efficient choices\nwhen it comes to development. And deployment. And operation. And maintenance.\nIt also allows us to keep as many options open as possible, so our future choices are not limited by an overcommitment\nfrom the past.\nSo — we have our perspective defined. Let’s dive into a real-world problem!\nThe challenge\nYou are a young, promising programmer sitting in a dorm and one afternoon a stranger appears. “I run a small company\nthat delivers packages from furniture shops to customers. I need a database that will allow reservation of slots. Is it\nsomething you are able to deliver?” “Of course!” — what else could a young, promising programmer answer?\nThe false start\nThe customer needs a database, so what can we start with? The database schema, of course! We can identify entities with\nease: a transport slot, a schedule, a user (we need some authentication, right?), a … something? Okay, perhaps it is\nnot the easiest way. So why don’t we start with something else?\nLet’s choose the technology to use! Let’s go with React frontend, Java+Spring backend, some SQL as persistence. To\npresent a clickable version to our customer we need some warm-up work to set up an environment, create a deployable\nservice version or GUI mockups, configure persistence and so on. In general: to pay attention to technical details —\ncode necessary to set up something working, of which non-devs are usually not aware. It simply has to be done before we\nstart talking about nitty-gritty for business logic.\nThe use-case-driven approach\nWhat if instead of starting with what we already know — how to visualize relationships, how to build a web-system — we\nstarted with what we didn’t know? Simply — by asking questions such as: How is the system going to be used? By whom?\nUse cases\nIn other words — what are the use cases for the system? Let’s define the challenge once more using high-level actors\nand interactions:  and pick the first\nrequired interaction: shop makes a reservation. What is required to make a reservation? Hmm, I think that it would be\ngood to get the current schedule in the first place. Why am I using “get” instead of “display”? “Display” already\nsuggests a way of delivering output, when we hear “display” a computer screen comes to our minds, with a web\napplication. Single page web app, of course. “Get” is more neutral, it does not constrain our vision by a specific\npresentation method. Frankly — is there anything wrong with delivering the current schedule over the phone, for\nexample?\nGetting the schedule\nSo, we can start thinking about our schedule model — let it be a single instance representing a day with slots inside.\nGreat, we have our entities! How to get one? Well, we need to check if there is already a stored schedule and if so\n— retrieve it from the repository. If the schedule is not available we have to create one. Based on…? Exactly — we do\nnot know yet, all we can say is that it will probably be something flexible. Something to discuss with our customer\n— but this does not prevent us from going forward with our first use case. Logic is indeed simple:\n\nfun getSchedule(scheduleDay: LocalDate): DaySchedule {\n  val daySchedule = daySchedulerRepository.get(scheduleDay)\n  if (daySchedule != null) {\n    return daySchedule\n  }\n\n  val newSchedule = dayScheduleCreator.create(scheduleDay)\n  return daySchedulerRepository.save(newSchedule)\n}\n\n\n(full commit: GitHub)\nAnd even with this simple logic we identified a hidden assumption regarding the schedule definition: that there is a\nrecipe for creating a daily schedule. What is more we can test retrieval of a schedule — with definition of schedule\ncreator if required — without any irrelevant details, like database, UI, framework and so on. Test only business rules,\nwithout unnecessary details.\nReserving the slot\nTo finish the reservation we have to add at least one more use case — one for reservation of a free slot. Provided that\nwe re-use existing logic, the interaction is still simple:\n\nfun reserve(slotId: SlotId): DaySchedule {\n  val daySchedule = getScheduleUseCase.getSchedule(scheduleDay = slotId.day)\n\n  val modifiedSchedule = daySchedule.reserveSlot(slotId.index)\n\n  return dayScheduleRepository.save(modifiedSchedule)\n}\n\n\n(full commit: GitHub)\nAnd, as we can see — the slot reservation business rule (and constraint) is implemented at the domain model itself — so\nwe are safe, that any other interaction, any other use case, is not going to break these rules. This approach also\nsimplifies testing, as business rules can be verified in separation from the use case interaction logic.\nWhere is the “Clean Architecture”?\nLet‘s stop with business logic for a moment. We created quite thoughtful, extensible code for sure, but why are we\ntalking about “Clean” architecture? We already used Domain-Driven Design and Hexagonal architecture concepts. Is there\nanything more? Imagine that another person is going to help us with implementation. She is not aware of the source code\nyet and simply would like to take a look at the codebase. And she sees: \nIt looks like something to her, doesn‘t it? A kind of reservation system! It is not yet another domain service with\nsome methods that have no clear connection with possible uses — the list of classes itself describes what the system\ncan do.\nThe first assumption\nWe have a mocked implementation as the schedule creator. It is OK to test logic at the unit test level, but not enough\nto run a prototype.\nAfter a short call with our customer we know more about the daily schedule — there are six slots, two hours each,\nstarting at 8:oo a.m. We also know that this recipe for the daily schedule is very, very simple and it is going to be\nchanged soon (e.g. to accommodate for holidays, etc.). All these issues will be solved later, now we are at the\nprototype stage and our desired outcome is to have a working demo for our stranger.\nWhere to put this simple implementation of the schedule creator? So far, the domain used an interface for that. Are we\ngoing to put an implementation of this interface to the infrastructure package and treat it as something outside the\ndomain? Certainly not! It is not complicated and this is part of the domain itself, we simply replace the mocked\nimplementation of the schedule creator with class specification.\n\npackage eu.kowalcze.michal.arch.clean.example.domain.model\n\nclass DayScheduleCreator {\n    fun create(scheduleDay: LocalDate): DaySchedule = DaySchedule(\n        scheduleDay,\n        createStandardSlots()\n    )\n//...\n}\n\n\n(full commit: GitHub)\nThe prototype\nI will not be original here — for the first prototype version the REST API sounds like something reasonable. Do we care\nabout other infrastructure at the moment? Persistence? No! In the previous commits a map-based persistence layer is\nused for unit tests and this solution is good enough to start with. As long as the system is not restarted, of course.\nWhat is important at this stage? We are introducing an API — this is a separate layer, so it is crucial to ensure\nthat domain classes are not exposed to the outside world — and that we do not introduce a dependency on the API into\nthe domain.\n\npackage eu.kowalcze.michal.arch.clean.example.api\n\n@Controller\nclass GetScheduleEndpoint(private val getScheduleUseCase: GetScheduleUseCase) {\n\n    @GetMapping(\"/schedules/{localDate}\")\n    fun getSchedules(@PathVariable localDate: String): DayScheduleDto {\n        val scheduleDay = LocalDate.parse(localDate)\n        val daySchedule = getScheduleUseCase.getSchedule(scheduleDay)\n        return daySchedule.toApi()\n    }\n\n}\n\n\n(full commit: GitHub)\nThe abstractions\nUse Case\nChecking the implementation of endpoints (see comments in the code) we can see that conceptually each endpoint executes\nlogic according to the same structure: \nWell, why don’t we make some abstraction for this? Sounds like a crazy idea? Let‘s check! Based on our code and the\ndiagram above we can identify the UseCase abstraction — something that takes some input (domain input, to be precise)\nand converts it to a (domain) output.\n\ninterface UseCase<INPUT, OUTPUT> {\n    fun apply(input: INPUT): OUTPUT\n}\n\n\n(full commit: GitHub)\nUse Case Executor\nGreat! We have use cases and I just realized that I would like to have an email in my inbox each time an exception is\nthrown — and I do not want to depend on a spring-specific mechanism to do this. A common UseCaseExecutor will be a\ngreat help to address this non-functional requirement.\n\nclass UseCaseExecutor(private val notificationGateway: NotificationGateway) {\n    fun <INPUT, OUTPUT> execute(useCase: UseCase<INPUT, OUTPUT>, input: INPUT): OUTPUT {\n        try {\n            return useCase.apply(input)\n        } catch (e: Exception) {\n            notificationGateway.notify(useCase, e)\n            throw e\n        }\n    }\n}\n\n\n(full commit: GitHub)\nFramework-independent response\nIn order to handle the next requirements in our plan we have to change the logic a bit — add the possibility of returning\nspring-specific response entities from the executor itself. To make our code reusable in a non-spring world (ktor,\nanyone?) we separated the plain executor from spring specific decorator, so that it is possible to use this code easily\nin other frameworks.\n\ndata class UseCaseApiResult<API_OUTPUT>(\n    val responseCode: Int,\n    val output: API_OUTPUT,\n)\n\nclass SpringUseCaseExecutor(private val useCaseExecutor: UseCaseExecutor) {\n    fun <DOMAIN_INPUT, DOMAIN_OUTPUT, API_OUTPUT> execute(\n        useCase: UseCase<DOMAIN_INPUT, DOMAIN_OUTPUT>,\n        input: DOMAIN_INPUT,\n        toApiConversion: (domainOutput: DOMAIN_OUTPUT) -> UseCaseApiResult<API_OUTPUT>\n    ): ResponseEntity<API_OUTPUT> {\n        return useCaseExecutor.execute(useCase, input, toApiConversion).toSpringResponse()\n    }\n}\n\nprivate fun <API_OUTPUT> UseCaseApiResult<API_OUTPUT>.toSpringResponse(): ResponseEntity<API_OUTPUT> =\n    ResponseEntity.status(responseCode).body(output)\n\n\n(full commit: GitHub)\nHandle domain exceptions\nOoops. Our prototype is running and we observe exceptions resulting in HTTP 500 errors. It would be nice to convert\nthese to dedicated response codes in a reasonable way yet without using much of spring infrastructure, for simplified\nmaintenance (and possible future changes). This can be easily achieved by adding another parameter to use case\nexecution, like this:\n\nclass UseCaseExecutor(private val notificationGateway: NotificationGateway) {\n    fun <DOMAIN_INPUT, DOMAIN_OUTPUT> execute(\n        useCase: UseCase<DOMAIN_INPUT, DOMAIN_OUTPUT>,\n        input: DOMAIN_INPUT,\n        toApiConversion: (domainOutput: DOMAIN_OUTPUT) -> UseCaseApiResult<*>,\n        handledExceptions: (ExceptionHandler.() -> Any)? = null,\n    ): UseCaseApiResult<*> {\n\n        try {\n            val domainOutput = useCase.apply(input)\n            return toApiConversion(domainOutput)\n        } catch (e: Exception) {\n            // conceptual logic\n            val exceptionHandler = ExceptionHandler(e)\n            handledExceptions?.let { exceptionHandler.handledExceptions() }\n            return UseCaseApiResult(responseCodeIfExceptionIsHandled, exceptionHandler.message ?: e.message)\n        }\n    }\n}\n\n\n(full commit: GitHub)\nHandle DTO conversion exceptions\nBy simply replacing input with:\n\ninputProvider: Any.() -> DOMAIN_INPUT,\n\n\n(full commit: GitHub)\nwe are able to handle exceptions raised during creation of input domain objects in a uniform way, without any\nadditional try/catches at the endpoint level.\nThe outcome\nWhat is the result of our journey across some functional requirements and a bit more non-functional requirements? By\nlooking at the definition of an endpoint we have full documentation of its behaviour, including exceptions. Our code is\neasily portable to some different API (e.g. EJB), we have fully-auditable modifications, and we can exchange layers\nquite freely. Also analysis of whole service is simplified, as possible use cases are explicitely stated.\n\n@PutMapping(\"/schedules/{localDate}/{index}\", produces = [\"application/json\"], consumes = [\"application/json\"])\nfun getSchedules(@PathVariable localDate: String, @PathVariable index: Int): ResponseEntity<*> =\n    useCaseExecutor.execute(\n        useCase = reserveSlotUseCase,\n        inputProvider = { SlotId(LocalDate.parse(localDate), index) },\n        toApiConversion = {\n            val dayScheduleDto = it.toApi()\n            UseCaseApiResult(HttpServletResponse.SC_ACCEPTED, dayScheduleDto)\n        },\n        handledExceptions = {\n            exception(InvalidSlotIndexException::class, UNPROCESSABLE_ENTITY, \"INVALID-SLOT-ID\")\n            exception(SlotAlreadyReservedException::class, CONFLICT, \"SLOT-ALREADY-RESERVED\")\n        },\n    )\n\n\n(repository: GitHub)\nA simple evaluation of our solution with measures mentioned at the beginning:\nAspect\n      Evaluation\n      Has advantage\n    \nDevelopment\n      UseCase abstraction forces unification of approach across different teams in a more significant way than standard service approach.\n      ✓\n    \nDeployment\n      We did not consider deployment in our example. It certainly is not going to be different/harder than in case of hexagonal architecture.\n       \n    \nOperation\n      Use case-based approach reveals operation of the system, which reduces learning curve for both development and maintenance.\n      ✓\n    \nMaintenance\n      Entry threshold might be lower compared to hexagonal approach, as service is separated horizontally (into layers) and vertically (into use cases with common domain model).\n      ✓\n    \nKeeping options open\n      Similar to hexagonal architecture approach.\n       \n    \nTL;DR\nIt is like hexagonal architecture with one additional dimension, composed of use cases, giving better insight into\noperations of a system and streamlining development and maintenance. Solution that was created during this narrative\nallows for creation of a self-documenting API endpoint.\nHigh-level overview\nWith all this read we can switch our view to the high-level perspective:\n\nand describe abstractions. Starting from the inside we have:\nDomain Model, Services and Gateways, which are responsible for defining\nbusiness rules for the domain.\nUseCase, which orchestrates execution of business rules.\nUseCaseExecutor providing common behavior for all use cases.\nAPI connecting service with the outside world.\nImplementation of gateways, which connects with other services or persistence providers.\nConfiguration, responsible for gluing all elements together.\nI hope that you enjoy this simple story and find the concept of\nthe Clean Architecture useful.\nThank you for reading!","guid":"https://blog.allegro.tech/2021/12/clean-architecture-story.html","categories":["tech","architecture","clean-architecture","ddd","kotlin"],"isoDate":"2021-12-12T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Which skills to choose in order to be more valuable","link":"https://blog.allegro.tech/2021/12/choose-your-skills.html","pubDate":"Thu, 09 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Piotr Prusiński"],"photo":["https://blog.allegro.tech/img/authors/piotr.prusinski.jpg"],"url":["https://blog.allegro.tech/authors/piotr.prusinski"]}]},"content":"<p>At some point in your career, you realize that it’s time to try to advance through the hierarchy. You think you are\ndoing a good job. You are constantly developing and learning something new. But at the same time, someone you know, with\nmuch less experience and knowledge than you, has long been higher up the hierarchy than you. Then you ask yourself:\n<em>what is wrong with me</em>? In my case, the answer turned out to be properly gathering the expectations concerning my skills\nand work.</p>\n\n<p>Below, you can read my insight about it. You will find out how you can steer your development, so that you become much\nmore valuable to your team, leader or company and how to combine all this with your talents and interests.</p>\n\n<h2 id=\"how-to-start-the-development\">How to start the development</h2>\n\n<p>As a member of a project team, you want to deliver a fully valuable solution. The key to success is to discover what’s\nbehind the word <em>valuable</em> to your client. Is it enough to gather requirements? Unfortunately not. To find out exactly\nwhat stands behind the word <em>valuable</em> you need to use a prototyping method, such as an iterative\n<a href=\"https://www.wikipedia.org/wiki/Minimum_viable_product\">MVP</a> (Minimum Viable Product) approach. For example, suppose you\nhave to paint your daughter’s room. You have several choices. You can choose the colour by yourself and paint the room\nwithout asking her for her opinion. The chance she will like the colour is small. So you will make a non-compliant\nproduct. You can bring her a sampler and ask her to choose a colour. The final colour when applied to the wall, may turn\nout to be different and then, the product will also not be as expected. You can also ask her about the colour and buy a\nfew samplers, paint a piece of wall with them and ask if any meet the requirements, if yes then paint the whole room\nwith the chosen colour, if not, buy another 5 colours and so on over and over until you get it right. This is the MVP\nprocess. First you gather requirements, then you prototype, which means you make a real product. You present it and\nreceive feedback (you learn) and decide what to do next. You repeat this process until you meet the client’s\nexpectations. This approach gives you the assurance that you have done exactly what was needed.</p>\n\n<p>Imagine that your skills and the work you do are products. We already know that to make a valuable product you need to\ngather requirements and prototype with the client. Your customer who cares most about your work and skills are the team,\nthe leader and the company you work for. However, what you do must also suit you. If you want your work and your\nknowledge to be of value to them, you need to gather their requirements first. Then quickly show the result and make\nsure it’s what they wanted. You are probably wondering what this prototyping is for in the case of your work? You have\ndone some work, it has been accepted so what more is there to talk about. There could be lots of reasons. You could have\nspent too much time on it, it’s even possible that what you did is wrong. You would have to put in a huge amount of time\nand work to improve it, which is why it was accepted as it is. Without prototyping, your work might appear to be of good\nquality to you, but the team might not be happy with it.</p>\n\n<p>Let’s start with requirements gathering. Unfortunately it is impossible to gather universal requirements for a given\nposition since these requirements are different at different companies and even teams. The same person who has certain\nskills and performs certain tasks may be a senior in one company, where in another, he or she may be a junior. In one of\nthe companies I worked for, a good programmer was required to know the products and the business, then the tenure at the\ncompany mattered as well and being a support for the business. Performing a large number of tasks was not so important.\nIn another company, there were people who knew their product very well and were an excellent support to the business.\nHowever, they did not perform their tasks on time and did not take on issues that required them to undertake a lot of\nresponsibility. They still had junior or mid-level status.</p>\n\n<p><img src=\"/img/articles/2021-12-09-choose-your-skills/expectations.png\" alt=\"Expectations\" />\nThis begs the question: how do I gather my skills requirements?</p>\n\n<h2 id=\"assess-yourself-with-some-evaluation-sheets\">Assess yourself with some evaluation sheets</h2>\n\n<p>At Allegro, in the case of programmers and testers, we have a list of expectations prepared within the leadership\ncommunity. With such a list, it is much easier to collect requirements and talk about your own development towards\npromotion (in the team or in the company).</p>\n\n<p>In my case, after working at Allegro for a while, I decided that it might be a good idea to finally get my development\non track. My leader asked me to assess myself using a requirements sheet. This allowed me to identify the current stage\nI was at. The same sheet about me was completed by my leader. Then we calibrated ourselves. This sheet is divided into 3\nmain parts:</p>\n\n<ul>\n  <li>Technical knowledge</li>\n  <li>Attitude</li>\n  <li>Product knowledge</li>\n</ul>\n\n<p>Each part has a list of skills, each with an extensive description. For example, one of the descriptions of technical\nknowledge looked like this:</p>\n\n<ul>\n  <li>You know your technological stack very well (systems, applications, languages, tools). Having a technical problem to\nsolve, you know what your technological abilities are, and you know how to use them to achieve your goal.</li>\n</ul>\n\n<p>I rated each skill on a scale of 1 to 5. Ideally, I would like each skill to be close to a 5. If I rated myself\nsomewhere at a 4 and my leader rated me a 3, during calibration, I found out that the requirements for that skill were\ndifferent from what I thought. He said, there were things I hadn’t used yet. To have a 4 at this point I need to know\nthem e.g. Cassandra database. With that I learned the first relevant requirement that I didn’t know about. After the\nwhole process of gathering requirements this way, I already knew what I needed to work on. Now all I had to do was\nselect a few of them and start working. But how to choose these requirements, which I will implement with passion and\nactually do them well?</p>\n\n<h2 id=\"how-to-choose-skills-to-improve\">How to choose skills to improve?</h2>\n\n<p>There are many strategies on how to choose skills to develop. All in all, you need to use the right method for the right\ntype of skill to get a good result. The first step is to get a mentor. Usually it will be your leader, but it can also\nbe an experienced team member, e.g. a senior.</p>\n\n<p>Leaders at Allegro gain their development knowledge from many available training sessions. You should find a similar\nperson in your environment. Such a person will help you on your way and will simulate the customer of the product that\nyou will be making. Show him or her a list of things you should develop. Ask them which are the most important for your\nteam. It is possible that the list of things will become very short. Now just pick two or three things and prepare\nprototypes. You can choose them intuitively or match them with your talents. Just take a Gallup\ntest (<a href=\"https://www.gallup.com/cliftonstrengths/en/home.aspx\">CliftonStrengths</a>) to discover them. This test will help\nyou choose a strategy for developing a particular skill. This test will work well for soft skills. For hard skills\na <a href=\"https://en.wikipedia.org/wiki/Lean_startup\">lean startup strategy</a> may be better. Below are two examples of what this\ncan look like in practice.</p>\n\n<h2 id=\"how-gallup-test-can-help-you-find-activities-that-improve-you\">How Gallup test can help you find activities that improve you</h2>\n\n<p>The <a href=\"https://www.gallup.com/cliftonstrengths/en/home.aspx\">Gallup test</a>\nhelps you find activities that you naturally enjoy doing. This natural response to certain types of information,\nsituations and people in the context of the Gallup test is called a talent. The test result is divided into four main\ndomains:</p>\n\n<ul>\n  <li>Executing</li>\n  <li>Influencing</li>\n  <li>Relationship Building</li>\n  <li>Strategic Thinking</li>\n</ul>\n\n<p>Some of these will have a direct impact on your work, others will not. For example a description of your character in\nthe context of building relationships with people will be interesting information and will certainly raise your\nawareness, but will not help us in our challenge. Other talents that come out of the challenge may fit into the skill\nyou are developing or show how you should develop it.</p>\n\n<p>Of course with these talents, the knowledge itself that you have a talent does not immediately translate into skills,\nfor example finding out that you have a talent for standing on your head, it does not mean that you will immediately\nstand on your head. You have to develop this talent, and you probably are already doing similar things because you\nsimply have a talent for it, and you enjoy it. So the Gallup test shows the things you should focus on. They are so\ngeneric that you can easily match them to your problems.</p>\n\n<p><img src=\"/img/articles/2021-12-09-choose-your-skills/gallup.png\" alt=\"Gallup\" /></p>\n\n<p>Here’s what it was like for me. I did the Gallup test, and it came out that I am a learner. I like to learn, and I\nshould share knowledge - which should make me happy. Comparing this talent to the list of skills I should develop, I\nfound out that I don’t share knowledge with others. I decided that a good form of knowledge transfer is to make\npresentations. I completed training in making presentations. I also estimated the time for preparation and set a\ndeadline. While preparing, I realized that this knowledge sharing strategy was not for me. I finished this presentation\nand decided to change my knowledge sharing strategy. Such a change in strategy is called a pivot. I discussed the\nproblem with my mentor and decided to make a pivot towards writing articles. Being in this situation you have two\nchoices: either you do a pivot, which means you change your strategy, or you persevere, which means you continue to\ndevelop using your current strategy, after getting proper feedback from your mentor and team of course. Let us now look\nat the second type of skill.</p>\n\n<h2 id=\"how-startup-method-can-help-you-develop-your-skills\">How startup method can help you develop your skills</h2>\n\n<p>Let us divide hard skills into two groups. In the first group there are skills you acquire while learning by yourself,\ne.g. a new programming language. For this you need internal motivation and discipline to achieve the expected high\nlevel. The second group is the one where you acquire skills automatically e.g. you use some framework in your project.\nFor the first group you can use the startup method.\n<img src=\"/img/articles/2021-12-09-choose-your-skills/startup.png\" alt=\"Startup\" />\nAn example scenario of what the process of developing hard skills might look like:</p>\n\n<ol>\n  <li>Choose one skill with your mentor (the order does not matter). Additionally, collect the requirements.</li>\n  <li>Estimate how long it will take you to develop that skill. Put it on your calendar.</li>\n  <li>Together with your mentor, set a first stage, i.e. the first prototype of your work and work on it for e.g. 2-3\nweeks.</li>\n  <li>After this time, show what you have done to your mentor or the whole team.</li>\n  <li>Draw conclusions. Collect feedback. Do you like doing it? Did the result meet their expectations?</li>\n  <li>If not, make a pivot to another strategy or take another skill. If so, improve what you collected during the feedback\nand keep developing.</li>\n</ol>\n\n<p>In the worst case scenario you will go through all the skills. You will then have a fairly broad knowledge base. I have\nnever heard the opinion that someone learned something and now regrets it. In a positive scenario you will probably find\nsomething you do well and enjoy it.</p>\n\n<p>We will now look at the second group of skills, which you develop automatically while working on a project. Here again,\nyou should first consult with your mentor on the best strategy to follow. There are two strategies in such cases: to\ngain deep knowledge in one of these skills or to learn a little about all of them. The mentor should know what the team\nexpects from a specialist in the given position. This automatically affects the implementation of the tasks in the\nproject:</p>\n\n<ol>\n  <li>You perform tasks only in the domain you know and after some time you become a specialist in this area, but you do\nnot take tasks from another domain that is new to you. Your value is the speed of task completion.</li>\n  <li>You always take tasks from a new domain, you are brave and not afraid of challenges. You will then have a broad\ngeneral knowledge of all products but will complete tasks much more slowly.</li>\n</ol>\n\n<p>This is just one example. It is important that the mentor is clear about his requirements and that he gives you feedback\nafter you have done a few of these tasks to make sure that this is what you wanted.</p>\n\n<h2 id=\"pivot-or-persevere\">Pivot or persevere</h2>\n\n<p>Finally, let me explain the startup terms for pivot. The term pivot means to change from one strategy to another. For\nexample, I did one pivot during my development process. I was developing the skill of knowledge sharing. I had several\nstrategies to choose from:</p>\n\n<ul>\n  <li>Making presentations</li>\n  <li>Writing articles</li>\n  <li>Training preparation</li>\n</ul>\n\n<p>I made a presentation and after collecting feedback I had to make a <code class=\"language-plaintext highlighter-rouge\">pivot</code> and changed my strategy to another one. I\nchose to share knowledge by writing articles. This article is the result of that. If this strategy works for me then I\nwill do a <code class=\"language-plaintext highlighter-rouge\">persevere</code> and continue using the chosen strategy.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>If you feel that you should already be in a higher position it is likely that the expectations for your job are quite\ndifferent than you think. To change this you should first find a mentor and then, with their help, gather the\nrequirements. A good place to start is with a list of requirements or a list of skills to begin discussing with your\nmentor. From this list you should select a few skills and then choose appropriate strategies for their development. To\nmake it easier for you to choose a strategy, do a Gallup test. Additionally, you can use the startup MVP approach.\nUltimately, the key to success is to gather requirements from your leader, team or entire company and then show the\nresults as quickly as possible. If the chosen skill or strategy doesn’t suit you or the feedback collected is strongly\nnegative, don’t worry, make a pivot and choose something else. After a few iterations you will definitely achieve your\ngoal.</p>\n","contentSnippet":"At some point in your career, you realize that it’s time to try to advance through the hierarchy. You think you are\ndoing a good job. You are constantly developing and learning something new. But at the same time, someone you know, with\nmuch less experience and knowledge than you, has long been higher up the hierarchy than you. Then you ask yourself:\nwhat is wrong with me? In my case, the answer turned out to be properly gathering the expectations concerning my skills\nand work.\nBelow, you can read my insight about it. You will find out how you can steer your development, so that you become much\nmore valuable to your team, leader or company and how to combine all this with your talents and interests.\nHow to start the development\nAs a member of a project team, you want to deliver a fully valuable solution. The key to success is to discover what’s\nbehind the word valuable to your client. Is it enough to gather requirements? Unfortunately not. To find out exactly\nwhat stands behind the word valuable you need to use a prototyping method, such as an iterative\nMVP (Minimum Viable Product) approach. For example, suppose you\nhave to paint your daughter’s room. You have several choices. You can choose the colour by yourself and paint the room\nwithout asking her for her opinion. The chance she will like the colour is small. So you will make a non-compliant\nproduct. You can bring her a sampler and ask her to choose a colour. The final colour when applied to the wall, may turn\nout to be different and then, the product will also not be as expected. You can also ask her about the colour and buy a\nfew samplers, paint a piece of wall with them and ask if any meet the requirements, if yes then paint the whole room\nwith the chosen colour, if not, buy another 5 colours and so on over and over until you get it right. This is the MVP\nprocess. First you gather requirements, then you prototype, which means you make a real product. You present it and\nreceive feedback (you learn) and decide what to do next. You repeat this process until you meet the client’s\nexpectations. This approach gives you the assurance that you have done exactly what was needed.\nImagine that your skills and the work you do are products. We already know that to make a valuable product you need to\ngather requirements and prototype with the client. Your customer who cares most about your work and skills are the team,\nthe leader and the company you work for. However, what you do must also suit you. If you want your work and your\nknowledge to be of value to them, you need to gather their requirements first. Then quickly show the result and make\nsure it’s what they wanted. You are probably wondering what this prototyping is for in the case of your work? You have\ndone some work, it has been accepted so what more is there to talk about. There could be lots of reasons. You could have\nspent too much time on it, it’s even possible that what you did is wrong. You would have to put in a huge amount of time\nand work to improve it, which is why it was accepted as it is. Without prototyping, your work might appear to be of good\nquality to you, but the team might not be happy with it.\nLet’s start with requirements gathering. Unfortunately it is impossible to gather universal requirements for a given\nposition since these requirements are different at different companies and even teams. The same person who has certain\nskills and performs certain tasks may be a senior in one company, where in another, he or she may be a junior. In one of\nthe companies I worked for, a good programmer was required to know the products and the business, then the tenure at the\ncompany mattered as well and being a support for the business. Performing a large number of tasks was not so important.\nIn another company, there were people who knew their product very well and were an excellent support to the business.\nHowever, they did not perform their tasks on time and did not take on issues that required them to undertake a lot of\nresponsibility. They still had junior or mid-level status.\n\nThis begs the question: how do I gather my skills requirements?\nAssess yourself with some evaluation sheets\nAt Allegro, in the case of programmers and testers, we have a list of expectations prepared within the leadership\ncommunity. With such a list, it is much easier to collect requirements and talk about your own development towards\npromotion (in the team or in the company).\nIn my case, after working at Allegro for a while, I decided that it might be a good idea to finally get my development\non track. My leader asked me to assess myself using a requirements sheet. This allowed me to identify the current stage\nI was at. The same sheet about me was completed by my leader. Then we calibrated ourselves. This sheet is divided into 3\nmain parts:\nTechnical knowledge\nAttitude\nProduct knowledge\nEach part has a list of skills, each with an extensive description. For example, one of the descriptions of technical\nknowledge looked like this:\nYou know your technological stack very well (systems, applications, languages, tools). Having a technical problem to\nsolve, you know what your technological abilities are, and you know how to use them to achieve your goal.\nI rated each skill on a scale of 1 to 5. Ideally, I would like each skill to be close to a 5. If I rated myself\nsomewhere at a 4 and my leader rated me a 3, during calibration, I found out that the requirements for that skill were\ndifferent from what I thought. He said, there were things I hadn’t used yet. To have a 4 at this point I need to know\nthem e.g. Cassandra database. With that I learned the first relevant requirement that I didn’t know about. After the\nwhole process of gathering requirements this way, I already knew what I needed to work on. Now all I had to do was\nselect a few of them and start working. But how to choose these requirements, which I will implement with passion and\nactually do them well?\nHow to choose skills to improve?\nThere are many strategies on how to choose skills to develop. All in all, you need to use the right method for the right\ntype of skill to get a good result. The first step is to get a mentor. Usually it will be your leader, but it can also\nbe an experienced team member, e.g. a senior.\nLeaders at Allegro gain their development knowledge from many available training sessions. You should find a similar\nperson in your environment. Such a person will help you on your way and will simulate the customer of the product that\nyou will be making. Show him or her a list of things you should develop. Ask them which are the most important for your\nteam. It is possible that the list of things will become very short. Now just pick two or three things and prepare\nprototypes. You can choose them intuitively or match them with your talents. Just take a Gallup\ntest (CliftonStrengths) to discover them. This test will help\nyou choose a strategy for developing a particular skill. This test will work well for soft skills. For hard skills\na lean startup strategy may be better. Below are two examples of what this\ncan look like in practice.\nHow Gallup test can help you find activities that improve you\nThe Gallup test\nhelps you find activities that you naturally enjoy doing. This natural response to certain types of information,\nsituations and people in the context of the Gallup test is called a talent. The test result is divided into four main\ndomains:\nExecuting\nInfluencing\nRelationship Building\nStrategic Thinking\nSome of these will have a direct impact on your work, others will not. For example a description of your character in\nthe context of building relationships with people will be interesting information and will certainly raise your\nawareness, but will not help us in our challenge. Other talents that come out of the challenge may fit into the skill\nyou are developing or show how you should develop it.\nOf course with these talents, the knowledge itself that you have a talent does not immediately translate into skills,\nfor example finding out that you have a talent for standing on your head, it does not mean that you will immediately\nstand on your head. You have to develop this talent, and you probably are already doing similar things because you\nsimply have a talent for it, and you enjoy it. So the Gallup test shows the things you should focus on. They are so\ngeneric that you can easily match them to your problems.\n\nHere’s what it was like for me. I did the Gallup test, and it came out that I am a learner. I like to learn, and I\nshould share knowledge - which should make me happy. Comparing this talent to the list of skills I should develop, I\nfound out that I don’t share knowledge with others. I decided that a good form of knowledge transfer is to make\npresentations. I completed training in making presentations. I also estimated the time for preparation and set a\ndeadline. While preparing, I realized that this knowledge sharing strategy was not for me. I finished this presentation\nand decided to change my knowledge sharing strategy. Such a change in strategy is called a pivot. I discussed the\nproblem with my mentor and decided to make a pivot towards writing articles. Being in this situation you have two\nchoices: either you do a pivot, which means you change your strategy, or you persevere, which means you continue to\ndevelop using your current strategy, after getting proper feedback from your mentor and team of course. Let us now look\nat the second type of skill.\nHow startup method can help you develop your skills\nLet us divide hard skills into two groups. In the first group there are skills you acquire while learning by yourself,\ne.g. a new programming language. For this you need internal motivation and discipline to achieve the expected high\nlevel. The second group is the one where you acquire skills automatically e.g. you use some framework in your project.\nFor the first group you can use the startup method.\n\nAn example scenario of what the process of developing hard skills might look like:\nChoose one skill with your mentor (the order does not matter). Additionally, collect the requirements.\nEstimate how long it will take you to develop that skill. Put it on your calendar.\nTogether with your mentor, set a first stage, i.e. the first prototype of your work and work on it for e.g. 2-3\nweeks.\nAfter this time, show what you have done to your mentor or the whole team.\nDraw conclusions. Collect feedback. Do you like doing it? Did the result meet their expectations?\nIf not, make a pivot to another strategy or take another skill. If so, improve what you collected during the feedback\nand keep developing.\nIn the worst case scenario you will go through all the skills. You will then have a fairly broad knowledge base. I have\nnever heard the opinion that someone learned something and now regrets it. In a positive scenario you will probably find\nsomething you do well and enjoy it.\nWe will now look at the second group of skills, which you develop automatically while working on a project. Here again,\nyou should first consult with your mentor on the best strategy to follow. There are two strategies in such cases: to\ngain deep knowledge in one of these skills or to learn a little about all of them. The mentor should know what the team\nexpects from a specialist in the given position. This automatically affects the implementation of the tasks in the\nproject:\nYou perform tasks only in the domain you know and after some time you become a specialist in this area, but you do\nnot take tasks from another domain that is new to you. Your value is the speed of task completion.\nYou always take tasks from a new domain, you are brave and not afraid of challenges. You will then have a broad\ngeneral knowledge of all products but will complete tasks much more slowly.\nThis is just one example. It is important that the mentor is clear about his requirements and that he gives you feedback\nafter you have done a few of these tasks to make sure that this is what you wanted.\nPivot or persevere\nFinally, let me explain the startup terms for pivot. The term pivot means to change from one strategy to another. For\nexample, I did one pivot during my development process. I was developing the skill of knowledge sharing. I had several\nstrategies to choose from:\nMaking presentations\nWriting articles\nTraining preparation\nI made a presentation and after collecting feedback I had to make a pivot and changed my strategy to another one. I\nchose to share knowledge by writing articles. This article is the result of that. If this strategy works for me then I\nwill do a persevere and continue using the chosen strategy.\nSummary\nIf you feel that you should already be in a higher position it is likely that the expectations for your job are quite\ndifferent than you think. To change this you should first find a mentor and then, with their help, gather the\nrequirements. A good place to start is with a list of requirements or a list of skills to begin discussing with your\nmentor. From this list you should select a few skills and then choose appropriate strategies for their development. To\nmake it easier for you to choose a strategy, do a Gallup test. Additionally, you can use the startup MVP approach.\nUltimately, the key to success is to gather requirements from your leader, team or entire company and then show the\nresults as quickly as possible. If the chosen skill or strategy doesn’t suit you or the feedback collected is strongly\nnegative, don’t worry, make a pivot and choose something else. After a few iterations you will definitely achieve your\ngoal.","guid":"https://blog.allegro.tech/2021/12/choose-your-skills.html","categories":["tech","education","skills","startup","lean startup","mvp"],"isoDate":"2021-12-08T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Moving towards Micronaut","link":"https://blog.allegro.tech/2021/11/micronaut.html","pubDate":"Mon, 22 Nov 2021 00:00:00 +0100","authors":{"author":[{"name":["Konrad Kamiński"],"photo":["https://blog.allegro.tech/img/authors/konrad.kaminski.jpg"],"url":["https://blog.allegro.tech/authors/konrad.kaminski"]}]},"content":"<p><a href=\"https://micronaut.io\">Micronaut</a> is one of the new application frameworks that have recently sprung up. It promises\nlow memory usage and faster application startup. At <a href=\"https://allegro.tech/\">Allegro</a> we decided to give it a try. In this article we’ll learn what\ncame out of it and if it’s worth considering when creating microservices-based systems.</p>\n\n<h2 id=\"paradise-city\">Paradise city</h2>\n<p>At Allegro we run a few hundred microservices, most of which use Spring Framework. We also have services created in other technologies.\nAnd to make things more complicated we run them on a few different types of clouds - our own <a href=\"http://mesos.apache.org/\">Mesos</a>-based as well as private and public <a href=\"https://kubernetes.io/\">k8s</a>-based ones.\nTherefore in order for all of it to work consistently and smoothly we created a number of supporting libraries and at the same time we defined a kind of\ncontract for all services. This way, if there is ever a need or will to create a service with a new shiny technology, it should be feasible with as little\nwork as possible. You can read more about this approach in <a href=\"https://blog.allegro.tech/2020/07/common-code-approach.html\">a great article by Piotr Betkier</a>.</p>\n\n<h2 id=\"you-gotta-fight-for-your-right-to-party\">(You Gotta) Fight for Your Right (To Party!)</h2>\n<p>In order to be up to date with current technologies, at Allegro we run hackathons where we try out the “trendy” solutions. Over a year ago\nwe decided to taste Micronaut. The framework represents one of the new approaches to some of the inherent problems of existing solutions:\nit steers clear of using Java Reflection and does as much as it can at compile or rather build time. Major things achieved this way are:</p>\n<ul>\n  <li>lower memory usage - Java Reflection in most current JDK implementations is a memory hog; Micronaut has its own implementation of Java Reflection-like API\nwhich doesn’t suffer from that problem,</li>\n  <li>faster startup - Java Reflection is also not a speed daemon; doing things ahead of time means less has to be done at runtime,</li>\n  <li>ability to create native apps - <a href=\"https://www.graalvm.org\">GraalVM</a>, another new kid on the block, allows creating native binaries out of a JVM-based application; however, there\nare some caveats and one of them is… Java Reflection (basically if your application uses it, it has to provide some metadata for the native compiler). Since\nMicronaut has its own implementation, the problem is simply non-existent.</li>\n</ul>\n\n<p>We wanted to see how difficult it is to create a new microservice with Micronaut that would run on our on-premise cloud and do something meaningful. So during\nour hackathon we defined the following goals for our simple app:</p>\n<ul>\n  <li>it should be possible to deploy the app on our on-premise cloud,</li>\n  <li>the app should provide and use basic functionalities, such as:\n    <ul>\n      <li>telemetry,</li>\n      <li>configuration management,</li>\n      <li>REST endpoints,</li>\n      <li>ability to call other microservices,</li>\n      <li>ability to send messages to <a href=\"https://github.com/allegro/hermes\">Hermes</a>,</li>\n      <li>database access (we voted for MongoDB),</li>\n      <li>(optionally) ability to be compiled into a native binary with GraalVM.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>After a very satisfying hackathon we carried the day. Our microservice had all the above-mentioned functionalities - some of them obviously\nin a makeshift form, but that didn’t matter. We achieved all the goals.</p>\n\n<h2 id=\"highway-to-hell\">Highway to Hell</h2>\n<p>The result of the hackathon pushed us forward to make something even bolder. We wanted to have a real Micronaut-based application in our production environment.\nTo make things harder - we wanted to convert an existing Spring-based system to a Micronaut-based one. Though we reached our destination, the road\nwas quite bumpy. Let’s see what awaits those who take that path.</p>\n\n<h3 id=\"paranoid\">Paranoid</h3>\n<p>To ease a migration from Spring a special <a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">micronaut-spring</a> project has been created.\nIt supports a limited selection of Spring annotations and functionality so that in theory one can just replace Spring dependencies with Micronaut ones.\nSpecifically among the most interesting features are:</p>\n<ul>\n  <li>standard inversion of control annotations: <code class=\"language-plaintext highlighter-rouge\">@Component</code>, <code class=\"language-plaintext highlighter-rouge\">@Service</code>, <code class=\"language-plaintext highlighter-rouge\">@Repository</code>,<code class=\"language-plaintext highlighter-rouge\">@Bean</code>, <code class=\"language-plaintext highlighter-rouge\">@Autowired</code>, <code class=\"language-plaintext highlighter-rouge\">@Configuration</code>, <code class=\"language-plaintext highlighter-rouge\">@Primary</code> and many others\nare converted into their Micronaut counterparts,</li>\n  <li>standard Spring interfaces: <code class=\"language-plaintext highlighter-rouge\">@Environment</code>, <code class=\"language-plaintext highlighter-rouge\">@ApplicationEventPublisher</code>, <code class=\"language-plaintext highlighter-rouge\">@ApplicationContext</code>, <code class=\"language-plaintext highlighter-rouge\">@BeanFactory</code> and their <code class=\"language-plaintext highlighter-rouge\">@*Aware</code> versions are also\nadapted to their Micronaut counterparts,</li>\n  <li>MVC controller annotations: <code class=\"language-plaintext highlighter-rouge\">@RestController</code>, <code class=\"language-plaintext highlighter-rouge\">@GetMapping</code>, <code class=\"language-plaintext highlighter-rouge\">@PostMapping</code> and many others are converted into their Micronaut counterparts.</li>\n</ul>\n\n<p>This makes the whole exercise simpler, but unfortunately it also comes at a price. Not all features are supported (e.g. Spring’s <code class=\"language-plaintext highlighter-rouge\">@PathVariable</code> is not)\nand for those which are, they sometimes have subtle differences. For this reason oftentimes you simply have to revert to the regular manual code\nconversion. The problem is that this kind of approach will lead you to a mixed solution - you’ll have both Micronaut and Spring annotations in your code.\nAnd then a question arises: which annotations should I use for the newly created code? Do we stick with the old Spring annotations if there is even\none instance of it in the current codebase? Or maybe treat this old code as a “necessary evil” and always put Micronaut annotations for the added functionality?</p>\n\n<p>We came to the conclusion that we did not want to use <em>micronaut-spring</em> at all. This\nof course led to more work, but in the end we think it was worth it. The converted application does not have any Spring dependencies, no “technical debt”.</p>\n\n<h3 id=\"sad-but-true\">Sad but True</h3>\n\n<p>One of the things not covered at all by <em>micronaut-spring</em> is exception handling in MVC.\nIn Spring our handlers looked something like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.springframework.http.HttpStatus.BAD_REQUEST</span>\n\n<span class=\"nd\">@ControllerAdvice</span>\n<span class=\"nd\">@Order</span><span class=\"p\">(</span><span class=\"nc\">Ordered</span><span class=\"p\">.</span><span class=\"nc\">HIGHEST_PRECEDENCE</span><span class=\"p\">)</span>\n<span class=\"kd\">class</span> <span class=\"nc\">DefaultExceptionHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@ExceptionHandler</span><span class=\"p\">(</span><span class=\"nc\">SomeException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">)</span>\n    <span class=\"nd\">@ResponseBody</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">handleSomeException</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">SomeException</span><span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">(</span><span class=\"nc\">ApiError</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">),</span> <span class=\"nc\">BAD_REQUEST</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>In Micronaut exception handling can be done locally (i.e. functions handling exception will only be used for the exceptions thrown by the controller the\nfunctions are defined in) or globally. Since our Spring handlers acted globally, the equivalent Micronaut code is as follows:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">io.micronaut.http.HttpStatus.BAD_REQUEST</span>\n<span class=\"k\">import</span> <span class=\"nn\">io.micronaut.http.annotation.Error</span> <span class=\"k\">as</span> <span class=\"nc\">HttpError</span>\n\n<span class=\"nd\">@Controller</span>\n<span class=\"kd\">class</span> <span class=\"nc\">DefaultExceptionHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@Error</span><span class=\"p\">(</span><span class=\"n\">global</span> <span class=\"p\">=</span> <span class=\"k\">true</span><span class=\"p\">)</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">handleSomeException</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">SomeException</span><span class=\"p\">):</span> <span class=\"nc\">HttpResponse</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nc\">HttpResponseFactory</span><span class=\"p\">.</span><span class=\"nc\">INSTANCE</span><span class=\"p\">.</span><span class=\"nf\">status</span><span class=\"p\">(</span><span class=\"nc\">BAD_REQUEST</span><span class=\"p\">,</span> <span class=\"nc\">ApiError</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">))</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"dirty-deeds-done-dirt-cheap\">Dirty Deeds Done Dirt Cheap</h3>\n\n<p>At Allegro we use many different types of databases. The application of this exercise used MongoDB. As it turned out we couldn’t have chosen worse. Don’t get me\nwrong - Micronaut supports most of the databases out there, but not all are treated equally well.</p>\n\n<p>Since our system used <a href=\"https://spring.io/projects/spring-data\">Spring Data</a>, we tried to find something similar from the Micronaut world. <a href=\"https://micronaut-projects.github.io/micronaut-data/latest/guide/\">Micronaut Data</a>\nis - as its authors say - “inspired by <em>GORM</em> and <em>Spring Data</em>”. Unfortunately the inspiration doesn’t go too far. And in case of MongoDB it actually <a href=\"https://github.com/micronaut-projects/micronaut-data/issues/220\">doesn’t even\ntake a step</a>. Instead, we used <a href=\"https://micronaut-projects.github.io/micronaut-mongodb/latest/guide/\">Micronaut MongoDB</a> library. This simple project will provide\nyour services only with either a <a href=\"https://mongodb.github.io/mongo-java-driver/4.3/apidocs/mongodb-driver-legacy/com/mongodb/MongoClient.html\">blocking MongoClient</a> or a\n<a href=\"https://mongodb.github.io/mongo-java-driver/4.3/apidocs/mongodb-driver-reactivestreams/com/mongodb/reactivestreams/client/MongoClient.html\">reactive MongoClient</a>\nalong with some healthchecks. Not enough even for a modest application.</p>\n\n<p>Fortunately some good people created <a href=\"https://litote.org/kmongo/\">kmongo</a> - a little library that helped us a lot in converting the database access part of our app.\nAt the end of the day, however, we had to create some support code to ease the migration.</p>\n\n<p>The original application database access code was in the form of reactive repositories:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.springframework.data.annotation.Id</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.springframework.data.mongodb.core.mapping.Document</span>\n\n<span class=\"nd\">@Document</span><span class=\"p\">(</span><span class=\"n\">collection</span> <span class=\"p\">=</span> <span class=\"s\">\"users\"</span><span class=\"p\">)</span>\n<span class=\"kd\">data class</span> <span class=\"nc\">User</span><span class=\"p\">(</span>\n    <span class=\"nd\">@Id</span> <span class=\"kd\">val</span> <span class=\"py\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">name</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">UserRepository</span><span class=\"p\">:</span> <span class=\"nc\">ReactiveMongoRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">,</span> <span class=\"nc\">String</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">findFirstByTypeOrderByNameDesc</span><span class=\"p\">(</span><span class=\"n\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>We wanted to preserve the interface and as much code as possible. Here is what we had to do to get this effect.</p>\n\n<p>First we decided that our components would use <code class=\"language-plaintext highlighter-rouge\">MongoDatabase</code> rather than <code class=\"language-plaintext highlighter-rouge\">MongoClient</code> offered by <em>Micronaut MongoDB</em>.\nWe had only one database so that was an obvious choice.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Factory</span>\n<span class=\"kd\">class</span> <span class=\"nc\">MongoConfig</span> <span class=\"p\">{</span>\n    <span class=\"nd\">@Singleton</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">mongoDatabase</span><span class=\"p\">(</span><span class=\"n\">mongoClient</span><span class=\"p\">:</span> <span class=\"nc\">MongoClient</span><span class=\"p\">,</span> <span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"nc\">DefaultMongoConfiguration</span><span class=\"p\">):</span> <span class=\"nc\">MongoDatabase</span> <span class=\"p\">=</span>\n        <span class=\"n\">mongoClient</span><span class=\"p\">.</span><span class=\"nf\">getDatabase</span><span class=\"p\">(</span><span class=\"n\">configuration</span><span class=\"p\">.</span><span class=\"n\">connectionString</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">().</span><span class=\"n\">database</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Then there was the question of configuring <em>kmongo</em>. It wasn’t as straightforward as we’d thought it would be. Let’s take a look at the\nfinal code.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Factory</span>\n<span class=\"kd\">class</span> <span class=\"nc\">KMongoFactory</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@Singleton</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">kCodecRegistry</span><span class=\"p\">():</span> <span class=\"nc\">CodecRegistry</span> <span class=\"p\">{</span>\n        <span class=\"nc\">ObjectMappingConfiguration</span><span class=\"p\">.</span><span class=\"nf\">addCustomCodec</span><span class=\"p\">(</span><span class=\"nc\">JodaDateSerializationCodec</span><span class=\"p\">)</span> <span class=\"c1\">// 1 - custom Joda DateTime coded</span>\n        <span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"nf\">registerBsonModule</span><span class=\"p\">(</span><span class=\"nc\">JodaModule</span><span class=\"p\">())</span>                  <span class=\"c1\">// 2 - register default Joda module</span>\n        <span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"nf\">registerBsonModule</span><span class=\"p\">(</span><span class=\"nc\">JodaDateSerializationModule</span><span class=\"p\">)</span>   <span class=\"c1\">// 3 - register custom Joda module</span>\n        <span class=\"nf\">with</span><span class=\"p\">(</span><span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"n\">bsonMapper</span><span class=\"p\">.</span><span class=\"n\">factory</span> <span class=\"k\">as</span> <span class=\"nc\">BsonFactory</span><span class=\"p\">)</span> <span class=\"p\">{</span>         <span class=\"c1\">// 4 - change BigDecimal handling</span>\n            <span class=\"nf\">disable</span><span class=\"p\">(</span><span class=\"nc\">BsonGenerator</span><span class=\"p\">.</span><span class=\"nc\">Feature</span><span class=\"p\">.</span><span class=\"nc\">WRITE_BIGDECIMALS_AS_DECIMAL128</span><span class=\"p\">)</span>\n            <span class=\"nf\">enable</span><span class=\"p\">(</span><span class=\"nc\">BsonGenerator</span><span class=\"p\">.</span><span class=\"nc\">Feature</span><span class=\"p\">.</span><span class=\"nc\">WRITE_BIGDECIMALS_AS_STRINGS</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n        <span class=\"k\">return</span> <span class=\"nc\">ClassMappingType</span><span class=\"p\">.</span><span class=\"nf\">codecRegistry</span><span class=\"p\">(</span><span class=\"nc\">MongoClientSettings</span><span class=\"p\">.</span><span class=\"nf\">getDefaultCodecRegistry</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p><code class=\"language-plaintext highlighter-rouge\">MongoDB</code> driver expects a <code class=\"language-plaintext highlighter-rouge\">CodecRegistry</code> which defines how to encode a Java object into Mongo <code class=\"language-plaintext highlighter-rouge\">BSON</code>, so that it can be persisted in a database. By default\n<em>kmongo</em> supports a simple, <a href=\"https://github.com/FasterXML/jackson\">Jackson</a> based converter. However, there were a few issues in\nour application which forced us to create some customizations:</p>\n\n<ul>\n  <li><a href=\"https://www.joda.org/joda-time/\">Joda</a> date types in entity classes - our app has a long history and it still uses <em>Joda</em> date types.\nUnfortunately they do not work with <em>kmongo</em>, so we had to teach it how to handle them. It required a few steps.\n    <ul>\n      <li>\n        <p>(1) <em>kmongo</em> had to know how to serialize a <em>Joda</em> date type to a <code class=\"language-plaintext highlighter-rouge\">MongoDB</code> date type:</p>\n\n        <div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">object</span> <span class=\"nc\">JodaDateSerializationCodec</span> <span class=\"p\">:</span> <span class=\"nc\">Codec</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">encode</span><span class=\"p\">(</span><span class=\"n\">writer</span><span class=\"p\">:</span> <span class=\"nc\">BsonWriter</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">DateTime</span><span class=\"p\">?,</span> <span class=\"n\">encoderContext</span><span class=\"p\">:</span> <span class=\"nc\">EncoderContext</span><span class=\"p\">?)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">value</span> <span class=\"p\">==</span> <span class=\"k\">null</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"n\">writer</span><span class=\"p\">.</span><span class=\"nf\">writeNull</span><span class=\"p\">()</span>\n        <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n            <span class=\"n\">writer</span><span class=\"p\">.</span><span class=\"nf\">writeDateTime</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">.</span><span class=\"n\">millis</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">getEncoderClass</span><span class=\"p\">():</span> <span class=\"nc\">Class</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">:</span> <span class=\"nc\">BsonReader</span><span class=\"p\">,</span> <span class=\"n\">decoderContext</span><span class=\"p\">:</span> <span class=\"nc\">DecoderContext</span><span class=\"p\">?):</span> <span class=\"nc\">DateTime</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nc\">DateTime</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">.</span><span class=\"nf\">readDateTime</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>(2) <em>Jackson</em> used by <em>kmongo</em> also had to know how to handle <em>Joda</em> date types,</li>\n      <li>\n        <p>(3) to make things harder sometimes we stored a datetime as a long value, therefore we had to add support for that as well:</p>\n\n        <div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">object</span> <span class=\"nc\">JodaDateSerializationModule</span> <span class=\"p\">:</span> <span class=\"nc\">SimpleModule</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"nf\">init</span> <span class=\"p\">{</span>\n        <span class=\"nf\">addSerializer</span><span class=\"p\">(</span><span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"nc\">JodaDateSerializer</span><span class=\"p\">())</span>\n        <span class=\"nf\">addDeserializer</span><span class=\"p\">(</span><span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"nc\">JodaDateDeserializer</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">JodaDateSerializer</span> <span class=\"p\">:</span> <span class=\"nc\">JsonSerializer</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;()</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">DateTime</span><span class=\"p\">,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?)</span> <span class=\"p\">{</span>\n        <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeObject</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">.</span><span class=\"nf\">toDate</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">JodaDateDeserializer</span> <span class=\"p\">:</span> <span class=\"nc\">JsonDeserializer</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;()</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">deserialize</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">:</span> <span class=\"nc\">JsonParser</span><span class=\"p\">,</span> <span class=\"n\">ctxt</span><span class=\"p\">:</span> <span class=\"nc\">DeserializationContext</span><span class=\"p\">?):</span> <span class=\"nc\">DateTime</span> <span class=\"p\">=</span>\n        <span class=\"k\">when</span> <span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"n\">currentToken</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"nc\">JsonToken</span><span class=\"p\">.</span><span class=\"nc\">VALUE_NUMBER_INT</span> <span class=\"p\">-&gt;</span> <span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"nf\">readValueAs</span><span class=\"p\">(</span><span class=\"nc\">Long</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">).</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"o\">::</span><span class=\"nc\">DateTime</span><span class=\"p\">)</span>\n            <span class=\"k\">else</span> <span class=\"p\">-&gt;</span> <span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"nf\">readValueAs</span><span class=\"p\">(</span><span class=\"nc\">Date</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">).</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"o\">::</span><span class=\"nc\">DateTime</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>(4) finally we stored <code class=\"language-plaintext highlighter-rouge\">BigDecimal</code> values as plain <code class=\"language-plaintext highlighter-rouge\">String</code>, which is not a default behaviour of <em>kmongo</em>, so we had to change it.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>As you can see some of the problems we had to face came from using either old technologies or not using them properly. It turned out there were more issues.</p>\n\n<p>For our entity IDs we usually used an artificial <code class=\"language-plaintext highlighter-rouge\">String</code> value. <code class=\"language-plaintext highlighter-rouge\">MongoDB</code> has special support for it in a form of <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/\"><code class=\"language-plaintext highlighter-rouge\">ObjectId</code></a>\ntype, which we gladly used in our application. But, here a new issue came up - in order to make our integration tests easier to read and write we used <code class=\"language-plaintext highlighter-rouge\">String</code>-type IDs\nnot conformant to <code class=\"language-plaintext highlighter-rouge\">ObjectId</code> restrictions (so for example our user IDs were <code class=\"language-plaintext highlighter-rouge\">user-1</code>, <code class=\"language-plaintext highlighter-rouge\">user-2</code>, etc.).\n<em>Spring Data</em> handles this transparently, but here we had to introduce one more customization. Our entity classes now\nhad to contain a special annotation indicating what serializer to use for our ID fields:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.bson.codecs.pojo.annotations.BsonId</span>\n\n<span class=\"kd\">data class</span> <span class=\"nc\">User</span><span class=\"p\">(</span>\n    <span class=\"nd\">@BsonId</span> <span class=\"nd\">@JsonSerialize</span><span class=\"p\">(</span><span class=\"n\">using</span> <span class=\"p\">=</span> <span class=\"nc\">CustomIdJsonSerializer</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">)</span> <span class=\"kd\">val</span> <span class=\"py\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">name</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">CustomIdJsonSerializer</span> <span class=\"p\">:</span> <span class=\"nc\">StdScalarSerializer</span><span class=\"p\">&lt;</span><span class=\"nc\">String</span><span class=\"p\">&gt;(</span><span class=\"nc\">String</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"k\">false</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">?,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?)</span> <span class=\"p\">=</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">value</span> <span class=\"p\">!=</span> <span class=\"k\">null</span> <span class=\"p\">&amp;&amp;</span> <span class=\"nc\">ObjectId</span><span class=\"p\">.</span><span class=\"nf\">isValid</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">))</span> <span class=\"p\">{</span> <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeObjectId</span><span class=\"p\">(</span><span class=\"nc\">ObjectId</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">))</span> <span class=\"p\">}</span>\n        <span class=\"k\">else</span> <span class=\"p\">{</span> <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeString</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">)</span> <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serializeWithType</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">?,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?,</span> <span class=\"n\">typeSer</span><span class=\"p\">:</span> <span class=\"nc\">TypeSerializer</span><span class=\"p\">?)</span> <span class=\"p\">=</span>\n        <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">gen</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">isEmpty</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Boolean</span> <span class=\"p\">=</span> <span class=\"n\">value</span><span class=\"p\">.</span><span class=\"nf\">isEmpty</span><span class=\"p\">()</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">acceptJsonFormatVisitor</span><span class=\"p\">(</span><span class=\"n\">visitor</span><span class=\"p\">:</span> <span class=\"nc\">JsonFormatVisitorWrapper</span><span class=\"p\">?,</span> <span class=\"n\">typeHint</span><span class=\"p\">:</span> <span class=\"nc\">JavaType</span><span class=\"p\">?)</span> <span class=\"p\">=</span> <span class=\"nf\">visitStringFormat</span><span class=\"p\">(</span><span class=\"n\">visitor</span><span class=\"p\">,</span> <span class=\"n\">typeHint</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>With the basics set up we could now focus on how to make the <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes work with as little effort as possible. We decided to create a base <code class=\"language-plaintext highlighter-rouge\">BaseRepository</code>\nclass letting us write concrete <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes easier:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">abstract</span> <span class=\"kd\">class</span> <span class=\"nc\">BaseRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;(</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">mongoDatabase</span><span class=\"p\">:</span> <span class=\"nc\">MongoDatabase</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">collectionName</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">clazz</span><span class=\"p\">:</span> <span class=\"nc\">Class</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span>\n<span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">open</span> <span class=\"k\">fun</span> <span class=\"nf\">findById</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">findOne</span><span class=\"p\">(</span><span class=\"nf\">eq</span><span class=\"p\">(</span><span class=\"s\">\"_id\"</span><span class=\"p\">,</span> <span class=\"n\">id</span><span class=\"p\">.</span><span class=\"nf\">maybeObjectId</span><span class=\"p\">()))</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">findOne</span><span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">:</span> <span class=\"nc\">Bson</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">withCollection</span> <span class=\"p\">{</span>\n        <span class=\"nf\">find</span><span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">).</span><span class=\"nf\">toMono</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">R</span><span class=\"p\">&gt;</span> <span class=\"nf\">withCollection</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">:</span> <span class=\"nc\">MongoCollection</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">R</span><span class=\"p\">):</span> <span class=\"nc\">R</span> <span class=\"p\">=</span>\n                <span class=\"n\">mongoDatabase</span>\n                    <span class=\"p\">.</span><span class=\"nf\">getCollection</span><span class=\"p\">(</span><span class=\"n\">collectionName</span><span class=\"p\">,</span> <span class=\"n\">clazz</span><span class=\"p\">)</span>\n                    <span class=\"p\">.</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Finally we wrote the <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes. A rewritten version of the <code class=\"language-plaintext highlighter-rouge\">UserRepository</code> mentioned at the beginning of this section looked like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Context</span>\n<span class=\"kd\">class</span> <span class=\"nc\">UserRepository</span><span class=\"p\">(</span>\n    <span class=\"n\">mongoDatabase</span><span class=\"p\">:</span> <span class=\"nc\">MongoDatabase</span>\n<span class=\"p\">):</span> <span class=\"nc\">BaseRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;(</span><span class=\"n\">mongoDatabase</span><span class=\"p\">,</span> <span class=\"s\">\"users\"</span><span class=\"p\">,</span> <span class=\"nc\">User</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">findFirstByTypeOrderByNameDesc</span><span class=\"p\">(</span><span class=\"n\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nf\">withCollection</span> <span class=\"p\">{</span>\n            <span class=\"nf\">find</span><span class=\"p\">(</span><span class=\"nf\">and</span><span class=\"p\">(</span><span class=\"nf\">eq</span><span class=\"p\">(</span><span class=\"s\">\"type\"</span><span class=\"p\">,</span> <span class=\"n\">type</span><span class=\"p\">)))</span>\n                <span class=\"p\">.</span><span class=\"nf\">sort</span><span class=\"p\">(</span><span class=\"nf\">descending</span><span class=\"p\">(</span><span class=\"s\">\"name\"</span><span class=\"p\">))</span>\n                <span class=\"p\">.</span><span class=\"nf\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"p\">.</span><span class=\"nf\">toMono</span><span class=\"p\">()</span>\n        <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"fear-of-the-dark\">Fear of the Dark</h3>\n\n<p><a href=\"https://spockframework.org/spock/docs/2.0/index.html\">Spock</a> is our framework of choice for writing tests. We still tend to use it even in <code class=\"language-plaintext highlighter-rouge\">Kotlin</code> applications,\nalthough sometimes the resulting code is not as clear as it’d be if not for <code class=\"language-plaintext highlighter-rouge\">Groovy</code> (`coroutines!). So how does\n<em>Micronaut</em> work with <em>Spock</em>? Actually, quite well.</p>\n\n<p>For testing there is a <a href=\"https://github.com/micronaut-projects/micronaut-test\">micronaut-test</a> project, which provides testing extensions for <em>Spock</em>\nand many other testing libraries. <a href=\"https://docs.spring.io/spring-framework/docs/current/reference/html/testing.html\">The general approach to writing test cases with Spring</a> which we were familiar with,\nis very similar in <em>micronaut-test</em>. Let’s have a look at a simple test case:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@MicronautTest</span> <span class=\"c1\">// 1</span>\n<span class=\"kd\">class</span> <span class=\"nc\">SimpleIntSpec</span> <span class=\"kd\">extends</span> <span class=\"n\">Specification</span> <span class=\"o\">{</span>\n    <span class=\"nd\">@Inject</span> <span class=\"c1\">// 2</span>\n    <span class=\"n\">UserService</span> <span class=\"n\">userService</span>\n\n    <span class=\"kt\">def</span> <span class=\"s2\">\"should persist a user\"</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n        <span class=\"nl\">given:</span>\n        <span class=\"n\">userService</span><span class=\"o\">.</span><span class=\"na\">createUser</span><span class=\"o\">(</span><span class=\"s2\">\"user-1\"</span><span class=\"o\">,</span> <span class=\"s2\">\"John\"</span><span class=\"o\">,</span> <span class=\"s2\">\"Doe\"</span><span class=\"o\">)</span>\n\n        <span class=\"nl\">when:</span>\n        <span class=\"kt\">def</span> <span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"n\">userService</span><span class=\"o\">.</span><span class=\"na\">getUser</span><span class=\"o\">(</span><span class=\"s2\">\"user-1\"</span><span class=\"o\">)</span>\n\n        <span class=\"nl\">then:</span>\n        <span class=\"n\">user</span><span class=\"o\">.</span><span class=\"na\">firstName</span> <span class=\"o\">==</span> <span class=\"s2\">\"John\"</span>\n        <span class=\"n\">user</span><span class=\"o\">.</span><span class=\"na\">lastName</span> <span class=\"o\">==</span> <span class=\"s2\">\"Doe\"</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<p>There are two interesting things in this test case:</p>\n<ul>\n  <li>(1) <code class=\"language-plaintext highlighter-rouge\">@MicronautTest</code> is an annotation you have to put in your test classes to start <em>Micronaut</em> application,</li>\n  <li>(2) <code class=\"language-plaintext highlighter-rouge\">@Inject</code> is <a href=\"https://micronaut.io/\">Micronaut</a>’s version of <code class=\"language-plaintext highlighter-rouge\">@Autowired</code> (or… <code class=\"language-plaintext highlighter-rouge\">@Inject</code>, which is also supported by <code class=\"language-plaintext highlighter-rouge\">Spring</code>). Be aware that since\n<em>Micronaut</em> <code class=\"language-plaintext highlighter-rouge\">3.0.0</code> you should use <code class=\"language-plaintext highlighter-rouge\">@jakarta.inject.Inject</code> annotation instead of the former <code class=\"language-plaintext highlighter-rouge\">@javax.inject.Inject</code>.</li>\n</ul>\n\n<p>If your tests make API calls to your application via REST endpoints, and you run your web container on a random port (which is common), then the way to retrieve it\nis through the use of the injected <code class=\"language-plaintext highlighter-rouge\">EmbeddedServer</code>:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@MicronautTest</span>\n<span class=\"kd\">class</span> <span class=\"nc\">ApiIntSpec</span> <span class=\"kd\">extends</span> <span class=\"n\">Specification</span> <span class=\"o\">{</span>\n    <span class=\"nd\">@Inject</span>\n    <span class=\"n\">EmbeddedServer</span> <span class=\"n\">server</span>\n\n    <span class=\"kt\">def</span> <span class=\"s2\">\"should create a user using API call\"</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n        <span class=\"nl\">given:</span>\n        <span class=\"kt\">def</span> <span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://localhost:{$server.port}/users\"</span>\n\n        <span class=\"nl\">when:</span>\n        <span class=\"c1\">// here goes your test...</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"money\">Money</h2>\n\n<p>As a side effect, an additional benefit you get when you use <em>Micronaut</em> is a faster development cycle. As stated at the\nbeginning of this post, one of the main features of this framework is faster startup. Therefore, when writing test cases and then running tests,\ntheir execution time is lower than their Spring equivalent. This may not be significant if your tests are few, but sooner or later\ntheir number will grow and then the speed will become more visible and important. For large codebases time savings can be really impressive.</p>\n\n<h2 id=\"should-i-stay-or-should-i-go\">Should I stay or should I go</h2>\n\n<p>The experience we gained during migration to <em>Micronaut</em> gave us more courage and assurance. So when the time came to decide what technology\nto use for a quite large greenfield project, we didn’t hesitate (well, we actually did, but not for long).\nSix months later with the system running in the production environment we’re happy we started that long journey. And if you’re considering\n<em>Micronaut</em> for one of your projects, I can wholeheartedly recommend: go for it.</p>\n","contentSnippet":"Micronaut is one of the new application frameworks that have recently sprung up. It promises\nlow memory usage and faster application startup. At Allegro we decided to give it a try. In this article we’ll learn what\ncame out of it and if it’s worth considering when creating microservices-based systems.\nParadise city\nAt Allegro we run a few hundred microservices, most of which use Spring Framework. We also have services created in other technologies.\nAnd to make things more complicated we run them on a few different types of clouds - our own Mesos-based as well as private and public k8s-based ones.\nTherefore in order for all of it to work consistently and smoothly we created a number of supporting libraries and at the same time we defined a kind of\ncontract for all services. This way, if there is ever a need or will to create a service with a new shiny technology, it should be feasible with as little\nwork as possible. You can read more about this approach in a great article by Piotr Betkier.\n(You Gotta) Fight for Your Right (To Party!)\nIn order to be up to date with current technologies, at Allegro we run hackathons where we try out the “trendy” solutions. Over a year ago\nwe decided to taste Micronaut. The framework represents one of the new approaches to some of the inherent problems of existing solutions:\nit steers clear of using Java Reflection and does as much as it can at compile or rather build time. Major things achieved this way are:\nlower memory usage - Java Reflection in most current JDK implementations is a memory hog; Micronaut has its own implementation of Java Reflection-like API\nwhich doesn’t suffer from that problem,\nfaster startup - Java Reflection is also not a speed daemon; doing things ahead of time means less has to be done at runtime,\nability to create native apps - GraalVM, another new kid on the block, allows creating native binaries out of a JVM-based application; however, there\nare some caveats and one of them is… Java Reflection (basically if your application uses it, it has to provide some metadata for the native compiler). Since\nMicronaut has its own implementation, the problem is simply non-existent.\nWe wanted to see how difficult it is to create a new microservice with Micronaut that would run on our on-premise cloud and do something meaningful. So during\nour hackathon we defined the following goals for our simple app:\nit should be possible to deploy the app on our on-premise cloud,\nthe app should provide and use basic functionalities, such as:\n    \ntelemetry,\nconfiguration management,\nREST endpoints,\nability to call other microservices,\nability to send messages to Hermes,\ndatabase access (we voted for MongoDB),\n(optionally) ability to be compiled into a native binary with GraalVM.\nAfter a very satisfying hackathon we carried the day. Our microservice had all the above-mentioned functionalities - some of them obviously\nin a makeshift form, but that didn’t matter. We achieved all the goals.\nHighway to Hell\nThe result of the hackathon pushed us forward to make something even bolder. We wanted to have a real Micronaut-based application in our production environment.\nTo make things harder - we wanted to convert an existing Spring-based system to a Micronaut-based one. Though we reached our destination, the road\nwas quite bumpy. Let’s see what awaits those who take that path.\nParanoid\nTo ease a migration from Spring a special micronaut-spring project has been created.\nIt supports a limited selection of Spring annotations and functionality so that in theory one can just replace Spring dependencies with Micronaut ones.\nSpecifically among the most interesting features are:\nstandard inversion of control annotations: @Component, @Service, @Repository,@Bean, @Autowired, @Configuration, @Primary and many others\nare converted into their Micronaut counterparts,\nstandard Spring interfaces: @Environment, @ApplicationEventPublisher, @ApplicationContext, @BeanFactory and their @*Aware versions are also\nadapted to their Micronaut counterparts,\nMVC controller annotations: @RestController, @GetMapping, @PostMapping and many others are converted into their Micronaut counterparts.\nThis makes the whole exercise simpler, but unfortunately it also comes at a price. Not all features are supported (e.g. Spring’s @PathVariable is not)\nand for those which are, they sometimes have subtle differences. For this reason oftentimes you simply have to revert to the regular manual code\nconversion. The problem is that this kind of approach will lead you to a mixed solution - you’ll have both Micronaut and Spring annotations in your code.\nAnd then a question arises: which annotations should I use for the newly created code? Do we stick with the old Spring annotations if there is even\none instance of it in the current codebase? Or maybe treat this old code as a “necessary evil” and always put Micronaut annotations for the added functionality?\nWe came to the conclusion that we did not want to use micronaut-spring at all. This\nof course led to more work, but in the end we think it was worth it. The converted application does not have any Spring dependencies, no “technical debt”.\nSad but True\nOne of the things not covered at all by micronaut-spring is exception handling in MVC.\nIn Spring our handlers looked something like this:\n\nimport org.springframework.http.HttpStatus.BAD_REQUEST\n\n@ControllerAdvice\n@Order(Ordered.HIGHEST_PRECEDENCE)\nclass DefaultExceptionHandler {\n\n    @ExceptionHandler(SomeException::class)\n    @ResponseBody\n    fun handleSomeException(e: SomeException): ResponseEntity<*> = ResponseEntity(ApiError(e.message), BAD_REQUEST)\n}\n\n\nIn Micronaut exception handling can be done locally (i.e. functions handling exception will only be used for the exceptions thrown by the controller the\nfunctions are defined in) or globally. Since our Spring handlers acted globally, the equivalent Micronaut code is as follows:\n\nimport io.micronaut.http.HttpStatus.BAD_REQUEST\nimport io.micronaut.http.annotation.Error as HttpError\n\n@Controller\nclass DefaultExceptionHandler {\n\n    @Error(global = true)\n    fun handleSomeException(e: SomeException): HttpResponse<*> =\n        HttpResponseFactory.INSTANCE.status(BAD_REQUEST, ApiError(e.message))\n}\n\n\nDirty Deeds Done Dirt Cheap\nAt Allegro we use many different types of databases. The application of this exercise used MongoDB. As it turned out we couldn’t have chosen worse. Don’t get me\nwrong - Micronaut supports most of the databases out there, but not all are treated equally well.\nSince our system used Spring Data, we tried to find something similar from the Micronaut world. Micronaut Data\nis - as its authors say - “inspired by GORM and Spring Data”. Unfortunately the inspiration doesn’t go too far. And in case of MongoDB it actually doesn’t even\ntake a step. Instead, we used Micronaut MongoDB library. This simple project will provide\nyour services only with either a blocking MongoClient or a\nreactive MongoClient\nalong with some healthchecks. Not enough even for a modest application.\nFortunately some good people created kmongo - a little library that helped us a lot in converting the database access part of our app.\nAt the end of the day, however, we had to create some support code to ease the migration.\nThe original application database access code was in the form of reactive repositories:\n\nimport org.springframework.data.annotation.Id\nimport org.springframework.data.mongodb.core.mapping.Document\n\n@Document(collection = \"users\")\ndata class User(\n    @Id val id: String,\n    val name: String,\n    val type: String\n)\n\nclass UserRepository: ReactiveMongoRepository<User, String> {\n    fun findFirstByTypeOrderByNameDesc(type: String): Mono<User>\n}\n\n\nWe wanted to preserve the interface and as much code as possible. Here is what we had to do to get this effect.\nFirst we decided that our components would use MongoDatabase rather than MongoClient offered by Micronaut MongoDB.\nWe had only one database so that was an obvious choice.\n\n@Factory\nclass MongoConfig {\n    @Singleton\n    fun mongoDatabase(mongoClient: MongoClient, configuration: DefaultMongoConfiguration): MongoDatabase =\n        mongoClient.getDatabase(configuration.connectionString.get().database)\n}\n\n\nThen there was the question of configuring kmongo. It wasn’t as straightforward as we’d thought it would be. Let’s take a look at the\nfinal code.\n\n@Factory\nclass KMongoFactory {\n\n    @Singleton\n    fun kCodecRegistry(): CodecRegistry {\n        ObjectMappingConfiguration.addCustomCodec(JodaDateSerializationCodec) // 1 - custom Joda DateTime coded\n        KMongoConfiguration.registerBsonModule(JodaModule())                  // 2 - register default Joda module\n        KMongoConfiguration.registerBsonModule(JodaDateSerializationModule)   // 3 - register custom Joda module\n        with(KMongoConfiguration.bsonMapper.factory as BsonFactory) {         // 4 - change BigDecimal handling\n            disable(BsonGenerator.Feature.WRITE_BIGDECIMALS_AS_DECIMAL128)\n            enable(BsonGenerator.Feature.WRITE_BIGDECIMALS_AS_STRINGS)\n        }\n        return ClassMappingType.codecRegistry(MongoClientSettings.getDefaultCodecRegistry())\n    }\n}\n\n\nMongoDB driver expects a CodecRegistry which defines how to encode a Java object into Mongo BSON, so that it can be persisted in a database. By default\nkmongo supports a simple, Jackson based converter. However, there were a few issues in\nour application which forced us to create some customizations:\nJoda date types in entity classes - our app has a long history and it still uses Joda date types.\nUnfortunately they do not work with kmongo, so we had to teach it how to handle them. It required a few steps.\n    \n(1) kmongo had to know how to serialize a Joda date type to a MongoDB date type:\n\nobject JodaDateSerializationCodec : Codec<DateTime> {\n    override fun encode(writer: BsonWriter, value: DateTime?, encoderContext: EncoderContext?) {\n        if (value == null) {\n            writer.writeNull()\n        } else {\n            writer.writeDateTime(value.millis)\n        }\n    }\n\n    override fun getEncoderClass(): Class<DateTime> {\n        return DateTime::class.java\n    }\n\n    override fun decode(reader: BsonReader, decoderContext: DecoderContext?): DateTime {\n        return DateTime(reader.readDateTime())\n    }\n}\n\n        \n(2) Jackson used by kmongo also had to know how to handle Joda date types,\n(3) to make things harder sometimes we stored a datetime as a long value, therefore we had to add support for that as well:\n\nobject JodaDateSerializationModule : SimpleModule() {\n    init {\n        addSerializer(DateTime::class.java, JodaDateSerializer())\n        addDeserializer(DateTime::class.java, JodaDateDeserializer())\n    }\n}\n\nclass JodaDateSerializer : JsonSerializer<DateTime>() {\n    override fun serialize(value: DateTime, gen: JsonGenerator, serializers: SerializerProvider?) {\n        gen.writeObject(value.toDate())\n    }\n}\n\nclass JodaDateDeserializer : JsonDeserializer<DateTime>() {\n    override fun deserialize(parser: JsonParser, ctxt: DeserializationContext?): DateTime =\n        when (parser.currentToken) {\n            JsonToken.VALUE_NUMBER_INT -> parser.readValueAs(Long::class.java).let(::DateTime)\n            else -> parser.readValueAs(Date::class.java).let(::DateTime)\n        }\n}\n\n        \n(4) finally we stored BigDecimal values as plain String, which is not a default behaviour of kmongo, so we had to change it.\nAs you can see some of the problems we had to face came from using either old technologies or not using them properly. It turned out there were more issues.\nFor our entity IDs we usually used an artificial String value. MongoDB has special support for it in a form of ObjectId\ntype, which we gladly used in our application. But, here a new issue came up - in order to make our integration tests easier to read and write we used String-type IDs\nnot conformant to ObjectId restrictions (so for example our user IDs were user-1, user-2, etc.).\nSpring Data handles this transparently, but here we had to introduce one more customization. Our entity classes now\nhad to contain a special annotation indicating what serializer to use for our ID fields:\n\nimport org.bson.codecs.pojo.annotations.BsonId\n\ndata class User(\n    @BsonId @JsonSerialize(using = CustomIdJsonSerializer::class) val id: String,\n    val name: String,\n    val type: String\n)\n\nclass CustomIdJsonSerializer : StdScalarSerializer<String>(String::class.java, false) {\n    override fun serialize(value: String?, gen: JsonGenerator, serializers: SerializerProvider?) =\n        if (value != null && ObjectId.isValid(value)) { gen.writeObjectId(ObjectId(value)) }\n        else { gen.writeString(value) }\n\n    override fun serializeWithType(value: String?, gen: JsonGenerator, serializers: SerializerProvider?, typeSer: TypeSerializer?) =\n        serialize(value, gen, serializers)\n\n    override fun isEmpty(value: String): Boolean = value.isEmpty()\n\n    override fun acceptJsonFormatVisitor(visitor: JsonFormatVisitorWrapper?, typeHint: JavaType?) = visitStringFormat(visitor, typeHint)\n}\n\n\nWith the basics set up we could now focus on how to make the *Repository classes work with as little effort as possible. We decided to create a base BaseRepository\nclass letting us write concrete *Repository classes easier:\n\nabstract class BaseRepository<T>(\n    private val mongoDatabase: MongoDatabase,\n    private val collectionName: String,\n    private val clazz: Class<T>\n) {\n    open fun findById(id: String): Mono<T> = findOne(eq(\"_id\", id.maybeObjectId()))\n\n    fun findOne(filter: Bson): Mono<T> = withCollection {\n        find(filter).toMono()\n    }\n\n    fun <R> withCollection(fn: MongoCollection<T>.() -> R): R =\n                mongoDatabase\n                    .getCollection(collectionName, clazz)\n                    .let(fn)\n}\n\n\nFinally we wrote the *Repository classes. A rewritten version of the UserRepository mentioned at the beginning of this section looked like this:\n\n@Context\nclass UserRepository(\n    mongoDatabase: MongoDatabase\n): BaseRepository<User>(mongoDatabase, \"users\", User::class.java) {\n\n    fun findFirstByTypeOrderByNameDesc(type: String): Mono<User> =\n        withCollection {\n            find(and(eq(\"type\", type)))\n                .sort(descending(\"name\"))\n                .limit(1)\n                .toMono()\n        }\n}\n\n\nFear of the Dark\nSpock is our framework of choice for writing tests. We still tend to use it even in Kotlin applications,\nalthough sometimes the resulting code is not as clear as it’d be if not for Groovy (`coroutines!). So how does\nMicronaut work with Spock? Actually, quite well.\nFor testing there is a micronaut-test project, which provides testing extensions for Spock\nand many other testing libraries. The general approach to writing test cases with Spring which we were familiar with,\nis very similar in micronaut-test. Let’s have a look at a simple test case:\n\n@MicronautTest // 1\nclass SimpleIntSpec extends Specification {\n    @Inject // 2\n    UserService userService\n\n    def \"should persist a user\"() {\n        given:\n        userService.createUser(\"user-1\", \"John\", \"Doe\")\n\n        when:\n        def user = userService.getUser(\"user-1\")\n\n        then:\n        user.firstName == \"John\"\n        user.lastName == \"Doe\"\n    }\n}\n\n\nThere are two interesting things in this test case:\n(1) @MicronautTest is an annotation you have to put in your test classes to start Micronaut application,\n(2) @Inject is Micronaut’s version of @Autowired (or… @Inject, which is also supported by Spring). Be aware that since\nMicronaut 3.0.0 you should use @jakarta.inject.Inject annotation instead of the former @javax.inject.Inject.\nIf your tests make API calls to your application via REST endpoints, and you run your web container on a random port (which is common), then the way to retrieve it\nis through the use of the injected EmbeddedServer:\n\n@MicronautTest\nclass ApiIntSpec extends Specification {\n    @Inject\n    EmbeddedServer server\n\n    def \"should create a user using API call\"() {\n        given:\n        def url = \"http://localhost:{$server.port}/users\"\n\n        when:\n        // here goes your test...\n    }\n}\n\n\nMoney\nAs a side effect, an additional benefit you get when you use Micronaut is a faster development cycle. As stated at the\nbeginning of this post, one of the main features of this framework is faster startup. Therefore, when writing test cases and then running tests,\ntheir execution time is lower than their Spring equivalent. This may not be significant if your tests are few, but sooner or later\ntheir number will grow and then the speed will become more visible and important. For large codebases time savings can be really impressive.\nShould I stay or should I go\nThe experience we gained during migration to Micronaut gave us more courage and assurance. So when the time came to decide what technology\nto use for a quite large greenfield project, we didn’t hesitate (well, we actually did, but not for long).\nSix months later with the system running in the production environment we’re happy we started that long journey. And if you’re considering\nMicronaut for one of your projects, I can wholeheartedly recommend: go for it.","guid":"https://blog.allegro.tech/2021/11/micronaut.html","categories":["tech","backend","performance","micronaut","kotlin","graalvm"],"isoDate":"2021-11-21T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"OAuth rate-limiting","link":"https://blog.allegro.tech/2021/11/oauth-rate-limiting.html","pubDate":"Tue, 09 Nov 2021 00:00:00 +0100","authors":{"author":[{"name":["Marek Walkowiak"],"photo":["https://blog.allegro.tech/img/authors/marek.walkowiak.jpg"],"url":["https://blog.allegro.tech/authors/marek.walkowiak"]},{"name":["Daniel Faderski"],"photo":["https://blog.allegro.tech/img/authors/daniel.faderski.jpg"],"url":["https://blog.allegro.tech/authors/daniel.faderski"]}]},"content":"<p>Every e-commerce platform needs some kind of central authorization system. At <a href=\"https://allegro.tech/\">Allegro</a> we use\nOAuth and have our own implementation that stays true to the\n<a href=\"https://datatracker.ietf.org/doc/html/rfc6749\">RFC</a>. Allegro has millions of users. There are also a lot of requests\nthat go through OAuth services. At some point there comes a need to have better control over how much traffic we want to\nallow in a certain time window, while maintaining full performance of the platform. Here is where the idea of\nrate-limiting comes in handy.</p>\n\n<h2 id=\"prologue\">Prologue</h2>\n\n<p>According to OAuth RFC, to use OAuth you need to be a registered client. Some clients are very small external\nintegrators (simple shops), while others are in a different league and can produce millions of requests per day (Allegro\nmobile and other large partner apps). Every user can create their own OAuth client and use it to integrate with\nDevelopers API. Unfortunately, not all of them do that correctly as per RFC.</p>\n\n<p>Normally such clients are not a big issue, but in certain cases they can generate a lot of unwanted and unneeded\ntraffic. This traffic includes, but is not limited to, creating huge numbers of new access tokens that are then thrown\naway instead of being reused.</p>\n\n<p>According to the RFC, the access tokens generated with most grant types (e.g. authorization code grant) should be reused\nup until their expiration period. When they expire, the\nprovided <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-1.5\">refresh token</a> should be used to receive a new\naccess token via refresh token grant.</p>\n\n<p>Some of the clients rarely refresh tokens or even don’t reuse them at all. This causes a lot of unnecessary traffic (\noften in the form of sudden spikes)\nthat can lead to potential issues on both sides. We had pretty good monitoring of this issue, but we needed better tools\nto deal with that problem as well as to educate the clients to make proper use of OAuth.</p>\n\n<h2 id=\"planning-the-solution\">Planning the solution</h2>\n\n<p>Before tackling the problem directly we needed a little more information and careful planning. Firstly, we wanted to\nmake sure that our solution would solve the problem. Secondly, blocking too many clients could end up in disaster. To be\ncertain that our solution was error-free, we started by making sure that we knew what we want to achieve. Here are the\nproperties we expected from our solution:</p>\n\n<ul>\n  <li>It cannot block trusted clients such as Allegro apps.</li>\n  <li>It should not negatively affect performance.</li>\n  <li>It should be configurable per client since different clients have different traffic characteristics.</li>\n  <li>It should be able to distinguish between user and non-user use of OAuth. Client credentials grant used without context\nof a user, for example should be treated differently than user-based authorization code grant. In effect, the limiting\n“per user” should be introduced in the second case.</li>\n  <li>It should directly cause an improvement in traffic spikes.</li>\n  <li>It should work well in a highly distributed environment with dozens of service instances and database nodes.</li>\n  <li>It cannot be too costly or require too much additional infrastructure like additional databases, external systems,\netc.</li>\n</ul>\n\n<h2 id=\"tackling-the-problem\">Tackling the problem</h2>\n\n<p>To meet those needs we needed a robust solution. Since RFC leaves a lot of room for implementation by end-users, it does\nnot specify how such rate-limiting should work.</p>\n\n<p>As with every problem of such kind, it’s worth starting with in-depth research of existing solutions. There are various\nstrategies and approaches to this problem in many different scenarios, but only a few of them were applicable to our\ncase and enabled us to fulfill our goals.</p>\n\n<p>As usual, we also explored the existing implementations of such solutions in the form of enterprise or open-source\nlibraries and frameworks. Unfortunately, we did not find any that would meet all of our needs and at the same time be\nflexible enough to easily integrate into our ecosystem. It’s also worth noting that we were limited by certain\nconstraints such as long-term costs of additional resources.</p>\n\n<p>In the end, we decided to go with implementing our own solution. There were several algorithms to choose from that could\nserve as its base:</p>\n\n<ol>\n  <li>Precise query-based per-user counter — query the database for a token count for each request. Not suitable, because\nit would cause way too much database traffic.</li>\n  <li>Fixed window based on TTL documents — uses a rate-limit counter that is cleared every fixed period. While less\nchallenging for the database, it shares a common vulnerability with the first algorithm: sudden TTL-caused spikes in\nallowed requests that consequently cause all of the following requests to be blocked (rate-limit exhaustion).</li>\n  <li>Sliding window log — stores all requests as a timestamped log entry in a database, that are later aggregated when\nneeded. Too costly, because again it would cause too heavy a load on the database.</li>\n  <li>Sliding window — uses a rate-limit counter that is a time-based weighted moving window. It has the advantage of\nrelatively low database load of the fixed window algorithm without its spike-related flaws.</li>\n</ol>\n\n<p>We carefully weighed the pros and cons of each of those options and finally decided to make a PoC of the fourth option:\nthe sliding window algorithm.</p>\n\n<p>The algorithm is based on counters that each hold a count of requests in their respective time frames. At each point of\ntime, an abstract window is calculated from two neighboring frames: current and previous. The further away the window is\nfrom the previous frame, the more important the counter from the current frame is. The weight is based strictly on\nproportions. If, for example, the span of the current window is 75% of the previous frame and 25% of the current frame,\nthen the value of the current rate-limit counter is a sum of 75% of the previous frame counter and 25% of the current\nframe counter. The results are put into a hashmap with users as keys and counters as values, which acts as a cache.</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/sliding-window-algorithm.png\" alt=\"Sliding window algorithm\" />\nIn the picture above the current counter value would be:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>value = previousCounter * partOfPreviousFrame + currentCounter * partOfCurrentFrame\nvalue = 12 * 0.75 + 5 * 0.25 = 10.25\n</code></pre></div></div>\n\n<p>The time complexity of the sliding window algorithm is O(1) since it’s a simple hashmap get operation, while its space\ncomplexity is O(n) where n is the number of user counters held by an instance. It works well in distributed systems\nbecause it’s based on simple counters that are relatively easy to synchronize. Its cost is low since we only need to\nstore simple counters and can easily minimize database queries, as well as improve performance with caching in a\nstraightforward manner. Thanks to the fact that the sliding window takes into account the time relative to the current\ntimestamp, we can flatten the token creation spikes. Its main disadvantage is that it indirectly relies on the even\ndistribution of requests in time, which makes it less sensitive to spike traffic within one time window.</p>\n\n<h2 id=\"following-the-plan\">Following the plan</h2>\n\n<h3 id=\"remote-state-synchronization\">Remote state synchronization</h3>\n\n<p>There are two kinds of states that need to be tracked for the rate-limiting to work. The first one is the global state.\nIt’s basically a global counter of all requests made by a client to rate-limited OAuth endpoints within a certain\nperiod. This state is persisted in the database and from its perspective represents the most recent rate-limit status.</p>\n\n<p>The second type is the local state of each instance, one part of which is the local counter of incoming requests for a\nclient within a time window. Since instances are not directly connected to the client and every one of them can make\nrequests to different instances concurrently, the local counter of each instance for that client will most probably have\na different value. Rate-limiting is a global functionality in the sense that we are interested in the value of global\ncounters and not of individual counters. Consider the following scenario: we would like to set a rate limit of 50 RPS\nfor a client while having 10 instances. If we relied only on local counters to do the rate-limiting, it would be\npossible for the traffic of 500 RPS to be split evenly between instances and not trigger rate-limiting - and that’s not\nthe result we are aiming for.</p>\n\n<p>To decide whether to reject an incoming request or not, the local instance needs a second part of the state - a global\ncounter. When making a decision, it sums both the local and the global ones to check whether new requests would go\nbeyond the rate-limit boundaries.</p>\n\n<p>Consequently, the instances need to be aware of the requests made to the others and for that to work, the global counter\nneeds to be updated regularly. Each instance will periodically try to flush the sum of global and local counters from\ntheir state to the database. Here is where things get tricky. If several instances are trying to save different global\ncounters, there is a risk that one of them will attempt to overwrite the changes that were just saved by the other. To\ncounter that, we use a mechanism called optimistic locking that is\ndescribed <a href=\"#resolving-the-conflicts\">later in this post</a>. Once the state is persisted successfully, the local state is\ncleared. At this point, it consists of the incoming requests counter that is equal to 0 and the global snapshot counter\nthat is equal to the value persisted in the database for that client.</p>\n\n<h3 id=\"caching-clients-global-state\">Caching clients’ global state</h3>\n\n<p>Any instance should be aware of the rate limit state for the whole cluster. It represents the global number of requests\nmade by a particular client and its users. We keep a mapping between a pair (client_id, username) and the number of\nrequests made in memory, which makes our algorithm efficient. There is one caveat: keeping all the clients and their\nusers would take too much space, so we only keep those clients and users who are actively making requests to the OAuth\nserver. As soon as they stop calling our servers we delete them from the instance’s cache.</p>\n\n<h3 id=\"sharing-the-state\">Sharing the state</h3>\n\n<p>Our internal OAuth service works in a distributed manner. There are many instances of the OAuth servers and we should\nhave mechanisms to coordinate rate-limiting between them. In practice, it means that if a client is making a request to\nserver instance A, the server instance B should consider it when calculating the allowed number of requests in the\ncurrent time window. Below are the key points that sum up this description:</p>\n\n<ul>\n  <li>We have a global request counter per each client/user.</li>\n  <li>The counter is stored in the Mongo database.</li>\n  <li>Each instance shares the counter - it reads and writes its current value.</li>\n  <li>The counter depicts the window in the current timestamp.</li>\n</ul>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"clientId\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"some-client\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"username\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"some-user\"</span><span class=\"w\">\n  </span><span class=\"p\">},</span><span class=\"w\">\n  </span><span class=\"nl\">\"version\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">NumberLong(</span><span class=\"mi\">405</span><span class=\"err\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"expire\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">ISODate(</span><span class=\"s2\">\"2021-05-29T14:24:01.376Z\"</span><span class=\"err\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"requestCounters\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"1622211780\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"1622211840\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The counter consists of a few fields:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">_id</code> - to uniquely identify the client and the user connected with the request.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">version</code> - for the optimistic locking purposes.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">expire</code> - used to remove the counter if it has no updates, which means that the client is not currently creating new\ntokens.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">requestCounters</code> - mapping of the number of requests at consecutive points in time.</li>\n</ul>\n\n<p>The counter is cached so we don’t have to fetch it each time the request from the client is made, since it would kill\nperformance. Each instance refreshes the counter asynchronously - reads and writes to the Mongo database are made in a\ndedicated thread. Each one also holds the counters only for the clients they have to handle. A particular instance\nremoves the client’s state after it stops sending requests.</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/sharing-the-state.png\" alt=\"Sharing the state example\" />\n<img src=\"/img/articles/2021-11-09-oauth-rate-limiting/sharing-the-state-table.png\" alt=\"Sharing the state example table\" /></p>\n\n<p>This table depicts an example flow of counters on two instances (A, B) of our OAuth service. Each of them has its own\ncounter\n(accordingly <strong>cnt1</strong> and <strong>cnt2</strong>), <strong>mng</strong> is a state in the sharded Mongo database and <strong>total req</strong> is the sum of\nall requests made to all instances.</p>\n\n<p>The state of each instance consists of two counters (<code class=\"language-plaintext highlighter-rouge\">g</code> / <code class=\"language-plaintext highlighter-rouge\">i</code>). The first one (<code class=\"language-plaintext highlighter-rouge\">g</code> for “global”) describes how many\nrequests have been globally made and are persisted in the database. The second one tracks how many requests have come to\nthat instance since the last flush (<code class=\"language-plaintext highlighter-rouge\">i</code> for “in-flight”). The total number of requests from its perspective is the sum\nof <code class=\"language-plaintext highlighter-rouge\">g</code> and <code class=\"language-plaintext highlighter-rouge\">i</code> and it is the value that is going to be persisted.</p>\n\n<p>Below is the description of what happens in this scenario, step by step:</p>\n\n<ol>\n  <li>There is an initial state with no requests made to either instance.</li>\n  <li>One request is made to instance A and its i<sub>A</sub> counter is increased to 1.</li>\n  <li>Next, instance A pushes its total counter to the database, setting its <code class=\"language-plaintext highlighter-rouge\">g</code> counter to 1 and its <code class=\"language-plaintext highlighter-rouge\">i</code> counter to 0.</li>\n  <li>Instance B periodically pulls the <code class=\"language-plaintext highlighter-rouge\">g</code> counter from the database. Now, the new counter is pulled and the instance’s\ncurrent state is <code class=\"language-plaintext highlighter-rouge\">g</code> = 1, i<sub>B</sub> = 0.</li>\n  <li>Next, instance A receives one request, and at the same time, instance B receives three requests. At this time\ninstance A knows of 2 requests, 1 of which came since the last flush. Instance B knows of 4 requests, 3 of them came\nsince the last flush.</li>\n  <li>Instance B pushes its total counter (3+1=4) to Mongo. As a consequence, its state is set to (4 / 0). Instance A has\nnot yet flushed its counter to persistent storage. Remember that both instances do it independently whenever a fixed\ninterval passes.</li>\n  <li>Now, Instance A tries to push its state (1+1=2) to the database. If the push was successful, it would overwrite the\nstate previously written by the instance B (in step 5), resulting in data loss. We want our counters to be precise so\nwe need to use optimistic locking here. It causes the push to fail, if the version of its state differs from the\nversion persisted in the database. If that scenario occurs, the instance knows that it should refresh its global\ncounter before trying to save it again.</li>\n  <li>Instance A refreshed its state. It is now equal to (4/1), which means 4 requests are persisted in the database and 1\nthat has not been flushed yet. Now it can safely push its total counter (4+1=5) to the database and set its state\nto (5/0).</li>\n  <li>At this point both instances have pushed their state to the database. Notice, however, that instance B has not yet\npulled the counter written by instance B in the previous step.</li>\n  <li>After the last pull, the counter states on both instances are consistent with the database state and reflect the\nglobal number of requests made by a client.</li>\n</ol>\n\n<h3 id=\"persisting-the-state\">Persisting the state</h3>\n\n<p>To properly persist the state in a distributed environment with minimal impact on application performance, we needed to\ntake several strategies into consideration.</p>\n\n<h4 id=\"resolving-the-conflicts\">Resolving the conflicts</h4>\n\n<p>As we’ve already mentioned we use optimistic locking to prevent the state from being overwritten by the different\ninstances. It’s quite a common problem in a the world of distributed systems. It works by using version numbers kept in\nMongoDB, which designates how many updates have been made from the beginning of its creation. After each update the\nversion increases by one.</p>\n\n<p>Before save to the database:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n   </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"err\">...</span><span class=\"p\">},</span><span class=\"w\">\n   </span><span class=\"nl\">\"version\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"nl\">\"requestCount\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"err\">...</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>After the save:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n   </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"err\">...</span><span class=\"p\">},</span><span class=\"w\">\n   </span><span class=\"nl\">\"version\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"nl\">\"requestCount\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"err\">...</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>But how does a particular instance save the document atomically? How does it know that there was an update made by\nanother instance in the meantime? For this purpose, we use a Mongo query to do\nthe <a href=\"https://en.wikipedia.org/wiki/Compare-and-swap\">CAS</a> update that looks like:</p>\n\n<div class=\"language-js highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">ratelimits</span><span class=\"p\">.</span><span class=\"nx\">update</span><span class=\"p\">(</span>\n <span class=\"o\">&lt;</span><span class=\"nx\">filter</span> <span class=\"nx\">query</span><span class=\"o\">&gt;</span>\n <span class=\"o\">&lt;</span><span class=\"nx\">update</span> <span class=\"nx\">operation</span><span class=\"o\">&gt;</span>\n<span class=\"p\">)</span>\n\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">ratelimits</span><span class=\"p\">.</span><span class=\"nx\">update</span><span class=\"p\">(</span>\n <span class=\"p\">{</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"p\">{...},</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">version</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span>\n <span class=\"p\">},</span>\n <span class=\"p\">{</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">version</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">requestCount</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"mi\">10</span>\n <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If the query returns 0 elements updated we know there was a collision and there was a concurrent save which changed the\nstate. If this happens we need to:</p>\n\n<ol>\n  <li>Update state from database to local instance,</li>\n  <li>Apply inflight recorded changes (the ones not yet persisted to the database),</li>\n  <li>Save the state,</li>\n  <li>If the query returns 1 element updated, the save was successful without any collisions,</li>\n</ol>\n\n<h4 id=\"saving-in-batches\">Saving in batches</h4>\n\n<p>As we already mentioned, a single instance of the OAuth server handles many clients. So it has to synchronize the state\nfor each of them. Doing the above save operation for each client individually would cause a massive number of queries\nand would quickly saturate our resources. That’s why we\nuse <a href=\"https://docs.mongodb.com/manual/core/bulk-write-operations/#bulk-write-operations\">Mongo bulk operations</a>. They\nallow defining many operations in a single query.</p>\n\n<h2 id=\"going-into-production\">Going into production</h2>\n\n<h3 id=\"dry-run\">Dry-run</h3>\n\n<p>Before actually blocking the clients we needed a way to check real outputs from our solution. We implemented the dry-run\nmode, which enabled us do this without affecting any clients, while also giving us metrics depicting which of them would\nbe blocked given a particular limit and time window. We took the following steps:</p>\n\n<ol>\n  <li>Set high rate limit up to the point when no clients would be blocked (running in dry-run mode all the time).</li>\n  <li>Lowered the threshold to the point that showed us the outstanding clients, while not blocking the others.</li>\n  <li>Communicated with clients abusing our future rate limit policy and gave them a chance to optimize the way they use\nthe OAuth server.</li>\n  <li>Switched modes from dry-run to actively blocking the clients.</li>\n</ol>\n\n<h3 id=\"canary-deployment\">Canary deployment</h3>\n\n<p>It would be risky to deploy this kind of feature to the whole OAuth cluster without ensuring it’s properly working on a\nfew instances. We used canary deployment which allows deploying a version of a service to only a few production\ninstances. During this deployment, we monitored CPU usage and response time metrics. After ensuring there was meaningful\ndisparity we rolled out the full feature to all production instances.</p>\n\n<h3 id=\"observability\">Observability</h3>\n\n<p>To monitor and verify our solution we needed a bunch of metrics telling us how many clients were affected by rate limit\npolicy and which of them are getting closer to being blocked. In practice, two charts shown below were enough to monitor\ntheir behaviour:</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/ratelimit-denied-rate.png\" alt=\"Ratelimit Denied Rate\" /></p>\n\n<p>This chart shows clients and their blocked rate. Each color depicts a different blocked client.</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/ratelimit-allowed-rate.png\" alt=\"Ratelimit Allowed Rate\" /></p>\n\n<p>This one on the other hand depicts the allowed rate. The limit line is helpful to see if any client is getting closer to\nit and will be blocked soon. It’s worth mentioning that the default rate limit policy isn’t sufficient for all of the\nclients. Some of them require special treatment and for them we can configure different thresholds, which is why a few\nof them go beyond the actual default limit (the red line).</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Rate-limiting is a common problem, but surprisingly not so trivial to solve, especially in a high-scale, distributed\nenvironment. Plenty of popular solutions can be found, but most of them deal with it only from a perspective of a single\nmachine and were not well-suited for our system. Coming up with the above approach took quite a bit of research and\nplanning, but in the end, its deployment allowed us to effectively achieve our goal. We are pretty content with the\nfinal product, as it is both effective and fast, but are constantly tweaking it and looking for new ways to optimize\nthat process.</p>\n","contentSnippet":"Every e-commerce platform needs some kind of central authorization system. At Allegro we use\nOAuth and have our own implementation that stays true to the\nRFC. Allegro has millions of users. There are also a lot of requests\nthat go through OAuth services. At some point there comes a need to have better control over how much traffic we want to\nallow in a certain time window, while maintaining full performance of the platform. Here is where the idea of\nrate-limiting comes in handy.\nPrologue\nAccording to OAuth RFC, to use OAuth you need to be a registered client. Some clients are very small external\nintegrators (simple shops), while others are in a different league and can produce millions of requests per day (Allegro\nmobile and other large partner apps). Every user can create their own OAuth client and use it to integrate with\nDevelopers API. Unfortunately, not all of them do that correctly as per RFC.\nNormally such clients are not a big issue, but in certain cases they can generate a lot of unwanted and unneeded\ntraffic. This traffic includes, but is not limited to, creating huge numbers of new access tokens that are then thrown\naway instead of being reused.\nAccording to the RFC, the access tokens generated with most grant types (e.g. authorization code grant) should be reused\nup until their expiration period. When they expire, the\nprovided refresh token should be used to receive a new\naccess token via refresh token grant.\nSome of the clients rarely refresh tokens or even don’t reuse them at all. This causes a lot of unnecessary traffic (\noften in the form of sudden spikes)\nthat can lead to potential issues on both sides. We had pretty good monitoring of this issue, but we needed better tools\nto deal with that problem as well as to educate the clients to make proper use of OAuth.\nPlanning the solution\nBefore tackling the problem directly we needed a little more information and careful planning. Firstly, we wanted to\nmake sure that our solution would solve the problem. Secondly, blocking too many clients could end up in disaster. To be\ncertain that our solution was error-free, we started by making sure that we knew what we want to achieve. Here are the\nproperties we expected from our solution:\nIt cannot block trusted clients such as Allegro apps.\nIt should not negatively affect performance.\nIt should be configurable per client since different clients have different traffic characteristics.\nIt should be able to distinguish between user and non-user use of OAuth. Client credentials grant used without context\nof a user, for example should be treated differently than user-based authorization code grant. In effect, the limiting\n“per user” should be introduced in the second case.\nIt should directly cause an improvement in traffic spikes.\nIt should work well in a highly distributed environment with dozens of service instances and database nodes.\nIt cannot be too costly or require too much additional infrastructure like additional databases, external systems,\netc.\nTackling the problem\nTo meet those needs we needed a robust solution. Since RFC leaves a lot of room for implementation by end-users, it does\nnot specify how such rate-limiting should work.\nAs with every problem of such kind, it’s worth starting with in-depth research of existing solutions. There are various\nstrategies and approaches to this problem in many different scenarios, but only a few of them were applicable to our\ncase and enabled us to fulfill our goals.\nAs usual, we also explored the existing implementations of such solutions in the form of enterprise or open-source\nlibraries and frameworks. Unfortunately, we did not find any that would meet all of our needs and at the same time be\nflexible enough to easily integrate into our ecosystem. It’s also worth noting that we were limited by certain\nconstraints such as long-term costs of additional resources.\nIn the end, we decided to go with implementing our own solution. There were several algorithms to choose from that could\nserve as its base:\nPrecise query-based per-user counter — query the database for a token count for each request. Not suitable, because\nit would cause way too much database traffic.\nFixed window based on TTL documents — uses a rate-limit counter that is cleared every fixed period. While less\nchallenging for the database, it shares a common vulnerability with the first algorithm: sudden TTL-caused spikes in\nallowed requests that consequently cause all of the following requests to be blocked (rate-limit exhaustion).\nSliding window log — stores all requests as a timestamped log entry in a database, that are later aggregated when\nneeded. Too costly, because again it would cause too heavy a load on the database.\nSliding window — uses a rate-limit counter that is a time-based weighted moving window. It has the advantage of\nrelatively low database load of the fixed window algorithm without its spike-related flaws.\nWe carefully weighed the pros and cons of each of those options and finally decided to make a PoC of the fourth option:\nthe sliding window algorithm.\nThe algorithm is based on counters that each hold a count of requests in their respective time frames. At each point of\ntime, an abstract window is calculated from two neighboring frames: current and previous. The further away the window is\nfrom the previous frame, the more important the counter from the current frame is. The weight is based strictly on\nproportions. If, for example, the span of the current window is 75% of the previous frame and 25% of the current frame,\nthen the value of the current rate-limit counter is a sum of 75% of the previous frame counter and 25% of the current\nframe counter. The results are put into a hashmap with users as keys and counters as values, which acts as a cache.\n\nIn the picture above the current counter value would be:\n\nvalue = previousCounter * partOfPreviousFrame + currentCounter * partOfCurrentFrame\nvalue = 12 * 0.75 + 5 * 0.25 = 10.25\n\n\nThe time complexity of the sliding window algorithm is O(1) since it’s a simple hashmap get operation, while its space\ncomplexity is O(n) where n is the number of user counters held by an instance. It works well in distributed systems\nbecause it’s based on simple counters that are relatively easy to synchronize. Its cost is low since we only need to\nstore simple counters and can easily minimize database queries, as well as improve performance with caching in a\nstraightforward manner. Thanks to the fact that the sliding window takes into account the time relative to the current\ntimestamp, we can flatten the token creation spikes. Its main disadvantage is that it indirectly relies on the even\ndistribution of requests in time, which makes it less sensitive to spike traffic within one time window.\nFollowing the plan\nRemote state synchronization\nThere are two kinds of states that need to be tracked for the rate-limiting to work. The first one is the global state.\nIt’s basically a global counter of all requests made by a client to rate-limited OAuth endpoints within a certain\nperiod. This state is persisted in the database and from its perspective represents the most recent rate-limit status.\nThe second type is the local state of each instance, one part of which is the local counter of incoming requests for a\nclient within a time window. Since instances are not directly connected to the client and every one of them can make\nrequests to different instances concurrently, the local counter of each instance for that client will most probably have\na different value. Rate-limiting is a global functionality in the sense that we are interested in the value of global\ncounters and not of individual counters. Consider the following scenario: we would like to set a rate limit of 50 RPS\nfor a client while having 10 instances. If we relied only on local counters to do the rate-limiting, it would be\npossible for the traffic of 500 RPS to be split evenly between instances and not trigger rate-limiting - and that’s not\nthe result we are aiming for.\nTo decide whether to reject an incoming request or not, the local instance needs a second part of the state - a global\ncounter. When making a decision, it sums both the local and the global ones to check whether new requests would go\nbeyond the rate-limit boundaries.\nConsequently, the instances need to be aware of the requests made to the others and for that to work, the global counter\nneeds to be updated regularly. Each instance will periodically try to flush the sum of global and local counters from\ntheir state to the database. Here is where things get tricky. If several instances are trying to save different global\ncounters, there is a risk that one of them will attempt to overwrite the changes that were just saved by the other. To\ncounter that, we use a mechanism called optimistic locking that is\ndescribed later in this post. Once the state is persisted successfully, the local state is\ncleared. At this point, it consists of the incoming requests counter that is equal to 0 and the global snapshot counter\nthat is equal to the value persisted in the database for that client.\nCaching clients’ global state\nAny instance should be aware of the rate limit state for the whole cluster. It represents the global number of requests\nmade by a particular client and its users. We keep a mapping between a pair (client_id, username) and the number of\nrequests made in memory, which makes our algorithm efficient. There is one caveat: keeping all the clients and their\nusers would take too much space, so we only keep those clients and users who are actively making requests to the OAuth\nserver. As soon as they stop calling our servers we delete them from the instance’s cache.\nSharing the state\nOur internal OAuth service works in a distributed manner. There are many instances of the OAuth servers and we should\nhave mechanisms to coordinate rate-limiting between them. In practice, it means that if a client is making a request to\nserver instance A, the server instance B should consider it when calculating the allowed number of requests in the\ncurrent time window. Below are the key points that sum up this description:\nWe have a global request counter per each client/user.\nThe counter is stored in the Mongo database.\nEach instance shares the counter - it reads and writes its current value.\nThe counter depicts the window in the current timestamp.\n\n{\n  \"_id\" : {\n    \"clientId\" : \"some-client\",\n    \"username\" : \"some-user\"\n  },\n  \"version\" : NumberLong(405),\n  \"expire\" : ISODate(\"2021-05-29T14:24:01.376Z\"),\n  \"requestCounters\" : {\n    \"1622211780\" : 1,\n    \"1622211840\" : 1\n  }\n}\n\n\nThe counter consists of a few fields:\n_id - to uniquely identify the client and the user connected with the request.\nversion - for the optimistic locking purposes.\nexpire - used to remove the counter if it has no updates, which means that the client is not currently creating new\ntokens.\nrequestCounters - mapping of the number of requests at consecutive points in time.\nThe counter is cached so we don’t have to fetch it each time the request from the client is made, since it would kill\nperformance. Each instance refreshes the counter asynchronously - reads and writes to the Mongo database are made in a\ndedicated thread. Each one also holds the counters only for the clients they have to handle. A particular instance\nremoves the client’s state after it stops sending requests.\n\n\nThis table depicts an example flow of counters on two instances (A, B) of our OAuth service. Each of them has its own\ncounter\n(accordingly cnt1 and cnt2), mng is a state in the sharded Mongo database and total req is the sum of\nall requests made to all instances.\nThe state of each instance consists of two counters (g / i). The first one (g for “global”) describes how many\nrequests have been globally made and are persisted in the database. The second one tracks how many requests have come to\nthat instance since the last flush (i for “in-flight”). The total number of requests from its perspective is the sum\nof g and i and it is the value that is going to be persisted.\nBelow is the description of what happens in this scenario, step by step:\nThere is an initial state with no requests made to either instance.\nOne request is made to instance A and its iA counter is increased to 1.\nNext, instance A pushes its total counter to the database, setting its g counter to 1 and its i counter to 0.\nInstance B periodically pulls the g counter from the database. Now, the new counter is pulled and the instance’s\ncurrent state is g = 1, iB = 0.\nNext, instance A receives one request, and at the same time, instance B receives three requests. At this time\ninstance A knows of 2 requests, 1 of which came since the last flush. Instance B knows of 4 requests, 3 of them came\nsince the last flush.\nInstance B pushes its total counter (3+1=4) to Mongo. As a consequence, its state is set to (4 / 0). Instance A has\nnot yet flushed its counter to persistent storage. Remember that both instances do it independently whenever a fixed\ninterval passes.\nNow, Instance A tries to push its state (1+1=2) to the database. If the push was successful, it would overwrite the\nstate previously written by the instance B (in step 5), resulting in data loss. We want our counters to be precise so\nwe need to use optimistic locking here. It causes the push to fail, if the version of its state differs from the\nversion persisted in the database. If that scenario occurs, the instance knows that it should refresh its global\ncounter before trying to save it again.\nInstance A refreshed its state. It is now equal to (4/1), which means 4 requests are persisted in the database and 1\nthat has not been flushed yet. Now it can safely push its total counter (4+1=5) to the database and set its state\nto (5/0).\nAt this point both instances have pushed their state to the database. Notice, however, that instance B has not yet\npulled the counter written by instance B in the previous step.\nAfter the last pull, the counter states on both instances are consistent with the database state and reflect the\nglobal number of requests made by a client.\nPersisting the state\nTo properly persist the state in a distributed environment with minimal impact on application performance, we needed to\ntake several strategies into consideration.\nResolving the conflicts\nAs we’ve already mentioned we use optimistic locking to prevent the state from being overwritten by the different\ninstances. It’s quite a common problem in a the world of distributed systems. It works by using version numbers kept in\nMongoDB, which designates how many updates have been made from the beginning of its creation. After each update the\nversion increases by one.\nBefore save to the database:\n\n{\n   \"_id\" : {...},\n   \"version\" : 0,\n   \"requestCount\": 0,\n   ...\n}\n\n\nAfter the save:\n\n{\n   \"_id\" : {...},\n   \"version\" : 1,\n   \"requestCount\": 5,\n   ...\n}\n\n\nBut how does a particular instance save the document atomically? How does it know that there was an update made by\nanother instance in the meantime? For this purpose, we use a Mongo query to do\nthe CAS update that looks like:\n\n\ndb.ratelimits.update(\n <filter query>\n <update operation>\n)\n\ndb.ratelimits.update(\n {\n   \"_id\": {...},\n   \"version\": 1\n },\n {\n   \"version\": 2,\n   \"requestCount\": 10\n }\n)\n\n\nIf the query returns 0 elements updated we know there was a collision and there was a concurrent save which changed the\nstate. If this happens we need to:\nUpdate state from database to local instance,\nApply inflight recorded changes (the ones not yet persisted to the database),\nSave the state,\nIf the query returns 1 element updated, the save was successful without any collisions,\nSaving in batches\nAs we already mentioned, a single instance of the OAuth server handles many clients. So it has to synchronize the state\nfor each of them. Doing the above save operation for each client individually would cause a massive number of queries\nand would quickly saturate our resources. That’s why we\nuse Mongo bulk operations. They\nallow defining many operations in a single query.\nGoing into production\nDry-run\nBefore actually blocking the clients we needed a way to check real outputs from our solution. We implemented the dry-run\nmode, which enabled us do this without affecting any clients, while also giving us metrics depicting which of them would\nbe blocked given a particular limit and time window. We took the following steps:\nSet high rate limit up to the point when no clients would be blocked (running in dry-run mode all the time).\nLowered the threshold to the point that showed us the outstanding clients, while not blocking the others.\nCommunicated with clients abusing our future rate limit policy and gave them a chance to optimize the way they use\nthe OAuth server.\nSwitched modes from dry-run to actively blocking the clients.\nCanary deployment\nIt would be risky to deploy this kind of feature to the whole OAuth cluster without ensuring it’s properly working on a\nfew instances. We used canary deployment which allows deploying a version of a service to only a few production\ninstances. During this deployment, we monitored CPU usage and response time metrics. After ensuring there was meaningful\ndisparity we rolled out the full feature to all production instances.\nObservability\nTo monitor and verify our solution we needed a bunch of metrics telling us how many clients were affected by rate limit\npolicy and which of them are getting closer to being blocked. In practice, two charts shown below were enough to monitor\ntheir behaviour:\n\nThis chart shows clients and their blocked rate. Each color depicts a different blocked client.\n\nThis one on the other hand depicts the allowed rate. The limit line is helpful to see if any client is getting closer to\nit and will be blocked soon. It’s worth mentioning that the default rate limit policy isn’t sufficient for all of the\nclients. Some of them require special treatment and for them we can configure different thresholds, which is why a few\nof them go beyond the actual default limit (the red line).\nConclusion\nRate-limiting is a common problem, but surprisingly not so trivial to solve, especially in a high-scale, distributed\nenvironment. Plenty of popular solutions can be found, but most of them deal with it only from a perspective of a single\nmachine and were not well-suited for our system. Coming up with the above approach took quite a bit of research and\nplanning, but in the end, its deployment allowed us to effectively achieve our goal. We are pretty content with the\nfinal product, as it is both effective and fast, but are constantly tweaking it and looking for new ways to optimize\nthat process.","guid":"https://blog.allegro.tech/2021/11/oauth-rate-limiting.html","categories":["tech","architecture","oauth","microservices"],"isoDate":"2021-11-08T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999785422127","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"229d607a-333b-431b-9abe-78137730f5fd","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:56:17.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785422127","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999785421861","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"a6b2b59e-28e3-4bfa-89ab-b13ab97f06c8","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:54:52.000Z","location":{"city":"Warszawa, Poznań, Kraków, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785421861","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999779485268","name":"Chief Architect","uuid":"5ea8adaa-e9ae-4cb2-a1dc-b27600247ffb","refNumber":"REF2835R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T13:53:22.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Gdańsk, Łódź, Katowice, Lublin","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"chief architect, architekt, architect, platform, architektura"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779485268","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779448775","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"c8e577cc-c93a-43e7-8e73-e430989798d7","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:36.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448775","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779448676","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"7cb35dfc-f53c-4b51-81ac-61b683060f4c","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:00.000Z","location":{"city":"Warszawa, Poznań, Kraków, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448676","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1638356820000,"duration":7200000,"id":"282421464","name":"Allegro Tech Labs #9 Online: System design workshop","date_in_series_pattern":false,"status":"past","time":1639501200000,"local_date":"2021-12-14","local_time":"18:00","updated":1639514166000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":62,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/282421464/","description":"❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: https://app.evenea.pl/event/allegro-tech-labs-9/ ❗ To już druga edycja naszych warsztatów System Design Workshop! Czy chcesz poznać tajniki tworzenia systemów?…","how_to_find_us":"https://app.evenea.pl/event/allegro-tech-labs-9/","visibility":"public","member_pay_fee":false},{"created":1635344914000,"duration":7200000,"id":"281692274","name":"Allegro Tech Live #23 - Przygody backendowców w C#","date_in_series_pattern":false,"status":"past","time":1636045200000,"local_date":"2021-11-04","local_time":"18:00","updated":1636056125000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":29,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281692274/","description":"------ Rejestracja: https://app.evenea.pl/event/allegro-tech-live-23/------- Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Kiedyś spotykaliśmy się w naszych biurach, a teraz…","visibility":"public","member_pay_fee":false},{"created":1634290537000,"duration":5400000,"id":"281441586","name":"Allegro Tech Live #22 - Jak wygląda codzienność lidera w Allegro?","date_in_series_pattern":false,"status":"past","time":1634832000000,"local_date":"2021-10-21","local_time":"18:00","updated":1634841007000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":67,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281441586/","description":"!!!! Rejestracja: https://app.evenea.pl/event/allegro-tech-live-22/ !!!! Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale…","visibility":"public","member_pay_fee":false},{"created":1633336352000,"duration":7200000,"id":"281199452","name":"Allegro Tech Live #21 - Jak zautomatyzować bezpieczeństwo IT?","date_in_series_pattern":false,"status":"past","time":1633622400000,"local_date":"2021-10-07","local_time":"18:00","updated":1633634017000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281199452/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}