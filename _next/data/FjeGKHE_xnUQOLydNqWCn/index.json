{"pageProps":{"posts":[{"title":"How we refactored the search form UI component","link":"https://blog.allegro.tech/2021/10/refactoring-opbox-search.html","pubDate":"Tue, 26 Oct 2021 00:00:00 +0200","authors":{"author":[{"name":["Volodymyr Khytskyi"],"photo":["https://blog.allegro.tech/img/authors/volodymyr.khytskyi.jpg"],"url":["https://blog.allegro.tech/authors/volodymyr.khytskyi"]}]},"content":"<p>This article describes a classic case of refactoring a search form UI component, a critical part of every e-commerce\nplatform. In it I’ll explain the precursor of change, analysis process, as well as aspects to pay attention to and\nprinciples to apply while designing a new solution. If you are planning to conduct refactoring of a codebase or just\ncurious to learn more about frontend internals at <a href=\"https://allegro.tech\">Allegro</a>, you might learn a thing or two from\nthis article. Sounds interesting? Hop on!</p>\n\n<h2 id=\"how-does-the-search-form-function-at-allegro\">How does the search form function at Allegro?</h2>\n\n<p>For starters, so that we all are on the same page, let me briefly explain how the search form at Allegro works and what\nfunctionality it is responsible for. Under the hood, it is one of our many\n<a href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">OpBox</a> components and its\ntechnical name is opbox-search.</p>\n\n<p>From UI standpoint it consists of four parts:</p>\n\n<ul>\n  <li>an input field</li>\n  <li>a scope selector</li>\n  <li>a submit button</li>\n  <li>a dropdown with a list of suggestions</li>\n</ul>\n\n<p><img src=\"/img/articles/2021-10-26-refactoring-opbox-search/component-breakdown.png\" alt=\"Component Breakdown\" title=\"Component Breakdown\" /></p>\n\n<p>Functionality-wise, whenever a user clicks/taps into the input or types a search phrase, a dropdown with a list of\nsuggestions shows up and the user can navigate through by using keyboard/mouse/touchscreen. The suggestion list\nitself, at most, consists of two sections: phrases searched in the past and popular/matching suggestions. Those are\nfetched in real time as the user interacts with the form. Additionally, there is also a preconfigured option to search\nthe phrase in products’ descriptions.</p>\n\n<p>The form also provides a possibility to narrow down the search scope to a particular department, a certain\nuser, a charity organization, etc. Depending on the selected scope, when user click the submission button, they are\nredirected to an appropriate product/user/charity listing page and the search phrase is sent as a URL query parameter.</p>\n\n<p>At this point you might be wondering: this sounds quite easy, how come you messed up such a simple component?</p>\n\n<h2 id=\"a-bit-of-history-and-the-precursor-of-refactoring\">A bit of history and the precursor of refactoring</h2>\n\n<p>The initial commit took place back in 2017 and up until the point of refactoring the project, there were 2292 commits\nspread across 189 pull requests merged to the main branch. All those contributions were made by different independent\nteams. Over time the component evolved, some external APIs changed, some features became deprecated and new ones were\nadded. At one point it also changed its ownership to another development team. As expected, all those factors left some\nmarks on the codebase.</p>\n\n<p>One of ample examples of troublesome conditions was the store entity that is responsible for handling the runtime state.\nIn reality, besides performing its primary function it also handled network calls and contained pieces of business logic\nnon-relevant for search scoping and suggestions listing.</p>\n\n<p>To make matters worse, the internals of the entity were publicly exposed and therefore any dependent, e.g. the search\ninput or the scope selector, was free to manipulate the store arbitrarily. Not hard to imagine, using such “shortcuts”\nhas lead to degradation of codebase readability and maintainability.</p>\n\n<p>At one point in time it reached its critical mass and we started considering the cost of maintaining the existing\ncodebase vs. the cost of redeveloping it from scratch.</p>\n\n<h2 id=\"refactoring-expectations\">Refactoring expectations</h2>\n\n<p>Refactoring itself doesn’t provide any business value. Instead it is an investment that should save engineers’ time and\neffort. That is why the primary goal was to gain back the confidence in further development of the component by:</p>\n\n<ul>\n  <li>streamlining the maintenance of existing features</li>\n  <li>streamlining and simplifying the development of new business features</li>\n  <li>using tools, design patterns and principles to help engineers develop stable and correctly functioning features</li>\n</ul>\n\n<p>Subsequently, achieving the goal would:</p>\n\n<ul>\n  <li>shorten the delivery time</li>\n  <li>create, from an architectural standpoint, a new solution resilient to multiple future contributions from different teams</li>\n</ul>\n\n<h2 id=\"into-the-technical-analysis\">Into the technical analysis</h2>\n\n<p>So by this point we learned what the functional requirements are and identified the issues we have with the current\nsolution. We also outlined refactoring expectations, hence we could start laying down the conceptual foundation of a new\nsolution. We began by asking ourselves a few questions that could help us formalize our technical end goals.</p>\n\n<h3 id=\"what-issues-did-we-want-to-avoid-this-time-what-did-we-want-the-new-solution-to-improve-upon\">What issues did we want to avoid this time? What did we want the new solution to improve upon?</h3>\n\n<p>Going back to the store example, we clearly didn’t want to allow any dependent to mutate its data in any abritrary way\nnor did we want the store to contain pieces of logic that are not of its concern, e.g. networking. Instead we wanted to\nmove these irrelevant pieces into more suitable locations, decrease entanglement of different parts of the codebase,\nstructure it into logical entities that are more human readable, draw boundaries between those entities, define rules\nof communication and make sure we expose to the public API only what we intended to.</p>\n\n<h3 id=\"what-development-principles-and-design-patterns-could-we-have-incorporated\">What development principles and design patterns could we have incorporated?</h3>\n\n<h4 id=\"the-single-responsibility-principle-srp\">The single-responsibility principle (SRP)</h4>\n\n<p>The “S” of the “SOLID” acronym. As the name hints, a class (or a unit of code) should be responsible only for a single\npiece of functionality. A simple and powerful principle, yet quite often overlooked. Say, if we build a piece of logic\nassociated with an input HTML element, its only responsibility should be to tightly interact with the element, e.g.\nlisten to its DOM events and update its value if needed. At the same time, you don’t want this logic to affect the\nsubmission behavior of a form element inside of which the input element is placed.</p>\n\n<h4 id=\"the-separation-of-concerns-principle-soc\">The separation of concerns principle (SoC)</h4>\n\n<p>SoC goes well in pair with SRP and states that one should not place functionalities of different domains under the same\nlogical entity (say, an object, a class or a module). For example, we need to render a piece of information on the\nscreen but beforehand the information needs to be fetched over the network. Since the view and network layers have\ndifferent concerns we don’t want to place both of them under a single logical entity. Let these two be separate ones\nwith established dependency relation to one another via a public API.</p>\n\n<h4 id=\"the-loose-coupling-principle\">The loose coupling principle</h4>\n\n<p>Loose coupling means that a single logical entity knows as little as possible about other entities and communication\nbetween them follows strict rules. One important characteristic we wanted to achieve here is to minimize negative\neffects on application’s runtime in case an entity malfunctions. As an analogy, we could imagine a graph of airports\nthat are connected to each other via a set of flight routes. Say, there are direct routes from airport A to B, C and D.\nIf the airport C gets closed due to renovation of a runway, the routes to B and D are not affected in any way. Moreover,\nsome passengers might not even know that C is not operating at that moment.</p>\n\n<h4 id=\"event-driven-dataflow\">Event-driven dataflow</h4>\n\n<p>Since we are dealing with a UI component that has several moving parts, applying event-driven techniques come in handy\nbecause of their asynchronous nature and because messaging channels are subscription-based. Here is another analogy: you\nare at your place waiting for a hand-to-hand package delivery. Instead of opening the door every now and then to check\nif there is a courier behind it you would probably wait first for a doorbell to ring, right? Only once it rings (an\nevent occurs), you would open the door to obtain the package.</p>\n\n<h3 id=\"which-development-constraints-if-any-did-we-want-to-introduce-intentionally\">Which development constraints (if any) did we want to introduce intentionally?</h3>\n\n<p>We wanted the architecture of the new solution to follow a certain set of rules in order to maintain its structure. We\nalso wanted to make it harder for engineers to break those rules. In the short term it might be a drawback, but we are\ninterested in keeping the codebase well-maintained in the long run. By carefully designing the APIs and applying static\ntype analysis we were not only able to meet the requirement but also lifted some complexity from engineers’ shoulders,\nand this is where TypeScript shines brightly.</p>\n\n<h2 id=\"applying-all-of-the-above-in-a-new-store-solution\">Applying all of the above in a new store solution</h2>\n\n<p>Based on the conclusions reached above we were ready to start the actual work and started it from the core, that is the\nstore entity. As with any store solutions, let us list functional characteristics that one should provide.</p>\n\n<p>The store should:</p>\n<ul>\n  <li>hold current runtime state</li>\n  <li>be initialized with a default state</li>\n  <li>provide a public API that allows dependents to update its state during runtime in a <em>controlled way</em></li>\n  <li>expose information channels to propagate the state change to every subscriber</li>\n  <li>not expose any implementation details</li>\n</ul>\n\n<p>Defining the structure of the store’s data is as simple as declaring a TypeScript interface, but we need to be able to\nexpose information that the state has mutated in a particular way. That is, we need to build event-driven communication\nbetween the store and its dependents. For this purpose we could use an event emitter and define as many topics as needed.\nIn our case, having a dedicated topic per state property turned out to work perfectly as we didn’t have that many\nproperties in the first place. And since we wanted to have the state and event emitter under the same umbrella, we\nencapsulated them into the following class declaration:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kr\">interface</span> <span class=\"nx\">State</span> <span class=\"p\">{</span>\n  <span class=\"nl\">foo</span><span class=\"p\">:</span> <span class=\"nx\">boolean</span><span class=\"p\">;</span>\n  <span class=\"nl\">bar</span><span class=\"p\">:</span> <span class=\"kr\">string</span><span class=\"p\">;</span>\n  <span class=\"c1\">// ...</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">type</span> <span class=\"nx\">Topic</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"p\">[</span><span class=\"nx\">K</span> <span class=\"k\">in</span> <span class=\"nx\">Capitalize</span><span class=\"o\">&lt;</span><span class=\"kr\">keyof</span> <span class=\"nx\">State</span><span class=\"o\">&gt;</span><span class=\"p\">]:</span> <span class=\"nx\">Uncapitalize</span><span class=\"o\">&lt;</span><span class=\"nx\">K</span><span class=\"o\">&gt;</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nx\">Store</span> <span class=\"p\">{</span>\n  <span class=\"kd\">constructor</span><span class=\"p\">(</span>\n    <span class=\"k\">private</span> <span class=\"nx\">state</span><span class=\"p\">:</span> <span class=\"nx\">State</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">readonly</span> <span class=\"nx\">emitter</span><span class=\"p\">:</span> <span class=\"nx\">Emitter</span><span class=\"o\">&lt;</span><span class=\"nb\">Record</span><span class=\"o\">&lt;</span>\n       <span class=\"kr\">keyof</span> <span class=\"nx\">State</span><span class=\"p\">,</span>\n       <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">:</span> <span class=\"nx\">State</span><span class=\"p\">)</span> <span class=\"o\">=&gt;</span> <span class=\"k\">void</span>\n    <span class=\"o\">&gt;&gt;</span><span class=\"p\">,</span>\n  <span class=\"p\">)</span> <span class=\"p\">{}</span>\n\n  <span class=\"k\">public</span> <span class=\"nx\">on</span><span class=\"p\">(...</span><span class=\"nx\">args</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"k\">return</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(...</span><span class=\"nx\">args</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>The last piece of the puzzle is that we needed to automate event emission triggering. At the same time, we aimed to\nminimize the size of the boilerplate code needed to set up this behavior for each state property. Since every property\nhas a corresponding topic, that is where we were able to leverage the power of JavaScript’s accessor descriptor and\nwithin a setter we could trigger the emission:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">Object</span>\n  <span class=\"p\">.</span><span class=\"nx\">values</span><span class=\"p\">(</span><span class=\"nx\">Topic</span><span class=\"p\">)</span>\n  <span class=\"p\">.</span><span class=\"nx\">forEach</span><span class=\"p\">(</span><span class=\"nx\">key</span> <span class=\"o\">=&gt;</span> <span class=\"nb\">Object</span><span class=\"p\">.</span><span class=\"nx\">defineProperty</span><span class=\"p\">(</span><span class=\"nx\">Store</span><span class=\"p\">.</span><span class=\"nx\">prototype</span><span class=\"p\">,</span> <span class=\"nx\">key</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n    <span class=\"kd\">get</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n      <span class=\"k\">return</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span><span class=\"p\">[</span><span class=\"nx\">key</span><span class=\"p\">];</span>\n    <span class=\"p\">},</span>\n    <span class=\"kd\">set</span><span class=\"p\">(</span><span class=\"nx\">value</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"p\">...</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"nx\">key</span><span class=\"p\">]:</span> <span class=\"nx\">value</span> <span class=\"p\">};</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"nx\">key</span><span class=\"p\">,</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">}));</span>\n</code></pre></div></div>\n\n<p>After putting it all together, not only did we achieve the above-mentioned characteristics but also a simple way to\nwork with the store:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">foo</span> <span class=\"o\">=</span> <span class=\"kc\">true</span><span class=\"p\">;</span>\n<span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"nx\">Topic</span><span class=\"p\">.</span><span class=\"nx\">Foo</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">:</span> <span class=\"nx\">State</span><span class=\"p\">)</span> <span class=\"o\">=&gt;</span> <span class=\"p\">...);</span>\n</code></pre></div></div>\n\n<p>At this point the store is ready and we would like to test how well the solution performs in reality.</p>\n\n<h2 id=\"event-driven-communication-between-the-store-and-ui-parts\">Event-driven communication between the store and UI parts</h2>\n\n<p>Now we could focus on developing the UI parts of our component. Luckily, business requirements provide enough hints where\nto place each piece of functionality. Let’s take a look at how we shaped the search input and the suggestion list, and\nhow these UI parts cooperate with each other and the store.</p>\n\n<p>Recall that functionality-wise the suggestion list is a dropdown that should be rendered whenever a user types a search\nphrase or clicks/taps into the input element. We also need to fetch best matching suggestions whenever the input value\nchanges.</p>\n\n<p>We started with the search input as it doesn’t have any dependencies besides the store. With functional requirements in\nmind, we aimed it to be good at doing just two things:</p>\n\n<ul>\n  <li>Proxying DOM events such as focus, blur, click, keydown, etc.</li>\n  <li>Notifying the store whenever the input value changes</li>\n</ul>\n\n<p>Since updating the store is pretty much straightforward, let’s take a look at how we handled DOM events. Events such as\n<code class=\"language-plaintext highlighter-rouge\">click</code>, <code class=\"language-plaintext highlighter-rouge\">focus</code>, <code class=\"language-plaintext highlighter-rouge\">blur</code> convey the fact that there was some sort of interaction with the input HTML element. Unlike\n<code class=\"language-plaintext highlighter-rouge\">input</code> event, where one is interested in knowing what the current value is, the above-mentioned ones don’t include any\nrelated information. We only need to be able to communicate to the dependents the fact that such an event took place\nand that is why, similarly to the store, the search input has an event emitter of its own. Now, you might be thinking:\nwhy would you want to have multiple sources of information given you already have the event-driven store solution? There\nare a couple of reasons:</p>\n\n<ul>\n  <li>Since those DOM events don’t contain additional information, there is nothing we need to put into our store. It\naligns well with the single-responsibility principle, according to which the store only fulfills its primary function\nand has no hint of existence of the search input.</li>\n  <li>By bypassing the store we shortened an event message travel distance from the source to a subscriber.</li>\n  <li>It also aligns well with a mental model, where if one wants to react to an event that happened in the search input,\none subscribes to its publicly available communication channels.</li>\n</ul>\n\n<p>The gist of this approach and binding with the store can be achieved in just a few lines of code. Here we add several\nDOM event listeners and, depending on the event type, decide whether we need to proxy them into the event emitter or\nupdate the store’s state:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nx\">SearchInput</span> <span class=\"p\">{</span>\n  <span class=\"kd\">constructor</span><span class=\"p\">(</span>\n    <span class=\"k\">private</span> <span class=\"k\">readonly</span> <span class=\"nx\">inputNode</span><span class=\"p\">:</span> <span class=\"nx\">HTMLInputElement</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"nx\">store</span><span class=\"p\">:</span> <span class=\"nx\">Store</span><span class=\"p\">,</span>\n    <span class=\"k\">public</span> <span class=\"k\">readonly</span> <span class=\"nx\">emitter</span><span class=\"p\">:</span> <span class=\"nx\">Emitter</span><span class=\"o\">&lt;</span><span class=\"nb\">Record</span><span class=\"o\">&lt;</span>\n      <span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span> <span class=\"o\">|</span> <span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span> <span class=\"o\">|</span> <span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n      <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">void</span><span class=\"p\">,</span>\n    <span class=\"o\">&gt;&gt;</span><span class=\"p\">,</span>\n  <span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span><span class=\"p\">));</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span><span class=\"p\">));</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">));</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">input</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">({</span> <span class=\"na\">currentTarget</span><span class=\"p\">:</span> <span class=\"p\">{</span> <span class=\"nx\">value</span> <span class=\"p\">}})</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">input</span> <span class=\"o\">=</span> <span class=\"nx\">value</span><span class=\"p\">);</span>\n  <span class=\"p\">}</span>\n  <span class=\"c1\">// ...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Now, we can focus our attention on the suggestions list. Communication-wise, all it needs to do is to subscribe to\nseveral topics provided by the store and the search input:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"p\">{</span> <span class=\"nx\">fetchSuggestions</span> <span class=\"p\">}</span> <span class=\"k\">from</span> <span class=\"dl\">'</span><span class=\"s1\">./network</span><span class=\"dl\">'</span><span class=\"p\">;</span>\n\n<span class=\"kd\">class</span> <span class=\"nx\">SuggestionsList</span> <span class=\"p\">{</span>\n  <span class=\"kd\">constructor</span><span class=\"p\">(</span>\n    <span class=\"k\">private</span> <span class=\"nx\">store</span><span class=\"p\">:</span> <span class=\"nx\">Store</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">readonly</span> <span class=\"nx\">searchInput</span><span class=\"p\">:</span> <span class=\"nx\">SearchInput</span><span class=\"p\">,</span>\n    <span class=\"c1\">// ...</span>\n  <span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">searchInput</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderVisible</span><span class=\"p\">());</span>\n    <span class=\"nx\">searchInput</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderVisible</span><span class=\"p\">());</span>\n    <span class=\"nx\">searchInput</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderHidden</span><span class=\"p\">());</span>\n    <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">input</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">fetchItemsList</span><span class=\"p\">());</span>\n    <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">suggestionsListData</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderItemsList</span><span class=\"p\">();</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderVisible</span><span class=\"p\">();</span>\n    <span class=\"p\">});</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"k\">private</span> <span class=\"k\">async</span> <span class=\"nx\">fetchItemsList</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">suggesiontsListData</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"nx\">fetchSuggestions</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">input</span><span class=\"p\">);</span>\n  <span class=\"p\">}</span>\n  <span class=\"c1\">// ...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>When a <code class=\"language-plaintext highlighter-rouge\">click</code> or <code class=\"language-plaintext highlighter-rouge\">focus</code> event message arrives from the search input, <code class=\"language-plaintext highlighter-rouge\">SuggestionsList</code> renders a dropdown UI element.\nOn the other hand, <code class=\"language-plaintext highlighter-rouge\">blur</code> event occurrence hides it. A change of input value in the store leads to fetching suggestions\nover the network and lastly, whenever suggestions data is available, <code class=\"language-plaintext highlighter-rouge\">SuggestionsList</code> renders the item list and makes\nthe dropdown visible.</p>\n\n<p>Note that because we apply the separation of concerns principle, the <code class=\"language-plaintext highlighter-rouge\">fetchItemsList</code> method only delegates a job to an\nexternal dependency responsible for network communication. Upon successful response, <code class=\"language-plaintext highlighter-rouge\">SuggestionsList</code> doesn’t\nimmediately start rendering the data, instead the data is put into the store and <code class=\"language-plaintext highlighter-rouge\">SuggestionList</code> listens to that data\nchange. With such data circulation we ensure that:</p>\n\n<ul>\n  <li>The store is a single source of truth (and data)</li>\n  <li>Suggestions’ data is propagated to every dependent</li>\n  <li>We avoid possible redundant rendering cycles</li>\n</ul>\n\n<p>With that, our functional requirement is implemented.</p>\n\n<h2 id=\"the-final-solution\">The final solution</h2>\n\n<p>Reapplying the above principles and techniques to develop the remaining functional requirements, we ended up with a\nsolution that can be illustrated as follows:</p>\n\n<p><img src=\"/img/articles/2021-10-26-refactoring-opbox-search/architecture-diagram.png\" alt=\"Architecture Diagram\" title=\"Architecture Diagram\" /></p>\n\n<p>Were we able to meet our expectations? At the end of the day, after careful problem analysis, testing out POCs and\ndevelopment preparations, the implementation process itself went quite smoothly. Multiple people participated and\nwe are quite satisfied with the end result. Will it withstand future challenges? Only time will tell.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>At Allegro we value our customer experience and that is why we pay a lot of attention to performance of our frontend\nsolutions. At the same time, as software engineers, we want to stay productive and, therefore, we also care about our\ndevelopment experience. Achieving good results in both worlds is where the real challenge lies.</p>\n","contentSnippet":"This article describes a classic case of refactoring a search form UI component, a critical part of every e-commerce\nplatform. In it I’ll explain the precursor of change, analysis process, as well as aspects to pay attention to and\nprinciples to apply while designing a new solution. If you are planning to conduct refactoring of a codebase or just\ncurious to learn more about frontend internals at Allegro, you might learn a thing or two from\nthis article. Sounds interesting? Hop on!\nHow does the search form function at Allegro?\nFor starters, so that we all are on the same page, let me briefly explain how the search form at Allegro works and what\nfunctionality it is responsible for. Under the hood, it is one of our many\nOpBox components and its\ntechnical name is opbox-search.\nFrom UI standpoint it consists of four parts:\nan input field\na scope selector\na submit button\na dropdown with a list of suggestions\n\nFunctionality-wise, whenever a user clicks/taps into the input or types a search phrase, a dropdown with a list of\nsuggestions shows up and the user can navigate through by using keyboard/mouse/touchscreen. The suggestion list\nitself, at most, consists of two sections: phrases searched in the past and popular/matching suggestions. Those are\nfetched in real time as the user interacts with the form. Additionally, there is also a preconfigured option to search\nthe phrase in products’ descriptions.\nThe form also provides a possibility to narrow down the search scope to a particular department, a certain\nuser, a charity organization, etc. Depending on the selected scope, when user click the submission button, they are\nredirected to an appropriate product/user/charity listing page and the search phrase is sent as a URL query parameter.\nAt this point you might be wondering: this sounds quite easy, how come you messed up such a simple component?\nA bit of history and the precursor of refactoring\nThe initial commit took place back in 2017 and up until the point of refactoring the project, there were 2292 commits\nspread across 189 pull requests merged to the main branch. All those contributions were made by different independent\nteams. Over time the component evolved, some external APIs changed, some features became deprecated and new ones were\nadded. At one point it also changed its ownership to another development team. As expected, all those factors left some\nmarks on the codebase.\nOne of ample examples of troublesome conditions was the store entity that is responsible for handling the runtime state.\nIn reality, besides performing its primary function it also handled network calls and contained pieces of business logic\nnon-relevant for search scoping and suggestions listing.\nTo make matters worse, the internals of the entity were publicly exposed and therefore any dependent, e.g. the search\ninput or the scope selector, was free to manipulate the store arbitrarily. Not hard to imagine, using such “shortcuts”\nhas lead to degradation of codebase readability and maintainability.\nAt one point in time it reached its critical mass and we started considering the cost of maintaining the existing\ncodebase vs. the cost of redeveloping it from scratch.\nRefactoring expectations\nRefactoring itself doesn’t provide any business value. Instead it is an investment that should save engineers’ time and\neffort. That is why the primary goal was to gain back the confidence in further development of the component by:\nstreamlining the maintenance of existing features\nstreamlining and simplifying the development of new business features\nusing tools, design patterns and principles to help engineers develop stable and correctly functioning features\nSubsequently, achieving the goal would:\nshorten the delivery time\ncreate, from an architectural standpoint, a new solution resilient to multiple future contributions from different teams\nInto the technical analysis\nSo by this point we learned what the functional requirements are and identified the issues we have with the current\nsolution. We also outlined refactoring expectations, hence we could start laying down the conceptual foundation of a new\nsolution. We began by asking ourselves a few questions that could help us formalize our technical end goals.\nWhat issues did we want to avoid this time? What did we want the new solution to improve upon?\nGoing back to the store example, we clearly didn’t want to allow any dependent to mutate its data in any abritrary way\nnor did we want the store to contain pieces of logic that are not of its concern, e.g. networking. Instead we wanted to\nmove these irrelevant pieces into more suitable locations, decrease entanglement of different parts of the codebase,\nstructure it into logical entities that are more human readable, draw boundaries between those entities, define rules\nof communication and make sure we expose to the public API only what we intended to.\nWhat development principles and design patterns could we have incorporated?\nThe single-responsibility principle (SRP)\nThe “S” of the “SOLID” acronym. As the name hints, a class (or a unit of code) should be responsible only for a single\npiece of functionality. A simple and powerful principle, yet quite often overlooked. Say, if we build a piece of logic\nassociated with an input HTML element, its only responsibility should be to tightly interact with the element, e.g.\nlisten to its DOM events and update its value if needed. At the same time, you don’t want this logic to affect the\nsubmission behavior of a form element inside of which the input element is placed.\nThe separation of concerns principle (SoC)\nSoC goes well in pair with SRP and states that one should not place functionalities of different domains under the same\nlogical entity (say, an object, a class or a module). For example, we need to render a piece of information on the\nscreen but beforehand the information needs to be fetched over the network. Since the view and network layers have\ndifferent concerns we don’t want to place both of them under a single logical entity. Let these two be separate ones\nwith established dependency relation to one another via a public API.\nThe loose coupling principle\nLoose coupling means that a single logical entity knows as little as possible about other entities and communication\nbetween them follows strict rules. One important characteristic we wanted to achieve here is to minimize negative\neffects on application’s runtime in case an entity malfunctions. As an analogy, we could imagine a graph of airports\nthat are connected to each other via a set of flight routes. Say, there are direct routes from airport A to B, C and D.\nIf the airport C gets closed due to renovation of a runway, the routes to B and D are not affected in any way. Moreover,\nsome passengers might not even know that C is not operating at that moment.\nEvent-driven dataflow\nSince we are dealing with a UI component that has several moving parts, applying event-driven techniques come in handy\nbecause of their asynchronous nature and because messaging channels are subscription-based. Here is another analogy: you\nare at your place waiting for a hand-to-hand package delivery. Instead of opening the door every now and then to check\nif there is a courier behind it you would probably wait first for a doorbell to ring, right? Only once it rings (an\nevent occurs), you would open the door to obtain the package.\nWhich development constraints (if any) did we want to introduce intentionally?\nWe wanted the architecture of the new solution to follow a certain set of rules in order to maintain its structure. We\nalso wanted to make it harder for engineers to break those rules. In the short term it might be a drawback, but we are\ninterested in keeping the codebase well-maintained in the long run. By carefully designing the APIs and applying static\ntype analysis we were not only able to meet the requirement but also lifted some complexity from engineers’ shoulders,\nand this is where TypeScript shines brightly.\nApplying all of the above in a new store solution\nBased on the conclusions reached above we were ready to start the actual work and started it from the core, that is the\nstore entity. As with any store solutions, let us list functional characteristics that one should provide.\nThe store should:\nhold current runtime state\nbe initialized with a default state\nprovide a public API that allows dependents to update its state during runtime in a controlled way\nexpose information channels to propagate the state change to every subscriber\nnot expose any implementation details\nDefining the structure of the store’s data is as simple as declaring a TypeScript interface, but we need to be able to\nexpose information that the state has mutated in a particular way. That is, we need to build event-driven communication\nbetween the store and its dependents. For this purpose we could use an event emitter and define as many topics as needed.\nIn our case, having a dedicated topic per state property turned out to work perfectly as we didn’t have that many\nproperties in the first place. And since we wanted to have the state and event emitter under the same umbrella, we\nencapsulated them into the following class declaration:\n\ninterface State {\n  foo: boolean;\n  bar: string;\n  // ...\n}\n\ntype Topic = {\n  [K in Capitalize<keyof State>]: Uncapitalize<K>;\n}\n\nclass Store {\n  constructor(\n    private state: State,\n    private readonly emitter: Emitter<Record<\n       keyof State,\n       (state: State) => void\n    >>,\n  ) {}\n\n  public on(...args) { return this.emitter.on(...args);\n}\n\n\nThe last piece of the puzzle is that we needed to automate event emission triggering. At the same time, we aimed to\nminimize the size of the boilerplate code needed to set up this behavior for each state property. Since every property\nhas a corresponding topic, that is where we were able to leverage the power of JavaScript’s accessor descriptor and\nwithin a setter we could trigger the emission:\n\nObject\n  .values(Topic)\n  .forEach(key => Object.defineProperty(Store.prototype, key, {\n    get() {\n      return this.state[key];\n    },\n    set(value) {\n      this.state = { ...this.state, [key]: value };\n      this.emitter.emit(key, this.state);\n    }\n  }));\n\n\nAfter putting it all together, not only did we achieve the above-mentioned characteristics but also a simple way to\nwork with the store:\n\nstore.foo = true;\nstore.on(Topic.Foo, (state: State) => ...);\n\n\nAt this point the store is ready and we would like to test how well the solution performs in reality.\nEvent-driven communication between the store and UI parts\nNow we could focus on developing the UI parts of our component. Luckily, business requirements provide enough hints where\nto place each piece of functionality. Let’s take a look at how we shaped the search input and the suggestion list, and\nhow these UI parts cooperate with each other and the store.\nRecall that functionality-wise the suggestion list is a dropdown that should be rendered whenever a user types a search\nphrase or clicks/taps into the input element. We also need to fetch best matching suggestions whenever the input value\nchanges.\nWe started with the search input as it doesn’t have any dependencies besides the store. With functional requirements in\nmind, we aimed it to be good at doing just two things:\nProxying DOM events such as focus, blur, click, keydown, etc.\nNotifying the store whenever the input value changes\nSince updating the store is pretty much straightforward, let’s take a look at how we handled DOM events. Events such as\nclick, focus, blur convey the fact that there was some sort of interaction with the input HTML element. Unlike\ninput event, where one is interested in knowing what the current value is, the above-mentioned ones don’t include any\nrelated information. We only need to be able to communicate to the dependents the fact that such an event took place\nand that is why, similarly to the store, the search input has an event emitter of its own. Now, you might be thinking:\nwhy would you want to have multiple sources of information given you already have the event-driven store solution? There\nare a couple of reasons:\nSince those DOM events don’t contain additional information, there is nothing we need to put into our store. It\naligns well with the single-responsibility principle, according to which the store only fulfills its primary function\nand has no hint of existence of the search input.\nBy bypassing the store we shortened an event message travel distance from the source to a subscriber.\nIt also aligns well with a mental model, where if one wants to react to an event that happened in the search input,\none subscribes to its publicly available communication channels.\nThe gist of this approach and binding with the store can be achieved in just a few lines of code. Here we add several\nDOM event listeners and, depending on the event type, decide whether we need to proxy them into the event emitter or\nupdate the store’s state:\n\nclass SearchInput {\n  constructor(\n    private readonly inputNode: HTMLInputElement,\n    private store: Store,\n    public readonly emitter: Emitter<Record<\n      'click' | 'focus' | 'blur',\n      () => void,\n    >>,\n  ) {\n    inputNode.addEventListener('click', () => emitter.emit('click'));\n    inputNode.addEventListener('focus', () => emitter.emit('focus'));\n    inputNode.addEventListener('blur', () => emitter.emit('blur'));\n    inputNode.addEventListener('input', ({ currentTarget: { value }}) => store.input = value);\n  }\n  // ...\n}\n\n\nNow, we can focus our attention on the suggestions list. Communication-wise, all it needs to do is to subscribe to\nseveral topics provided by the store and the search input:\n\nimport { fetchSuggestions } from './network';\n\nclass SuggestionsList {\n  constructor(\n    private store: Store,\n    private readonly searchInput: SearchInput,\n    // ...\n  ) {\n    searchInput.emitter.on('click', () => this.renderVisible());\n    searchInput.emitter.on('focus', () => this.renderVisible());\n    searchInput.emitter.on('blur', () => this.renderHidden());\n    store.on('input', () => this.fetchItemsList());\n    store.on('suggestionsListData', () => {\n      this.renderItemsList();\n      this.renderVisible();\n    });\n  }\n\n  private async fetchItemsList() {\n    this.store.suggesiontsListData = await fetchSuggestions(this.store.input);\n  }\n  // ...\n}\n\n\nWhen a click or focus event message arrives from the search input, SuggestionsList renders a dropdown UI element.\nOn the other hand, blur event occurrence hides it. A change of input value in the store leads to fetching suggestions\nover the network and lastly, whenever suggestions data is available, SuggestionsList renders the item list and makes\nthe dropdown visible.\nNote that because we apply the separation of concerns principle, the fetchItemsList method only delegates a job to an\nexternal dependency responsible for network communication. Upon successful response, SuggestionsList doesn’t\nimmediately start rendering the data, instead the data is put into the store and SuggestionList listens to that data\nchange. With such data circulation we ensure that:\nThe store is a single source of truth (and data)\nSuggestions’ data is propagated to every dependent\nWe avoid possible redundant rendering cycles\nWith that, our functional requirement is implemented.\nThe final solution\nReapplying the above principles and techniques to develop the remaining functional requirements, we ended up with a\nsolution that can be illustrated as follows:\n\nWere we able to meet our expectations? At the end of the day, after careful problem analysis, testing out POCs and\ndevelopment preparations, the implementation process itself went quite smoothly. Multiple people participated and\nwe are quite satisfied with the end result. Will it withstand future challenges? Only time will tell.\nSummary\nAt Allegro we value our customer experience and that is why we pay a lot of attention to performance of our frontend\nsolutions. At the same time, as software engineers, we want to stay productive and, therefore, we also care about our\ndevelopment experience. Achieving good results in both worlds is where the real challenge lies.","guid":"https://blog.allegro.tech/2021/10/refactoring-opbox-search.html","categories":["tech","frontend","architecture","refactoring","developmentexperience","typescript"],"isoDate":"2021-10-25T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Comparing MongoDB composite indexes","link":"https://blog.allegro.tech/2021/10/comparing-mongodb-composite-indexes.html","pubDate":"Mon, 18 Oct 2021 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>One of the key elements ensuring efficient operation of the services we work on every day at\n<a href=\"https://allegro.tech/\">Allegro</a> is fast responses from the database.\nWe spend a lot of time to properly model the data so that storing and querying take as little time as possible.\nYou can read more about why good schema design is important in one of my earlier\n<a href=\"/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html\">posts</a>.\nIt’s also equally important to make sure that all queries are covered with indexes of the correct type whenever\npossible. Indexes are used to quickly search the database and under certain conditions even allow results to be\nreturned directly from the index, without the need to access the data itself. However, indexes are not\nall the same and it’s important to learn more about their different types in order to make the right choices later on.</p>\n\n<h2 id=\"whats-the-difference\">What’s the difference?</h2>\n<p>I’ve had a conversation with a colleague of mine the other day, about the point of using composite keys in a MongoDB\ndatabase. I’ve always been a firm believer that it’s a good idea to use a composite key wherever possible because\nsearching this way is very fast. My colleague, on the other hand, advocates using artificial keys and creating\nseparate <a href=\"https://docs.mongodb.com/manual/core/index-compound/\">composite indexes</a> on fields for\nwhich I would use a composite key.\nAfter a brief disagreement, I realized that other than my intuition, I had no arguments to defend my beliefs.\nI decided to see how indexes on composite keys differ from composite indexes created on regular fields in practice.</p>\n\n<p>As an example for our considerations, we will use an entity describing a person by first and last name, let’s also\nassume that this pair is unique.</p>\n\n<p>Such data can be stored in a collection (let’s call it <code class=\"language-plaintext highlighter-rouge\">composite</code>) with a composite key that contains both fields.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"John\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"surname\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Doe\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>A unique index will be automatically created on pair of both fields along with the collection.</p>\n\n<p>Many developers would most likely prefer to use an artificial key, and index the <code class=\"language-plaintext highlighter-rouge\">name</code> and <code class=\"language-plaintext highlighter-rouge\">surname</code> fields\nseparately, as shown in collection <code class=\"language-plaintext highlighter-rouge\">artificial</code>:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">ObjectId(</span><span class=\"s2\">\"615eb18b76e172647f7462e2\"</span><span class=\"err\">)</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"name\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"John\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"surname\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Doe\"</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>When it comes to the second model, only the artificial key will be automatically covered by the index. To be\nable to efficiently search the collection by first and last name, we need to manually create a composite index on both\nfields. To maintain consistency with the first collection, of course, uniqueness also needs to be enforced:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">createIndex</span><span class=\"p\">({</span><span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"na\">unique</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>At first glance, we can clearly see that the first way of storing data is more compact, since we only store two fields\nof interest, and only one index is required in addition to the data. For the second collection, in addition to the data\nitself, we need to store an artificial key, moreover two indexes are required here: on the artificial key and on\nthe <code class=\"language-plaintext highlighter-rouge\">name</code> and <code class=\"language-plaintext highlighter-rouge\">surname</code> fields.</p>\n\n<p>We can now move on to comparing the execution plans of queries to two collections, so let’s take a look at the result\nof the <code class=\"language-plaintext highlighter-rouge\">explain</code> commands:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">composite</span><span class=\"p\">.</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>and:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Let’s start with the second result first. We can see that the optimiser chose to use the index we manually created.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"winningPlan\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"FETCH\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"inputStage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IXSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"keyPattern\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"w\">\n      </span><span class=\"p\">},</span><span class=\"w\">\n      </span><span class=\"nl\">\"indexName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"name_1_surname_1\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>[…]</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"executionStats\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionSuccess\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>In the case of a collection with a composite key, however, the plan is different; it contains the word <code class=\"language-plaintext highlighter-rouge\">IDHACK</code>:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"winningPlan\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IDHACK\"</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>[…]</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"executionStats\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionSuccess\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>It means that the optimiser skipped the index selection phase (although in our case there were no other indexes, but\nit doesn’t matter) and decided to use the key index. This operation is considered to be the fastest one possible.\nWhenever there is a key in the query, its index will be used (while ignoring conditions on other fields).</p>\n\n<p>Let’s also take a look at the notation <code class=\"language-plaintext highlighter-rouge\">\"nReturned\" : 1</code>, which means that both queries returned a single document.</p>\n\n<p>We already know that queries to both collections will be handled with an index. However, I’ve been wondering if there\nare any differences between these indexes?</p>\n\n<p>The first should be search time: since whenever there is a key in the condition list, its index will be used,\ntheoretically, key’s index should be the fastest. We’ll get to that topic in a moment. For now, let’s see what happens\nif we only want to search one field at a time:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">composite</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">artificial</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>In the case of the first query, the index was admittedly used, but we got no results, as evidenced by the notation:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionStages\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IDHACK\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>This happens because a composite key requires all its components to be used by the query, so it is impossible to search\nby a single field.</p>\n\n<p>The situation is different when querying the second collection, here the index was also used, however this time the\ndocument was found:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"winningPlan\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"FETCH\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"inputStage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IXSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"keyPattern\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"w\">\n      </span><span class=\"p\">},</span><span class=\"w\">\n      </span><span class=\"nl\">\"indexName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"name_1_surname_1\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>[…]</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"executionStats\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionSuccess\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>What if we decide to search by <code class=\"language-plaintext highlighter-rouge\">surname</code> only?</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">composite</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">artificial</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>In a collection with a composite key, we have a situation similar to the previous one - the index was used, but we\ndidn’t receive any document. The reason, of course, is the same: we didn’t use all the key fields.</p>\n\n<p>By querying the collection with a separate composite index, we got the document we were looking for, but it turns out\nthat this time the index was not used, and instead the database had to search through the entire collection:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"winningPlan\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"stage\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"COLLSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"filter\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n            </span><span class=\"nl\">\"surname\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n                </span><span class=\"nl\">\"$eq\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Doe\"</span><span class=\"w\">\n            </span><span class=\"p\">}</span><span class=\"w\">\n        </span><span class=\"p\">},</span><span class=\"w\">\n        </span><span class=\"nl\">\"direction\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"forward\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>This is because with composite indexes, while it is possible not to use all indexed\nfields, it is only permissible to skip values in the reverse order, that is, reading indexed fields from the right.\nOur index was created on the fields in the following order:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">createIndex</span><span class=\"p\">({</span><span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"na\">unique</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>It is therefore possible to omit the condition on the <code class=\"language-plaintext highlighter-rouge\">surname</code> field and search only by <code class=\"language-plaintext highlighter-rouge\">name</code>, but it’s not possible\nthe other way around.</p>\n\n<p>We managed to find out the first difference between two types of indexes: composite key indexes are less flexible,\nrequire all values to be specified, while in regular composite indexes we can omit values from the right.</p>\n\n<p>Let’s also check if the order of conditions matter?</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">composite</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">artificial</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>And here’s another surprise: even though all the components of the key were provided, but in reverse order, the\nfirst query did not find the document.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionStages\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IDHACK\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>With a regular composite index, the optimiser was able to reverse the field order itself and use the appropriate\nindex to find the document: <code class=\"language-plaintext highlighter-rouge\">\"nReturned\" : 1</code></p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"winningPlan\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"stage\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"FETCH\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"inputStage\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IXSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"keyPattern\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n            </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"w\">\n            </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"w\">\n          </span><span class=\"p\">},</span><span class=\"w\">\n          </span><span class=\"nl\">\"indexName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"name_1_surname_1\"</span><span class=\"w\">\n        </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>For the time being, indexes on composite keys are losing against regular ones. This is a good time to get back to the\nquestion about the search time of the two indexes. Now that we’ve established that indexes on composite keys are less\nflexible, it’s a good idea to figure out what we gain in return for such limitations. We already know that <code class=\"language-plaintext highlighter-rouge\">IDHACK</code>\nskips all indexes and always uses a key, so one might think that this is the fastest available way to get to the\ndocument. I decided to check this on my own.</p>\n\n<h2 id=\"its-time-to-experiment\">It’s time to experiment</h2>\n\n<p>I filled both previously used collections with 10 million documents. I used the following scripts for this purpose:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">composite</span><span class=\"p\">.</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span><span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">name_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">),</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">surname_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)}})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span><span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"k\">new</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(),</span> <span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">name_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">),</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">surname_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>It is worth noting here that I’m adding documents in batches. This is definitely faster than a list of single inserts\nand is useful when we need to generate a large amount of data quickly. Also note that I am using existing collections\nso my index on <code class=\"language-plaintext highlighter-rouge\">name</code> and <code class=\"language-plaintext highlighter-rouge\">surname</code> fields already exists.</p>\n\n<p>I measured the execution time of both scripts using following commands (to avoid network latency I performed the\nmeasurements on my laptop, with a local instance of MongoDB version 4.4.0 running):</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill2.js\n</code></pre></div></div>\n\n<p>Filling collections with documents took <strong>58,95s</strong> and <strong>76,48s</strong> respectively. So when it comes to <code class=\"language-plaintext highlighter-rouge\">insert</code> operation time,\nthe composite key collection clearly wins. The reason for this, of course, is a simpler document structure and only one\nindex, instead of two.</p>\n\n<p>I was more interested in read times, because in a typical case, data is usually read more often than written. For each collection,\nI prepared a script containing a list of <code class=\"language-plaintext highlighter-rouge\">find</code> commands for 1 million documents in random order. Of course, the order of\nqueries in both scripts is the same:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.composite.find({_id: {name: 'name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'}})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.artificial.find({name: 'name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find2.js\n</code></pre></div></div>\n\n<p>Finally, I measured the execution time of both scripts using the commands:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find2.js\n</code></pre></div></div>\n\n<p>The results surprised me. The script running on the first collection took <strong>11.19s</strong> and the second took <strong>10.15s</strong>.\nAlthough the differences are small, I was sure that using a composite key would be much faster than searching\nthrough regular fields and would make up for all the inconvenience of less flexible keys. Meanwhile, it turns\nout that a collection built with an artificial key and a separate index on two fields wins in terms of search speed.</p>\n\n<p>I had one more idea. Maybe searching a document by a key that doesn’t exist would be faster?\nTo test this, I generated another two scripts, this time searching for non-existent documents:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.composite.find({_id: {name: 'missing_name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'missing_surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'}})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find-missing1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.artificial.find({name: 'missing_name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'missing_surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find-missing2.js\n</code></pre></div></div>\n\n<p>Unfortunately, the composite key collection also lost in this case: <strong>12.44s</strong> vs. <strong>10.26s</strong>.</p>\n\n<p>Finally, I decided to run one more test. Since when using composite key we have to pass the entire key to the query\nand cannot search by its fragment, I decided to create a third collection, this time its key being the concatenation of\nfirst and last name:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">concatenation</span><span class=\"p\">.</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span><span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">name_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"dl\">'</span><span class=\"s1\">-</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"dl\">'</span><span class=\"s1\">surname_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>I then prepared another script containing 1 million search operations (in the same order as the previous attempts,\nof course):</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.concatenation.find({_id: 'name_</span><span class=\"nv\">$x</span><span class=\"s2\">-surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find3.js\n</code></pre></div></div>\n\n<p>This time I was finally able to get a result better than the one achieved with the artificial key approach: <strong>9.71s</strong>.\nAll results are summarized in the chart below:</p>\n\n<p><img src=\"/img/articles/2021-10-18-comparing-mongodb-composite-indexes/final-results.png\" alt=\"Collection stats\" /></p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>It appears that the <code class=\"language-plaintext highlighter-rouge\">IDHACK</code> operation is indeed the fastest possible way to access a document, but only for a simple\nkey. As soon as we add an additional field into the key, the benefits of the <code class=\"language-plaintext highlighter-rouge\">IDHACK</code> operation disappear and the\nregular index works better.</p>\n\n<p>In general, my experiments did not confirm the advantages of using a composite key. The artificial key model is more\nflexible in querying and returns results faster. However, if you want to search even faster, you may consider using a\nsimple key created by concatenating selected fields.</p>\n\n<p>I realize, of course, that I did not use the latest available version of the mongo server. For this reason, I am\nposting all the scripts I used in my attempts and encourage you to experiment on your own. Perhaps composite keys\nperform better in newer versions of the database?</p>\n","contentSnippet":"One of the key elements ensuring efficient operation of the services we work on every day at\nAllegro is fast responses from the database.\nWe spend a lot of time to properly model the data so that storing and querying take as little time as possible.\nYou can read more about why good schema design is important in one of my earlier\nposts.\nIt’s also equally important to make sure that all queries are covered with indexes of the correct type whenever\npossible. Indexes are used to quickly search the database and under certain conditions even allow results to be\nreturned directly from the index, without the need to access the data itself. However, indexes are not\nall the same and it’s important to learn more about their different types in order to make the right choices later on.\nWhat’s the difference?\nI’ve had a conversation with a colleague of mine the other day, about the point of using composite keys in a MongoDB\ndatabase. I’ve always been a firm believer that it’s a good idea to use a composite key wherever possible because\nsearching this way is very fast. My colleague, on the other hand, advocates using artificial keys and creating\nseparate composite indexes on fields for\nwhich I would use a composite key.\nAfter a brief disagreement, I realized that other than my intuition, I had no arguments to defend my beliefs.\nI decided to see how indexes on composite keys differ from composite indexes created on regular fields in practice.\nAs an example for our considerations, we will use an entity describing a person by first and last name, let’s also\nassume that this pair is unique.\nSuch data can be stored in a collection (let’s call it composite) with a composite key that contains both fields.\n\n{\n    \"_id\" : {\n        \"name\" : \"John\",\n        \"surname\" : \"Doe\"\n    }\n}\n\n\nA unique index will be automatically created on pair of both fields along with the collection.\nMany developers would most likely prefer to use an artificial key, and index the name and surname fields\nseparately, as shown in collection artificial:\n\n{\n    \"_id\" : ObjectId(\"615eb18b76e172647f7462e2\"),\n    \"name\" : \"John\",\n    \"surname\" : \"Doe\"\n}\n\n\nWhen it comes to the second model, only the artificial key will be automatically covered by the index. To be\nable to efficiently search the collection by first and last name, we need to manually create a composite index on both\nfields. To maintain consistency with the first collection, of course, uniqueness also needs to be enforced:\n\ndb.artificial.createIndex({name: 1, surname: 1}, {unique: true})\n\n\nAt first glance, we can clearly see that the first way of storing data is more compact, since we only store two fields\nof interest, and only one index is required in addition to the data. For the second collection, in addition to the data\nitself, we need to store an artificial key, moreover two indexes are required here: on the artificial key and on\nthe name and surname fields.\nWe can now move on to comparing the execution plans of queries to two collections, so let’s take a look at the result\nof the explain commands:\n\ndb.composite.find({\n    \"_id\" : {\n        \"name\" : \"John\",\n        \"surname\" : \"Doe\"\n    }\n}).explain(\"executionStats\")\n\n\nand:\n\ndb.artificial.find({\"name\" : \"John\", \"surname\" : \"Doe\"}).explain(\"executionStats\")\n\n\nLet’s start with the second result first. We can see that the optimiser chose to use the index we manually created.\n\n{\n  \"winningPlan\": {\n    \"stage\": \"FETCH\",\n    \"inputStage\": {\n      \"stage\": \"IXSCAN\",\n      \"keyPattern\": {\n        \"name\": 1.0,\n        \"surname\": 1.0\n      },\n      \"indexName\": \"name_1_surname_1\"\n    }\n  }\n}\n\n\n[…]\n\n{\n  \"executionStats\" : {\n    \"executionSuccess\": true,\n    \"nReturned\": 1\n  }\n}\n\n\nIn the case of a collection with a composite key, however, the plan is different; it contains the word IDHACK:\n\n{\n  \"winningPlan\": {\n    \"stage\": \"IDHACK\"\n  }\n}\n\n\n[…]\n\n{\n  \"executionStats\" : {\n    \"executionSuccess\": true,\n    \"nReturned\": 1\n  }\n}\n\n\nIt means that the optimiser skipped the index selection phase (although in our case there were no other indexes, but\nit doesn’t matter) and decided to use the key index. This operation is considered to be the fastest one possible.\nWhenever there is a key in the query, its index will be used (while ignoring conditions on other fields).\nLet’s also take a look at the notation \"nReturned\" : 1, which means that both queries returned a single document.\nWe already know that queries to both collections will be handled with an index. However, I’ve been wondering if there\nare any differences between these indexes?\nThe first should be search time: since whenever there is a key in the condition list, its index will be used,\ntheoretically, key’s index should be the fastest. We’ll get to that topic in a moment. For now, let’s see what happens\nif we only want to search one field at a time:\n\ndb.getCollection('composite').find({\n    \"_id\" : {\n        \"name\" : \"John\"\n    }\n}).explain(\"executionStats\")\n\n\n\ndb.getCollection('artificial').find({\"name\" : \"John\"}).explain(\"executionStats\")\n\n\nIn the case of the first query, the index was admittedly used, but we got no results, as evidenced by the notation:\n\n{\n    \"executionStages\": {\n        \"stage\": \"IDHACK\",\n        \"nReturned\": 0\n    }\n}\n\n\nThis happens because a composite key requires all its components to be used by the query, so it is impossible to search\nby a single field.\nThe situation is different when querying the second collection, here the index was also used, however this time the\ndocument was found:\n\n{\n  \"winningPlan\": {\n    \"stage\": \"FETCH\",\n    \"inputStage\": {\n      \"stage\": \"IXSCAN\",\n      \"keyPattern\": {\n        \"name\": 1.0,\n        \"surname\": 1.0\n      },\n      \"indexName\": \"name_1_surname_1\"\n    }\n  }\n}\n\n\n[…]\n\n{\n  \"executionStats\" : {\n    \"executionSuccess\": true,\n    \"nReturned\": 1\n  }\n}\n\n\nWhat if we decide to search by surname only?\n\ndb.getCollection('composite').find({\n    \"_id\" : {\n        \"surname\" : \"Doe\"\n    }\n}).explain(\"executionStats\")\n\n\n\ndb.getCollection('artificial').find({\"surname\" : \"Doe\"}).explain(\"executionStats\")\n\n\nIn a collection with a composite key, we have a situation similar to the previous one - the index was used, but we\ndidn’t receive any document. The reason, of course, is the same: we didn’t use all the key fields.\nBy querying the collection with a separate composite index, we got the document we were looking for, but it turns out\nthat this time the index was not used, and instead the database had to search through the entire collection:\n\n{\n    \"winningPlan\" : {\n        \"stage\" : \"COLLSCAN\",\n        \"filter\" : {\n            \"surname\" : {\n                \"$eq\" : \"Doe\"\n            }\n        },\n        \"direction\" : \"forward\"\n    }\n}\n\n\nThis is because with composite indexes, while it is possible not to use all indexed\nfields, it is only permissible to skip values in the reverse order, that is, reading indexed fields from the right.\nOur index was created on the fields in the following order:\n\ndb.artificial.createIndex({name: 1, surname: 1}, {unique: true})\n\n\nIt is therefore possible to omit the condition on the surname field and search only by name, but it’s not possible\nthe other way around.\nWe managed to find out the first difference between two types of indexes: composite key indexes are less flexible,\nrequire all values to be specified, while in regular composite indexes we can omit values from the right.\nLet’s also check if the order of conditions matter?\n\ndb.getCollection('composite').find({\n    \"_id\" : {\n        \"surname\" : \"Doe\",\n        \"name\" : \"John\"\n    }\n}).explain(\"executionStats\")\n\n\n\ndb.getCollection('artificial').find({\"surname\" : \"Doe\", \"name\" : \"John\"}).explain(\"executionStats\")\n\n\nAnd here’s another surprise: even though all the components of the key were provided, but in reverse order, the\nfirst query did not find the document.\n\n{\n    \"executionStages\" : {\n      \"stage\": \"IDHACK\",\n      \"nReturned\": 0\n    }\n}\n\n\nWith a regular composite index, the optimiser was able to reverse the field order itself and use the appropriate\nindex to find the document: \"nReturned\" : 1\n\n{\n    \"winningPlan\" : {\n        \"stage\" : \"FETCH\",\n        \"inputStage\" : {\n          \"stage\": \"IXSCAN\",\n          \"keyPattern\": {\n            \"name\": 1.0,\n            \"surname\": 1.0\n          },\n          \"indexName\": \"name_1_surname_1\"\n        }\n    }\n}\n\n\nFor the time being, indexes on composite keys are losing against regular ones. This is a good time to get back to the\nquestion about the search time of the two indexes. Now that we’ve established that indexes on composite keys are less\nflexible, it’s a good idea to figure out what we gain in return for such limitations. We already know that IDHACK\nskips all indexes and always uses a key, so one might think that this is the fastest available way to get to the\ndocument. I decided to check this on my own.\nIt’s time to experiment\nI filled both previously used collections with 10 million documents. I used the following scripts for this purpose:\n\nvar bulk = db.composite.initializeUnorderedBulkOp();\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({_id: {name: 'name_' + NumberInt(i), surname: 'surname_' + NumberInt(i)}})\n}\nbulk.execute();\n\n\n\nvar bulk = db.artificial.initializeUnorderedBulkOp();\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({_id: new ObjectId(), name: 'name_' + NumberInt(i), surname: 'surname_' + NumberInt(i)})\n}\nbulk.execute();\n\n\nIt is worth noting here that I’m adding documents in batches. This is definitely faster than a list of single inserts\nand is useful when we need to generate a large amount of data quickly. Also note that I am using existing collections\nso my index on name and surname fields already exists.\nI measured the execution time of both scripts using following commands (to avoid network latency I performed the\nmeasurements on my laptop, with a local instance of MongoDB version 4.4.0 running):\n\ntime mongo test fill1.js\n\n\n\ntime mongo test fill2.js\n\n\nFilling collections with documents took 58,95s and 76,48s respectively. So when it comes to insert operation time,\nthe composite key collection clearly wins. The reason for this, of course, is a simpler document structure and only one\nindex, instead of two.\nI was more interested in read times, because in a typical case, data is usually read more often than written. For each collection,\nI prepared a script containing a list of find commands for 1 million documents in random order. Of course, the order of\nqueries in both scripts is the same:\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.composite.find({_id: {name: 'name_$x', surname: 'surname_$x'}})\"\ndone >> find1.js\n\n\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.artificial.find({name: 'name_$x', surname: 'surname_$x'})\"\ndone >> find2.js\n\n\nFinally, I measured the execution time of both scripts using the commands:\n\ntime mongo test find1.js\n\n\n\ntime mongo test find2.js\n\n\nThe results surprised me. The script running on the first collection took 11.19s and the second took 10.15s.\nAlthough the differences are small, I was sure that using a composite key would be much faster than searching\nthrough regular fields and would make up for all the inconvenience of less flexible keys. Meanwhile, it turns\nout that a collection built with an artificial key and a separate index on two fields wins in terms of search speed.\nI had one more idea. Maybe searching a document by a key that doesn’t exist would be faster?\nTo test this, I generated another two scripts, this time searching for non-existent documents:\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.composite.find({_id: {name: 'missing_name_$x', surname: 'missing_surname_$x'}})\"\ndone >> find-missing1.js\n\n\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.artificial.find({name: 'missing_name_$x', surname: 'missing_surname_$x'})\"\ndone >> find-missing2.js\n\n\nUnfortunately, the composite key collection also lost in this case: 12.44s vs. 10.26s.\nFinally, I decided to run one more test. Since when using composite key we have to pass the entire key to the query\nand cannot search by its fragment, I decided to create a third collection, this time its key being the concatenation of\nfirst and last name:\n\nvar bulk = db.concatenation.initializeUnorderedBulkOp();\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({_id: 'name_' + NumberInt(i) + '-' + 'surname_' + NumberInt(i)})\n}\nbulk.execute();\n\n\nI then prepared another script containing 1 million search operations (in the same order as the previous attempts,\nof course):\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.concatenation.find({_id: 'name_$x-surname_$x'})\"\ndone >> find3.js\n\n\nThis time I was finally able to get a result better than the one achieved with the artificial key approach: 9.71s.\nAll results are summarized in the chart below:\n\nConclusion\nIt appears that the IDHACK operation is indeed the fastest possible way to access a document, but only for a simple\nkey. As soon as we add an additional field into the key, the benefits of the IDHACK operation disappear and the\nregular index works better.\nIn general, my experiments did not confirm the advantages of using a composite key. The artificial key model is more\nflexible in querying and returns results faster. However, if you want to search even faster, you may consider using a\nsimple key created by concatenating selected fields.\nI realize, of course, that I did not use the latest available version of the mongo server. For this reason, I am\nposting all the scripts I used in my attempts and encourage you to experiment on your own. Perhaps composite keys\nperform better in newer versions of the database?","guid":"https://blog.allegro.tech/2021/10/comparing-mongodb-composite-indexes.html","categories":["tech","mongodb","index","performance","query tuning"],"isoDate":"2021-10-17T22:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"How to ruin your Elasticsearch performance — Part II: Breaking things, one at a time","link":"https://blog.allegro.tech/2021/10/how-to-ruin-elasticsearch-performance-part-ii.html","pubDate":"Thu, 07 Oct 2021 00:00:00 +0200","authors":{"author":[{"name":["Michał Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"<p>It’s easy to find resources about <em>improving</em> <a href=\"https://www.elastic.co/elastic-stack\">Elasticsearch</a> performance, but what if you wanted to <em>reduce</em> it?\nIn <a href=\"/2021/09/how-to-ruin-elasticsearch-performance-part-i.html\">Part I</a> of this two-part series we looked under the hood in order to learn how\nES works internally. Now, in Part II, is the time to apply this knowledge in practice and ruin our ES performance. Most tips should also be applicable to\n<a href=\"https://solr.apache.org/\">Solr</a>, raw <a href=\"https://lucene.apache.org/\">Lucene</a>, or, for that matter, to any other full-text search engine as well.</p>\n\n<h2 id=\"using-complex-boolean-queries\">Using complex boolean queries</h2>\n\n<p>A consequence of the algorithms outlined in <a href=\"/2021/09/how-to-ruin-elasticsearch-performance-part-i.html\">Part I</a> is that simple queries, such as finding\na document containing two or three specific words, are relatively cheap to compute. We can easily increase the cost by making the queries more complex. This\ncomplexity is easily achieved by using <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/query-dsl-bool-query.html\">boolean queries</a> which allow\narbitrary boolean expressions, including nested subexpressions.</p>\n\n<p>A “flat” query, even with many words, boils down to a single AND/OR operation (though potentially with many lists). Since a search engine is usually\nsmart enough to sort these lists by length, it can execute such queries really fast. But what happens if we use a complex boolean expression? Let’s compare\ntwo example queries:</p>\n\n<ul>\n  <li>Q1: find books which contain all of the words: “I like apples”</li>\n  <li>Q2: find books which contain all of the words: “I like apples” or “I like oranges” and were published in 2020 or 2021 and are at least 100 pages long</li>\n</ul>\n\n<p>Let us now look at the boolean expressions which correspond to these queries:</p>\n\n<ul>\n  <li>Q1: <code class=\"language-plaintext highlighter-rouge\">I AND like AND apples</code></li>\n  <li>Q2: <code class=\"language-plaintext highlighter-rouge\">((I AND like AND apples) OR (I AND like AND oranges)) AND (2020 OR 2021) AND (pages &gt;= 100)</code></li>\n</ul>\n\n<p>Clearly, there is much more stuff to compute in the case of Q2, and the query will take more time, accordingly. Even though from the end user’s perspective,\nwe only added a few filters, the complexity of the query rose dramatically. For Q1, we need to perform 2 AND operations. For Q2, we end up with\n6 AND operations and 2 OR operations. However, this query is probably still much worse for performance than it may seem.</p>\n\n<p>Let’s start analyzing this query from the bottom up.</p>\n\n<p>The expression <code class=\"language-plaintext highlighter-rouge\">2020 OR 2021</code> is a little gem that looks innocent, but is actually quite expensive. As you remember, the cost of an OR operation is proportional\nto the sum of the sizes of input lists. The lists of books published in a year are probably quite long, so the cost of merging two will be quite high.\nAs a bonus, we get an even longer list as a result and this long list will take part in any computations that follow. So here are the takeaways:</p>\n<ul>\n  <li>OR operations are costly,</li>\n  <li>even more so when inputs are large document sets;</li>\n  <li>subqueries (parentheses in the logical expression) cause temporary postings lists to be created, which then take part in further calculations and so their\nsizes affect query performance.</li>\n</ul>\n\n<p>Looking further at our query, we see that even more temporary document ID lists will have to be created: one for each pair of parentheses. These results have\nto be computed, and since they are temporary partial results, they will have to be stored in memory since they cannot be retrieved from the index directly.</p>\n\n<p>Also note that subqueries can hinder many optimizations search engines employ. I mentioned earlier that Lucene sorts postings lists by length when AND-ing\nthem together. This can only work reliably if list lengths are known. For a postings list of a single word, its length is stored in the index and known exactly\nup-front. For a nested subexpression, however, the number of matches is not known before the subexpression is evaluated. But, Lucene needs the number of matches\nin order to prepare the optimal query plan. This leads to a chicken-and-egg problem which Lucene solves by estimating the size of subquery results list based\non the sizes of its constituents. For example, it estimates the result size of a subquery with OR-s as the sum of its input sizes. Being just an estimate, this\nnumber may differ from the actual value, and thus cause suboptimal query performance further up the stack. Takeaway:</p>\n<ul>\n  <li>subqueries are great at hindering global query optimizations.</li>\n</ul>\n\n<p>Another reason why subqueries may negatively affect performance becomes apparent with queries such as <code class=\"language-plaintext highlighter-rouge\">(a AND b) AND (c AND d)</code>. Since AND is an associative\noperation, the expression above gives the same result as <code class=\"language-plaintext highlighter-rouge\">a AND b AND c AND d</code>. In the\nversion without parentheses, the optimization of sorting lists by size works globally since all inputs are at the same nesting level, potentially achieving\nbetter performance than the version with nested subexpressions which can only sort the lists within each pair of parentheses separately.</p>\n\n<p>You may wonder why anyone would add these parentheses, but such constructs may arise naturally due to the way your code is structured if individual subqueries\nare built by separate methods or classes because they serve different business needs.</p>\n\n<p>Looking at how long postings lists affect query performance, especially with OR operator, you can see one of the reasons for introducing\n<a href=\"https://en.wikipedia.org/wiki/Stop_word\">stopwords</a> into search configuration. Words such as <em>the</em> are very common and on one hand they introduce practically\nno meaning at all to the query (with rare exceptions), matching almost all documents anyway, and on the other, they could add immense computational cost.</p>\n\n<p>Obviously, the longest postings list possible is the one containing all documents in the index. And indeed, pure negative queries such as\n“all documents but those with the word x” tend to be very expensive. Surprisingly, AND-ing the full set of documents (the result of a\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/query-dsl-match-all-query.html\">match_all query</a>) with results of another query is very fast.\nThis is because of a <a href=\"https://github.com/apache/lucene/blob/5e0e7a5479bca798ccfe385629a0ca2ba5870bc0/lucene/core/src/java/org/apache/lucene/search/BooleanQuery.java#L449\">special optimization</a>\nwhich uses the identity <code class=\"language-plaintext highlighter-rouge\">ALL AND a = a</code> to simplify those queries so that the expensive computation can be completely avoided.\nThis kind of query rewriting can transform a number of query patterns to queries with the same result but better performance characteristics.\nHowever, this only works for a set of rather simple cases: for example if you do not use <code class=\"language-plaintext highlighter-rouge\">match_all</code> query, but create some other query which also happens\nto match all documents, this optimization will not be triggered. Complex query structure with subqueries can effectively disable such optimizations as well.</p>\n\n<p>Thinking about indexing and index segments, you have to notice that merging partial results from each segment is an operation similar to OR-ing (though it\nadditionally has to account for document removal and updates). This leads to the conclusion that having many segments hurts search performance, especially\nfor popular keywords whose postings lists are large to start with. Indeed, this actually happens. Performance may vary significantly depending on the number\nof segments, and the optimum is having just a single segment in your index. In Elastic, you can use the\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/indices-forcemerge.html\">force merge</a> API to reduce the number of segments after indexing.\nI have actually worked with a product in which data was never indexed incrementally, but instead the whole index was rebuilt from scratch and force-merged to a single\nsegment after each update. This was a relatively small index with high search traffic, so big gains in search performance (on the order of two times shorter response\ntimes) were a justifiable reason for this seemingly wasteful indexation process.</p>\n\n<h2 id=\"complex-queries-in-disguise\">Complex queries in disguise</h2>\n\n<p>Some queries seem simple, but are actually very complex for the search engine to handle. One example is prefix queries such as <code class=\"language-plaintext highlighter-rouge\">cat*</code> (which matches documents\ncontaining any words starting with <code class=\"language-plaintext highlighter-rouge\">cat</code>). It turns out that unless you do something special, such a query is likely to be handled as an OR-query with all words\nmatching the prefix: <code class=\"language-plaintext highlighter-rouge\">(cat OR catamaran OR catapult OR category OR ...)</code>. Keeping in mind that queries with the OR operator can be expensive, you see the risk:\nthere may be lots and lots of words in the resulting expression, increasing the cost of merging their corresponding postings lists. In most datasets, a query such as <code class=\"language-plaintext highlighter-rouge\">a*</code>,\nwith probably thousands of individual postings lists, each containing millions of documents, can take ages to finish and even bring down the whole cluster.</p>\n\n<p>Another type of query that looks simple at first glance but can (or rather, used to) be very costly is range searches in numeric and date fields. Let’s say you want to limit\nyour query to only documents modified between 2020-01-01 and 2020-12-31. How costly could that be? The inverted index maps individual values to lists of\ndocument IDs. If each value in the index corresponds to a single day, and the documents are evenly spread throughout the year, there will be 366 lists to join\nwith the OR operator. If the data is indexed with millisecond resolution, there will be many more, with performance becoming even worse.</p>\n\n<p>Fortunately, these issues have been known for a long time, and there are a number of solutions in place. For text fields, you can enable\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/index-prefixes.html\">prefix indexing</a> which creates special structures in the index which\ncontain merged postings lists so that they don’t have to be computed at query time. Range queries on numeric and date fields are now optimized by default\nin Elasticsearch by creating <a href=\"https://lucene.apache.org/core/2_9_4/api/core/org/apache/lucene/search/NumericRangeQuery.html#precisionStepDesc\">additional structures in the index</a>\nas well, though with a particularly nasty data set, you might still be able to trigger some issues. Note that these solutions are space-time tradeoffs\n(speeding up searches at the cost of larger index), and as with any tradeoff, there is always some risk of shooting yourself in the foot. Also, new versions\nintroduce new optimizations, so <a href=\"https://www.elastic.co/blog/apache-lucene-numeric-filters\">behavior may well change between ES versions</a>.\nInterestingly, some preconceptions related to performance are very persistent (not only in the full-text search field), and you may run into people\nrecommending optimizations which made sense ten years ago, but may be counterproductive now. For example,\n<a href=\"https://discuss.elastic.co/t/efficient-date-range-handling/3465\">range searches have been efficient for ten years</a>, and apart from extreme cases, you should\nnot need to worry about them too much now.</p>\n\n<p>As a side note, ES tries to protect you from yourself and by default disables some types of queries that are likely to be costly: you have to\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-dsl.html#query-dsl-allow-expensive-queries\">explicitly enable them</a> if you know what\nyou’re doing and want to use them.</p>\n\n<h2 id=\"returning-lots-of-search-results\">Returning lots of search results</h2>\n\n<p>Elasticsearch indexes may be huge, often searching millions and billions of documents, but usually only a tiny fraction of these documents match each query’s\ncriteria, and out of those, only a handful (10 or so) are returned to the end user. Increasing the number of documents returned is detrimental to search performance\nin many ways:</p>\n<ul>\n  <li>Some algorithms, such as <a href=\"https://en.wikipedia.org/wiki/Partial_sorting\">finding the top N results</a> when sorting, have complexity which depends on N:\nthey are faster if N is much smaller than the total number of matches, and become slower as N grows.</li>\n  <li>Some operations a full-text search engine performs are proportional to the number of documents returned (linear complexity). As you remember, just finding matches is very fast since it\nuses inverted indices, but in order to actually return the documents’ contents, they have to be fetched from document store, and this operation scales linearly\nwith the number of documents returned. So, if you fetch 100 documents instead of 10, this part takes around ten times longer. Same goes for highlighting\nquery terms. The amount of data transferred over the network scales similarly.</li>\n  <li>Aggregations such as grouping documents by a field’s value may also have linear complexity (the number of documents returned being the input size).</li>\n</ul>\n\n<p>Also note that paging the results (e.g. retrieving 100 pages of 10 documents each instead of a single request asking for 1000 documents) helps only a little.\nThe problem is that in order to find documents on positions 991-1000, Elastic has to find the complete list of results 1-1000 first, and only then take the last\n10 items. This means the cost of fetching documents from storage is indeed proportional to 10, but the cost of performing set operations on postings lists\nand aggregations as well as memory usage is still proportional to 1000.</p>\n\n<p>So, if you think you can have millions of documents in ES and can just retrieve them all (or some large subset) using a simple query, you may be in for a surprise.\nThere are <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/paginate-search-results.html\">specialized APIs for such a use case</a>, but they all have\ntheir limitations.</p>\n\n<h2 id=\"assuming-elastic-knows-as-much-about-your-data-as-you-do\">Assuming Elastic knows as much about your data as you do</h2>\n\n<p>Much of the discussion up to this point revolved around replacing boolean expressions with their equivalents that have different performance characteristics.\nSome of these transformations are always correct since both expressions can be proven equal by means of boolean algebra.\nHowever, sometimes two forms of a query are equivalent only within a specific data set. Despite many smart optimizations used by modern full-text search\nengines, by using knowledge about your dataset you can often achieve more in terms of increasing or decreasing search performance than by relying on\nmathematics alone.</p>\n\n<p>A simple example of using this knowledge in practice is improving performance by removing subqueries which are redundant due to the nature of the data.\nSuppose your index contains both printed and online publications. Only online publications have a URL. By following the simplest logic, if you wanted to find\nan online publication by URL, you would issue a query such as <code class=\"language-plaintext highlighter-rouge\">type:online AND url:value</code>. This will work, although you could query just <code class=\"language-plaintext highlighter-rouge\">url:value</code> as well.\nHowever, it requires that you know something about your data, namely that only online publications have any value set in the <code class=\"language-plaintext highlighter-rouge\">url</code> field.\nObviously, this simplified query will be faster than the original.</p>\n\n<p>Where you can’t avoid complex queries, you can still use your domain knowledge to improve or reduce performance. For example, since the cost of merge operations\ndepends on sizes of inputs, knowing that a particular subquery is likely to return many or few results (query selectivity) and modifying the query in a way\nin which the sizes of consecutive intermediate results diminish faster may boost performance, while relying only on Elastic’s optimizations may result in\nsub-par performance.</p>\n\n<p>Suppose (a real-world example) there is an index with two types of documents whose counts differ wildly: documents of type 1 make up 99% of the index while\ntype 2 amounts to just 1% of all documents. Certain queries must be limited to just a single type. The obvious way to filter these results is to add a clause\nsuch as <code class=\"language-plaintext highlighter-rouge\">... AND type:1</code> and <code class=\"language-plaintext highlighter-rouge\">... AND type:2</code>, correspondingly, but replacing the first one with <code class=\"language-plaintext highlighter-rouge\">... AND NOT type:2</code> may be faster since the results list for\ntype 2 is much shorter than for type 1. If the filters can be combined (e.g. by the user checking checkboxes in a GUI), and the user selects both types,\nmeaning effectively no filtering by type, it is probably much more efficient to simply remove the filter from the query than to add a <code class=\"language-plaintext highlighter-rouge\">... AND (type:1 OR type:2)</code>\nclause.</p>\n\n<p>As you may have already realized, not only boolean queries’ but also range queries’ performance may depend a lot on your data, for example on a field’s\ncardinality (the number of unique values). One of the more spectacular ways of shooting yourself in the foot is applying a pattern which normally helps\nperformance, but in your particular case, due to a specific distribution of a field’s values, does just the opposite. Such situations may be very difficult\nto discover if you do not precisely track performance before and after each significant change. Sneakily placing such a pattern in your code can be a great way\nto end up with low performance difficult to explain.</p>\n\n<p>For a real life example, consider the rule of thumb that if you don’t care about a subquery’s score, using <code class=\"language-plaintext highlighter-rouge\">filter</code> subqueries within a <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-dsl-bool-query.html\">bool query</a>\nresults in faster response times than using <code class=\"language-plaintext highlighter-rouge\">must</code> subqueries since the former <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-filter-context.html\">do not need to update matching documents’ scores</a>.\nIn our advertising system, we match ads in a way mostly consistent with the way we match organic results. We match ads by keywords, but we also take\ninto account criteria such as delivery methods selected by the user. In the latter case, the fact that a sponsored offer\nis available with some delivery method only affects which offers match, but does not affect their scores. This is a perfect use case for <code class=\"language-plaintext highlighter-rouge\">filter</code> queries.\nHowever, we also use <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-dsl-function-score-query.html\">function score query</a>.\nFunction score query allows us to combine a document’s score resulting from how well it matches our keywords with additional factors.\nFunction score query accepts an embedded query — only documents matching this query have their scores modified. Symbolically, we could express it as our\ncomplete query being: <code class=\"language-plaintext highlighter-rouge\">function_score_query(keyword_subquery AND filters_subquery)</code>. At one point, I wanted to optimize the performance of this query, and\nfollowing the abovementioned rule of thumb, thought that it would make sense to move <code class=\"language-plaintext highlighter-rouge\">filters_subquery</code> outside of <code class=\"language-plaintext highlighter-rouge\">function_score_query</code> since filters need\nnot participate in score calculations. This resulted in the query <code class=\"language-plaintext highlighter-rouge\">filters_subquery AND function_score_query(keyword_subquery)</code> and should have\nimproved search performance. However, upon running performance tests, to my surprise I realized these changes actually made performance worse. The reason was,\nwith the filters moved outside <code class=\"language-plaintext highlighter-rouge\">function_score_query</code>, <code class=\"language-plaintext highlighter-rouge\">function_score_query</code> had to modify the scores of a larger number of documents and for the particular\ndata I had in my index, the added cost of rescoring more documents was greater than the savings achieved by not having to calculate the score for these\ndocuments in the first place. This just shows that with performance tuning, <a href=\"https://en.wiktionary.org/wiki/your_mileage_may_vary\">YMMV</a>, always.</p>\n\n<h2 id=\"treating-search-and-indexing-as-two-separate-problems\">Treating search and indexing as two separate problems</h2>\n\n<p>You might be tempted to think of Elasticsearch as yet another database. If you do, you are likely to run into many issues, including performance problems.\nOne of the main things that set ES apart from most databases, whether they be SQL or NoSQL, is the\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/7.x/near-real-time.html\">search-indexing asymmetry</a>.\nIn contrast to a normal database, in Elasticsearch you can’t just insert a bunch of documents: this process triggers indexing, creates new segments, potentially\ntriggers segment merges, has to <a href=\"https://www.alibabacloud.com/blog/elasticsearch-distributed-consistency-principles-analysis-3---data_594360\">propagate replicas and handle consistency within the cluster</a>,\netc. This may all affect performance in interesting ways. While indices of some kind are used in pretty much all databases, in ES they play\na central role. Another important difference to databases is that Elastic data model favors, and often forces, very much denormalized data. This is\ncommon with NoSQL databases but in ES, it is even more extreme.</p>\n\n<p>In particular, since ES is — in most cases — more about search performance than anything else, it is a common optimization to move cost from search time to\nindexing time when possible, and many such optimizations result in an even more denormalized data model.</p>\n\n<p>For example, let’s consider an index of offers such as you may find in an online store. Each offer may be available with a free return option, but\nthere’s a catch: while the client only sees a single checkbox in the UI, internally there are several types of free returns, e.g. free return by package locker\nand free return by post. The natural way to handle this would be to index each of these two flags and then to search for offers having either of those flags.\nIt would also be a step towards our goal of ruining Elasticsearch search performance, especially if the number of values was 200 rather than 2.</p>\n\n<p>The reason it works this way is that there are lots and lots of offers matching any of these flags: probably around 90% match one and around 90% match the other\n(with, obviously, a large number matching both). Going back to the <a href=\"/2021/09/how-to-ruin-elasticsearch-performance-part-i.html#or-operator\">section about OR operator</a>, you will notice that having two very long\ninput lists is about the worst case for OR operator efficiency. A usually reasonable trade-off in such a case is to move the cost to indexing-time, and\nto index with the document just a single flag, “free return”, which will improve search performance (at the cost of reducing indexing performance by just a tiny amount).\nNote that this was a very simple case and sometimes indexing denormalized data may increase index size significantly, in which case the trade-off may become\nless obvious.</p>\n\n<p>Another quirk is the mutual interaction between indexing and search performance. Interaction between reads and writes happens in practically any database,\nbut with Elastic, it is easier for it to become an issue due to the relatively high CPU and I/O cost of indexing. Ignoring this fact and treating\nsearch and indexing performance as two independent issues is a recipe for poor performance in both areas.</p>\n\n<h2 id=\"jumping-right-into-optimization-without-checking-first\">Jumping right into optimization without checking first</h2>\n\n<p>One effective method of achieving inferior performance, which works not only with Elasticsearch, is jumping\nright into optimization without first analyzing the problem, and, even better, not checking if there is a problem at all. It is a boring thing to repeat\nover and over, but the only way to improve performance is to:</p>\n<ul>\n  <li>first, measure the baseline you are starting from (avoiding common pitfalls along the way),</li>\n  <li>decide whether the values are satisfactory or not,</li>\n  <li>define target values if they are not, and</li>\n  <li>systematically measure and improve until success or surrender.</li>\n</ul>\n\n<p>Optimizing without <a href=\"https://esrally.readthedocs.io/en/stable/\">measurement</a> and without defining goals, on the other hand, is a good method of wasting your time, and consequently, achieving sub-par\nperformance. While there are some simple improvements which amount to “don’t do stupid things” and can be applied practically always without any risk,\nmost are trade-offs: you gain something at the expense of something else. If you apply them inappropriately, you may end up with expenses but without the gains.\nMany optimizations’ effectiveness varies a lot depending on the kind of data in the index or specific query patterns generated by your users, so, for example\nyou may introduce an optimization whose effect is negligible, but whose cost (e.g. in increased complexity and thus maintenance cost) is significant.</p>\n\n<h2 id=\"blindly-trusting-what-you-read-on-the-web\">Blindly trusting what you read on the web</h2>\n\n<p>This leads us to the last tip: if you really want to ruin your ES performance, always trust strangers on the internet and apply their advice\nduly and without hesitation. Obviously, this applies to this very post as well. Another good practice is to never check publication dates, or the ES versions\nthat particular tips apply to.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>I hope <a href=\"/2021/09/how-to-ruin-elasticsearch-performance-part-i.html\">Part I</a> gave you some background on how Elastic works under the hood. In Part II, we discussed various techniques which can affect its performance in\nreal-world scenarios. Armed with this knowledge, you will be able to make or break Elasticsearch performance: the choice is yours.</p>\n","contentSnippet":"It’s easy to find resources about improving Elasticsearch performance, but what if you wanted to reduce it?\nIn Part I of this two-part series we looked under the hood in order to learn how\nES works internally. Now, in Part II, is the time to apply this knowledge in practice and ruin our ES performance. Most tips should also be applicable to\nSolr, raw Lucene, or, for that matter, to any other full-text search engine as well.\nUsing complex boolean queries\nA consequence of the algorithms outlined in Part I is that simple queries, such as finding\na document containing two or three specific words, are relatively cheap to compute. We can easily increase the cost by making the queries more complex. This\ncomplexity is easily achieved by using boolean queries which allow\narbitrary boolean expressions, including nested subexpressions.\nA “flat” query, even with many words, boils down to a single AND/OR operation (though potentially with many lists). Since a search engine is usually\nsmart enough to sort these lists by length, it can execute such queries really fast. But what happens if we use a complex boolean expression? Let’s compare\ntwo example queries:\nQ1: find books which contain all of the words: “I like apples”\nQ2: find books which contain all of the words: “I like apples” or “I like oranges” and were published in 2020 or 2021 and are at least 100 pages long\nLet us now look at the boolean expressions which correspond to these queries:\nQ1: I AND like AND apples\nQ2: ((I AND like AND apples) OR (I AND like AND oranges)) AND (2020 OR 2021) AND (pages >= 100)\nClearly, there is much more stuff to compute in the case of Q2, and the query will take more time, accordingly. Even though from the end user’s perspective,\nwe only added a few filters, the complexity of the query rose dramatically. For Q1, we need to perform 2 AND operations. For Q2, we end up with\n6 AND operations and 2 OR operations. However, this query is probably still much worse for performance than it may seem.\nLet’s start analyzing this query from the bottom up.\nThe expression 2020 OR 2021 is a little gem that looks innocent, but is actually quite expensive. As you remember, the cost of an OR operation is proportional\nto the sum of the sizes of input lists. The lists of books published in a year are probably quite long, so the cost of merging two will be quite high.\nAs a bonus, we get an even longer list as a result and this long list will take part in any computations that follow. So here are the takeaways:\nOR operations are costly,\neven more so when inputs are large document sets;\nsubqueries (parentheses in the logical expression) cause temporary postings lists to be created, which then take part in further calculations and so their\nsizes affect query performance.\nLooking further at our query, we see that even more temporary document ID lists will have to be created: one for each pair of parentheses. These results have\nto be computed, and since they are temporary partial results, they will have to be stored in memory since they cannot be retrieved from the index directly.\nAlso note that subqueries can hinder many optimizations search engines employ. I mentioned earlier that Lucene sorts postings lists by length when AND-ing\nthem together. This can only work reliably if list lengths are known. For a postings list of a single word, its length is stored in the index and known exactly\nup-front. For a nested subexpression, however, the number of matches is not known before the subexpression is evaluated. But, Lucene needs the number of matches\nin order to prepare the optimal query plan. This leads to a chicken-and-egg problem which Lucene solves by estimating the size of subquery results list based\non the sizes of its constituents. For example, it estimates the result size of a subquery with OR-s as the sum of its input sizes. Being just an estimate, this\nnumber may differ from the actual value, and thus cause suboptimal query performance further up the stack. Takeaway:\nsubqueries are great at hindering global query optimizations.\nAnother reason why subqueries may negatively affect performance becomes apparent with queries such as (a AND b) AND (c AND d). Since AND is an associative\noperation, the expression above gives the same result as a AND b AND c AND d. In the\nversion without parentheses, the optimization of sorting lists by size works globally since all inputs are at the same nesting level, potentially achieving\nbetter performance than the version with nested subexpressions which can only sort the lists within each pair of parentheses separately.\nYou may wonder why anyone would add these parentheses, but such constructs may arise naturally due to the way your code is structured if individual subqueries\nare built by separate methods or classes because they serve different business needs.\nLooking at how long postings lists affect query performance, especially with OR operator, you can see one of the reasons for introducing\nstopwords into search configuration. Words such as the are very common and on one hand they introduce practically\nno meaning at all to the query (with rare exceptions), matching almost all documents anyway, and on the other, they could add immense computational cost.\nObviously, the longest postings list possible is the one containing all documents in the index. And indeed, pure negative queries such as\n“all documents but those with the word x” tend to be very expensive. Surprisingly, AND-ing the full set of documents (the result of a\nmatch_all query) with results of another query is very fast.\nThis is because of a special optimization\nwhich uses the identity ALL AND a = a to simplify those queries so that the expensive computation can be completely avoided.\nThis kind of query rewriting can transform a number of query patterns to queries with the same result but better performance characteristics.\nHowever, this only works for a set of rather simple cases: for example if you do not use match_all query, but create some other query which also happens\nto match all documents, this optimization will not be triggered. Complex query structure with subqueries can effectively disable such optimizations as well.\nThinking about indexing and index segments, you have to notice that merging partial results from each segment is an operation similar to OR-ing (though it\nadditionally has to account for document removal and updates). This leads to the conclusion that having many segments hurts search performance, especially\nfor popular keywords whose postings lists are large to start with. Indeed, this actually happens. Performance may vary significantly depending on the number\nof segments, and the optimum is having just a single segment in your index. In Elastic, you can use the\nforce merge API to reduce the number of segments after indexing.\nI have actually worked with a product in which data was never indexed incrementally, but instead the whole index was rebuilt from scratch and force-merged to a single\nsegment after each update. This was a relatively small index with high search traffic, so big gains in search performance (on the order of two times shorter response\ntimes) were a justifiable reason for this seemingly wasteful indexation process.\nComplex queries in disguise\nSome queries seem simple, but are actually very complex for the search engine to handle. One example is prefix queries such as cat* (which matches documents\ncontaining any words starting with cat). It turns out that unless you do something special, such a query is likely to be handled as an OR-query with all words\nmatching the prefix: (cat OR catamaran OR catapult OR category OR ...). Keeping in mind that queries with the OR operator can be expensive, you see the risk:\nthere may be lots and lots of words in the resulting expression, increasing the cost of merging their corresponding postings lists. In most datasets, a query such as a*,\nwith probably thousands of individual postings lists, each containing millions of documents, can take ages to finish and even bring down the whole cluster.\nAnother type of query that looks simple at first glance but can (or rather, used to) be very costly is range searches in numeric and date fields. Let’s say you want to limit\nyour query to only documents modified between 2020-01-01 and 2020-12-31. How costly could that be? The inverted index maps individual values to lists of\ndocument IDs. If each value in the index corresponds to a single day, and the documents are evenly spread throughout the year, there will be 366 lists to join\nwith the OR operator. If the data is indexed with millisecond resolution, there will be many more, with performance becoming even worse.\nFortunately, these issues have been known for a long time, and there are a number of solutions in place. For text fields, you can enable\nprefix indexing which creates special structures in the index which\ncontain merged postings lists so that they don’t have to be computed at query time. Range queries on numeric and date fields are now optimized by default\nin Elasticsearch by creating additional structures in the index\nas well, though with a particularly nasty data set, you might still be able to trigger some issues. Note that these solutions are space-time tradeoffs\n(speeding up searches at the cost of larger index), and as with any tradeoff, there is always some risk of shooting yourself in the foot. Also, new versions\nintroduce new optimizations, so behavior may well change between ES versions.\nInterestingly, some preconceptions related to performance are very persistent (not only in the full-text search field), and you may run into people\nrecommending optimizations which made sense ten years ago, but may be counterproductive now. For example,\nrange searches have been efficient for ten years, and apart from extreme cases, you should\nnot need to worry about them too much now.\nAs a side note, ES tries to protect you from yourself and by default disables some types of queries that are likely to be costly: you have to\nexplicitly enable them if you know what\nyou’re doing and want to use them.\nReturning lots of search results\nElasticsearch indexes may be huge, often searching millions and billions of documents, but usually only a tiny fraction of these documents match each query’s\ncriteria, and out of those, only a handful (10 or so) are returned to the end user. Increasing the number of documents returned is detrimental to search performance\nin many ways:\nSome algorithms, such as finding the top N results when sorting, have complexity which depends on N:\nthey are faster if N is much smaller than the total number of matches, and become slower as N grows.\nSome operations a full-text search engine performs are proportional to the number of documents returned (linear complexity). As you remember, just finding matches is very fast since it\nuses inverted indices, but in order to actually return the documents’ contents, they have to be fetched from document store, and this operation scales linearly\nwith the number of documents returned. So, if you fetch 100 documents instead of 10, this part takes around ten times longer. Same goes for highlighting\nquery terms. The amount of data transferred over the network scales similarly.\nAggregations such as grouping documents by a field’s value may also have linear complexity (the number of documents returned being the input size).\nAlso note that paging the results (e.g. retrieving 100 pages of 10 documents each instead of a single request asking for 1000 documents) helps only a little.\nThe problem is that in order to find documents on positions 991-1000, Elastic has to find the complete list of results 1-1000 first, and only then take the last\n10 items. This means the cost of fetching documents from storage is indeed proportional to 10, but the cost of performing set operations on postings lists\nand aggregations as well as memory usage is still proportional to 1000.\nSo, if you think you can have millions of documents in ES and can just retrieve them all (or some large subset) using a simple query, you may be in for a surprise.\nThere are specialized APIs for such a use case, but they all have\ntheir limitations.\nAssuming Elastic knows as much about your data as you do\nMuch of the discussion up to this point revolved around replacing boolean expressions with their equivalents that have different performance characteristics.\nSome of these transformations are always correct since both expressions can be proven equal by means of boolean algebra.\nHowever, sometimes two forms of a query are equivalent only within a specific data set. Despite many smart optimizations used by modern full-text search\nengines, by using knowledge about your dataset you can often achieve more in terms of increasing or decreasing search performance than by relying on\nmathematics alone.\nA simple example of using this knowledge in practice is improving performance by removing subqueries which are redundant due to the nature of the data.\nSuppose your index contains both printed and online publications. Only online publications have a URL. By following the simplest logic, if you wanted to find\nan online publication by URL, you would issue a query such as type:online AND url:value. This will work, although you could query just url:value as well.\nHowever, it requires that you know something about your data, namely that only online publications have any value set in the url field.\nObviously, this simplified query will be faster than the original.\nWhere you can’t avoid complex queries, you can still use your domain knowledge to improve or reduce performance. For example, since the cost of merge operations\ndepends on sizes of inputs, knowing that a particular subquery is likely to return many or few results (query selectivity) and modifying the query in a way\nin which the sizes of consecutive intermediate results diminish faster may boost performance, while relying only on Elastic’s optimizations may result in\nsub-par performance.\nSuppose (a real-world example) there is an index with two types of documents whose counts differ wildly: documents of type 1 make up 99% of the index while\ntype 2 amounts to just 1% of all documents. Certain queries must be limited to just a single type. The obvious way to filter these results is to add a clause\nsuch as ... AND type:1 and ... AND type:2, correspondingly, but replacing the first one with ... AND NOT type:2 may be faster since the results list for\ntype 2 is much shorter than for type 1. If the filters can be combined (e.g. by the user checking checkboxes in a GUI), and the user selects both types,\nmeaning effectively no filtering by type, it is probably much more efficient to simply remove the filter from the query than to add a ... AND (type:1 OR type:2)\nclause.\nAs you may have already realized, not only boolean queries’ but also range queries’ performance may depend a lot on your data, for example on a field’s\ncardinality (the number of unique values). One of the more spectacular ways of shooting yourself in the foot is applying a pattern which normally helps\nperformance, but in your particular case, due to a specific distribution of a field’s values, does just the opposite. Such situations may be very difficult\nto discover if you do not precisely track performance before and after each significant change. Sneakily placing such a pattern in your code can be a great way\nto end up with low performance difficult to explain.\nFor a real life example, consider the rule of thumb that if you don’t care about a subquery’s score, using filter subqueries within a bool query\nresults in faster response times than using must subqueries since the former do not need to update matching documents’ scores.\nIn our advertising system, we match ads in a way mostly consistent with the way we match organic results. We match ads by keywords, but we also take\ninto account criteria such as delivery methods selected by the user. In the latter case, the fact that a sponsored offer\nis available with some delivery method only affects which offers match, but does not affect their scores. This is a perfect use case for filter queries.\nHowever, we also use function score query.\nFunction score query allows us to combine a document’s score resulting from how well it matches our keywords with additional factors.\nFunction score query accepts an embedded query — only documents matching this query have their scores modified. Symbolically, we could express it as our\ncomplete query being: function_score_query(keyword_subquery AND filters_subquery). At one point, I wanted to optimize the performance of this query, and\nfollowing the abovementioned rule of thumb, thought that it would make sense to move filters_subquery outside of function_score_query since filters need\nnot participate in score calculations. This resulted in the query filters_subquery AND function_score_query(keyword_subquery) and should have\nimproved search performance. However, upon running performance tests, to my surprise I realized these changes actually made performance worse. The reason was,\nwith the filters moved outside function_score_query, function_score_query had to modify the scores of a larger number of documents and for the particular\ndata I had in my index, the added cost of rescoring more documents was greater than the savings achieved by not having to calculate the score for these\ndocuments in the first place. This just shows that with performance tuning, YMMV, always.\nTreating search and indexing as two separate problems\nYou might be tempted to think of Elasticsearch as yet another database. If you do, you are likely to run into many issues, including performance problems.\nOne of the main things that set ES apart from most databases, whether they be SQL or NoSQL, is the\nsearch-indexing asymmetry.\nIn contrast to a normal database, in Elasticsearch you can’t just insert a bunch of documents: this process triggers indexing, creates new segments, potentially\ntriggers segment merges, has to propagate replicas and handle consistency within the cluster,\netc. This may all affect performance in interesting ways. While indices of some kind are used in pretty much all databases, in ES they play\na central role. Another important difference to databases is that Elastic data model favors, and often forces, very much denormalized data. This is\ncommon with NoSQL databases but in ES, it is even more extreme.\nIn particular, since ES is — in most cases — more about search performance than anything else, it is a common optimization to move cost from search time to\nindexing time when possible, and many such optimizations result in an even more denormalized data model.\nFor example, let’s consider an index of offers such as you may find in an online store. Each offer may be available with a free return option, but\nthere’s a catch: while the client only sees a single checkbox in the UI, internally there are several types of free returns, e.g. free return by package locker\nand free return by post. The natural way to handle this would be to index each of these two flags and then to search for offers having either of those flags.\nIt would also be a step towards our goal of ruining Elasticsearch search performance, especially if the number of values was 200 rather than 2.\nThe reason it works this way is that there are lots and lots of offers matching any of these flags: probably around 90% match one and around 90% match the other\n(with, obviously, a large number matching both). Going back to the section about OR operator, you will notice that having two very long\ninput lists is about the worst case for OR operator efficiency. A usually reasonable trade-off in such a case is to move the cost to indexing-time, and\nto index with the document just a single flag, “free return”, which will improve search performance (at the cost of reducing indexing performance by just a tiny amount).\nNote that this was a very simple case and sometimes indexing denormalized data may increase index size significantly, in which case the trade-off may become\nless obvious.\nAnother quirk is the mutual interaction between indexing and search performance. Interaction between reads and writes happens in practically any database,\nbut with Elastic, it is easier for it to become an issue due to the relatively high CPU and I/O cost of indexing. Ignoring this fact and treating\nsearch and indexing performance as two independent issues is a recipe for poor performance in both areas.\nJumping right into optimization without checking first\nOne effective method of achieving inferior performance, which works not only with Elasticsearch, is jumping\nright into optimization without first analyzing the problem, and, even better, not checking if there is a problem at all. It is a boring thing to repeat\nover and over, but the only way to improve performance is to:\nfirst, measure the baseline you are starting from (avoiding common pitfalls along the way),\ndecide whether the values are satisfactory or not,\ndefine target values if they are not, and\nsystematically measure and improve until success or surrender.\nOptimizing without measurement and without defining goals, on the other hand, is a good method of wasting your time, and consequently, achieving sub-par\nperformance. While there are some simple improvements which amount to “don’t do stupid things” and can be applied practically always without any risk,\nmost are trade-offs: you gain something at the expense of something else. If you apply them inappropriately, you may end up with expenses but without the gains.\nMany optimizations’ effectiveness varies a lot depending on the kind of data in the index or specific query patterns generated by your users, so, for example\nyou may introduce an optimization whose effect is negligible, but whose cost (e.g. in increased complexity and thus maintenance cost) is significant.\nBlindly trusting what you read on the web\nThis leads us to the last tip: if you really want to ruin your ES performance, always trust strangers on the internet and apply their advice\nduly and without hesitation. Obviously, this applies to this very post as well. Another good practice is to never check publication dates, or the ES versions\nthat particular tips apply to.\nSummary\nI hope Part I gave you some background on how Elastic works under the hood. In Part II, we discussed various techniques which can affect its performance in\nreal-world scenarios. Armed with this knowledge, you will be able to make or break Elasticsearch performance: the choice is yours.","guid":"https://blog.allegro.tech/2021/10/how-to-ruin-elasticsearch-performance-part-ii.html","categories":["tech","full-text search","elasticsearch","elastic","es","performance"],"isoDate":"2021-10-06T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How to ruin your Elasticsearch performance — Part I: Know your enemy","link":"https://blog.allegro.tech/2021/09/how-to-ruin-elasticsearch-performance-part-i.html","pubDate":"Thu, 30 Sep 2021 00:00:00 +0200","authors":{"author":[{"name":["Michał Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"<p>It’s easy to find resources about <em>improving</em> <a href=\"https://www.elastic.co/elastic-stack\">Elasticsearch</a> performance, but what if you wanted to <em>reduce</em> it?\nThis is Part I of a two-post series, and will present some ES internals. In <a href=\"/2021/10/how-to-ruin-elasticsearch-performance-part-ii.html\">Part II</a>\nwe’ll deduce from them a collection of select tips which can help you ruin your ES performance in no time. Most should also be applicable to\n<a href=\"https://solr.apache.org/\">Solr</a>, raw <a href=\"https://lucene.apache.org/\">Lucene</a>, or, for that matter, to any other full-text search engine as well.</p>\n\n<p>Surprisingly, a number of people seem to have discovered these tactics already, and you may even find some of them used in your own production code.</p>\n\n<h2 id=\"know-your-enemy-know-your-battlefield\">Know your enemy, know your battlefield</h2>\n\n<p>In order to deal a truly devastating blow to Elastic’s performance, we first need to understand what goes on under the hood. Since full-text search is\na complex topic, consider this introduction both simplified and incomplete.</p>\n\n<h2 id=\"index-and-document-contents\">Index and document contents</h2>\n\n<p>In most full-text search engines data is split into two separate areas: the index, which makes it possible to find documents (represented by some sort of document ID)\nmatching specified criteria, and <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/mapping-store.html\">document storage</a> which makes it possible\nto retrieve the contents (values of all fields) of a document with specified ID.\nThis distinction improves performance, since usually document IDs will appear multiple times in the index, and it would not make much sense to duplicate all\ndocument contents. IDs can also fit into fixed-width fields which makes managing certain data structures easier. This separation also enables further\nspace savings: it is possible to specify that certain fields will never be searched, and therefore do not need to be in the index, while others might never\nneed to be returned in search results and thus can be omitted from document storage.</p>\n\n<p>For certain operations it may be necessary to <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/doc-values.html\">store field values within the index itself</a>,\nwhich is yet another approach.</p>\n\n<h2 id=\"inverted-index\">Inverted index</h2>\n\n<p>The basic data structure full-text search uses is the <a href=\"https://en.wikipedia.org/wiki/Inverted_index\">inverted index</a>. Basically, it is a map from keywords\nto sorted lists of document IDs, so-called postings lists. The specific data structures used to implement this mapping are many, but are not relevant here.\nWhat matters is that for a single-word query this index can find matching documents very fast: it actually contains a ready-to-use answer. The same\nstructure can, of course, be used not only for words: in a numeric index, for example, we may have a ready-to-use list with IDs of documents containing\nthe value 123 in a specific field.</p>\n\n<p><img src=\"/img/articles/2021-09-30-how-to-ruin-elasticsearch-performance/postings-lists.webp\" alt=\"Postings lists — lists of document IDs containing each individual word\" /></p>\n\n<h2 id=\"indexing\">Indexing</h2>\n\n<p>The mechanism for finding all documents containing a single word, described above, is very neat, but it can be so fast and simple only because\nthere is a ready answer for our query in the index. However, in order for it to end up in there, ES needs to perform a rather complex operation called <em>indexing</em>.\nWe won’t get into the details here, but suffice to say this process is both complex when it comes to the logic it implements, and resource-intensive, since\nit requires information about all documents to be gathered in a single place.</p>\n\n<p>This has far-reaching consequences. Adding a new document, which may contain hundreds or thousands of words, to the index, would mean that hundreds or thousands\nof postings lists would have to be updated. This would be prohibitively expensive in terms of performance. Therefore, full-text search engines usually employ\na different strategy: once built, an index is effectively immutable. When documents are added, removed, or modified, a new tiny index containing just the changes\nis created. At query time, results from the main and the incremental indices are merged. Any number of these incremental indices, called <a href=\"https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/codecs/lucene87/package-summary.html#Segments\">segments</a> in\nElastic jargon, can be created, but the cost of merging results at search time grows quickly with their number. Therefore, a special process of segment merging\nmust be present in order to ensure that the number of segments (and thus also search latency) does not get out of control. This, obviously, further increases\ncomplexity of the whole system.</p>\n\n<h2 id=\"operations-on-postings-lists\">Operations on postings lists</h2>\n\n<p>So far we talked about the relatively simple case of searching for documents matching a single search term. But what if we wanted to find documents\ncontaining multiple search terms? This is where Elastic needs to combine several postings lists into a single one. Interestingly, the same issue arises\neven for a single search term if your index has multiple segments.</p>\n\n<p>A postings list represents a set of document IDs, and most ways of matching documents to search terms correspond to boolean operations on those sets.\nFor example, finding documents which contain both <code class=\"language-plaintext highlighter-rouge\">term1</code> and <code class=\"language-plaintext highlighter-rouge\">term2</code> corresponds to the logical operation <code class=\"language-plaintext highlighter-rouge\">set1 AND set2</code> (intersection of sets) where\n<code class=\"language-plaintext highlighter-rouge\">set1</code> and <code class=\"language-plaintext highlighter-rouge\">set2</code> are the sets of documents matching individual search terms. Likewise, finding documents containing any word out of several corresponds\nto the logical <code class=\"language-plaintext highlighter-rouge\">OR</code> operation (sum of sets) and documents which contain one term, but do not contain another, correspond to <code class=\"language-plaintext highlighter-rouge\">AND NOT</code> operator (difference\nof sets).</p>\n\n<p>There are many ways these operations can be implemented in practice, with search engines using lots of optimizations. However, some constraints on the\ncomplexity remain. Let’s take a look at one possible implementation and see what conclusions can be drawn.</p>\n\n<p>While the operations described below can be generalized to work on multiple lists at once, for simplicity we’ll just discuss operations which are binary,\ni.e. which take two arguments. Conclusions remain the same for higher arity.</p>\n\n<p>In the algorithms below, we’ll assume each postings list is an actual list of integers (doc IDs), sorted in ascending order, and that their sizes\nare <em>n</em> and <em>m</em>.</p>\n\n<h3 id=\"or-operator\">OR</h3>\n\n<p><img src=\"/img/articles/2021-09-30-how-to-ruin-elasticsearch-performance/list-merging-or.webp\" alt=\"Example algorithm for computing results of OR operation\" /></p>\n\n<p>The way to merge two sorted lists in an OR operation is straightforward (and it is also the reason for the lists to be sorted in the first place).\nFor each list we need to keep a pointer which will indicate the current position. Both pointers start at the beginning of their corresponding lists.\nIn each step we compare the values (integer IDs) indicated by the pointers, and add the smaller one to the result list. Then, we move that pointer forward. If the values\nare equal, we add the value to the result just once and move both pointers. When one pointer reaches the end of its list, we copy the remainder of the second\nlist to the end of result list, and we’re done. Like each of the input lists, the result is a sorted list without duplicates.</p>\n\n<p>If you are familiar with <a href=\"https://en.wikipedia.org/wiki/Merge_sort\">merge sort</a>, you will notice that this algorithm corresponds to its merge phase.</p>\n\n<p>Since the result is a sum of the two sets, its size is at least <em>max(m, n)</em> (in the case one set is a subset of the other) and at most\n<em>m + n</em> (in the case the two sets are disjoint). Due to the fact that the cursor has to go through all entries in each of the lists, the\nalgorithm’s complexity is O(n+m). Even for any other algorithm, since the size of the result list may reach <em>m + n</em>, and we have to generate that list,\ncomplexity of O(n+m) is expected.</p>\n\n<p>The result does not depend on the order of lists (OR operation is symmetric), and performance is not (much) affected by it, either.</p>\n\n<h3 id=\"and-and-and-not\">AND and AND NOT</h3>\n\n<p><img src=\"/img/articles/2021-09-30-how-to-ruin-elasticsearch-performance/list-merging-and.webp\" alt=\"Example algorithm for computing results of AND / AND NOT operations\" /></p>\n\n<p>Calculating the intersection of two sets (what corresponds to the logical AND operator) or their difference (AND NOT) are very similar operations.\nJust as when calculating the sum of sets, we need to maintain two pointers, one for each list. In each step of the iteration we look at the current value\nin the first list and then try to find that value in the second list, starting from the second list’s pointer’s position. If we find the value, we add it\nto the result list, and move the second list’s pointer to the corresponding position. If the value can’t be found, we advance the pointer to the first\nitem after which the searched-for value would be. Once the second list’s pointer reaches the end, we are done.</p>\n\n<p>The algorithmic complexity of these two operations differs quite a bit from that of OR operation. First of all, a new sub-operation of searching for a value\nin the second list is used. It can be implemented in a number of ways depending on the data structures (flat array, skip list, various trees, etc.). For a simple\nsorted list stored in an array, we could use binary search whose complexity is O(log(<em>m</em>)). Things get a little more complicated since we only search starting\nfrom current position rather than from the beginning, but let’s skim over this for now. What matters is that we perform a search in the second list\nfor each item from the first one: the complexity is no longer symmetric between the two lists. If we change the order of the lists, the result of AND operation\ndoes not change, but the cost of performing the calculation does.\nIt pays to have the shorter of the two list as the first in order, and the difference can be huge. The cost also depends very much on the data, i.e. how\nbig the intersection of the two lists is. If the intersection is empty, even looking for the first list’s first element in the second list will move the second\nlist’s pointer to the end, and the algorithm will finish very quickly. The upper limit on the size of the result list (which in turn puts a lower limit on\nalgorithmic complexity) is <em>min(m, n)</em>.</p>\n\n<p>If we’re performing the AND NOT rather than AND operation, the match condition has to be reversed from <em>found</em> to <em>not found</em>. While the result changes when\nwe exchange the two lists, algorithmic properties are the same as for AND, especially the asymmetry in how the first and second list’s size affects the\ncomputational cost.</p>\n\n<p>In order to improve performance, many search engines will automatically change the order in which operations are evaluated if it does not alter the end result.\nThis is an example of <em>query rewriting</em>. In particular, Lucene (and thus also Elasticsearch and Solr)\n<a href=\"https://github.com/apache/lucene/blob/5e0e7a5479bca798ccfe385629a0ca2ba5870bc0/lucene/core/src/java/org/apache/lucene/search/ConjunctionDISI.java#L153\">reorder lists passed to AND operator</a>\nso that they are sorted by length in ascending order.</p>\n\n<h2 id=\"beyond-the-basics\">Beyond the basics</h2>\n\n<p>There are lots of factors which affect Elasticsearch performance and can be exploited in order to make that performance worse, and which I haven’t mentioned\nhere. These include scoring, phrase search, run-time scripting, sharding and replication, hardware, and disk vs memory access just to name a few.\nThere are also lots of minor quirks which are too numerous to list here. Given that you can\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/plugin-authors.html\">extend ES with custom plugins</a> that can execute arbitrary code, the\nopportunities for breaking things are endless.</p>\n\n<p>On the other hand, search engines employ <a href=\"https://www.elastic.co/blog/elasticsearch-query-execution-order\">a number of optimizations</a> which may counter\nsome of our efforts at achieving low performance.</p>\n\n<p>Then again, some of these optimizations may themselves lead to surprising results. For example, often there are two different ways of performing a task,\nand a heuristic is used to choose one or the other. Such heuristics are often simple threshold values: for example, if the number of sub-clauses in a\nquery is above 16, <a href=\"https://github.com/apache/lucene/blob/d5d6dc079395c47cd6d12dcce3bcfdd2c7d9dc63/lucene/core/src/java/org/apache/lucene/search/BooleanWeight.java#L358\">it will not be cached</a>.\nLikewise, certain pieces of data may be represented as lists and then suddenly switch to a bitset representation.\nSuch behaviors may be confusing since they make performance analysis more difficult.</p>\n\n<p>Anyway, even the basic knowledge presented above should allow you to deal some heavy damage to your search performance, so let’s get started.</p>\n\n<h2 id=\"summary\">Summary</h2>\n<p>I hope the first part of this post gave you some background on how Elastic works under the hood. In <a href=\"/2021/10/how-to-ruin-elasticsearch-performance-part-ii.html\">Part II</a>,\nwe’ll look at how to apply this knowledge in practice to making Elasticsearch performance as bad as possible.</p>\n","contentSnippet":"It’s easy to find resources about improving Elasticsearch performance, but what if you wanted to reduce it?\nThis is Part I of a two-post series, and will present some ES internals. In Part II\nwe’ll deduce from them a collection of select tips which can help you ruin your ES performance in no time. Most should also be applicable to\nSolr, raw Lucene, or, for that matter, to any other full-text search engine as well.\nSurprisingly, a number of people seem to have discovered these tactics already, and you may even find some of them used in your own production code.\nKnow your enemy, know your battlefield\nIn order to deal a truly devastating blow to Elastic’s performance, we first need to understand what goes on under the hood. Since full-text search is\na complex topic, consider this introduction both simplified and incomplete.\nIndex and document contents\nIn most full-text search engines data is split into two separate areas: the index, which makes it possible to find documents (represented by some sort of document ID)\nmatching specified criteria, and document storage which makes it possible\nto retrieve the contents (values of all fields) of a document with specified ID.\nThis distinction improves performance, since usually document IDs will appear multiple times in the index, and it would not make much sense to duplicate all\ndocument contents. IDs can also fit into fixed-width fields which makes managing certain data structures easier. This separation also enables further\nspace savings: it is possible to specify that certain fields will never be searched, and therefore do not need to be in the index, while others might never\nneed to be returned in search results and thus can be omitted from document storage.\nFor certain operations it may be necessary to store field values within the index itself,\nwhich is yet another approach.\nInverted index\nThe basic data structure full-text search uses is the inverted index. Basically, it is a map from keywords\nto sorted lists of document IDs, so-called postings lists. The specific data structures used to implement this mapping are many, but are not relevant here.\nWhat matters is that for a single-word query this index can find matching documents very fast: it actually contains a ready-to-use answer. The same\nstructure can, of course, be used not only for words: in a numeric index, for example, we may have a ready-to-use list with IDs of documents containing\nthe value 123 in a specific field.\n\nIndexing\nThe mechanism for finding all documents containing a single word, described above, is very neat, but it can be so fast and simple only because\nthere is a ready answer for our query in the index. However, in order for it to end up in there, ES needs to perform a rather complex operation called indexing.\nWe won’t get into the details here, but suffice to say this process is both complex when it comes to the logic it implements, and resource-intensive, since\nit requires information about all documents to be gathered in a single place.\nThis has far-reaching consequences. Adding a new document, which may contain hundreds or thousands of words, to the index, would mean that hundreds or thousands\nof postings lists would have to be updated. This would be prohibitively expensive in terms of performance. Therefore, full-text search engines usually employ\na different strategy: once built, an index is effectively immutable. When documents are added, removed, or modified, a new tiny index containing just the changes\nis created. At query time, results from the main and the incremental indices are merged. Any number of these incremental indices, called segments in\nElastic jargon, can be created, but the cost of merging results at search time grows quickly with their number. Therefore, a special process of segment merging\nmust be present in order to ensure that the number of segments (and thus also search latency) does not get out of control. This, obviously, further increases\ncomplexity of the whole system.\nOperations on postings lists\nSo far we talked about the relatively simple case of searching for documents matching a single search term. But what if we wanted to find documents\ncontaining multiple search terms? This is where Elastic needs to combine several postings lists into a single one. Interestingly, the same issue arises\neven for a single search term if your index has multiple segments.\nA postings list represents a set of document IDs, and most ways of matching documents to search terms correspond to boolean operations on those sets.\nFor example, finding documents which contain both term1 and term2 corresponds to the logical operation set1 AND set2 (intersection of sets) where\nset1 and set2 are the sets of documents matching individual search terms. Likewise, finding documents containing any word out of several corresponds\nto the logical OR operation (sum of sets) and documents which contain one term, but do not contain another, correspond to AND NOT operator (difference\nof sets).\nThere are many ways these operations can be implemented in practice, with search engines using lots of optimizations. However, some constraints on the\ncomplexity remain. Let’s take a look at one possible implementation and see what conclusions can be drawn.\nWhile the operations described below can be generalized to work on multiple lists at once, for simplicity we’ll just discuss operations which are binary,\ni.e. which take two arguments. Conclusions remain the same for higher arity.\nIn the algorithms below, we’ll assume each postings list is an actual list of integers (doc IDs), sorted in ascending order, and that their sizes\nare n and m.\nOR\n\nThe way to merge two sorted lists in an OR operation is straightforward (and it is also the reason for the lists to be sorted in the first place).\nFor each list we need to keep a pointer which will indicate the current position. Both pointers start at the beginning of their corresponding lists.\nIn each step we compare the values (integer IDs) indicated by the pointers, and add the smaller one to the result list. Then, we move that pointer forward. If the values\nare equal, we add the value to the result just once and move both pointers. When one pointer reaches the end of its list, we copy the remainder of the second\nlist to the end of result list, and we’re done. Like each of the input lists, the result is a sorted list without duplicates.\nIf you are familiar with merge sort, you will notice that this algorithm corresponds to its merge phase.\nSince the result is a sum of the two sets, its size is at least max(m, n) (in the case one set is a subset of the other) and at most\nm + n (in the case the two sets are disjoint). Due to the fact that the cursor has to go through all entries in each of the lists, the\nalgorithm’s complexity is O(n+m). Even for any other algorithm, since the size of the result list may reach m + n, and we have to generate that list,\ncomplexity of O(n+m) is expected.\nThe result does not depend on the order of lists (OR operation is symmetric), and performance is not (much) affected by it, either.\nAND and AND NOT\n\nCalculating the intersection of two sets (what corresponds to the logical AND operator) or their difference (AND NOT) are very similar operations.\nJust as when calculating the sum of sets, we need to maintain two pointers, one for each list. In each step of the iteration we look at the current value\nin the first list and then try to find that value in the second list, starting from the second list’s pointer’s position. If we find the value, we add it\nto the result list, and move the second list’s pointer to the corresponding position. If the value can’t be found, we advance the pointer to the first\nitem after which the searched-for value would be. Once the second list’s pointer reaches the end, we are done.\nThe algorithmic complexity of these two operations differs quite a bit from that of OR operation. First of all, a new sub-operation of searching for a value\nin the second list is used. It can be implemented in a number of ways depending on the data structures (flat array, skip list, various trees, etc.). For a simple\nsorted list stored in an array, we could use binary search whose complexity is O(log(m)). Things get a little more complicated since we only search starting\nfrom current position rather than from the beginning, but let’s skim over this for now. What matters is that we perform a search in the second list\nfor each item from the first one: the complexity is no longer symmetric between the two lists. If we change the order of the lists, the result of AND operation\ndoes not change, but the cost of performing the calculation does.\nIt pays to have the shorter of the two list as the first in order, and the difference can be huge. The cost also depends very much on the data, i.e. how\nbig the intersection of the two lists is. If the intersection is empty, even looking for the first list’s first element in the second list will move the second\nlist’s pointer to the end, and the algorithm will finish very quickly. The upper limit on the size of the result list (which in turn puts a lower limit on\nalgorithmic complexity) is min(m, n).\nIf we’re performing the AND NOT rather than AND operation, the match condition has to be reversed from found to not found. While the result changes when\nwe exchange the two lists, algorithmic properties are the same as for AND, especially the asymmetry in how the first and second list’s size affects the\ncomputational cost.\nIn order to improve performance, many search engines will automatically change the order in which operations are evaluated if it does not alter the end result.\nThis is an example of query rewriting. In particular, Lucene (and thus also Elasticsearch and Solr)\nreorder lists passed to AND operator\nso that they are sorted by length in ascending order.\nBeyond the basics\nThere are lots of factors which affect Elasticsearch performance and can be exploited in order to make that performance worse, and which I haven’t mentioned\nhere. These include scoring, phrase search, run-time scripting, sharding and replication, hardware, and disk vs memory access just to name a few.\nThere are also lots of minor quirks which are too numerous to list here. Given that you can\nextend ES with custom plugins that can execute arbitrary code, the\nopportunities for breaking things are endless.\nOn the other hand, search engines employ a number of optimizations which may counter\nsome of our efforts at achieving low performance.\nThen again, some of these optimizations may themselves lead to surprising results. For example, often there are two different ways of performing a task,\nand a heuristic is used to choose one or the other. Such heuristics are often simple threshold values: for example, if the number of sub-clauses in a\nquery is above 16, it will not be cached.\nLikewise, certain pieces of data may be represented as lists and then suddenly switch to a bitset representation.\nSuch behaviors may be confusing since they make performance analysis more difficult.\nAnyway, even the basic knowledge presented above should allow you to deal some heavy damage to your search performance, so let’s get started.\nSummary\nI hope the first part of this post gave you some background on how Elastic works under the hood. In Part II,\nwe’ll look at how to apply this knowledge in practice to making Elasticsearch performance as bad as possible.","guid":"https://blog.allegro.tech/2021/09/how-to-ruin-elasticsearch-performance-part-i.html","categories":["tech","full-text search","elasticsearch","elastic","es","performance"],"isoDate":"2021-09-29T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999781697706","name":"Product Manager - Technology Consumer Experience","uuid":"6ce4b325-b6dd-42f4-bf16-f6fcab53e8a6","refNumber":"REF2814W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-26T06:41:38.000Z","location":{"city":"Warszawa, Poznań, Toruń, Kraków","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572889","label":"IT - Product/Project Management"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"executive","label":"Executive"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572889","valueLabel":"IT - Product/Project Management"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999781697706","language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999780112164","name":"Software Engineer (Java/Kotlin) - IT Business Services","uuid":"12a8f356-39f3-4a0d-980c-38d16bcb0e04","refNumber":"REF2838W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-18T13:22:19.000Z","location":{"city":"Poznań, Warszawa, Toruń, Kraków, Wrocław, Lublin, Łódź, Gdańsk","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"inżynier, java"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999780112164","creator":{"name":"Dariusz Fudalej"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999780051418","name":"Research Engineer - Machine Learning (Computer Vision)","uuid":"0ea27b35-a033-4340-b00a-e6e879d4d314","refNumber":"REF2880R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-18T08:13:05.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"CV, Computer Vision, ML, AI, DS, Machine Learning, PyTorch, Python, Deep Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999780051418","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779485268","name":"Chief Architect","uuid":"5ea8adaa-e9ae-4cb2-a1dc-b27600247ffb","refNumber":"REF2835R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T13:53:22.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Gdańsk, Łódź, Katowice, Lublin","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"chief architect, architekt, architect, platform, architektura"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779485268","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779451297","name":"UX Designer (Allegro Lokalnie)","uuid":"5984cb62-0e39-471c-bb78-be04a567c4b7","refNumber":"REF2844Z","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:48:50.000Z","location":{"city":"Poznań, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572906","label":"UX - Research & Design"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"3df4cd37-be2f-409a-8871-60e1c2319007","valueLabel":"Nie"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"bcab9d4f-8624-4d85-8e3d-dbf12239b972","valueLabel":"Nie"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"50675cc6-acd3-46a5-901c-54e68167e826","valueLabel":"Nie"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"c4ab8d4b-916c-49d3-9ffb-bad301fb62f6","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572906","valueLabel":"UX - Research & Design"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"25416dd5-3ba7-4e71-ab12-7b3af24269dc","valueLabel":"Nie"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"UX Designer, Projektant UX, User Experience, Technology, Technologia, UX Design, Design"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779451297","creator":{"name":"Martyna Maziarska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1634290537000,"duration":5400000,"id":"281441586","name":"Allegro Tech Live #22 - Jak wygląda codzienność lidera w Allegro?","date_in_series_pattern":false,"status":"past","time":1634832000000,"local_date":"2021-10-21","local_time":"18:00","updated":1634841007000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":67,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281441586/","description":"!!!! Rejestracja: https://app.evenea.pl/event/allegro-tech-live-22/ !!!! Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale…","visibility":"public","member_pay_fee":false},{"created":1633336352000,"duration":7200000,"id":"281199452","name":"Allegro Tech Live #21 - Jak zautomatyzować bezpieczeństwo IT?","date_in_series_pattern":false,"status":"past","time":1633622400000,"local_date":"2021-10-07","local_time":"18:00","updated":1633634017000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281199452/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false},{"created":1629120828000,"duration":93600000,"id":"280149404","name":"Allegro Tech Meeting 2021","date_in_series_pattern":false,"status":"past","time":1632927600000,"local_date":"2021-09-29","local_time":"17:00","updated":1633024970000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":144,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/280149404/","description":"❗Uwaga, aby wziąć udział w wydarzeniu zarejestruj się tutaj: https://app.evenea.pl/event/atm-2021/ ❗ Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to…","how_to_find_us":"https://www.youtube.com/c/AllegroTechBlog","visibility":"public","member_pay_fee":false},{"created":1623957759000,"duration":7200000,"id":"278903176","name":"Allegro Tech Live #20: Wydajność Backendu","date_in_series_pattern":false,"status":"past","time":1624982400000,"local_date":"2021-06-29","local_time":"18:00","updated":1624994207000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":125,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278903176/","description":"Allegro Tech Live w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my zagościmy…","how_to_find_us":"https://youtu.be/VklKR_fO5OI","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}