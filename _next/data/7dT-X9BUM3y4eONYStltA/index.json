{"pageProps":{"posts":[{"title":"How does B-tree make your queries fast?","link":"https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html","pubDate":"Mon, 27 Nov 2023 00:00:00 +0100","authors":{"author":[{"name":["Mateusz Ku≈∫mik"],"photo":["https://blog.allegro.tech/img/authors/mateusz.kuzmik.jpg"],"url":["https://blog.allegro.tech/authors/mateusz.kuzmik"]}]},"content":"<p><strong>B-tree</strong> is a structure that helps to search through great amounts of data.\nIt was invented over 40 years ago, yet it is still employed by the majority of modern databases.\nAlthough there are newer index structures, like LSM trees,\n<strong>B-tree</strong> is unbeaten when handling most of the database queries.</p>\n\n<p>After reading this post, you will know how <strong>B-tree</strong> organises the data and how it performs search queries.</p>\n\n<h2 id=\"origins\">Origins</h2>\n\n<p>In order to understand <strong>B-tree</strong> let‚Äôs focus on <strong>Binary Search Tree (BST)</strong> first.</p>\n\n<p>Wait, isn‚Äôt it the same?</p>\n\n<p>What does ‚ÄúB‚Äù stand for then?</p>\n\n<p>According to <a href=\"https://en.wikipedia.org/wiki/B-tree\">wikipedia.org</a>, Edward M.¬†McCreight, the inventor of B-tree, once said:</p>\n\n<blockquote>\n  <p>‚Äúthe more you think about what the B in B-trees means, the better you understand B-trees.‚Äù</p>\n</blockquote>\n\n<p>Confusing <strong>B-tree</strong> with <strong>BST</strong> is a really common misconception.\nAnyway, in my opinion, BST is a great starting point for reinventing B-tree.\nLet‚Äôs start with a simple example of BST:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-basic.webp\" alt=\"Binary Search Tree with three nodes\" class=\"small-image\" /></p>\n\n<p>The greater number is always on the right, the lower on the left. It may become clearer when we add more numbers.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger.webp\" alt=\"Binary Search Tree with seven nodes\" class=\"small-image\" /></p>\n\n<p>This tree contains seven numbers, but we need to visit at most three nodes to locate any number.\nThe following example visualizes searching for 14.\nI used SQL to define the query in order to think about this tree as if it were an actual database index.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger-searching.webp\" alt=\"Searching for single node within Binary Search Tree with seven nodes\" class=\"small-image\" /></p>\n\n<h2 id=\"hardware\">Hardware</h2>\n\n<p>In theory, using Binary Search Tree for running our queries looks fine. Its time complexity (when searching) is \\(O(log\nn)\\), <a href=\"https://en.wikipedia.org/wiki/B-tree\">same as B-tree</a>. However, in practice, this data structure needs to work on actual hardware. An index must be\nstored somewhere on your machine.</p>\n\n<p>The computer has three places where the data can be stored:</p>\n\n<ul>\n  <li>CPU caches</li>\n  <li>RAM (memory)</li>\n  <li>Disk (storage)</li>\n</ul>\n\n<p>The cache is managed fully by CPUs. Moreover, it is relatively small, usually a few megabytes.\nIndex may contain gigabytes of data, so it won‚Äôt fit there.</p>\n\n<p>Databases vastly use Memory (RAM). It has some great advantages:</p>\n\n<ul>\n  <li>assures fast random access (more on that in the next paragraph)</li>\n  <li>its size may be pretty big (e.g. AWS RDS cloud service <a href=\"https://aws.amazon.com/rds/instance-types/\">provides instances</a>\nwith a few terabytes of memory available).</li>\n</ul>\n\n<p>Cons? You lose the data when the power supply goes off. Moreover, when compared to the disk, it is pretty expensive.</p>\n\n<p>Finally, the cons of a memory are the pros of a disk storage.\nIt‚Äôs cheap, and data will remain there even if we lose the power.\nHowever, there are no free lunches!\nThe catch is that we need to be careful about random and sequential access.\nReading from the disk is fast, but only under certain conditions!\nI‚Äôll try to explain them simply.</p>\n\n<h3 id=\"random-and-sequential-access\">Random and sequential access</h3>\n\n<p>Memory may be visualized as a line of containers for values, where every container is numbered.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory.webp\" alt=\"Simple memory visualization\" class=\"small-image\" /></p>\n\n<p>Now let‚Äôs assume we want to read data from containers 1, 4, and 6. It requires random access:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory-random-access.webp\" alt=\"Random access visualized on a small chunk of a memory\" class=\"small-image\" /></p>\n\n<p>And then let‚Äôs compare it with reading containers 3, 4, and 5. It may be done sequentially:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/memory-sequential-access.webp\" alt=\"Sequential access visualized on a small chunk of a memory\" class=\"small-image\" /></p>\n\n<p>The difference between a ‚Äúrandom jump‚Äù and a ‚Äúsequential read‚Äù can be explained based on Hard Disk Drive.\nIt consists of the head and the disk.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/hdd-disk.webp\" alt=\"Hard Disk Drive with cover removed, Public Domain image from https://en.wikipedia.org/wiki/Hard_disk_drive#/media/File:Laptop-hard-drive-exposed.jpg\" class=\"small-image\" /></p>\n\n<p>‚ÄúRandom jump‚Äù requires moving the head to the given place on the disk.\n‚ÄúSequential read‚Äù is simply spinning the disk, allowing the head to read consecutive values.\nWhen reading megabytes of data, the difference between these two types of access is enormous.\nUsing ‚Äúsequential reads‚Äù lowers the time needed to fetch the data significantly.</p>\n\n<p>Differences in speed between random and sequential access were researched in the article ‚ÄúThe Pathologies of Big Data‚Äù\nby Adam Jacobs, <a href=\"https://queue.acm.org/detail.cfm?id=1563874\">published in Acm Queue</a>.\nIt revealed a few mind-blowing facts:</p>\n\n<ul>\n  <li>Sequential access on HDD may be hundreds of thousands of times faster than random access. ü§Ø</li>\n  <li>It may be faster to read sequentially from the disk than randomly from the memory.</li>\n</ul>\n\n<p>Who even uses HDD nowadays?\nWhat about SSD?\nThis research shows that reading fully sequentially from HDD may be faster than SSD.\nHowever, please note that the article is from 2009 and SSD developed significantly through the last decade,\nthus these results are probably outdated.</p>\n\n<p>To sum up, the key takeaway is <strong>to prefer sequential access wherever we can</strong>.\nIn the next paragraph, I will explain how to apply it to our index structure.</p>\n\n<h2 id=\"optimizing-a-tree-for-sequential-access\">Optimizing a tree for sequential access</h2>\n\n<p>Binary Search Tree may be represented in memory in the same way\nas <a href=\"https://en.wikipedia.org/wiki/Binary_heap\">the heap</a>:</p>\n\n<ul>\n  <li>parent node position is \\(i\\)</li>\n  <li>left node position is \\(2i\\)</li>\n  <li>right node position is \\(2i+1\\)</li>\n</ul>\n\n<p>That‚Äôs how these positions are calculated based on the example (the parent node starts at 1):</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-1.webp\" alt=\"Binary tree representation in the memory‚Äîpart 1/2\" class=\"small-image\" /></p>\n\n<p>According to the calculated positions, nodes are aligned into the memory:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-2.webp\" alt=\"Binary tree representation in the memory‚Äîpart 2/2\" class=\"small-image\" /></p>\n\n<p>Do you remember the query visualized a few chapters ago?</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-bigger-searching.webp\" alt=\"Searching for single node within Binary Search Tree with seven nodes\" class=\"small-image\" /></p>\n\n<p>That‚Äôs what it looks like on the memory level:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-representation-in-memory-query.webp\" alt=\"Binary tree representation in the memory - querying\" class=\"small-image\" /></p>\n\n<p>When performing the query, memory addresses 1, 3, and 6 need to be visited.\nVisiting three nodes is not a problem; however, as we store more data, the tree gets higher.\nStoring more than one million values requires a tree of height at least 20. It means\nthat 20 values from different places in memory must be read.\nIt causes completely random access!</p>\n\n<h3 id=\"pages\">Pages</h3>\n\n<p>While a tree grows in height, random access is causing more and more delay.\nThe solution to reduce this problem is simple: grow the tree in width rather than in height.\nIt may be achieved by packing more than one value into a single node.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-in-node.webp\" alt=\"A tree with three values in single node\" /></p>\n\n<p>It brings us the following benefits:</p>\n\n<ul>\n  <li>the tree is shallower (two levels instead of three)</li>\n  <li>it still has a lot of space for new values without the need for growing further</li>\n</ul>\n\n<p>The query performed on such index looks as follows:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-query.webp\" alt=\"A query performed on a tree with three values in a single node\" /></p>\n\n<p>Please note that every time we visit a node, we need to load all its values.\nIn this example, we need to load 4 values (or 6 if the tree is full) in order to reach the one we are looking for.\nBelow, you will find a visualization of this tree in a memory:</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/tree-with-3-values-memory.webp\" alt=\"A tree with three values in a single node represented in a memory\" /></p>\n\n<p>Compared to <a href=\"#optimizing-a-tree-for-sequential-access\">the previous example</a> (where the tree grows in height),\nthis search should be faster.\nWe need random access only twice (jump to cells 0 and 9) and then sequentially read the rest of values.</p>\n\n<p>This solution works better and better as our database grows. If you want to store one million values, then you need:</p>\n\n<ul>\n  <li>Binary Search Tree which has <strong>20</strong> levels</li>\n</ul>\n\n<p>OR</p>\n\n<ul>\n  <li>3-value node Tree which has <strong>10</strong> levels</li>\n</ul>\n\n<p>Values from a single node make a page.\nIn the example above, each page consists of three values.\nA page is a set of values placed on a disk next to each other,\nso the database may reach the whole page at once with one sequential read.</p>\n\n<p>And how does it refer to the reality?\n<a href=\"https://www.postgresql.org/docs/current/storage-toast.html#:~:text=PostgreSQL%20uses%20a%20fixed%20page,tuples%20to%20span%20multiple%20pages.\">Postgres page size is 8kB</a>.\nLet‚Äôs assume that 20% is for metadata, so it‚Äôs 6kB left.\nHalf of the page is needed to store\npointers to node‚Äôs children, so it gives us 3kB for values.\nBIGINT size is 8 bytes, thus we may store ~375 values in a\nsingle page.</p>\n\n<p>Assuming that some pretty big tables in a database have one billion rows,\nhow many levels in the Postgres tree do we need to store them?\nAccording to the calculations above,\nif we create a tree that can handle 375 values in a single node,\nit may store <strong>1 billion</strong> values with a tree that has only <strong>four</strong> levels.\nBinary Search Tree would require 30 levels for such amount of data.</p>\n\n<p>To sum up, placing multiple values in a single node of the tree helped us to reduce its height, thus using the benefits of sequential access.\nMoreover, a B-tree may grow not only in height, but also in width (by using larger pages).</p>\n\n<h2 id=\"balancing\">Balancing</h2>\n\n<p>There are two types of operations in databases: writing and reading.\nIn the previous section, we addressed the problems with reading the data from the B-tree.\nNonetheless, writing is also a crucial part.\nWhen writing the data to a database, B-tree needs to be constantly updated with new values.</p>\n\n<p>The tree shape depends on the order of values added to the tree.\nIt‚Äôs easily visible in a binary tree.\nWe may obtain trees with different depths if the values are added in an incorrect order.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/bst-imbalance.webp\" alt=\"Two Binary Trees with shapes depending on the order of inserted values.\" class=\"small-image\" /></p>\n\n<p>When the tree has different depths on different nodes, it is called an unbalanced tree.\nThere are basically two ways of returning such a tree to a balanced state:</p>\n\n<ol>\n  <li>Rebuilding it from the very beginning just by adding the values in the correct order.</li>\n  <li>Keeping it balanced all the time, as the new values are added.</li>\n</ol>\n\n<p>B-tree implements the second option. A feature that makes the tree balanced all the time is called self-balancing.</p>\n\n<h3 id=\"self-balancing-algorithm-by-example\">Self-balancing algorithm by example</h3>\n\n<p>Building a B-tree can be started simply by creating a single node\nand adding new values until there is no free space in it.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-1.webp\" alt=\"Self-balancing, step 1, Add new values until there is a free space in existing nodes.\" class=\"small-image\" /></p>\n\n<p>If there is no space on the corresponding page, it needs to be split.\nTo perform a split, a ‚Äûsplit point‚Äù is chosen.\nIn that case, it will be 12, because it is in the middle.\nThe ‚ÄûSplit point‚Äù is a value that will be moved to the upper page.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2a.webp\" alt=\"Self-balancing, step 2a, Splitting the page.\" class=\"small-image\" /></p>\n\n<p>Now, it gets us to an interesting point where there is no upper page.\nIn such a case, a new one needs to be generated (and it becomes the new root page!).</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2b.webp\" alt=\"Self-balancing, step 2b, Generating a new root page.\" class=\"small-image\" /></p>\n\n<p>And finally, there is some free space in the three, so value 14 may be added.</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-2c.webp\" alt=\"Self-balancing, step 2c, Adding the 14 to B-tree.\" class=\"small-image\" /></p>\n\n<p>Following this algorithm, we may constantly add new values to the B-tree, and it will remain balanced all the time!</p>\n\n<p><img src=\"/img/articles/2023-11-27-how-does-btree-make-your-queries-fast/self-balancing-step-final.webp\" alt=\"Self-balancing, Final state of the B-tree, after adding multiple values.\" /></p>\n\n<p><em>At this point, you may have a valid concern that there is a lot of free space that has no chance to be\nfilled.\nFor example, values 14, 15, and 16, are on different pages, so these pages will remain with only one value and two free spaces forever.</em></p>\n\n<p><em>It was caused by the split location choice.\nWe always split the page in the middle.\nBut every time we do a split, we may choose any split location we want.</em></p>\n\n<p><em>Postgres has an algorithm that is run every time a split is performed!\nIts implementation may be found in the <a href=\"https://github.com/postgres/postgres/blob/54ccfd65868c013a8c6906bc894bc5ea3640740a/src/backend/access/nbtree/nbtsplitloc.c#L87\">_bt_findsplitloc() function in Postgres source code</a>.\nIts goal is to leave as little free space as possible.</em></p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>In this article, you learned how a B-tree works.\nAll in all, it may be simply described as a Binary Search Tree with two changes:</p>\n\n<ul>\n  <li>every node may contain more than one value</li>\n  <li>inserting a new value is followed by a self-balancing algorithm.</li>\n</ul>\n\n<p>Although the structures used by modern databases are usually some variants of a B-tree (like B+tree), they are still based on the original conception.\nIn my opinion, one great strength of a B-tree is the fact that it was designed directly to handle large amounts of data on actual hardware.\nIt may be the reason why the B-tree has remained with us for such a long time.</p>\n","contentSnippet":"B-tree is a structure that helps to search through great amounts of data.\nIt was invented over 40 years ago, yet it is still employed by the majority of modern databases.\nAlthough there are newer index structures, like LSM trees,\nB-tree is unbeaten when handling most of the database queries.\nAfter reading this post, you will know how B-tree organises the data and how it performs search queries.\nOrigins\nIn order to understand B-tree let‚Äôs focus on Binary Search Tree (BST) first.\nWait, isn‚Äôt it the same?\nWhat does ‚ÄúB‚Äù stand for then?\nAccording to wikipedia.org, Edward M.¬†McCreight, the inventor of B-tree, once said:\n‚Äúthe more you think about what the B in B-trees means, the better you understand B-trees.‚Äù\nConfusing B-tree with BST is a really common misconception.\nAnyway, in my opinion, BST is a great starting point for reinventing B-tree.\nLet‚Äôs start with a simple example of BST:\n\nThe greater number is always on the right, the lower on the left. It may become clearer when we add more numbers.\n\nThis tree contains seven numbers, but we need to visit at most three nodes to locate any number.\nThe following example visualizes searching for 14.\nI used SQL to define the query in order to think about this tree as if it were an actual database index.\n\nHardware\nIn theory, using Binary Search Tree for running our queries looks fine. Its time complexity (when searching) is \\(O(log\nn)\\), same as B-tree. However, in practice, this data structure needs to work on actual hardware. An index must be\nstored somewhere on your machine.\nThe computer has three places where the data can be stored:\nCPU caches\nRAM (memory)\nDisk (storage)\nThe cache is managed fully by CPUs. Moreover, it is relatively small, usually a few megabytes.\nIndex may contain gigabytes of data, so it won‚Äôt fit there.\nDatabases vastly use Memory (RAM). It has some great advantages:\nassures fast random access (more on that in the next paragraph)\nits size may be pretty big (e.g. AWS RDS cloud service provides instances\nwith a few terabytes of memory available).\nCons? You lose the data when the power supply goes off. Moreover, when compared to the disk, it is pretty expensive.\nFinally, the cons of a memory are the pros of a disk storage.\nIt‚Äôs cheap, and data will remain there even if we lose the power.\nHowever, there are no free lunches!\nThe catch is that we need to be careful about random and sequential access.\nReading from the disk is fast, but only under certain conditions!\nI‚Äôll try to explain them simply.\nRandom and sequential access\nMemory may be visualized as a line of containers for values, where every container is numbered.\n\nNow let‚Äôs assume we want to read data from containers 1, 4, and 6. It requires random access:\n\nAnd then let‚Äôs compare it with reading containers 3, 4, and 5. It may be done sequentially:\n\nThe difference between a ‚Äúrandom jump‚Äù and a ‚Äúsequential read‚Äù can be explained based on Hard Disk Drive.\nIt consists of the head and the disk.\n\n‚ÄúRandom jump‚Äù requires moving the head to the given place on the disk.\n‚ÄúSequential read‚Äù is simply spinning the disk, allowing the head to read consecutive values.\nWhen reading megabytes of data, the difference between these two types of access is enormous.\nUsing ‚Äúsequential reads‚Äù lowers the time needed to fetch the data significantly.\nDifferences in speed between random and sequential access were researched in the article ‚ÄúThe Pathologies of Big Data‚Äù\nby Adam Jacobs, published in Acm Queue.\nIt revealed a few mind-blowing facts:\nSequential access on HDD may be hundreds of thousands of times faster than random access. ü§Ø\nIt may be faster to read sequentially from the disk than randomly from the memory.\nWho even uses HDD nowadays?\nWhat about SSD?\nThis research shows that reading fully sequentially from HDD may be faster than SSD.\nHowever, please note that the article is from 2009 and SSD developed significantly through the last decade,\nthus these results are probably outdated.\nTo sum up, the key takeaway is to prefer sequential access wherever we can.\nIn the next paragraph, I will explain how to apply it to our index structure.\nOptimizing a tree for sequential access\nBinary Search Tree may be represented in memory in the same way\nas the heap:\nparent node position is \\(i\\)\nleft node position is \\(2i\\)\nright node position is \\(2i+1\\)\nThat‚Äôs how these positions are calculated based on the example (the parent node starts at 1):\n\nAccording to the calculated positions, nodes are aligned into the memory:\n\nDo you remember the query visualized a few chapters ago?\n\nThat‚Äôs what it looks like on the memory level:\n\nWhen performing the query, memory addresses 1, 3, and 6 need to be visited.\nVisiting three nodes is not a problem; however, as we store more data, the tree gets higher.\nStoring more than one million values requires a tree of height at least 20. It means\nthat 20 values from different places in memory must be read.\nIt causes completely random access!\nPages\nWhile a tree grows in height, random access is causing more and more delay.\nThe solution to reduce this problem is simple: grow the tree in width rather than in height.\nIt may be achieved by packing more than one value into a single node.\n\nIt brings us the following benefits:\nthe tree is shallower (two levels instead of three)\nit still has a lot of space for new values without the need for growing further\nThe query performed on such index looks as follows:\n\nPlease note that every time we visit a node, we need to load all its values.\nIn this example, we need to load 4 values (or 6 if the tree is full) in order to reach the one we are looking for.\nBelow, you will find a visualization of this tree in a memory:\n\nCompared to the previous example (where the tree grows in height),\nthis search should be faster.\nWe need random access only twice (jump to cells 0 and 9) and then sequentially read the rest of values.\nThis solution works better and better as our database grows. If you want to store one million values, then you need:\nBinary Search Tree which has 20 levels\nOR\n3-value node Tree which has 10 levels\nValues from a single node make a page.\nIn the example above, each page consists of three values.\nA page is a set of values placed on a disk next to each other,\nso the database may reach the whole page at once with one sequential read.\nAnd how does it refer to the reality?\nPostgres page size is 8kB.\nLet‚Äôs assume that 20% is for metadata, so it‚Äôs 6kB left.\nHalf of the page is needed to store\npointers to node‚Äôs children, so it gives us 3kB for values.\nBIGINT size is 8 bytes, thus we may store ~375 values in a\nsingle page.\nAssuming that some pretty big tables in a database have one billion rows,\nhow many levels in the Postgres tree do we need to store them?\nAccording to the calculations above,\nif we create a tree that can handle 375 values in a single node,\nit may store 1 billion values with a tree that has only four levels.\nBinary Search Tree would require 30 levels for such amount of data.\nTo sum up, placing multiple values in a single node of the tree helped us to reduce its height, thus using the benefits of sequential access.\nMoreover, a B-tree may grow not only in height, but also in width (by using larger pages).\nBalancing\nThere are two types of operations in databases: writing and reading.\nIn the previous section, we addressed the problems with reading the data from the B-tree.\nNonetheless, writing is also a crucial part.\nWhen writing the data to a database, B-tree needs to be constantly updated with new values.\nThe tree shape depends on the order of values added to the tree.\nIt‚Äôs easily visible in a binary tree.\nWe may obtain trees with different depths if the values are added in an incorrect order.\n\nWhen the tree has different depths on different nodes, it is called an unbalanced tree.\nThere are basically two ways of returning such a tree to a balanced state:\nRebuilding it from the very beginning just by adding the values in the correct order.\nKeeping it balanced all the time, as the new values are added.\nB-tree implements the second option. A feature that makes the tree balanced all the time is called self-balancing.\nSelf-balancing algorithm by example\nBuilding a B-tree can be started simply by creating a single node\nand adding new values until there is no free space in it.\n\nIf there is no space on the corresponding page, it needs to be split.\nTo perform a split, a ‚Äûsplit point‚Äù is chosen.\nIn that case, it will be 12, because it is in the middle.\nThe ‚ÄûSplit point‚Äù is a value that will be moved to the upper page.\n\nNow, it gets us to an interesting point where there is no upper page.\nIn such a case, a new one needs to be generated (and it becomes the new root page!).\n\nAnd finally, there is some free space in the three, so value 14 may be added.\n\nFollowing this algorithm, we may constantly add new values to the B-tree, and it will remain balanced all the time!\n\nAt this point, you may have a valid concern that there is a lot of free space that has no chance to be\nfilled.\nFor example, values 14, 15, and 16, are on different pages, so these pages will remain with only one value and two free spaces forever.\nIt was caused by the split location choice.\nWe always split the page in the middle.\nBut every time we do a split, we may choose any split location we want.\nPostgres has an algorithm that is run every time a split is performed!\nIts implementation may be found in the _bt_findsplitloc() function in Postgres source code.\nIts goal is to leave as little free space as possible.\nSummary\nIn this article, you learned how a B-tree works.\nAll in all, it may be simply described as a Binary Search Tree with two changes:\nevery node may contain more than one value\ninserting a new value is followed by a self-balancing algorithm.\nAlthough the structures used by modern databases are usually some variants of a B-tree (like B+tree), they are still based on the original conception.\nIn my opinion, one great strength of a B-tree is the fact that it was designed directly to handle large amounts of data on actual hardware.\nIt may be the reason why the B-tree has remained with us for such a long time.","guid":"https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html","categories":["tech"],"isoDate":"2023-11-26T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Beyond the Code - An Engineer‚Äôs Battle Against Knowledge Loss","link":"https://blog.allegro.tech/2023/10/battle-against-knowledge-loss.html","pubDate":"Mon, 30 Oct 2023 00:00:00 +0100","authors":{"author":[{"name":["Krzysztof Przychodzki"],"photo":["https://blog.allegro.tech/img/authors/krzysztof.przychodzki.jpg"],"url":["https://blog.allegro.tech/authors/krzysztof.przychodzki"]}]},"content":"<p>The idea for this article arose during a meeting where we learned that our supervisor would be leaving the company to pursue new opportunities. In response, a\ncolleague lamented that what we would miss most is the knowledge departing with the leader. Unfortunately, that‚Äôs how it goes. Not only do we lose a colleague,\nbut we also lose valuable knowledge and experience. However, this isn‚Äôt a story about my supervisor; it‚Äôs a story about all those individuals who are experts in\ntheir fields, who understand the paths to success and paths that lead to catastrophic failures. When they leave, they take with them knowledge that you won‚Äôt\nfind in any book, note, or Jira ticket. And this leads to a fundamental question: <em>What can be done to avoid this ‚Äúblack hole‚Äù of knowledge? How can we ensure\nit doesn‚Äôt vanish along with them?</em> That‚Äôs what this article is all about.</p>\n\n<h2 id=\"business-decisions-somebody-made-and-didnt-tell-you\">Business Decisions Somebody Made‚Ä¶ and didn‚Äôt tell you</h2>\n\n<p>Specifically for this article I created the term <em>Biological Data Storage</em> or <em>BDS</em> for short. This term encompasses nearly every employee in a company. I\nunderstand that nobody wants to be seen as just a resource, and certainly not as part of the <em>Biological Data Storage</em>. However, in the context of a company‚Äôs\nresources, an employee can be likened to a technical data repository, but with the valuable addition of context.</p>\n\n<p>I wanted to examine this issue more broadly from an engineer‚Äôs perspective. We often hear about Conway‚Äôs Law:</p>\n\n<blockquote>\n  <p>Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization‚Äôs communication structure. <sup id=\"fnref:1\" role=\"doc-noteref\"><a href=\"#fn:1\" class=\"footnote\" rel=\"footnote\">1</a></sup></p>\n</blockquote>\n\n<p>And I perceive the loss of knowledge as a depletion of communication, which can ultimately result in its imperfections within the created system.</p>\n\n<p>The engineering approach is marked by our commitment to gauging the impact of various events and assessing their real significance using specific metrics. When\ndealing with the challenge of an employee departing, consider these metrics to evaluate organizational effectiveness:</p>\n\n<ol>\n  <li>Time to Problem Resolution:\n    <ul>\n      <li>measures how quickly issues or challenges are resolved and helps identify the efficiency of problem-solving processes.</li>\n    </ul>\n  </li>\n  <li>Knowledge Transfer Rate:\n    <ul>\n      <li>measures how long it takes for a new employee to become self-sufficient and also indicates the effectiveness of knowledge transfer and onboarding.</li>\n    </ul>\n  </li>\n</ol>\n\n<p>I think these metrics provide valuable insights into organizational efficiency and its capacity to seamlessly integrate new team members.\nIn the context of Conway‚Äôs Law, the loss of knowledge becomes a critical factor influencing not only communication but also the very design of systems within\nthe company.</p>\n\n<p>Consider this: when a team member with a wealth of knowledge and expertise departs, they take with them not just facts and figures but also their unique\ninsights, problem-solving approaches, and understanding of the organization‚Äôs intricacies. The lack of such knowledge can disrupt the flow of information within\nteams and across departments. As a result, the communication structure can falter, hindering the organization‚Äôs ability to respond to challenges effectively.</p>\n\n<p>Moreover, the design of systems can be profoundly impacted. Engineers and developers who were privy to invaluable knowledge may have made design choices based\non their expertise. These decisions may not have been documented or clearly understood by others, and when their authors leave, may become opaque. This can lead\nto difficulties in maintaining and developing these systems, potentially causing inefficiencies and vulnerabilities.</p>\n\n<p>Now, when we introduce the <em>Knowledge Transfer Rate</em> metric into this context, it becomes evident that measuring how long it takes for a new employee to become\nself-sufficient is crucial. The longer this duration, the more pronounced the knowledge gap becomes, affecting both communication and system design.\nOrganizations must recognize that knowledge isn‚Äôt just about data; it‚Äôs about understanding and context, and its loss can significantly impede the smooth\nfunctioning of teams and the evolution of systems.</p>\n\n<h2 id=\"organizations-have-no-memory\">Organizations have no memory</h2>\n\n<p>You might ask, ‚ÄúWhat‚Äôs the impact of losing this knowledge on a company?‚Äù Is it that business processes start collapsing like houses of cards? Innovation loses\nits wings? The company‚Äôs efficiency plummets like leaves in an autumn storm?\nThe answer to the above questions in 98% of cases is - of course, no - because we can manage such risks. Companies have ways of dealing with them, but do they,\nreally?</p>\n\n<p><em>Organizations have no memory</em> is a quote from Trevor Kletz‚Äôs book <em>Lessons from Disaster</em>, which highlights the concept of organizational memory and how\nincidents and accidents can recur due to the lack of effective learning from past mistakes within an organization. Prof. Kletz highlights the organization‚Äôs\ninability to learn from accidents, even those occurring within the company. I sometimes feel that a similar pattern emerges when knowledge departs from our\ncompany. Perhaps because it can‚Äôt be easily measured in money, it‚Äôs often downplayed.</p>\n\n<p>While Kletz‚Äôs book pertains to chemical engineering, I see several universal truths that apply to any situation and industry. For example, another quote, ‚ÄúWhat\nyou don‚Äôt have, can‚Äôt leak‚Äù is remarkably similar to the idea that code you don‚Äôt have is <em>maintainless</em> and won‚Äôt have bugs. There are likely analogous\nprinciples in our field.</p>\n\n<p>However, even at this stage, the process of knowledge acquisition can be accelerated. There are several ways to do it, such as creating procedures, diagrams,\ncharts, and documentation.</p>\n\n<p>Documentation is like treasure maps in the business world. Creating documentation is one thing, but keeping it up-to-date within an organization (regardless of\nits size) is a challenge. Encouraging the team to regularly update documentation is also a challenge. Even the best-prepared documentation often lacks many\ndetails, like the rationale behind specific business decisions, why a particular database or framework was chosen, or why we use technology <em>Y</em> instead of the\nmore prevalent <em>X</em> throughout the company.</p>\n\n<p>So, while documentation is like treasure maps for your company, recorded, organized, and structured information about processes, systems, and practices within\nthe company are akin to Architecture Decision Records (ADRs). ADRs are like the flight recorders of our business. They contain records of critical decisions\nmade during system design or significant technological choices.</p>\n\n<p>Why is this important? When creating new things, we make numerous decisions that may appear irrational without the right context later on. ADRs are like opening\na box that explains why these decisions were made. It‚Äôs the key to understanding the company‚Äôs history and evolution. In the context of our <em>BDS</em>, ADRs are\nlike recordings of experts‚Äô thoughts when making key decisions. When these experts leave, these recordings become a treasure trove of knowledge, helping us\navoid repeating the same mistakes.</p>\n\n<p>A common scenario emerges: the team tasked with addressing the problem must invest valuable time in rediscovering solutions, experimenting with potential fixes,\nor even resorting to trial and error. This not only prolongs the problem-solving process but can also result in suboptimal resolutions, increased frustration,\nand a negative impact on overall productivity. Thanks to documentation and ADR we can significantly reduce this time.</p>\n\n<h2 id=\"an-alternative-to-lengthy-documentation\">An alternative to lengthy documentation?</h2>\n\n<blockquote>\n  <p>No one reads.\nIf someone does read, he doesn‚Äôt understand.\nIf he understands, he immediately forgets.</p>\n</blockquote>\n\n<p>Unfortunately, just as is the case in the above quote from Stanis≈Çaw Lem <sup id=\"fnref:2\" role=\"doc-noteref\"><a href=\"#fn:2\" class=\"footnote\" rel=\"footnote\">2</a></sup>, the problem with documentation, procedures, and ADRs is that people need to\nfamiliarize themselves with them. I suppose that even at SpaceX, it‚Äôs doubtful this would be considered the most thrilling reading material, or maybe I am just\nmistaken. Anyway, even if someone manages to get through the documentation, they‚Äôll only retain what they understand. We‚Äôre presented with the work of others,\nwith their imposed ways of thinking and decision-making. Often, questions arise to which no one knows the answers, and the people who do know are no longer with\nthe company.</p>\n\n<p>Since we now know our mental limitations, instead of forcing people to sift through stacks of documentation, we can\nuse <a href=\"https://blog.allegro.tech/2022/07/event-storming-workshops.html\">EventStorming</a>. This technique helps understand business processes, identify events and\nactivities, and integrate knowledge in an understandable way. We focus on behaviors, on what changes and why. Together, we develop a solution and understand the\nprocesses because we see them from start to finish. Understanding a process through EventStorming is faster and easier than reading documentation. During an\nEventStorming session, most questions find answers, and knowledge can be conveyed to many people simultaneously, whether they are technical or not. The most\nsignificant artifact of such sessions is that you can discuss why the process looks the way it does, why a specific sequence was chosen, and not another ‚Äî\nessentially, a mega-mix of documentation, ADR, and conversation. I emphasize once more that this understanding of the process is developed collectively ‚Äî\neveryone feels as a part of the solution. In the case of our <em>BDS</em>, EventStorming is like capturing the thoughts of experts when making crucial decisions.</p>\n\n<h3 id=\"real-life-example\">Real life example</h3>\n\n<p>At <a href=\"https://allegro.tech/\">Allegro</a>, we recently had a situation where the entire development team responsible for a critical service was moved to a different\nproject. The new team, which inherited the service, had the opportunity to collaborate with the departing team for a period. However, in this context, we also\nconducted EventStorming sessions. To provide more detail, these sessions extended over two full days, each lasting 8 hours. The knowledge accumulated over the\npast five years was not merely confined to two plotter paper-sized sheets, each stretching 6 meters in length, but was primarily assimilated within the\nparticipants‚Äô minds in a seamless manner. I believe this facilitated the new team in gaining greater confidence when taking over the domain.</p>\n\n<p>Interestingly, you don‚Äôt need to spend a lot of time on EventStorming to uncover enough business knowledge. In the case mentioned earlier, the session lasted\ntwo days, but it involved an entire team. For an individual, a two-hour workshop can be enough to see the big picture of our process. Although EventStorming\nallows us to absorb a dose of knowledge relatively easily to know <em>what and why</em> is changing in our process, the devil is in the detail. To really understand\n<em>how</em> this process is changing, it‚Äôs best to start by doing small tasks under the guidance of an experienced person.</p>\n\n<h2 id=\"seeking-uml-like-alternatives\">Seeking UML-like Alternatives?</h2>\n\n<p>Unfortunately, EventStorming is not the answer to all knowledge loss-related problems. While I don‚Äôt question how fantastic this tool is, the knowledge acquired\nthrough it will remain only in the participants‚Äô minds. If it‚Äôs not somehow preserved in the form of documentation or ADRs, it may turn out to be just as\nfleeting as departing employees. What can be done about this? Our initial thoughts may lead us to create some form of description or documentation, which, as we\nknow, comes with the challenge of its preparation and the cognitive overload for someone trying to assimilate new knowledge.</p>\n\n<p>It seems that when dealing with the issue of knowledge loss and its effective transfer, it‚Äôs worth mentioning tools like BPMN, which stands for Business Process\nModel and Notation. BPMN provides a standardized graphical representation of business processes. By using BPMN diagrams, we can visually map\nworkflows and procedures. Such an approach not only simplifies the understanding of complex processes but also aids in comprehensive documentation. When\ncombined with other knowledge-sharing techniques, such as EventStorming, BPMN can be a powerful asset in preserving and transferring critical business\nknowledge.</p>\n\n<p>However, BPMN has an elaborate set of symbols and notation rules, which can make creating and interpreting diagrams complicated for some individuals. Creating\nadvanced BPMN diagrams and fully utilizing the notation‚Äôs potential requires specialized knowledge and experience. People unfamiliar with BPMN may struggle\nto use it effectively. Despite these inconveniences, BPMN still remains a valuable tool for modeling and documenting business processes in many organizations. I\nbelieve it complements the previously mentioned techniques perfectly.</p>\n\n<p>Just remember to have the right tools in your arsenal and, more importantly, to choose the appropriate tool for the situation, considering both its strengths\nand weaknesses.</p>\n\n<h2 id=\"one-more-thing\">One more thing‚Ä¶</h2>\n\n<p><em>Time to Problem Resolution</em> metric serves as a clear indicator of an organization‚Äôs efficiency in addressing challenges. A shorter time to resolution signifies\nthat issues are tackled swiftly, minimizing disruptions and ensuring that the organization operates smoothly.\n<em>Knowledge Transfer Rate</em> metric is a means to quantify and address the loss of knowledge, shedding light on its impact on communication structures and system\ndesign within an organization.</p>\n\n<p>Both metrics are directly influenced by the use of appropriate tools such as documentation, ADRs, EventStorming or BPMN. I have tried to highlight their\nadvantages and disadvantages in the context of knowledge transfer.</p>\n\n<p>However, there is another challenge - changing the company‚Äôs culture. Employees must know what tools they have and feel that sharing knowledge is key to\nsuccess. Leadership plays a crucial role here, as leaders need to actively promote and engage in knowledge sharing and open communication. If company leaders\nactively endorse and engage in knowledge sharing, other employees are more likely to follow suit. However, changing organizational culture is a time-consuming\nprocess. Patience and perseverance are essential until new behaviors and beliefs prevail over old ones.</p>\n\n<h2 id=\"this-can-be-done\">This can be done</h2>\n\n<p>As an engineer in an organisation, regardless of size, there are several proactive steps you can take to facilitate knowledge transfer. First and foremost,\nactively engage in open communication with your colleagues. Encourage discussion and information sharing, especially within your area of expertise, to ensure\nthat valuable insights are shared. Second, mentorship can be a powerful tool. Offer to mentor junior team members or be open to seeking guidance from more\nexperienced colleagues. In addition, participate in knowledge-sharing initiatives within the company, such as brown bag sessions, workshops or cross-functional\nprojects. Finally, consider creating or contributing to internal documentation and repositories. These resources can serve as valuable references for your\ncolleagues and future team members, ensuring that knowledge is retained within the organisation. By actively participating in these practices, you can play a\nkey role in preserving and transferring critical knowledge within your organisation.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>In this article, I aimed to discuss how knowledge loss in a company appears through an engineer‚Äôs eyes and why it can pose a threat. The term <em>Biological Data\nStorage</em> may sound unconventional, but it emphasises the critical role that every team member plays in preserving and transferring knowledge. It‚Äôs important to\nremember that employees are not just resources; they are the living repositories of valuable information, experience and expertise. In the world of <em>BDS</em>, every\nmember contributes to the collective body of knowledge, shaping the organisation‚Äôs communication structure.\nAs we say goodbye to departing colleagues, let‚Äôs also say goodbye to the notion that knowledge should be confined to individual minds. Instead, let‚Äôs adopt a\nculture of open communication, active knowledge sharing and the right tools, such as EventStorming and BPMN, to capture, preserve and share critical knowledge\nacross our organisation.</p>\n\n<h3 id=\"footnotes\">Footnotes</h3>\n\n<div class=\"footnotes\" role=\"doc-endnotes\">\n  <ol>\n    <li id=\"fn:1\" role=\"doc-endnote\">\n      <p>Quote from https://en.wikipedia.org/wiki/Conway%27s_law¬†<a href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n    <li id=\"fn:2\" role=\"doc-endnote\">\n      <p>https://en.wikipedia.org/wiki/Stanis%C5%82aw_Lem¬†<a href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\">&#8617;</a></p>\n    </li>\n  </ol>\n</div>\n","contentSnippet":"The idea for this article arose during a meeting where we learned that our supervisor would be leaving the company to pursue new opportunities. In response, a\ncolleague lamented that what we would miss most is the knowledge departing with the leader. Unfortunately, that‚Äôs how it goes. Not only do we lose a colleague,\nbut we also lose valuable knowledge and experience. However, this isn‚Äôt a story about my supervisor; it‚Äôs a story about all those individuals who are experts in\ntheir fields, who understand the paths to success and paths that lead to catastrophic failures. When they leave, they take with them knowledge that you won‚Äôt\nfind in any book, note, or Jira ticket. And this leads to a fundamental question: What can be done to avoid this ‚Äúblack hole‚Äù of knowledge? How can we ensure\nit doesn‚Äôt vanish along with them? That‚Äôs what this article is all about.\nBusiness Decisions Somebody Made‚Ä¶ and didn‚Äôt tell you\nSpecifically for this article I created the term Biological Data Storage or BDS for short. This term encompasses nearly every employee in a company. I\nunderstand that nobody wants to be seen as just a resource, and certainly not as part of the Biological Data Storage. However, in the context of a company‚Äôs\nresources, an employee can be likened to a technical data repository, but with the valuable addition of context.\nI wanted to examine this issue more broadly from an engineer‚Äôs perspective. We often hear about Conway‚Äôs Law:\nAny organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization‚Äôs communication structure. 1\nAnd I perceive the loss of knowledge as a depletion of communication, which can ultimately result in its imperfections within the created system.\nThe engineering approach is marked by our commitment to gauging the impact of various events and assessing their real significance using specific metrics. When\ndealing with the challenge of an employee departing, consider these metrics to evaluate organizational effectiveness:\nTime to Problem Resolution:\n    \nmeasures how quickly issues or challenges are resolved and helps identify the efficiency of problem-solving processes.\nKnowledge Transfer Rate:\n    \nmeasures how long it takes for a new employee to become self-sufficient and also indicates the effectiveness of knowledge transfer and onboarding.\nI think these metrics provide valuable insights into organizational efficiency and its capacity to seamlessly integrate new team members.\nIn the context of Conway‚Äôs Law, the loss of knowledge becomes a critical factor influencing not only communication but also the very design of systems within\nthe company.\nConsider this: when a team member with a wealth of knowledge and expertise departs, they take with them not just facts and figures but also their unique\ninsights, problem-solving approaches, and understanding of the organization‚Äôs intricacies. The lack of such knowledge can disrupt the flow of information within\nteams and across departments. As a result, the communication structure can falter, hindering the organization‚Äôs ability to respond to challenges effectively.\nMoreover, the design of systems can be profoundly impacted. Engineers and developers who were privy to invaluable knowledge may have made design choices based\non their expertise. These decisions may not have been documented or clearly understood by others, and when their authors leave, may become opaque. This can lead\nto difficulties in maintaining and developing these systems, potentially causing inefficiencies and vulnerabilities.\nNow, when we introduce the Knowledge Transfer Rate metric into this context, it becomes evident that measuring how long it takes for a new employee to become\nself-sufficient is crucial. The longer this duration, the more pronounced the knowledge gap becomes, affecting both communication and system design.\nOrganizations must recognize that knowledge isn‚Äôt just about data; it‚Äôs about understanding and context, and its loss can significantly impede the smooth\nfunctioning of teams and the evolution of systems.\nOrganizations have no memory\nYou might ask, ‚ÄúWhat‚Äôs the impact of losing this knowledge on a company?‚Äù Is it that business processes start collapsing like houses of cards? Innovation loses\nits wings? The company‚Äôs efficiency plummets like leaves in an autumn storm?\nThe answer to the above questions in 98% of cases is - of course, no - because we can manage such risks. Companies have ways of dealing with them, but do they,\nreally?\nOrganizations have no memory is a quote from Trevor Kletz‚Äôs book Lessons from Disaster, which highlights the concept of organizational memory and how\nincidents and accidents can recur due to the lack of effective learning from past mistakes within an organization. Prof. Kletz highlights the organization‚Äôs\ninability to learn from accidents, even those occurring within the company. I sometimes feel that a similar pattern emerges when knowledge departs from our\ncompany. Perhaps because it can‚Äôt be easily measured in money, it‚Äôs often downplayed.\nWhile Kletz‚Äôs book pertains to chemical engineering, I see several universal truths that apply to any situation and industry. For example, another quote, ‚ÄúWhat\nyou don‚Äôt have, can‚Äôt leak‚Äù is remarkably similar to the idea that code you don‚Äôt have is maintainless and won‚Äôt have bugs. There are likely analogous\nprinciples in our field.\nHowever, even at this stage, the process of knowledge acquisition can be accelerated. There are several ways to do it, such as creating procedures, diagrams,\ncharts, and documentation.\nDocumentation is like treasure maps in the business world. Creating documentation is one thing, but keeping it up-to-date within an organization (regardless of\nits size) is a challenge. Encouraging the team to regularly update documentation is also a challenge. Even the best-prepared documentation often lacks many\ndetails, like the rationale behind specific business decisions, why a particular database or framework was chosen, or why we use technology Y instead of the\nmore prevalent X throughout the company.\nSo, while documentation is like treasure maps for your company, recorded, organized, and structured information about processes, systems, and practices within\nthe company are akin to Architecture Decision Records (ADRs). ADRs are like the flight recorders of our business. They contain records of critical decisions\nmade during system design or significant technological choices.\nWhy is this important? When creating new things, we make numerous decisions that may appear irrational without the right context later on. ADRs are like opening\na box that explains why these decisions were made. It‚Äôs the key to understanding the company‚Äôs history and evolution. In the context of our BDS, ADRs are\nlike recordings of experts‚Äô thoughts when making key decisions. When these experts leave, these recordings become a treasure trove of knowledge, helping us\navoid repeating the same mistakes.\nA common scenario emerges: the team tasked with addressing the problem must invest valuable time in rediscovering solutions, experimenting with potential fixes,\nor even resorting to trial and error. This not only prolongs the problem-solving process but can also result in suboptimal resolutions, increased frustration,\nand a negative impact on overall productivity. Thanks to documentation and ADR we can significantly reduce this time.\nAn alternative to lengthy documentation?\nNo one reads.\nIf someone does read, he doesn‚Äôt understand.\nIf he understands, he immediately forgets.\nUnfortunately, just as is the case in the above quote from Stanis≈Çaw Lem 2, the problem with documentation, procedures, and ADRs is that people need to\nfamiliarize themselves with them. I suppose that even at SpaceX, it‚Äôs doubtful this would be considered the most thrilling reading material, or maybe I am just\nmistaken. Anyway, even if someone manages to get through the documentation, they‚Äôll only retain what they understand. We‚Äôre presented with the work of others,\nwith their imposed ways of thinking and decision-making. Often, questions arise to which no one knows the answers, and the people who do know are no longer with\nthe company.\nSince we now know our mental limitations, instead of forcing people to sift through stacks of documentation, we can\nuse EventStorming. This technique helps understand business processes, identify events and\nactivities, and integrate knowledge in an understandable way. We focus on behaviors, on what changes and why. Together, we develop a solution and understand the\nprocesses because we see them from start to finish. Understanding a process through EventStorming is faster and easier than reading documentation. During an\nEventStorming session, most questions find answers, and knowledge can be conveyed to many people simultaneously, whether they are technical or not. The most\nsignificant artifact of such sessions is that you can discuss why the process looks the way it does, why a specific sequence was chosen, and not another ‚Äî\nessentially, a mega-mix of documentation, ADR, and conversation. I emphasize once more that this understanding of the process is developed collectively ‚Äî\neveryone feels as a part of the solution. In the case of our BDS, EventStorming is like capturing the thoughts of experts when making crucial decisions.\nReal life example\nAt Allegro, we recently had a situation where the entire development team responsible for a critical service was moved to a different\nproject. The new team, which inherited the service, had the opportunity to collaborate with the departing team for a period. However, in this context, we also\nconducted EventStorming sessions. To provide more detail, these sessions extended over two full days, each lasting 8 hours. The knowledge accumulated over the\npast five years was not merely confined to two plotter paper-sized sheets, each stretching 6 meters in length, but was primarily assimilated within the\nparticipants‚Äô minds in a seamless manner. I believe this facilitated the new team in gaining greater confidence when taking over the domain.\nInterestingly, you don‚Äôt need to spend a lot of time on EventStorming to uncover enough business knowledge. In the case mentioned earlier, the session lasted\ntwo days, but it involved an entire team. For an individual, a two-hour workshop can be enough to see the big picture of our process. Although EventStorming\nallows us to absorb a dose of knowledge relatively easily to know what and why is changing in our process, the devil is in the detail. To really understand\nhow this process is changing, it‚Äôs best to start by doing small tasks under the guidance of an experienced person.\nSeeking UML-like Alternatives?\nUnfortunately, EventStorming is not the answer to all knowledge loss-related problems. While I don‚Äôt question how fantastic this tool is, the knowledge acquired\nthrough it will remain only in the participants‚Äô minds. If it‚Äôs not somehow preserved in the form of documentation or ADRs, it may turn out to be just as\nfleeting as departing employees. What can be done about this? Our initial thoughts may lead us to create some form of description or documentation, which, as we\nknow, comes with the challenge of its preparation and the cognitive overload for someone trying to assimilate new knowledge.\nIt seems that when dealing with the issue of knowledge loss and its effective transfer, it‚Äôs worth mentioning tools like BPMN, which stands for Business Process\nModel and Notation. BPMN provides a standardized graphical representation of business processes. By using BPMN diagrams, we can visually map\nworkflows and procedures. Such an approach not only simplifies the understanding of complex processes but also aids in comprehensive documentation. When\ncombined with other knowledge-sharing techniques, such as EventStorming, BPMN can be a powerful asset in preserving and transferring critical business\nknowledge.\nHowever, BPMN has an elaborate set of symbols and notation rules, which can make creating and interpreting diagrams complicated for some individuals. Creating\nadvanced BPMN diagrams and fully utilizing the notation‚Äôs potential requires specialized knowledge and experience. People unfamiliar with BPMN may struggle\nto use it effectively. Despite these inconveniences, BPMN still remains a valuable tool for modeling and documenting business processes in many organizations. I\nbelieve it complements the previously mentioned techniques perfectly.\nJust remember to have the right tools in your arsenal and, more importantly, to choose the appropriate tool for the situation, considering both its strengths\nand weaknesses.\nOne more thing‚Ä¶\nTime to Problem Resolution metric serves as a clear indicator of an organization‚Äôs efficiency in addressing challenges. A shorter time to resolution signifies\nthat issues are tackled swiftly, minimizing disruptions and ensuring that the organization operates smoothly.\nKnowledge Transfer Rate metric is a means to quantify and address the loss of knowledge, shedding light on its impact on communication structures and system\ndesign within an organization.\nBoth metrics are directly influenced by the use of appropriate tools such as documentation, ADRs, EventStorming or BPMN. I have tried to highlight their\nadvantages and disadvantages in the context of knowledge transfer.\nHowever, there is another challenge - changing the company‚Äôs culture. Employees must know what tools they have and feel that sharing knowledge is key to\nsuccess. Leadership plays a crucial role here, as leaders need to actively promote and engage in knowledge sharing and open communication. If company leaders\nactively endorse and engage in knowledge sharing, other employees are more likely to follow suit. However, changing organizational culture is a time-consuming\nprocess. Patience and perseverance are essential until new behaviors and beliefs prevail over old ones.\nThis can be done\nAs an engineer in an organisation, regardless of size, there are several proactive steps you can take to facilitate knowledge transfer. First and foremost,\nactively engage in open communication with your colleagues. Encourage discussion and information sharing, especially within your area of expertise, to ensure\nthat valuable insights are shared. Second, mentorship can be a powerful tool. Offer to mentor junior team members or be open to seeking guidance from more\nexperienced colleagues. In addition, participate in knowledge-sharing initiatives within the company, such as brown bag sessions, workshops or cross-functional\nprojects. Finally, consider creating or contributing to internal documentation and repositories. These resources can serve as valuable references for your\ncolleagues and future team members, ensuring that knowledge is retained within the organisation. By actively participating in these practices, you can play a\nkey role in preserving and transferring critical knowledge within your organisation.\nSummary\nIn this article, I aimed to discuss how knowledge loss in a company appears through an engineer‚Äôs eyes and why it can pose a threat. The term Biological Data\nStorage may sound unconventional, but it emphasises the critical role that every team member plays in preserving and transferring knowledge. It‚Äôs important to\nremember that employees are not just resources; they are the living repositories of valuable information, experience and expertise. In the world of BDS, every\nmember contributes to the collective body of knowledge, shaping the organisation‚Äôs communication structure.\nAs we say goodbye to departing colleagues, let‚Äôs also say goodbye to the notion that knowledge should be confined to individual minds. Instead, let‚Äôs adopt a\nculture of open communication, active knowledge sharing and the right tools, such as EventStorming and BPMN, to capture, preserve and share critical knowledge\nacross our organisation.\nFootnotes\nQuote from https://en.wikipedia.org/wiki/Conway%27s_law¬†‚Ü©\nhttps://en.wikipedia.org/wiki/Stanis%C5%82aw_Lem¬†‚Ü©","guid":"https://blog.allegro.tech/2023/10/battle-against-knowledge-loss.html","categories":["eventstorming","knowledge-preservation","tech","communication"],"isoDate":"2023-10-29T23:00:00.000Z","thumbnail":"images/post-headers/eventstorming.png"},{"title":"Online MongoDB migration","link":"https://blog.allegro.tech/2023/09/online-mongodb-migration.html","pubDate":"Thu, 14 Sep 2023 00:00:00 +0200","authors":{"author":[{"name":["Szymon Marcinkiewicz"],"photo":["https://blog.allegro.tech/img/authors/szymon.marcinkiewicz.jpg"],"url":["https://blog.allegro.tech/authors/szymon.marcinkiewicz"]}]},"content":"<p>MongoDB is the most popular database used at <a href=\"https://allegro.tech\">Allegro</a>. We have hundreds of MongoDB databases running on our on‚Äîpremise servers.\nIn 2022 we decided that we need to migrate all our MongoDB databases\nfrom existing shared clusters to new MongoDB clusters hosted on Kubernetes pods with separated resources.\nTo perform the migration of all databases we needed a tool for transfering all the data and keeping consistency between old and new databases.\nThat‚Äôs how <em>mongo-migration-stream</em> project was born.</p>\n\n<h2 id=\"why-do-we-needed-to-migrate-mongodb-databases-at-all\">Why do we needed to migrate MongoDB databases at all?</h2>\n\n<p>At Allegro we are managing tens of MongoDB clusters, with hundreds of MongoDB databases running on them.\nThis kind of approach, where one MongoDB cluster runs multiple MongoDB databases, allowed us to utilize resources\nmore effectively, while at the same time easing maintenance of clusters.</p>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/one_cluster_multiple_databases.png\" alt=\"Old approach\" /></p>\n\n<p>We‚Äôve been living with this approach for years, but over time, more and more databases were\ncreated on shared clusters, increasing the frequency of the noisy neighbour problem.</p>\n\n<h3 id=\"noisy-neighbour-problem\">Noisy neighbour problem</h3>\n\n<p>Generally speaking, a noisy neighbour situation appears while multiple applications run on shared infrastructure,\nand one of those applications starts to consume so many resources (like CPU, RAM or Storage),\nthat it causes starvation of other applications.</p>\n\n<p>At Allegro this problem started to be visible because over the years we‚Äôve created more and more new MongoDB databases\nwhich were hosted on a fixed number of clusters.</p>\n\n<p>The most common cause of the noisy neighbour problem in the Allegro infrastructure was long time high CPU usage caused by one of MongoDB databases on a given cluster.\nOn various occasions it occurred that a non-optimal query performed on a large collection was consuming too much CPU,\nnegatively affecting all the other databases on that cluster, making them slower or completely unresponsive.</p>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/cluster_cpu.png\" alt=\"Cluster CPU usage\" /></p>\n\n<h3 id=\"mongodb-on-kubernetes-as-a-solution-to-the-noisy-neighbour-problem\">MongoDB on Kubernetes as a solution to the noisy neighbour problem</h3>\n\n<p>To solve the noisy neighbour problem a separate team implemented a solution allowing Allegro engineers to create independent MongoDB clusters on Kubernetes.\nFrom now on, each MongoDB cluster is formed of multiple replicas and an arbiter spread among datacenters, serving only a single MongoDB database.\nRunning each database on a separate cluster with isolated resources managed by Kubernetes was our solution to the noisy neighbour problem.</p>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/k8s_cpu.png\" alt=\"Kubernetes CPU usage\" /></p>\n\n<p>At this point we knew what we needed to do to solve our problems ‚Äî we had to migrate all MongoDB databases from old shared clusters\nto new independent clusters on Kubernetes. Now came the time to answer the question: <em>How should we do it?</em></p>\n\n<h2 id=\"available-options\">Available options</h2>\n\n<p>Firstly, we prepared a list of requirements which a tool for migrating databases (referred to as <em>migrator</em>) had to meet in order to perform\nsuccessful migrations.</p>\n\n<h3 id=\"requirements\">Requirements</h3>\n\n<ul>\n  <li>Migrator must be able to migrate databases from older MongoDB versions to newer ones,</li>\n  <li>Migrator must be able to migrate <code class=\"language-plaintext highlighter-rouge\">ReplicaSets</code> and sharded clusters,</li>\n  <li>Migrator must copy indexes from source database to destination database,</li>\n  <li>Migrator must be able to handle more than 10k write operations per second,</li>\n  <li>Migration must be performed without any downtime,</li>\n  <li>Migration cannot affect database clients,</li>\n  <li>Database owners (software engineers) need to be able to perform migrations on their own.</li>\n</ul>\n\n<h3 id=\"existing-solutions\">Existing solutions</h3>\n\n<p>Having defined a list of requirements, we checked what tools were available on the market at the time.</p>\n\n<h4 id=\"py-mongo-sync\"><a href=\"https://github.com/caosiyang/py-mongo-sync\">py-mongo-sync</a></h4>\n\n<p>According to documentation <em>py-mongo-sync</em> is:</p>\n\n<blockquote>\n  <p>‚ÄúOplog‚Äîbased data sync tool that synchronizes data from a replica set to another deployment,\ne.g.: standalone, replica set, and sharded cluster.‚Äù</p>\n</blockquote>\n\n<p>As you can see, <em>py-mongo-sync</em> is not a tool that would suit our needs from end to end.\n<em>py-mongo-sync</em> focuses on the synchronization of the data stored on the <em>source database</em> after starting the tool.\nIt doesn‚Äôt copy already existing data from the <em>source</em> to the <em>destination database</em>.\nWhat‚Äôs more, at the time <em>py-mongo-sync</em> supported MongoDB versions between 2.4 to 3.4, which were older than those used at Allegro.</p>\n\n<h4 id=\"mongodb-cluster-to-cluster-sync\"><a href=\"https://www.mongodb.com/docs/cluster-to-cluster-sync/current/\">MongoDB Cluster-to-Cluster Sync</a></h4>\n\n<p>On July 22, 2022 MongoDB released <em>mongosync</em> v1.0 ‚Äî a tool for migrating and synchronizing data between MongoDB clusters.\nAs described in the <em>mongosync</em> documentation:</p>\n\n<blockquote>\n  <p>‚ÄúThe mongosync binary is the primary process used in Cluster‚Äîto‚ÄîCluster Sync. mongosync migrates data from one cluster\nto another and can keep the clusters in continuous sync.‚Äù</p>\n</blockquote>\n\n<p>This description sounded like a perfect fit for us! Unfortunately, after initial excitement\n(and hours spent on reading <a href=\"https://www.mongodb.com/docs/cluster-to-cluster-sync/current/reference/mongosync/\"><em>mongosync</em> documentation</a>)\nwe realized we couldn‚Äôt use <em>mongosync</em> as it was able to perform migration and synchronization process only if source database and destination database\nwere both in the exact same version.\nIt meant that there was no option to migrate databases from older MongoDB versions to the newest one, which was a no-go for us.</p>\n\n<p>When we realised that there wasn‚Äôt a tool which met all our requirements, we made a tough decision to implement our own online MongoDB migration tool\nnamed <em>mongo-migration-stream</em>.</p>\n\n<h2 id=\"mongo-migration-stream\">mongo-migration-stream</h2>\n\n<p><a href=\"https://github.com/allegro/mongo-migration-stream\"><em>mongo-migration-stream</em></a> is an open source tool from Allegro, that performs online migrations of MongoDB databases.\nIt‚Äôs a <a href=\"https://kotlinlang.org/\">Kotlin</a> application utilising\n<code class=\"language-plaintext highlighter-rouge\">mongodump</code> and <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> <a href=\"https://www.mongodb.com/docs/database-tools/\">MongoDB Command Line Database Tools</a>\nalong with <a href=\"https://www.mongodb.com/docs/manual/changeStreams/\">Mongo Change Streams</a> mechanism.\nIn this section I will explain how <em>mongo-migration-stream</em> works under the hood, by covering its functionalities from a high‚Äîlevel overview and\nproviding details about its low‚Äîlevel implementation.</p>\n\n<h3 id=\"mongo-migration-stream-terminology\">mongo-migration-stream terminology</h3>\n\n<ul>\n  <li><em>Source database</em> - MongoDB database which is a data source for migration,</li>\n  <li><em>Destination database</em> - MongoDB database which is a target for the data from <em>source database</em>,</li>\n  <li><em>Transfer</em> - a process of dumping data from <em>source database</em>, and restoring it on <em>destination database</em>,</li>\n  <li><em>Synchronization</em> - a process of keeping eventual consistency between <em>source database</em> and <em>destination database</em>,</li>\n  <li><em>Migration</em> - an end-to-end migration process combining both <em>transfer</em> and <em>synchronization</em> processes,</li>\n  <li><em>Migrator</em> - a tool for performing <em>migrations</em>.</li>\n</ul>\n\n<h3 id=\"building-blocks\">Building blocks</h3>\n\n<p>As I‚Äôve mentioned at the beginning of this section, <em>mongo-migration-stream</em> utilises <code class=\"language-plaintext highlighter-rouge\">mongodump</code>, <code class=\"language-plaintext highlighter-rouge\">mongorestore</code>, Mongo Change Streams\nand a custom Kotlin application to perform migrations.</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">mongodump</code> is used to dump <em>source database</em> in form of a binary file,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">mongorestore</code> is used to restore previously created dump on <em>destination database</em>,</li>\n  <li>Mongo Change Streams are used to keep eventual consistency between <em>source database</em> and <em>destination database</em>,</li>\n  <li>Kotlin application orchestrates, manages, and monitors all above processes.</li>\n</ul>\n\n<p><code class=\"language-plaintext highlighter-rouge\">mongodump</code> and <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> are responsible for the <em>transfer</em> part of migration,\nwhile Mongo Change Streams play the main role in the <em>synchronization</em> process.</p>\n\n<h3 id=\"birds-eye-view\">Bird‚Äôs eye view</h3>\n\n<p>To implement a <em>migrator</em>, we needed a robust procedure for <em>migrations</em> which ensures that no data is lost during a <em>migration</em>.\nWe have formulated a procedure consisting of six consecutive steps:</p>\n\n<ol>\n  <li>Start listening for Mongo Change Events on <em>source database</em> and save them in the queue,</li>\n  <li>Dump all the data from <em>source database</em> using <code class=\"language-plaintext highlighter-rouge\">mongodump</code>,</li>\n  <li>Restore all the data on <em>destination database</em> using <code class=\"language-plaintext highlighter-rouge\">mongorestore</code>,</li>\n  <li>Copy indexes definitions from <em>source database</em> and start creating them on <em>destination database</em>,</li>\n  <li>Start to push all the events stored in the queue (changes on <em>source database</em>) to the <em>destination database</em>,</li>\n  <li>Wait for the queue to empty to establish eventual consistency.</li>\n</ol>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/migration_process.png\" alt=\"Migration process\" /></p>\n\n<p>Our migration procedure works flawlessly because processing Mongo Change Events in a sequence guarantees migration idempotency.\nWithout this characteristic, we would have to change the order of steps 1 and 2 in the procedure, creating a possibility of losing data during migration.</p>\n\n<p>To explain this problem in more detail, let‚Äôs assume that the <em>source database</em> deals with a continuous high volume of writes.\nIf we had started the migration by performing dump in the first place and then started to listen for events,\nwe would have lost the events stored on the <em>source database</em> in the meantime.\nHowever, as we start the migration by listening for events on the <em>source database</em>, and then proceeding with the dump,\nwe do not lose any of the events stored on the <em>source database</em> during that time.</p>\n\n<p>The diagram below presents how such a kind of <em>write anomaly</em> could happen if we started dumping data before listening for Mongo Change Events.</p>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/avoiding_event_loss.png\" alt=\"Avoiding event loss\" /></p>\n\n<h3 id=\"implementation-details\">Implementation details</h3>\n\n<h4 id=\"concurrency\">Concurrency</h4>\n\n<p>From the beginning we wanted to make <em>mongo-migration-stream</em> fast ‚Äî we knew that it would need to cope with databases having more than 10k writes per second.\nAs a result <em>mongo-migration-stream</em> parallelizes migration of one MongoDB database into independent migrations of collections.\nEach database migration consists of multiple little <em>migrators</em> ‚Äî one <em>migrator</em> per collection in the database.</p>\n\n<p>The <em>transfer</em> process is performed in parallel for each collection, in separate <code class=\"language-plaintext highlighter-rouge\">mongodump</code> and <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> processes.\n<em>Synchronization</em> process was also implemented concurrently ‚Äî at the beginning of migration, each collection on <em>source database</em> is watched individually\nusing <a href=\"https://www.mongodb.com/docs/manual/changeStreams/#watch-a-collection--database--or-deployment\">Mongo Change Streams with collection target</a>.\nAll collections have their own separate queues in which Mongo Change Events are stored.\nAt the final phase of migration, each of these queues is processed independently.</p>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/concurrent_migrations.png\" alt=\"Concurrent migrations\" /></p>\n\n<h4 id=\"initial-data-transfer\">Initial data transfer</h4>\n\n<p>To perform transfer of the database, we‚Äôre executing <code class=\"language-plaintext highlighter-rouge\">mongodump</code> and <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> commands for each collection.\nFor that reason, machines on which <em>mongo-migration-stream</em> is running are required to have MongoDB Command Line Database Tools installed.</p>\n\n<p>Dumping data from collection <code class=\"language-plaintext highlighter-rouge\">collectionName</code> in <code class=\"language-plaintext highlighter-rouge\">source</code> database can be achieved by running a command:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>mongodump <span class=\"se\">\\</span>\n <span class=\"nt\">--uri</span> <span class=\"s2\">\"mongodb://mongo_rs36_1:36301,mongo_rs36_2:36302,mongo_rs36_3:36303/?replicaSet=replicaSet36\"</span> <span class=\"se\">\\</span>\n <span class=\"nt\">--db</span> <span class=\"nb\">source</span> <span class=\"se\">\\</span>\n <span class=\"nt\">--collection</span> collectionName <span class=\"se\">\\</span>\n <span class=\"nt\">--out</span> /home/user/mongomigrationstream/dumps <span class=\"se\">\\</span>\n <span class=\"nt\">--readPreference</span> secondary\n <span class=\"nt\">--username</span> username <span class=\"se\">\\</span>\n <span class=\"nt\">--config</span> /home/user/mongomigrationstream/password_config/dump.config <span class=\"se\">\\</span>\n <span class=\"nt\">--authenticationDatabase</span> admin\n</code></pre></div></div>\n\n<p>Starting a <code class=\"language-plaintext highlighter-rouge\">mongodump</code> process from Kotlin code is done with Java‚Äôs <code class=\"language-plaintext highlighter-rouge\">ProcessBuilder</code> feature.\n<code class=\"language-plaintext highlighter-rouge\">ProcessBuilder</code> requires us to provide a process program and arguments in the form of a list of Strings.\nWe construct this list using <code class=\"language-plaintext highlighter-rouge\">prepareCommand</code> function:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">prepareCommand</span><span class=\"p\">():</span> <span class=\"nc\">List</span><span class=\"p\">&lt;</span><span class=\"nc\">String</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">listOf</span><span class=\"p\">(</span>\n    <span class=\"n\">mongoToolsPath</span> <span class=\"p\">+</span> <span class=\"s\">\"mongodump\"</span><span class=\"p\">,</span>\n    <span class=\"s\">\"--uri\"</span><span class=\"p\">,</span> <span class=\"n\">dbProperties</span><span class=\"p\">.</span><span class=\"n\">uri</span><span class=\"p\">,</span>\n    <span class=\"s\">\"--db\"</span><span class=\"p\">,</span> <span class=\"n\">dbCollection</span><span class=\"p\">.</span><span class=\"n\">dbName</span><span class=\"p\">,</span>\n    <span class=\"s\">\"--collection\"</span><span class=\"p\">,</span> <span class=\"n\">dbCollection</span><span class=\"p\">.</span><span class=\"n\">collectionName</span><span class=\"p\">,</span>\n    <span class=\"s\">\"--out\"</span><span class=\"p\">,</span> <span class=\"n\">dumpPath</span><span class=\"p\">,</span>\n    <span class=\"s\">\"--readPreference\"</span><span class=\"p\">,</span> <span class=\"n\">readPreference</span>\n<span class=\"p\">)</span> <span class=\"p\">+</span> <span class=\"nf\">credentialsIfNotNull</span><span class=\"p\">(</span><span class=\"n\">dbProperties</span><span class=\"p\">.</span><span class=\"n\">authenticationProperties</span><span class=\"p\">,</span> <span class=\"n\">passwordConfigPath</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Having <code class=\"language-plaintext highlighter-rouge\">ProcessBuilder</code> with properly configured list of process program and arguments, we‚Äôre ready to start a new process\nusing the <code class=\"language-plaintext highlighter-rouge\">start()</code> function.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">runCommand</span><span class=\"p\">(</span><span class=\"n\">command</span><span class=\"p\">:</span> <span class=\"nc\">Command</span><span class=\"p\">):</span> <span class=\"nc\">CommandResult</span> <span class=\"p\">{</span>\n    <span class=\"kd\">val</span> <span class=\"py\">processBuilder</span> <span class=\"p\">=</span> <span class=\"nc\">ProcessBuilder</span><span class=\"p\">().</span><span class=\"nf\">command</span><span class=\"p\">(</span><span class=\"n\">command</span><span class=\"p\">.</span><span class=\"nf\">prepareCommand</span><span class=\"p\">())</span> <span class=\"c1\">// Configure ProcessBuilder with mongodump command in form of List&lt;String&gt;</span>\n    <span class=\"n\">currentProcess</span> <span class=\"p\">=</span> <span class=\"n\">processBuilder</span><span class=\"p\">.</span><span class=\"nf\">start</span><span class=\"p\">()</span> <span class=\"c1\">// Start a new process</span>\n    <span class=\"c1\">// ...</span>\n    <span class=\"kd\">val</span> <span class=\"py\">exitCode</span> <span class=\"p\">=</span> <span class=\"n\">currentProcess</span><span class=\"p\">.</span><span class=\"nf\">waitFor</span><span class=\"p\">()</span>\n    <span class=\"nf\">stopRunningCommand</span><span class=\"p\">()</span>\n    <span class=\"k\">return</span> <span class=\"nc\">CommandResult</span><span class=\"p\">(</span><span class=\"n\">exitCode</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>An analogous approach is implemented in <em>mongo-migration-stream</em> to execute the <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> command.</p>\n\n<h4 id=\"event-queue\">Event queue</h4>\n\n<p>During the process of migration <em>source database</em> can constantly receive changes, which <em>mongo-migration-stream</em> is listening to with Mongo Change Streams.\nEvents from the stream are saved in the queue for sending to the <em>destination database</em> at a later time.\nCurrently <em>mongo-migration-stream</em> provides two implementations of the queue,\nwhere one implementation stores the data in RAM, while the other one persists the data to disk.</p>\n\n<p>In-memory implementation can be used for databases with low traffic, or for testing purposes,\nor on machines with a sufficient amount of RAM (as events are stored as objects on the JVM heap).</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// In-memory queue implementation</span>\n<span class=\"k\">internal</span> <span class=\"kd\">class</span> <span class=\"nc\">InMemoryEventQueue</span><span class=\"p\">&lt;</span><span class=\"nc\">E</span><span class=\"p\">&gt;</span> <span class=\"p\">:</span> <span class=\"nc\">EventQueue</span><span class=\"p\">&lt;</span><span class=\"nc\">E</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">queue</span> <span class=\"p\">=</span> <span class=\"nc\">ConcurrentLinkedQueue</span><span class=\"p\">&lt;</span><span class=\"nc\">E</span><span class=\"p\">&gt;()</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">offer</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">:</span> <span class=\"nc\">E</span><span class=\"p\">):</span> <span class=\"nc\">Boolean</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">offer</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">)</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">poll</span><span class=\"p\">():</span> <span class=\"nc\">E</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">poll</span><span class=\"p\">()</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">peek</span><span class=\"p\">():</span> <span class=\"nc\">E</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">peek</span><span class=\"p\">()</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">size</span><span class=\"p\">():</span> <span class=\"nc\">Int</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"n\">size</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">removeAll</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">removeAll</span> <span class=\"p\">{</span> <span class=\"k\">true</span> <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>In our production setup we decided to use a persistent event queue, which is implemented on top of <a href=\"https://github.com/bulldog2011/bigqueue\">BigQueue project</a>.\nAs BigQueue only allows enqueuing and dequeuing byte arrays, we had to implement serialization and deserialization of the data from the events.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// Persistent queue implementation</span>\n<span class=\"k\">internal</span> <span class=\"kd\">class</span> <span class=\"nc\">BigQueueEventQueue</span><span class=\"p\">&lt;</span><span class=\"nc\">E</span> <span class=\"p\">:</span> <span class=\"nc\">Serializable</span><span class=\"p\">&gt;(</span><span class=\"n\">path</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">queueName</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">)</span> <span class=\"p\">:</span> <span class=\"nc\">EventQueue</span><span class=\"p\">&lt;</span><span class=\"nc\">E</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">queue</span> <span class=\"p\">=</span> <span class=\"nc\">BigQueueImpl</span><span class=\"p\">(</span><span class=\"n\">path</span><span class=\"p\">,</span> <span class=\"n\">queueName</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">offer</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">:</span> <span class=\"nc\">E</span><span class=\"p\">):</span> <span class=\"nc\">Boolean</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">enqueue</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"nf\">toByteArray</span><span class=\"p\">()).</span><span class=\"nf\">let</span> <span class=\"p\">{</span> <span class=\"k\">true</span> <span class=\"p\">}</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">poll</span><span class=\"p\">():</span> <span class=\"nc\">E</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">dequeue</span><span class=\"p\">().</span><span class=\"nf\">toE</span><span class=\"p\">()</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">peek</span><span class=\"p\">():</span> <span class=\"nc\">E</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">peek</span><span class=\"p\">().</span><span class=\"nf\">toE</span><span class=\"p\">()</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">size</span><span class=\"p\">():</span> <span class=\"nc\">Int</span> <span class=\"p\">=</span> <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">size</span><span class=\"p\">().</span><span class=\"nf\">toInt</span><span class=\"p\">()</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">removeAll</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">removeAll</span><span class=\"p\">()</span>\n        <span class=\"n\">queue</span><span class=\"p\">.</span><span class=\"nf\">gc</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"nc\">E</span><span class=\"p\">.</span><span class=\"nf\">toByteArray</span><span class=\"p\">():</span> <span class=\"nc\">ByteArray</span> <span class=\"p\">=</span> <span class=\"nc\">SerializationUtils</span><span class=\"p\">.</span><span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">)</span>\n    <span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"nc\">ByteArray</span><span class=\"p\">.</span><span class=\"nf\">toE</span><span class=\"p\">():</span> <span class=\"nc\">E</span> <span class=\"p\">=</span> <span class=\"nc\">SerializationUtils</span><span class=\"p\">.</span><span class=\"nf\">deserialize</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h4 id=\"migrating-indexes\">Migrating indexes</h4>\n\n<p>In early versions of <em>mongo-migration-stream</em>, to copy indexes from <em>source collection</em> to <em>destination collection</em>, we used\nan <a href=\"https://www.mongodb.com/docs/database-tools/mongorestore/#rebuild-indexes\">index rebuilding feature</a> from <code class=\"language-plaintext highlighter-rouge\">mongodump</code> and <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> tools.\nThis feature works on the principle that the result of <code class=\"language-plaintext highlighter-rouge\">mongodump</code> consists of both documents from the collection and definitions of indexes.\n<code class=\"language-plaintext highlighter-rouge\">mongorestore</code> can use those definitions to rebuild indexes on <em>destination collection</em>.</p>\n\n<p>Unfortunately it occurred that rebuilding indexes on <em>destination collection</em> after <em>transfer</em> phase (before starting <em>synchronization</em> process)\nwith the <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> tool lengthened the entire <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> process, preventing us from emptying the queue in the meantime.\nIt resulted in a growing queue of events to synchronize, ending up with overall longer migration times and higher resources utilisation.\nWe‚Äôve come to the conclusion, that we must rebuild indexes, while at the same time, keep sending events from the queue to <em>destination collection</em>.</p>\n\n<p>To migrate indexes without blocking <em>migration</em> process, we implemented a solution which for each collection,\nfetches all its indexes, and rebuilds them on <em>destination collection</em>.\nLooking from the application perspective, we use <code class=\"language-plaintext highlighter-rouge\">getRawSourceIndexes</code> function to fetch a list of Documents\n(representing indexes definitions) from <em>source collection</em>,\nand then recreate them on <em>destination collection</em> using <code class=\"language-plaintext highlighter-rouge\">createIndexOnDestinationCollection</code>.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"nf\">getRawSourceIndexes</span><span class=\"p\">(</span><span class=\"n\">sourceToDestination</span><span class=\"p\">:</span> <span class=\"nc\">SourceToDestination</span><span class=\"p\">):</span> <span class=\"nc\">List</span><span class=\"p\">&lt;</span><span class=\"nc\">Document</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"n\">sourceDb</span><span class=\"p\">.</span><span class=\"nf\">getCollection</span><span class=\"p\">(</span><span class=\"n\">sourceToDestination</span><span class=\"p\">.</span><span class=\"n\">source</span><span class=\"p\">.</span><span class=\"n\">collectionName</span><span class=\"p\">).</span><span class=\"nf\">listIndexes</span><span class=\"p\">()</span>\n        <span class=\"p\">.</span><span class=\"nf\">toList</span><span class=\"p\">()</span>\n        <span class=\"p\">.</span><span class=\"nf\">filterNot</span> <span class=\"p\">{</span> <span class=\"n\">it</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">(</span><span class=\"s\">\"key\"</span><span class=\"p\">,</span> <span class=\"nc\">Document</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span> <span class=\"p\">==</span> <span class=\"nc\">Document</span><span class=\"p\">().</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s\">\"_id\"</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">}</span>\n        <span class=\"p\">.</span><span class=\"nf\">map</span> <span class=\"p\">{</span>\n            <span class=\"n\">it</span><span class=\"p\">.</span><span class=\"nf\">remove</span><span class=\"p\">(</span><span class=\"s\">\"ns\"</span><span class=\"p\">)</span>\n            <span class=\"n\">it</span><span class=\"p\">.</span><span class=\"nf\">remove</span><span class=\"p\">(</span><span class=\"s\">\"v\"</span><span class=\"p\">)</span>\n            <span class=\"n\">it</span><span class=\"p\">[</span><span class=\"s\">\"background\"</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"k\">true</span>\n            <span class=\"n\">it</span>\n        <span class=\"p\">}</span>\n\n<span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"nf\">createIndexOnDestinationCollection</span><span class=\"p\">(</span>\n    <span class=\"n\">sourceToDestination</span><span class=\"p\">:</span> <span class=\"nc\">SourceToDestination</span><span class=\"p\">,</span>\n    <span class=\"n\">indexDefinition</span><span class=\"p\">:</span> <span class=\"nc\">Document</span>\n<span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"n\">destinationDb</span><span class=\"p\">.</span><span class=\"nf\">runCommand</span><span class=\"p\">(</span>\n        <span class=\"nc\">Document</span><span class=\"p\">().</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s\">\"createIndexes\"</span><span class=\"p\">,</span> <span class=\"n\">sourceToDestination</span><span class=\"p\">.</span><span class=\"n\">destination</span><span class=\"p\">.</span><span class=\"n\">collectionName</span><span class=\"p\">)</span>\n            <span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"s\">\"indexes\"</span><span class=\"p\">,</span> <span class=\"nf\">listOf</span><span class=\"p\">(</span><span class=\"n\">indexDefinition</span><span class=\"p\">))</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Our solution can rebuild indexes in both older and newer versions of MongoDB.\nTo support older MongoDB versions we specify <code class=\"language-plaintext highlighter-rouge\">{ background: true }</code> option, which does not block all operations on a given database during index creation.\nIn case where <em>destination database</em> is newer than or equal to MongoDB 4.2, the <code class=\"language-plaintext highlighter-rouge\">{ background: true }</code> option is ignored, and\n<a href=\"https://www.mongodb.com/docs/manual/core/index-creation/#comparison-to-foreground-and-background-builds\">optimized index build is used</a>.\nIn both scenarios rebuilding indexes does not block <em>synchronization</em> process, improving overall <em>migration</em> times.</p>\n\n<h4 id=\"verification-of-migration-state\">Verification of migration state</h4>\n\n<p>Throught <em>mongo-migration-stream</em> implementation we kept in mind that <em>migrator</em> user should be aware what‚Äôs happening within his/her migration.\nFor that purpose <em>mongo-migration-stream</em> exposes data about migration in multiple different ways:</p>\n\n<ul>\n  <li>\n    <p>Logs ‚Äî <em>migrator</em> logs all important information, so user can verify what‚Äôs going on,</p>\n  </li>\n  <li>\n    <p>Periodical checks ‚Äî when all migrated collections are in <em>synchronization</em> process, <em>migrator</em> starts a periodical check for each collection, verifying if all the data has been migrated, making collection on <em>destination database</em> ready to use,</p>\n  </li>\n  <li>\n    <p>Metrics ‚Äî various metrics about migration state are exposed through <a href=\"https://micrometer.io/\">Micrometer</a>.</p>\n  </li>\n</ul>\n\n<p>On top of all that, each migrator internal state change emits an event to in‚Äîmemory event bus. There are multiple types of events which <em>mongo-migration-stream</em> produces:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Event type</th>\n      <th>When the event is emitted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">StartEvent</code></td>\n      <td>Start of the migration</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">SourceToLocalStartEvent</code></td>\n      <td>Start watching for a collection specific Mongo Change Stream</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">DumpStartEvent</code></td>\n      <td>Start <code class=\"language-plaintext highlighter-rouge\">mongodump</code> for a collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">DumpUpdateEvent</code></td>\n      <td>Each <code class=\"language-plaintext highlighter-rouge\">mongodump</code> print to stdout</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">DumpFinishEvent</code></td>\n      <td>Finish <code class=\"language-plaintext highlighter-rouge\">mongodump</code> for a collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">RestoreStartEvent</code></td>\n      <td>Start <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> for a collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">RestoreUpdateEvent</code></td>\n      <td>Each <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> print to stdout</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">RestoreFinishEvent</code></td>\n      <td>Finish <code class=\"language-plaintext highlighter-rouge\">mongorestore</code> for a collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">IndexRebuildStartEvent</code></td>\n      <td>Start rebuilding indexes for a collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">IndexRebuildFinishEvent</code></td>\n      <td>Finish rebuilding indexes for a collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">LocalToDestinationStartEvent</code></td>\n      <td>Start sending events from queue to destination collection</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">StopEvent</code></td>\n      <td>Stop of the migration</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">PauseEvent</code></td>\n      <td>Pause of the migration</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">ResumeEvent</code></td>\n      <td>Resume of paused migration</td>\n    </tr>\n    <tr>\n      <td><code class=\"language-plaintext highlighter-rouge\">FailedEvent</code></td>\n      <td>Fail of collection migration</td>\n    </tr>\n  </tbody>\n</table>\n\n<h4 id=\"application-modules\">Application modules</h4>\n\n<p><em>mongo-migration-stream</em> code was split into two separate modules:</p>\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">mongo-migration-stream-core</code> module which can be used as a library in JVM application,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">mongo-migration-stream-cli</code> module which can be run as a standalone JAR.</li>\n</ul>\n\n<h2 id=\"mongo-migration-stream-in-production-at-allegro\">mongo-migration-stream in production at Allegro</h2>\n\n<p>Since internal launch in January 2023, we have migrated more than a hundred production databases using <em>mongo-migration-stream</em>.\nThe largest migrated collection stored more than one and a half billion documents.\nAt its peak moments, <em>migrator</em> was synchronizing a collection which emitted about 4 thousand Mongo Change Events per second.\nDuring one of our migrations, one collection queue size reached almost one hundred million events.\nAll of those events were later successfully synchronized into <em>destination collection</em>.</p>\n\n<p>At Allegro we use <em>mongo-migration-stream</em> as a library in a Web application with graphical user interface.\nThis approach allows Allegro engineers to manage database migrations on their own, without involving database team members.\nOn the screenshot below you can see our Web application GUI during a migration.</p>\n\n<p><img src=\"/img/articles/2023-09-14-online-mongodb-migration/mongo_migration_stream_ui.png\" alt=\"mongo-migration-stream Web Application at Allegro\" /></p>\n","contentSnippet":"MongoDB is the most popular database used at Allegro. We have hundreds of MongoDB databases running on our on‚Äîpremise servers.\nIn 2022 we decided that we need to migrate all our MongoDB databases\nfrom existing shared clusters to new MongoDB clusters hosted on Kubernetes pods with separated resources.\nTo perform the migration of all databases we needed a tool for transfering all the data and keeping consistency between old and new databases.\nThat‚Äôs how mongo-migration-stream project was born.\nWhy do we needed to migrate MongoDB databases at all?\nAt Allegro we are managing tens of MongoDB clusters, with hundreds of MongoDB databases running on them.\nThis kind of approach, where one MongoDB cluster runs multiple MongoDB databases, allowed us to utilize resources\nmore effectively, while at the same time easing maintenance of clusters.\n\nWe‚Äôve been living with this approach for years, but over time, more and more databases were\ncreated on shared clusters, increasing the frequency of the noisy neighbour problem.\nNoisy neighbour problem\nGenerally speaking, a noisy neighbour situation appears while multiple applications run on shared infrastructure,\nand one of those applications starts to consume so many resources (like CPU, RAM or Storage),\nthat it causes starvation of other applications.\nAt Allegro this problem started to be visible because over the years we‚Äôve created more and more new MongoDB databases\nwhich were hosted on a fixed number of clusters.\nThe most common cause of the noisy neighbour problem in the Allegro infrastructure was long time high CPU usage caused by one of MongoDB databases on a given cluster.\nOn various occasions it occurred that a non-optimal query performed on a large collection was consuming too much CPU,\nnegatively affecting all the other databases on that cluster, making them slower or completely unresponsive.\n\nMongoDB on Kubernetes as a solution to the noisy neighbour problem\nTo solve the noisy neighbour problem a separate team implemented a solution allowing Allegro engineers to create independent MongoDB clusters on Kubernetes.\nFrom now on, each MongoDB cluster is formed of multiple replicas and an arbiter spread among datacenters, serving only a single MongoDB database.\nRunning each database on a separate cluster with isolated resources managed by Kubernetes was our solution to the noisy neighbour problem.\n\nAt this point we knew what we needed to do to solve our problems ‚Äî we had to migrate all MongoDB databases from old shared clusters\nto new independent clusters on Kubernetes. Now came the time to answer the question: How should we do it?\nAvailable options\nFirstly, we prepared a list of requirements which a tool for migrating databases (referred to as migrator) had to meet in order to perform\nsuccessful migrations.\nRequirements\nMigrator must be able to migrate databases from older MongoDB versions to newer ones,\nMigrator must be able to migrate ReplicaSets and sharded clusters,\nMigrator must copy indexes from source database to destination database,\nMigrator must be able to handle more than 10k write operations per second,\nMigration must be performed without any downtime,\nMigration cannot affect database clients,\nDatabase owners (software engineers) need to be able to perform migrations on their own.\nExisting solutions\nHaving defined a list of requirements, we checked what tools were available on the market at the time.\npy-mongo-sync\nAccording to documentation py-mongo-sync is:\n‚ÄúOplog‚Äîbased data sync tool that synchronizes data from a replica set to another deployment,\ne.g.: standalone, replica set, and sharded cluster.‚Äù\nAs you can see, py-mongo-sync is not a tool that would suit our needs from end to end.\npy-mongo-sync focuses on the synchronization of the data stored on the source database after starting the tool.\nIt doesn‚Äôt copy already existing data from the source to the destination database.\nWhat‚Äôs more, at the time py-mongo-sync supported MongoDB versions between 2.4 to 3.4, which were older than those used at Allegro.\nMongoDB Cluster-to-Cluster Sync\nOn July 22, 2022 MongoDB released mongosync v1.0 ‚Äî a tool for migrating and synchronizing data between MongoDB clusters.\nAs described in the mongosync documentation:\n‚ÄúThe mongosync binary is the primary process used in Cluster‚Äîto‚ÄîCluster Sync. mongosync migrates data from one cluster\nto another and can keep the clusters in continuous sync.‚Äù\nThis description sounded like a perfect fit for us! Unfortunately, after initial excitement\n(and hours spent on reading mongosync documentation)\nwe realized we couldn‚Äôt use mongosync as it was able to perform migration and synchronization process only if source database and destination database\nwere both in the exact same version.\nIt meant that there was no option to migrate databases from older MongoDB versions to the newest one, which was a no-go for us.\nWhen we realised that there wasn‚Äôt a tool which met all our requirements, we made a tough decision to implement our own online MongoDB migration tool\nnamed mongo-migration-stream.\nmongo-migration-stream\nmongo-migration-stream is an open source tool from Allegro, that performs online migrations of MongoDB databases.\nIt‚Äôs a Kotlin application utilising\nmongodump and mongorestore MongoDB Command Line Database Tools\nalong with Mongo Change Streams mechanism.\nIn this section I will explain how mongo-migration-stream works under the hood, by covering its functionalities from a high‚Äîlevel overview and\nproviding details about its low‚Äîlevel implementation.\nmongo-migration-stream terminology\nSource database - MongoDB database which is a data source for migration,\nDestination database - MongoDB database which is a target for the data from source database,\nTransfer - a process of dumping data from source database, and restoring it on destination database,\nSynchronization - a process of keeping eventual consistency between source database and destination database,\nMigration - an end-to-end migration process combining both transfer and synchronization processes,\nMigrator - a tool for performing migrations.\nBuilding blocks\nAs I‚Äôve mentioned at the beginning of this section, mongo-migration-stream utilises mongodump, mongorestore, Mongo Change Streams\nand a custom Kotlin application to perform migrations.\nmongodump is used to dump source database in form of a binary file,\nmongorestore is used to restore previously created dump on destination database,\nMongo Change Streams are used to keep eventual consistency between source database and destination database,\nKotlin application orchestrates, manages, and monitors all above processes.\nmongodump and mongorestore are responsible for the transfer part of migration,\nwhile Mongo Change Streams play the main role in the synchronization process.\nBird‚Äôs eye view\nTo implement a migrator, we needed a robust procedure for migrations which ensures that no data is lost during a migration.\nWe have formulated a procedure consisting of six consecutive steps:\nStart listening for Mongo Change Events on source database and save them in the queue,\nDump all the data from source database using mongodump,\nRestore all the data on destination database using mongorestore,\nCopy indexes definitions from source database and start creating them on destination database,\nStart to push all the events stored in the queue (changes on source database) to the destination database,\nWait for the queue to empty to establish eventual consistency.\n\nOur migration procedure works flawlessly because processing Mongo Change Events in a sequence guarantees migration idempotency.\nWithout this characteristic, we would have to change the order of steps 1 and 2 in the procedure, creating a possibility of losing data during migration.\nTo explain this problem in more detail, let‚Äôs assume that the source database deals with a continuous high volume of writes.\nIf we had started the migration by performing dump in the first place and then started to listen for events,\nwe would have lost the events stored on the source database in the meantime.\nHowever, as we start the migration by listening for events on the source database, and then proceeding with the dump,\nwe do not lose any of the events stored on the source database during that time.\nThe diagram below presents how such a kind of write anomaly could happen if we started dumping data before listening for Mongo Change Events.\n\nImplementation details\nConcurrency\nFrom the beginning we wanted to make mongo-migration-stream fast ‚Äî we knew that it would need to cope with databases having more than 10k writes per second.\nAs a result mongo-migration-stream parallelizes migration of one MongoDB database into independent migrations of collections.\nEach database migration consists of multiple little migrators ‚Äî one migrator per collection in the database.\nThe transfer process is performed in parallel for each collection, in separate mongodump and mongorestore processes.\nSynchronization process was also implemented concurrently ‚Äî at the beginning of migration, each collection on source database is watched individually\nusing Mongo Change Streams with collection target.\nAll collections have their own separate queues in which Mongo Change Events are stored.\nAt the final phase of migration, each of these queues is processed independently.\n\nInitial data transfer\nTo perform transfer of the database, we‚Äôre executing mongodump and mongorestore commands for each collection.\nFor that reason, machines on which mongo-migration-stream is running are required to have MongoDB Command Line Database Tools installed.\nDumping data from collection collectionName in source database can be achieved by running a command:\n\nmongodump \\\n --uri \"mongodb://mongo_rs36_1:36301,mongo_rs36_2:36302,mongo_rs36_3:36303/?replicaSet=replicaSet36\" \\\n --db source \\\n --collection collectionName \\\n --out /home/user/mongomigrationstream/dumps \\\n --readPreference secondary\n --username username \\\n --config /home/user/mongomigrationstream/password_config/dump.config \\\n --authenticationDatabase admin\n\n\nStarting a mongodump process from Kotlin code is done with Java‚Äôs ProcessBuilder feature.\nProcessBuilder requires us to provide a process program and arguments in the form of a list of Strings.\nWe construct this list using prepareCommand function:\n\noverride fun prepareCommand(): List<String> = listOf(\n    mongoToolsPath + \"mongodump\",\n    \"--uri\", dbProperties.uri,\n    \"--db\", dbCollection.dbName,\n    \"--collection\", dbCollection.collectionName,\n    \"--out\", dumpPath,\n    \"--readPreference\", readPreference\n) + credentialsIfNotNull(dbProperties.authenticationProperties, passwordConfigPath)\n\n\nHaving ProcessBuilder with properly configured list of process program and arguments, we‚Äôre ready to start a new process\nusing the start() function.\n\nfun runCommand(command: Command): CommandResult {\n    val processBuilder = ProcessBuilder().command(command.prepareCommand()) // Configure ProcessBuilder with mongodump command in form of List<String>\n    currentProcess = processBuilder.start() // Start a new process\n    // ...\n    val exitCode = currentProcess.waitFor()\n    stopRunningCommand()\n    return CommandResult(exitCode)\n}\n\n\nAn analogous approach is implemented in mongo-migration-stream to execute the mongorestore command.\nEvent queue\nDuring the process of migration source database can constantly receive changes, which mongo-migration-stream is listening to with Mongo Change Streams.\nEvents from the stream are saved in the queue for sending to the destination database at a later time.\nCurrently mongo-migration-stream provides two implementations of the queue,\nwhere one implementation stores the data in RAM, while the other one persists the data to disk.\nIn-memory implementation can be used for databases with low traffic, or for testing purposes,\nor on machines with a sufficient amount of RAM (as events are stored as objects on the JVM heap).\n\n// In-memory queue implementation\ninternal class InMemoryEventQueue<E> : EventQueue<E> {\n    private val queue = ConcurrentLinkedQueue<E>()\n\n    override fun offer(element: E): Boolean = queue.offer(element)\n    override fun poll(): E = queue.poll()\n    override fun peek(): E = queue.peek()\n    override fun size(): Int = queue.size\n    override fun removeAll() {\n        queue.removeAll { true }\n    }\n}\n\n\nIn our production setup we decided to use a persistent event queue, which is implemented on top of BigQueue project.\nAs BigQueue only allows enqueuing and dequeuing byte arrays, we had to implement serialization and deserialization of the data from the events.\n\n// Persistent queue implementation\ninternal class BigQueueEventQueue<E : Serializable>(path: String, queueName: String) : EventQueue<E> {\n    private val queue = BigQueueImpl(path, queueName)\n\n    override fun offer(element: E): Boolean = queue.enqueue(element.toByteArray()).let { true }\n    override fun poll(): E = queue.dequeue().toE()\n    override fun peek(): E = queue.peek().toE()\n    override fun size(): Int = queue.size().toInt()\n    override fun removeAll() {\n        queue.removeAll()\n        queue.gc()\n    }\n\n    private fun E.toByteArray(): ByteArray = SerializationUtils.serialize(this)\n    private fun ByteArray.toE(): E = SerializationUtils.deserialize(this)\n}\n\n\nMigrating indexes\nIn early versions of mongo-migration-stream, to copy indexes from source collection to destination collection, we used\nan index rebuilding feature from mongodump and mongorestore tools.\nThis feature works on the principle that the result of mongodump consists of both documents from the collection and definitions of indexes.\nmongorestore can use those definitions to rebuild indexes on destination collection.\nUnfortunately it occurred that rebuilding indexes on destination collection after transfer phase (before starting synchronization process)\nwith the mongorestore tool lengthened the entire mongorestore process, preventing us from emptying the queue in the meantime.\nIt resulted in a growing queue of events to synchronize, ending up with overall longer migration times and higher resources utilisation.\nWe‚Äôve come to the conclusion, that we must rebuild indexes, while at the same time, keep sending events from the queue to destination collection.\nTo migrate indexes without blocking migration process, we implemented a solution which for each collection,\nfetches all its indexes, and rebuilds them on destination collection.\nLooking from the application perspective, we use getRawSourceIndexes function to fetch a list of Documents\n(representing indexes definitions) from source collection,\nand then recreate them on destination collection using createIndexOnDestinationCollection.\n\nprivate fun getRawSourceIndexes(sourceToDestination: SourceToDestination): List<Document> =\n    sourceDb.getCollection(sourceToDestination.source.collectionName).listIndexes()\n        .toList()\n        .filterNot { it.get(\"key\", Document::class.java) == Document().append(\"_id\", 1) }\n        .map {\n            it.remove(\"ns\")\n            it.remove(\"v\")\n            it[\"background\"] = true\n            it\n        }\n\nprivate fun createIndexOnDestinationCollection(\n    sourceToDestination: SourceToDestination,\n    indexDefinition: Document\n) {\n    destinationDb.runCommand(\n        Document().append(\"createIndexes\", sourceToDestination.destination.collectionName)\n            .append(\"indexes\", listOf(indexDefinition))\n    )\n}\n\n\nOur solution can rebuild indexes in both older and newer versions of MongoDB.\nTo support older MongoDB versions we specify { background: true } option, which does not block all operations on a given database during index creation.\nIn case where destination database is newer than or equal to MongoDB 4.2, the { background: true } option is ignored, and\noptimized index build is used.\nIn both scenarios rebuilding indexes does not block synchronization process, improving overall migration times.\nVerification of migration state\nThrought mongo-migration-stream implementation we kept in mind that migrator user should be aware what‚Äôs happening within his/her migration.\nFor that purpose mongo-migration-stream exposes data about migration in multiple different ways:\nLogs ‚Äî migrator logs all important information, so user can verify what‚Äôs going on,\nPeriodical checks ‚Äî when all migrated collections are in synchronization process, migrator starts a periodical check for each collection, verifying if all the data has been migrated, making collection on destination database ready to use,\nMetrics ‚Äî various metrics about migration state are exposed through Micrometer.\nOn top of all that, each migrator internal state change emits an event to in‚Äîmemory event bus. There are multiple types of events which mongo-migration-stream produces:\nEvent type\n      When the event is emitted\n    \nStartEvent\n      Start of the migration\n    \nSourceToLocalStartEvent\n      Start watching for a collection specific Mongo Change Stream\n    \nDumpStartEvent\n      Start mongodump for a collection\n    \nDumpUpdateEvent\n      Each mongodump print to stdout\n    \nDumpFinishEvent\n      Finish mongodump for a collection\n    \nRestoreStartEvent\n      Start mongorestore for a collection\n    \nRestoreUpdateEvent\n      Each mongorestore print to stdout\n    \nRestoreFinishEvent\n      Finish mongorestore for a collection\n    \nIndexRebuildStartEvent\n      Start rebuilding indexes for a collection\n    \nIndexRebuildFinishEvent\n      Finish rebuilding indexes for a collection\n    \nLocalToDestinationStartEvent\n      Start sending events from queue to destination collection\n    \nStopEvent\n      Stop of the migration\n    \nPauseEvent\n      Pause of the migration\n    \nResumeEvent\n      Resume of paused migration\n    \nFailedEvent\n      Fail of collection migration\n    \nApplication modules\nmongo-migration-stream code was split into two separate modules:\nmongo-migration-stream-core module which can be used as a library in JVM application,\nmongo-migration-stream-cli module which can be run as a standalone JAR.\nmongo-migration-stream in production at Allegro\nSince internal launch in January 2023, we have migrated more than a hundred production databases using mongo-migration-stream.\nThe largest migrated collection stored more than one and a half billion documents.\nAt its peak moments, migrator was synchronizing a collection which emitted about 4 thousand Mongo Change Events per second.\nDuring one of our migrations, one collection queue size reached almost one hundred million events.\nAll of those events were later successfully synchronized into destination collection.\nAt Allegro we use mongo-migration-stream as a library in a Web application with graphical user interface.\nThis approach allows Allegro engineers to manage database migrations on their own, without involving database team members.\nOn the screenshot below you can see our Web application GUI during a migration.","guid":"https://blog.allegro.tech/2023/09/online-mongodb-migration.html","categories":["tech","mongodb","nosql","kotlin","open source","mongo change streams"],"isoDate":"2023-09-13T22:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"The Acrobatics of Switching Between Management and Engineering","link":"https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html","pubDate":"Tue, 22 Aug 2023 00:00:00 +0200","authors":{"author":[{"name":["Micha≈Ç Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"<p>After six years as a Team Leader, I went back to hands-on engineering work, and I‚Äôm very happy about taking\nthis step. While it may appear surprising at first, it was a well-thought-out decision, and actually I‚Äôve already\nperformed such a maneuver once before.</p>\n\n<h2 id=\"background\">Background</h2>\n\n<p>A few years ago I stumbled upon <a href=\"https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/\">The Engineer/Manager Pendulum</a>\nby Charity Majors, and the follow-up post <a href=\"https://charity.wtf/2019/01/04/engineering-management-the-pendulum-or-the-ladder/\">Engineering Management: The Pendulum Or The Ladder</a>.\nI found both pieces interesting, and quite in line with my own experiences: in my previous job some time earlier I\nhad been a team leader, but consciously joined <a href=\"https://allegro.tech\">Allegro</a> as a software engineer. After a while, I\nbecame a team leader again, and recently, another few years down the line, I went back to engineering yet again.</p>\n\n<p>Both above-mentioned posts make very good points, so I recommend you read them first. What I want to add on top of\nthem are my personal experiences and some tips on organizing a transition between a management and an individual\ncontributor role. The journey in the other direction (from developer to team leader) has been discussed in depth\nelsewhere, so I won‚Äôt delve into that.</p>\n\n<h2 id=\"why\">Why</h2>\n\n<p>Why would I want to switch between developer and team leader roles? The problem is they are both interesting and have\n<a href=\"/2019/06/allegro-culture-tech-leaders-meeting.html\">their own highlights</a>, but you can‚Äôt do both at the\nsame time. When you become a manager, not only do you have less time for technical tasks, but you also pretty much\nlose the ability to focus on these tasks even if you do find a time slot. This is because, like it or not, you end\nup with a <a href=\"http://www.paulgraham.com/makersschedule.html\">manager‚Äôs schedule</a>.</p>\n\n<p><img src=\"/img/articles/2023-08-22-management-engineering-acrobatics/flying-trapeze-performers.jpg\" alt=\"Circus performers on the flying trapeze, Public Domain image from https://commons.wikimedia.org/wiki/File:Programma_van_Circus_Krone_in_Rotterdam_drie_Alizes_,_vliegende_trapeze_met_o.a.,_Bestanddeelnr_910-4372.jpg\" class=\"small-image\" /></p>\n\n<p>Over time, your technical skills start deteriorating, and if you miss the right moment, you may find yourself at\na point of no return. Like a circus artist on a flying trapeze, you have to time your actions right to avoid\ndisaster. I think this metaphor fits the situation better than that of the pendulum, which moves in its own rhythm,\nindependent of external influences.</p>\n\n<p>Obviously, you need to ask yourself whether you actually want to prevent your technical skills from deteriorating. Some\npeople move on to management, wave goodbye to getting their hands dirty, and are completely fine with that. Some tinker\nwith technology in their spare time. As for me, I like technology and would not only prefer to not lose what I have\nlearned so far, but actually want to learn something new. However, due to a number of other hobbies and already\nspending more time in front of the screen than I would like, doing tech in the afternoons was not a viable option for\nme. Hence, the decision to make technology an important part of my job again.</p>\n\n<h2 id=\"how\">How</h2>\n\n<p>A crucial factor to take into account when planning such a change is that it will take time. If you take your\nteam seriously, you can‚Äôt just disappear overnight. You need to think ahead. Do not ask yourself ‚Äúhave I already\nlost my tech skills beyond repair?‚Äù. Ask yourself ‚Äúhow will my tech skills be a year from now?‚Äù.</p>\n\n<p>Also, keep a cool head. Just as when moving in the opposite direction, from engineer to manager, consider all\nconsequences, both positive and negative. It has been repeated many times that becoming a manager is not a promotion\n(being a lateral move to another career path), but in practice sometimes it is. In particular, it may come with a\nhigher level or salary. Make sure you check all details of your target role in order to avoid unpleasant surprises when\ngoing back. In my case, I was aiming for the Principal Software Engineer role, which is the same level as the Team\nLeader role at Allegro, so there were no issues in this regard.</p>\n\n<p>When I started thinking about making the switch, once I had a rough idea of what I wanted, I talked to my superior.\nThis was an important step: it allowed him to plan ahead, and also to look for opportunities for making the\nreorganization easier. Some elements of the process would depend only on our actions, but some, such as finding a good\nreplacement team leader, would also depend on a number of factors outside our control. Knowing that my boss understood\nmy need, and supported it, mattered a lot, and made the wait and preparations easier.</p>\n\n<p>Chance favors the prepared mind, as Louis Pasteur supposedly said. There happened to be a team leader in\nanother part of the company who was thinking about moving on to a different area. Thanks to being aware of my plan,\nmy boss was able to grab the chance, and we had a perfect match. We discussed with the potential new leader the team\nand the project, and he found them interesting. We planned a transition period, as short as possible, but long enough\nfor me to transfer to him a reasonable part of my knowledge about the team and its work.</p>\n\n<p>Now that we had a specific plan, we could tell the team. It was important to let everyone know as soon as possible, but\nnot before we had a specific plan. Without it, this information would only stir uncertainty. Apart\nfrom telling the team as a whole, I also talked to each person individually, in order to resolve any questions or doubts\nand to try to reduce any problems resulting from the transition as much as possible.</p>\n\n<p>Waiting for the switch date, we kept meeting online with the new leader, transferring knowledge and preparing him for\nworking with the team. There‚Äôs actually quite a lot of stuff a leader needs to know: not only how the project\nworks on technical and business levels, but also current plans, who the stakeholders are and how to work with them,\nand each team member‚Äôs individual strengths and development plan. The new leader himself also started meeting\npeople he would now work with, both team members and our product‚Äôs stakeholders, and attending team meetings such as\nthe daily stand-up. Despite gradually moving on to other tasks after the switch date, I was still available to clarify\nany doubts, and our boss would also help out when necessary, so the new leader knew he would not be left on his own.\nWhile it required quite a bit of work, the switch went smoothly, and we didn‚Äôt notice any serious disturbance to the\nteam‚Äôs functioning.</p>\n\n<h2 id=\"the-aftermath\">The Aftermath</h2>\n\n<p>It‚Äôs been several months since the switch now. Me changing back to a technical role has certainly required extra work,\nfor me, my boss, and the new team leader. Despite our best efforts, it probably put a little extra strain on the team as\nwell. Nonetheless, I think it was a win-win, even more so thanks to us being able to spot and exploit a happy\ncoincidence. I am glad to be closer to technology again, and the new leader also got to try something new, just as he\nwanted.</p>\n\n<p>There is one more subtle advantage to the whole process. When people leave the team, some knowledge inevitably gets\nlost. One of the reasons is <em>tacit knowledge</em>: there are always things you know, but are not aware of knowing. You can\nuse this knowledge when it‚Äôs needed, but you will probably not transfer it to others because you are not even aware of\nits existence in the first place. Removing someone from the team in a controlled manner as happened here (and being\nstill able to reach out to them if needed) causes such latent knowledge to be discovered, and once discovered, to be\npropagated. This causes a little disruption short-term, but in the long run it reduces\n<a href=\"https://en.wikipedia.org/wiki/Information_silo\">knowledge silos</a> and increases the\n<a href=\"https://en.wikipedia.org/wiki/Bus_factor\">bus factor</a>.</p>\n\n<p>When I first started thinking about going back to hands-on technical work for the second time, I had some doubts about\nhow much my technical skills had already deteriorated and how difficult it would be to go back. It seems I made it, but\nnot by much. Had I delayed by one more year, I might have really struggled. It‚Äôs not a matter of knowledge: theory,\nespecially generic things that do not change that fast with technology, is not lost so quickly. Also, while a team\nleader, I tried to stay in touch with technology by taking part in task refinement, architecture discussions, on-call\nrotation, etc.</p>\n\n<p>However, I really felt, and to some degree still feel, a difference in practical, hands-on work, such as actually\nwriting code. There are many small quirks that you need to be aware of in order to accomplish things quickly that you\ndon‚Äôt even notice if you use them every day and know inside-out. Knowing all the little useful tools, the less often\nused features of your IDE, or what to do when something breaks unexpectedly, make a world of a difference, but this\npractical knowledge gets lost when not used and I had to rebuild it almost from scratch. Another thing that I still\nexperience is the difficulty in focusing on a single topic. Working on a manager‚Äôs schedule for several years has taken\nits toll, and now that I often have large contiguous blocks of time, I find myself not using them as effectively as I\ncould, because I have become accustomed to always doing multiple things at once and without a chance to stay focused\nanyway. It‚Äôs gradually getting better, but I still feel the impact, and this is probably my biggest surprise of the\nwhole process.</p>\n\n<h2 id=\"about-the-principal-software-engineer-role\">About the Principal Software Engineer role</h2>\n\n<p>My current role is that of Principal Software Engineer (PSE). It is a relatively new addition to the junior, mid,\nand senior roles we‚Äôve had so far. It has gone through a number of revisions, and is still evolving. Most people in this\nrole come from a Senior Software Engineer background, so my case of getting there after being a Team Leader is a bit\nuntypical. At many companies, roles like this are called Staff Software Engineer or similar. While still an individual\ncontributor role, a PSE differs from a senior in several ways.</p>\n\n<p>First of all, a PSE is expected to spend significant time on topics whose scope is much larger than a single team can\nhandle. Seniors can also do this, but it‚Äôs not a requirement for them. Such topics may be area-wide, such as planning a\nmajor change to a single subsystem‚Äôs architecture, or have a company-wide scope. Much work on this level consists of\ncoming up with ideas and discussing them while implementation is often left to others. So, while still technical, this\nrole encompasses less coding than that of a senior. Not very surprising given that generally moving up the career ladder\nmeans more coming up with ideas, teaching others, and planning work, while coding less yourself.</p>\n\n<p>Secondly, a PSE should be very autonomous. Most PSEs are not members of regular development teams since they move from\ntask to task depending on where they can help most. This means you cooperate with more people from different parts of\nthe company, but you don‚Äôt have the few peers you work with every day that most people have. You don‚Äôt get a backlog\nof tasks to work on, but have to plan your work yourself. People do come to you, asking for support or doing something\nfor their project, but that‚Äôs just one of many inputs.</p>\n\n<p>Thirdly, since there are few PSEs compared to other positions, for each person the role is a little different. On one\nhand this means you can‚Äôt fully know what to expect when you start. On the other, you get to shape the role yourself,\nand personally I enjoy this flexibility.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>I think the idea of swinging back and forth between engineering and management described in\n<a href=\"https://charity.wtf/2017/05/11/the-engineer-manager-pendulum/\">The Engineer/Manager Pendulum</a>,\nis valid, and my own experience backs it up fully. One element which I want to additionally stress, however, is that\nthe switch is more like a flying trapeze than a pendulum: timing is crucial, and missing the right moment can have\nserious consequences. Becoming a Principal Software Engineer was a unique experience, both on a technical\nlevel and as my de facto last task as a team leader. Who knows what the future holds? Perhaps some time from now I‚Äôll\ntake another swing on the flying trapeze and go back to management?</p>\n","contentSnippet":"After six years as a Team Leader, I went back to hands-on engineering work, and I‚Äôm very happy about taking\nthis step. While it may appear surprising at first, it was a well-thought-out decision, and actually I‚Äôve already\nperformed such a maneuver once before.\nBackground\nA few years ago I stumbled upon The Engineer/Manager Pendulum\nby Charity Majors, and the follow-up post Engineering Management: The Pendulum Or The Ladder.\nI found both pieces interesting, and quite in line with my own experiences: in my previous job some time earlier I\nhad been a team leader, but consciously joined Allegro as a software engineer. After a while, I\nbecame a team leader again, and recently, another few years down the line, I went back to engineering yet again.\nBoth above-mentioned posts make very good points, so I recommend you read them first. What I want to add on top of\nthem are my personal experiences and some tips on organizing a transition between a management and an individual\ncontributor role. The journey in the other direction (from developer to team leader) has been discussed in depth\nelsewhere, so I won‚Äôt delve into that.\nWhy\nWhy would I want to switch between developer and team leader roles? The problem is they are both interesting and have\ntheir own highlights, but you can‚Äôt do both at the\nsame time. When you become a manager, not only do you have less time for technical tasks, but you also pretty much\nlose the ability to focus on these tasks even if you do find a time slot. This is because, like it or not, you end\nup with a manager‚Äôs schedule.\n\nOver time, your technical skills start deteriorating, and if you miss the right moment, you may find yourself at\na point of no return. Like a circus artist on a flying trapeze, you have to time your actions right to avoid\ndisaster. I think this metaphor fits the situation better than that of the pendulum, which moves in its own rhythm,\nindependent of external influences.\nObviously, you need to ask yourself whether you actually want to prevent your technical skills from deteriorating. Some\npeople move on to management, wave goodbye to getting their hands dirty, and are completely fine with that. Some tinker\nwith technology in their spare time. As for me, I like technology and would not only prefer to not lose what I have\nlearned so far, but actually want to learn something new. However, due to a number of other hobbies and already\nspending more time in front of the screen than I would like, doing tech in the afternoons was not a viable option for\nme. Hence, the decision to make technology an important part of my job again.\nHow\nA crucial factor to take into account when planning such a change is that it will take time. If you take your\nteam seriously, you can‚Äôt just disappear overnight. You need to think ahead. Do not ask yourself ‚Äúhave I already\nlost my tech skills beyond repair?‚Äù. Ask yourself ‚Äúhow will my tech skills be a year from now?‚Äù.\nAlso, keep a cool head. Just as when moving in the opposite direction, from engineer to manager, consider all\nconsequences, both positive and negative. It has been repeated many times that becoming a manager is not a promotion\n(being a lateral move to another career path), but in practice sometimes it is. In particular, it may come with a\nhigher level or salary. Make sure you check all details of your target role in order to avoid unpleasant surprises when\ngoing back. In my case, I was aiming for the Principal Software Engineer role, which is the same level as the Team\nLeader role at Allegro, so there were no issues in this regard.\nWhen I started thinking about making the switch, once I had a rough idea of what I wanted, I talked to my superior.\nThis was an important step: it allowed him to plan ahead, and also to look for opportunities for making the\nreorganization easier. Some elements of the process would depend only on our actions, but some, such as finding a good\nreplacement team leader, would also depend on a number of factors outside our control. Knowing that my boss understood\nmy need, and supported it, mattered a lot, and made the wait and preparations easier.\nChance favors the prepared mind, as Louis Pasteur supposedly said. There happened to be a team leader in\nanother part of the company who was thinking about moving on to a different area. Thanks to being aware of my plan,\nmy boss was able to grab the chance, and we had a perfect match. We discussed with the potential new leader the team\nand the project, and he found them interesting. We planned a transition period, as short as possible, but long enough\nfor me to transfer to him a reasonable part of my knowledge about the team and its work.\nNow that we had a specific plan, we could tell the team. It was important to let everyone know as soon as possible, but\nnot before we had a specific plan. Without it, this information would only stir uncertainty. Apart\nfrom telling the team as a whole, I also talked to each person individually, in order to resolve any questions or doubts\nand to try to reduce any problems resulting from the transition as much as possible.\nWaiting for the switch date, we kept meeting online with the new leader, transferring knowledge and preparing him for\nworking with the team. There‚Äôs actually quite a lot of stuff a leader needs to know: not only how the project\nworks on technical and business levels, but also current plans, who the stakeholders are and how to work with them,\nand each team member‚Äôs individual strengths and development plan. The new leader himself also started meeting\npeople he would now work with, both team members and our product‚Äôs stakeholders, and attending team meetings such as\nthe daily stand-up. Despite gradually moving on to other tasks after the switch date, I was still available to clarify\nany doubts, and our boss would also help out when necessary, so the new leader knew he would not be left on his own.\nWhile it required quite a bit of work, the switch went smoothly, and we didn‚Äôt notice any serious disturbance to the\nteam‚Äôs functioning.\nThe Aftermath\nIt‚Äôs been several months since the switch now. Me changing back to a technical role has certainly required extra work,\nfor me, my boss, and the new team leader. Despite our best efforts, it probably put a little extra strain on the team as\nwell. Nonetheless, I think it was a win-win, even more so thanks to us being able to spot and exploit a happy\ncoincidence. I am glad to be closer to technology again, and the new leader also got to try something new, just as he\nwanted.\nThere is one more subtle advantage to the whole process. When people leave the team, some knowledge inevitably gets\nlost. One of the reasons is tacit knowledge: there are always things you know, but are not aware of knowing. You can\nuse this knowledge when it‚Äôs needed, but you will probably not transfer it to others because you are not even aware of\nits existence in the first place. Removing someone from the team in a controlled manner as happened here (and being\nstill able to reach out to them if needed) causes such latent knowledge to be discovered, and once discovered, to be\npropagated. This causes a little disruption short-term, but in the long run it reduces\nknowledge silos and increases the\nbus factor.\nWhen I first started thinking about going back to hands-on technical work for the second time, I had some doubts about\nhow much my technical skills had already deteriorated and how difficult it would be to go back. It seems I made it, but\nnot by much. Had I delayed by one more year, I might have really struggled. It‚Äôs not a matter of knowledge: theory,\nespecially generic things that do not change that fast with technology, is not lost so quickly. Also, while a team\nleader, I tried to stay in touch with technology by taking part in task refinement, architecture discussions, on-call\nrotation, etc.\nHowever, I really felt, and to some degree still feel, a difference in practical, hands-on work, such as actually\nwriting code. There are many small quirks that you need to be aware of in order to accomplish things quickly that you\ndon‚Äôt even notice if you use them every day and know inside-out. Knowing all the little useful tools, the less often\nused features of your IDE, or what to do when something breaks unexpectedly, make a world of a difference, but this\npractical knowledge gets lost when not used and I had to rebuild it almost from scratch. Another thing that I still\nexperience is the difficulty in focusing on a single topic. Working on a manager‚Äôs schedule for several years has taken\nits toll, and now that I often have large contiguous blocks of time, I find myself not using them as effectively as I\ncould, because I have become accustomed to always doing multiple things at once and without a chance to stay focused\nanyway. It‚Äôs gradually getting better, but I still feel the impact, and this is probably my biggest surprise of the\nwhole process.\nAbout the Principal Software Engineer role\nMy current role is that of Principal Software Engineer (PSE). It is a relatively new addition to the junior, mid,\nand senior roles we‚Äôve had so far. It has gone through a number of revisions, and is still evolving. Most people in this\nrole come from a Senior Software Engineer background, so my case of getting there after being a Team Leader is a bit\nuntypical. At many companies, roles like this are called Staff Software Engineer or similar. While still an individual\ncontributor role, a PSE differs from a senior in several ways.\nFirst of all, a PSE is expected to spend significant time on topics whose scope is much larger than a single team can\nhandle. Seniors can also do this, but it‚Äôs not a requirement for them. Such topics may be area-wide, such as planning a\nmajor change to a single subsystem‚Äôs architecture, or have a company-wide scope. Much work on this level consists of\ncoming up with ideas and discussing them while implementation is often left to others. So, while still technical, this\nrole encompasses less coding than that of a senior. Not very surprising given that generally moving up the career ladder\nmeans more coming up with ideas, teaching others, and planning work, while coding less yourself.\nSecondly, a PSE should be very autonomous. Most PSEs are not members of regular development teams since they move from\ntask to task depending on where they can help most. This means you cooperate with more people from different parts of\nthe company, but you don‚Äôt have the few peers you work with every day that most people have. You don‚Äôt get a backlog\nof tasks to work on, but have to plan your work yourself. People do come to you, asking for support or doing something\nfor their project, but that‚Äôs just one of many inputs.\nThirdly, since there are few PSEs compared to other positions, for each person the role is a little different. On one\nhand this means you can‚Äôt fully know what to expect when you start. On the other, you get to shape the role yourself,\nand personally I enjoy this flexibility.\nSummary\nI think the idea of swinging back and forth between engineering and management described in\nThe Engineer/Manager Pendulum,\nis valid, and my own experience backs it up fully. One element which I want to additionally stress, however, is that\nthe switch is more like a flying trapeze than a pendulum: timing is crucial, and missing the right moment can have\nserious consequences. Becoming a Principal Software Engineer was a unique experience, both on a technical\nlevel and as my de facto last task as a team leader. Who knows what the future holds? Perhaps some time from now I‚Äôll\ntake another swing on the flying trapeze and go back to management?","guid":"https://blog.allegro.tech/2023/08/management-engineering-acrobatics.html","categories":["tech","coding","management","developer","team leader","career path"],"isoDate":"2023-08-21T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999948427702","name":"Integration Engineer","uuid":"6861c4c8-f07a-4adb-817b-2d94c06d32f5","jobAdId":"22b63dc3-61fa-488b-b1cd-ef8588ccfe28","defaultJobAd":true,"refNumber":"REF2138N","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-12-04T13:24:30.613Z","location":{"city":"Warsaw, Pozna≈Ñ, Cracow, Gda≈Ñsk, Wroclaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3976147c-fe25-42a8-8c97-78273250960b","valueLabel":"4"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999948427702","creator":{"name":"Wiktoria Mitruk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999948395138","name":"Software Engineer (.NET) - Allegro Pay","uuid":"e19d2ead-31e1-461c-a8b4-ae5b734d1806","jobAdId":"64d12e40-8ee1-468f-8e95-52bff85e0d14","defaultJobAd":true,"refNumber":"REF3170Y","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-12-04T09:50:34.747Z","location":{"city":"Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999948395138","creator":{"name":"Wiktoria Mitruk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999947819422","name":"DevOps Engineer (Azure / .NET) - Allegro Pay","uuid":"db47553d-b8da-423a-a2dc-3efe85402009","jobAdId":"00657dd7-dbfa-4edf-abd3-022ac21aa5da","defaultJobAd":true,"refNumber":"REF4210C","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-11-30T13:21:18.240Z","location":{"city":"Warsaw","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999947819422","creator":{"name":"Martyna Stafa"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999947605953","name":"Mobile Software Engineer (Android)-Allegro Pay","uuid":"40d565e9-daf9-4c6f-a908-4e44c3ecc0f8","jobAdId":"fe5b9e62-14a4-4cb9-aecf-639cc6a12380","defaultJobAd":true,"refNumber":"REF4603V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-11-29T11:33:59.238Z","location":{"city":"Warsaw","region":"Masovian Voivodeship","country":"pl","remote":false,"latitude":"52.2296756","longitude":"21.0122287"},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999947605953","creator":{"name":"Dominika Fujarowicz"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999947036804","name":"Mobile Software Engineer (iOS)-Allegro Pay","uuid":"d0be06ea-845a-47ab-b95b-1d1143f4ae0b","jobAdId":"c849a9ac-95a8-468f-b7b5-240a1f3d3017","defaultJobAd":true,"refNumber":"REF4604N","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-11-27T10:47:40.171Z","location":{"city":"Warsaw","region":"Masovian Voivodeship","country":"pl","remote":false,"latitude":"52.2296756","longitude":"21.0122287"},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"6406f92e638cbb2f415a94a9","fieldLabel":"Job Area","valueId":"e8731ea4-48a9-476d-ab1d-9a40eb3426f1","valueLabel":"Technology"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"61583054f15cea434e0be36f","fieldLabel":"Career Level","valueId":"3c97f53f-19c2-4a25-9eb1-513f9fb38b80","valueLabel":"3-5"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"9c8396d4-11a6-443c-897c-15f29221a3fd","valueLabel":"Allegro Pay sp. z o.o."},{"fieldId":"61582f70e72a6b6d239c9857","fieldLabel":"Area","valueId":"76599a72-f283-4550-9303-52e2e0eb6e32","valueLabel":"Technology"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999947036804","creator":{"name":"Dominika Fujarowicz"},"language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1701092071000,"duration":7200000,"id":"297614064","name":"Allegro Tech Talks #40 - Testy: dynamiczne dashboardy & optymalizacja pracy","date_in_series_pattern":false,"status":"upcoming","time":1701968400000,"local_date":"2023-12-07","local_time":"18:00","updated":1701092071000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":13,"venue":{"id":27528185,"name":"Allegro Krak√≥w Office","lat":50.06517028808594,"lon":19.951927185058594,"repinned":true,"address_1":"Lubicz Park A (5 piƒôtro)","address_2":"ul. Lubicz 23","city":"Krak√≥w","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/297614064/","description":"**‚û° Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-40/](https://app.evenea.pl/event/allegro-tech-talk-40/) Jeszcze przed ≈õwiƒôtami zapraszamy Was na #40 wydarzenie z serii Allegro Tech Talk, podczas kt√≥rych dzielimy siƒô wiedzƒÖ, wzajemnie inspirujemy oraz integrujemy‚Ä¶","how_to_find_us":"Biuro Allegro znajduje siƒô w Centrum Biurowym Lubicz. \n\nObok budynku znajduje siƒô przystanek Lubicz. Przy przystanku zatrzymujƒÖ siƒô tramwaje 2, 4, 10, 14, 20, 52, 62, 64 oraz autobusy: 124, 152, 424, 601, 611, 662, 664.\n\n","visibility":"public","member_pay_fee":false},{"created":1700495058000,"duration":7200000,"id":"297480100","name":"Allegro Tech Talks #39 - Big Data: o podej≈õciu do pracy z danymi","date_in_series_pattern":false,"status":"past","time":1701363600000,"local_date":"2023-11-30","local_time":"18:00","updated":1701377876000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":50,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":52.23224639892578,"lon":20.992111206054688,"repinned":true,"address_1":"ul. ≈ªelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/297480100/","description":"**Rejestracja: [https://app.evenea.pl/event/allegro-tech-talk-39/](https://app.evenea.pl/event/allegro-tech-talk-39/)** BƒÖd≈∫cie z nami podczas #39 wydarzenia z serii **Allegro Tech Talk**, podczas kt√≥rych dzielimy siƒô wiedzƒÖ, wzajemnie inspirujemy oraz integrujemy podczas rozm√≥w przy‚Ä¶","how_to_find_us":"Biuro Allegro znajduje siƒô w kompleksie Fabryki Norblina (wej≈õcie Plater 3 od ul. ≈ªelaznej). W niedalekiej odleg≈Ço≈õci znajdujƒÖ siƒô dwie stacje metra linii M2, Rondo Daszy≈Ñskiego i Rondo ONZ. Autobusy, tramwaje i inne ≈õrodki transportu sprawdzisz te≈º na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false},{"created":1685697967000,"duration":7200000,"id":"293929321","name":"Allegro Tech Talks #38 - Mobile: o iOS bez spinki","date_in_series_pattern":false,"status":"past","time":1686760200000,"local_date":"2023-06-14","local_time":"18:30","updated":1686773845000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":17,"venue":{"id":27549223,"name":"Allegro Warsaw Office","lat":0,"lon":0,"repinned":true,"address_1":"ul. ≈ªelazna 51/53","city":"Warszawa","country":"pl","localized_country_name":"Poland"},"is_online_event":false,"eventType":"PHYSICAL","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/293929321/","description":"**‚û° Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-38/](https://app.evenea.pl/event/allegro-tech-talk-38/) Ostatnie przed przerwƒÖ wakacyjnƒÖ, stacjonarne spotkanie z cyklu Allegro Tech Talks, na kt√≥rych dzielimy siƒô wiedzƒÖ, wzajemnie inspirujemy oraz integrujemy podczas rozm√≥w‚Ä¶","how_to_find_us":"Biuro Allegro znajduje siƒô w kompleksie Fabryki Norblina (wej≈õcie Plater 3 od ul. ≈ªelaznej). W niedalekiej odleg≈Ço≈õci znajdujƒÖ siƒô dwie stacje metra linii M2, Rondo Daszy≈Ñskiego i Rondo ONZ. Autobusy, tramwaje i inne ≈õrodki transportu sprawdzisz te≈º na: https://fabrykanorblina.pl/dojazd","visibility":"public","member_pay_fee":false},{"created":1678978572000,"duration":111600000,"id":"292278882","name":"UX Research Confetti - III edycja ","date_in_series_pattern":false,"status":"past","time":1684915200000,"local_date":"2023-05-24","local_time":"10:00","updated":1685029049000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":33,"is_online_event":true,"eventType":"ONLINE","group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/292278882/","description":"**Rejestracja na wydarzenie ‚û° [https://app.evenea.pl/event/ux-research-confetti-3/]( https://app.evenea.pl/event/ux-research-confetti-3/ )**[ ]( https://app.evenea.pl/event/ux-research-confetti-3/ ) **üéâ Przedstawiamy 3. edycjƒô UX Research Confetti organizowanƒÖ przez¬†Allegro - bezp≈ÇatnƒÖ, polskƒÖ konferencjƒô po≈õwiƒôconƒÖ badaniom‚Ä¶","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"MBox: server-driven UI dla aplikacji mobilnych","link":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","pubDate":"Thu, 16 Nov 2023 00:00:00 GMT","content":"Czym jest i jak powsta≈Ç MBox: wewnƒôtrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? SkƒÖd wziƒÖ≈Ç siƒô pomys≈Ç na to rozwiƒÖzanie i na jakie bolƒÖczki odpowiada? Dlaczego zdecydowali≈õmy siƒô na budowanie tego typu rozwiƒÖzania in-house i z jakimi wyzwaniami mierzyli≈õmy siƒô w procesie tworzenia? Co wyr√≥≈ºnia zespo≈Çy pracujƒÖce nad tym narzƒôdziem i jak pracuje im siƒô bez Product Ownera? Pos≈Çuchajcie si√≥dmego odcinka Allegro Tech Podcast z udzia≈Çem Pauliny Sadowskiej i Tomasza Gƒôbarowskiego - Manager√≥w w obszarze Technical Platform Services w Allegro.","contentSnippet":"Czym jest i jak powsta≈Ç MBox: wewnƒôtrzna platforma server-driven UI dla aplikacji mobilnych w Allegro? SkƒÖd wziƒÖ≈Ç siƒô pomys≈Ç na to rozwiƒÖzanie i na jakie bolƒÖczki odpowiada? Dlaczego zdecydowali≈õmy siƒô na budowanie tego typu rozwiƒÖzania in-house i z jakimi wyzwaniami mierzyli≈õmy siƒô w procesie tworzenia? Co wyr√≥≈ºnia zespo≈Çy pracujƒÖce nad tym narzƒôdziem i jak pracuje im siƒô bez Product Ownera? Pos≈Çuchajcie si√≥dmego odcinka Allegro Tech Podcast z udzia≈Çem Pauliny Sadowskiej i Tomasza Gƒôbarowskiego - Manager√≥w w obszarze Technical Platform Services w Allegro.","guid":"https://podcast.allegro.tech/mbox-server-driven-ui-dla-aplikacji-mobilnych/","isoDate":"2023-11-16T00:00:00.000Z"},{"title":"O chatbotach i ich wp≈Çywie na Allegro","link":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","pubDate":"Wed, 11 Oct 2023 00:00:00 GMT","content":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieƒá w kontek≈õcie obszaru Customer Experience? W czym pomagajƒÖ nam chatboty, jak je rozwijamy i dbamy o ich jako≈õƒá? Kim sƒÖ Allina oraz Albert i co majƒÖ wsp√≥lnego z automatyzacjƒÖ? Za jakie rozwiƒÖzania otrzymali≈õmy nagrodƒô hiperautomatyzacji? O tym wszystkim pos≈Çuchacie w odcinku z udzia≈Çem Rafa≈Ça Gajewskiego - Managera w obszarze IT Services w Allegro.","contentSnippet":"Jakie procesy automatyzujemy w Allegro i co warto o nich wiedzieƒá w kontek≈õcie obszaru Customer Experience? W czym pomagajƒÖ nam chatboty, jak je rozwijamy i dbamy o ich jako≈õƒá? Kim sƒÖ Allina oraz Albert i co majƒÖ wsp√≥lnego z automatyzacjƒÖ? Za jakie rozwiƒÖzania otrzymali≈õmy nagrodƒô hiperautomatyzacji? O tym wszystkim pos≈Çuchacie w odcinku z udzia≈Çem Rafa≈Ça Gajewskiego - Managera w obszarze IT Services w Allegro.","guid":"https://podcast.allegro.tech/o-chatbotach-i-ich-wplywie-na-allegro/","isoDate":"2023-10-11T00:00:00.000Z"},{"title":"O roli analityk√≥w biznesowych w Allegro","link":"https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/","pubDate":"Thu, 24 Aug 2023 00:00:00 GMT","content":"Czym zajmujƒÖ siƒô analitycy danych w Allegro i za jakie projekty odpowiadajƒÖ? Z jakich rodzaj√≥w danych i narzƒôdzi korzystajƒÖ w codziennej pracy? Jakie (przyk≈Çadowe) obszary tematyczne pokrywamy danymi, kt√≥re analizujemy w Allegro? Jakich umiejƒôtno≈õci szukamy u analityk√≥w biznesowych w Allegro i jak mo≈ºna do nas do≈ÇƒÖczyƒá? O roli analityk√≥w biznesowych i pracy w skali Allegro opowiadajƒÖ Jakub Kr√≥l i Mateusz Falkowski - Senior Data Analysts w Allegro.","contentSnippet":"Czym zajmujƒÖ siƒô analitycy danych w Allegro i za jakie projekty odpowiadajƒÖ? Z jakich rodzaj√≥w danych i narzƒôdzi korzystajƒÖ w codziennej pracy? Jakie (przyk≈Çadowe) obszary tematyczne pokrywamy danymi, kt√≥re analizujemy w Allegro? Jakich umiejƒôtno≈õci szukamy u analityk√≥w biznesowych w Allegro i jak mo≈ºna do nas do≈ÇƒÖczyƒá? O roli analityk√≥w biznesowych i pracy w skali Allegro opowiadajƒÖ Jakub Kr√≥l i Mateusz Falkowski - Senior Data Analysts w Allegro.","guid":"https://podcast.allegro.tech/o-roli-analitykow-biznesowych-w-allegro/","isoDate":"2023-08-24T00:00:00.000Z"},{"title":"O spo≈Çeczno≈õci Allegro Tech i rozwoju in≈ºynier√≥w w Allegro","link":"https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/","pubDate":"Thu, 27 Jul 2023 00:00:00 GMT","content":"Na czym polega rola Principal Software Engineera w Allegro oraz co ma wsp√≥lnego z rozwijaniem siebie i dzieleniem siƒô wiedzƒÖ? Co warto wiedzieƒá o turystyce, kt√≥ra pojawia siƒô niemal w ka≈ºdym odcinku naszych podcast√≥w? Na czym polega, kto, kiedy i jak mo≈ºe z niej skorzystaƒá? Jak pracujemy z talentami Gallupa (tak≈ºe w zespo≈Çach technicznych)?  Co dajƒÖ nam wewnƒôtrzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy siƒô do≈õwiadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przy≈õwieca? O spo≈Çeczno≈õci Allegro Tech i mo≈ºliwo≈õciach rozwoju w Allegro z perspektywy in≈ºynier√≥w rozmawiali≈õmy z Marcinem Turkiem i Micha≈Çem Kosmulskim.","contentSnippet":"Na czym polega rola Principal Software Engineera w Allegro oraz co ma wsp√≥lnego z rozwijaniem siebie i dzieleniem siƒô wiedzƒÖ? Co warto wiedzieƒá o turystyce, kt√≥ra pojawia siƒô niemal w ka≈ºdym odcinku naszych podcast√≥w? Na czym polega, kto, kiedy i jak mo≈ºe z niej skorzystaƒá? Jak pracujemy z talentami Gallupa (tak≈ºe w zespo≈Çach technicznych)?  Co dajƒÖ nam wewnƒôtrzne DevDays, hackhathony, gildie, meetupy, konferencje i jak jeszcze wymieniamy siƒô do≈õwiadczeniami? Czym jest Allegro Tech Meeting i jaka idea mu przy≈õwieca? O spo≈Çeczno≈õci Allegro Tech i mo≈ºliwo≈õciach rozwoju w Allegro z perspektywy in≈ºynier√≥w rozmawiali≈õmy z Marcinem Turkiem i Micha≈Çem Kosmulskim.","guid":"https://podcast.allegro.tech/o-spolecznosci-allegro-tech-i-rozwoju-inzynierow-w-allegro/","isoDate":"2023-07-27T00:00:00.000Z"}]},"__N_SSG":true}