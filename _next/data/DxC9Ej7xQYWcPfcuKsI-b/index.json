{"pageProps":{"posts":[{"title":"Moving towards Micronaut","link":"https://blog.allegro.tech/2021/11/micronaut.html","pubDate":"Mon, 22 Nov 2021 00:00:00 +0100","authors":{"author":[{"name":["Konrad Kamiński"],"photo":["https://blog.allegro.tech/img/authors/konrad.kaminski.jpg"],"url":["https://blog.allegro.tech/authors/konrad.kaminski"]}]},"content":"<p><a href=\"https://micronaut.io\">Micronaut</a> is one of the new application frameworks that have recently sprang up. It promises\nlow memory usage and faster application startup. At <a href=\"https://allegro.tech/\">Allegro</a> we decided to give it a try. In this article we’ll learn what\ncame out of it and if it’s worth considering when creating microservices-based systems.</p>\n\n<h2 id=\"paradise-city\">Paradise city</h2>\n<p>At Allegro we run a few hundred microservices, most of which use Spring Framework. We also have services created in other technologies.\nAnd to make things more complicated we run them on a few different types of clouds - our own <a href=\"http://mesos.apache.org/\">Mesos</a>-based as well as private and public <a href=\"https://kubernetes.io/\">k8s</a>-based ones.\nTherefore in order for all of it to work consistently and smoothly we created a number of supporting libraries and at the same time we defined a kind of\ncontract for all services. This way, if there is ever a need or will to create a service with a new shiny technology, it should be feasible with as little\nwork as possible. You can read more about this approach in <a href=\"https://blog.allegro.tech/2020/07/common-code-approach.html\">a great article by Piotr Betkier</a>.</p>\n\n<h2 id=\"you-gotta-fight-for-your-right-to-party\">(You Gotta) Fight for Your Right (To Party!)</h2>\n<p>In order to be up to date with current technologies, at Allegro we run hackathons where we try out the “trendy” solutions. Over a year ago\nwe decided to taste Micronaut. The framework represents one of the new approaches to some of the inherent problems of existing solutions:\nit steers clear of using Java Reflection and does as much as it can at compile or rather build time. Major things achieved this way are:</p>\n<ul>\n  <li>lower memory usage - Java Reflection in most current JDK implementations is a memory hog; Micronaut has its own implementation of Java Reflection-like API\nwhich doesn’t suffer from that problem,</li>\n  <li>faster startup - Java Reflection is also not a speed daemon; doing things ahead of time means less has to be done at runtime,</li>\n  <li>ability to create native apps - <a href=\"https://www.graalvm.org\">GraalVM</a>, another new kid on the block, allows creating native binaries out of a JVM-based application; however, there\nare some caveats and one of them is… Java Reflection (basically if your application uses it, it has to provide some metadata for the native compiler). Since\nMicronaut has its own implementation, the problem is simply non-existent.</li>\n</ul>\n\n<p>We wanted to see how difficult it is to create a new microservice with Micronaut that would run on our on-premise cloud and do something meaningful. So during\nour hackathon we defined the following goals for our simple app:</p>\n<ul>\n  <li>it should be possible to deploy the app on our on-premise cloud,</li>\n  <li>the app should provide and use basic functionalities, such as:\n    <ul>\n      <li>telemetry,</li>\n      <li>configuration management,</li>\n      <li>REST endpoints,</li>\n      <li>ability to call other microservices,</li>\n      <li>ability to send messages to <a href=\"https://github.com/allegro/hermes\">Hermes</a>,</li>\n      <li>database access (we voted for MongoDB),</li>\n      <li>(optionally) ability to be compiled into a native binary with GraalVM.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>After a very satisfying hackathon we carried the day. Our microservice had all the above-mentioned functionalities - some of them obviously\nin a makeshift form, but that didn’t matter. We achieved all the goals.</p>\n\n<h2 id=\"highway-to-hell\">Highway to Hell</h2>\n<p>The result of the hackathon pushed us forward to make something even bolder. We wanted to have a real Micronaut-based application in our production environment.\nTo make things harder - we wanted to convert an existing Spring-based system to a Micronaut-based one. Though we reached our destination, the road\nwas quite bumpy. Let’s see what awaits those who take that path.</p>\n\n<h3 id=\"paranoid\">Paranoid</h3>\n<p>To ease a migration from Spring a special <a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">micronaut-spring</a> project has been created.\nIt supports a limited selection of Spring annotations and functionality so that in theory one can just replace Spring dependencies with Micronaut ones.\nSpecifically among the most interesting features are:</p>\n<ul>\n  <li>standard inversion of control annotations: <code class=\"language-plaintext highlighter-rouge\">@Component</code>, <code class=\"language-plaintext highlighter-rouge\">@Service</code>, <code class=\"language-plaintext highlighter-rouge\">@Repository</code>,<code class=\"language-plaintext highlighter-rouge\">@Bean</code>, <code class=\"language-plaintext highlighter-rouge\">@Autowired</code>, <code class=\"language-plaintext highlighter-rouge\">@Configuration</code>, <code class=\"language-plaintext highlighter-rouge\">@Primary</code> and many others\nare converted into their Micronaut counterparts,</li>\n  <li>standard Spring interfaces: <code class=\"language-plaintext highlighter-rouge\">@Environment</code>, <code class=\"language-plaintext highlighter-rouge\">@ApplicationEventPublisher</code>, <code class=\"language-plaintext highlighter-rouge\">@ApplicationContext</code>, <code class=\"language-plaintext highlighter-rouge\">@BeanFactory</code> and their <code class=\"language-plaintext highlighter-rouge\">@*Aware</code> versions are also\nadapted to their Micronaut counterparts,</li>\n  <li>MVC controller annotations: <code class=\"language-plaintext highlighter-rouge\">@RestController</code>, <code class=\"language-plaintext highlighter-rouge\">@GetMapping</code>, <code class=\"language-plaintext highlighter-rouge\">@PostMapping</code> and many others are converted into their Micronaut counterparts.</li>\n</ul>\n\n<p>This makes the whole exercise simpler, but unfortunately it also comes at a price. Not all features are supported (e.g. Spring’s <code class=\"language-plaintext highlighter-rouge\">@PathVariable</code> is not)\nand for those which are, they sometimes have subtle differences. For this reason oftentimes you simply have to revert to the regular manual code\nconversion. The problem is that this kind of approach will lead you to a mixed solution - you’ll have both Micronaut and Spring annotations in your code.\nAnd then a question arises: which annotations should I use for the newly created code? Do we stick with the old Spring annotations if there is even\none instance of it in the current codebase? Or maybe treat this old code as a “necessary evil” and always put Micronaut annotations for the added functionality?</p>\n\n<p>We came to the conclusion that we did not want to use <em>micronaut-spring</em> at all. This\nof course led to more work, but in the end we think it was worth it. The converted application does not have any Spring dependencies, no “technical debt”.</p>\n\n<h3 id=\"sad-but-true\">Sad but True</h3>\n\n<p>One of the things not covered at all by <em>micronaut-spring</em> is exception handling in MVC.\nIn Spring our handlers looked something like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.springframework.http.HttpStatus.BAD_REQUEST</span>\n\n<span class=\"nd\">@ControllerAdvice</span>\n<span class=\"nd\">@Order</span><span class=\"p\">(</span><span class=\"nc\">Ordered</span><span class=\"p\">.</span><span class=\"nc\">HIGHEST_PRECEDENCE</span><span class=\"p\">)</span>\n<span class=\"kd\">class</span> <span class=\"nc\">DefaultExceptionHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@ExceptionHandler</span><span class=\"p\">(</span><span class=\"nc\">SomeException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">)</span>\n    <span class=\"nd\">@ResponseBody</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">handleSomeException</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">SomeException</span><span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">(</span><span class=\"nc\">ApiError</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">),</span> <span class=\"nc\">BAD_REQUEST</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>In Micronaut exception handling can be done locally (i.e. functions handling exception will only be used for the exceptions thrown by the controller the\nfunctions are defined in) or globally. Since our Spring handlers acted globally, the equivalent Micronaut code is as follows:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">io.micronaut.http.HttpStatus.BAD_REQUEST</span>\n<span class=\"k\">import</span> <span class=\"nn\">io.micronaut.http.annotation.Error</span> <span class=\"k\">as</span> <span class=\"nc\">HttpError</span>\n\n<span class=\"nd\">@Controller</span>\n<span class=\"kd\">class</span> <span class=\"nc\">DefaultExceptionHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@Error</span><span class=\"p\">(</span><span class=\"n\">global</span> <span class=\"p\">=</span> <span class=\"k\">true</span><span class=\"p\">)</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">handleSomeException</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">SomeException</span><span class=\"p\">):</span> <span class=\"nc\">HttpResponse</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nc\">HttpResponseFactory</span><span class=\"p\">.</span><span class=\"nc\">INSTANCE</span><span class=\"p\">.</span><span class=\"nf\">status</span><span class=\"p\">(</span><span class=\"nc\">BAD_REQUEST</span><span class=\"p\">,</span> <span class=\"nc\">ApiError</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">))</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"dirty-deeds-done-dirt-cheap\">Dirty Deeds Done Dirt Cheap</h3>\n\n<p>At Allegro we use many different types of databases. The application of this exercise used MongoDB. As it turned out we couldn’t have chosen worse. Don’t get me\nwrong - Micronaut supports most of the databases out there, but not all are treated equally well.</p>\n\n<p>Since our system used <a href=\"https://spring.io/projects/spring-data\">Spring Data</a>, we tried to find something similar from the Micronaut world. <a href=\"https://micronaut-projects.github.io/micronaut-data/latest/guide/\">Micronaut Data</a>\nis - as its authors say - “inspired by <em>GORM</em> and <em>Spring Data</em>”. Unfortunately the inspiration doesn’t go too far. And in case of MongoDB it actually <a href=\"https://github.com/micronaut-projects/micronaut-data/issues/220\">doesn’t even\ntake a step</a>. Instead, we used <a href=\"https://micronaut-projects.github.io/micronaut-mongodb/latest/guide/\">Micronaut MongoDB</a> library. This simple project will provide\nyour services only with either a <a href=\"https://mongodb.github.io/mongo-java-driver/4.3/apidocs/mongodb-driver-legacy/com/mongodb/MongoClient.html\">blocking MongoClient</a> or a\n<a href=\"https://mongodb.github.io/mongo-java-driver/4.3/apidocs/mongodb-driver-reactivestreams/com/mongodb/reactivestreams/client/MongoClient.html\">reactive MongoClient</a>\nalong with some healthchecks. Not enough even for a modest application.</p>\n\n<p>Fortunately some good people created <a href=\"https://litote.org/kmongo/\">kmongo</a> - a little library that helped us a lot in converting the database access part of our app.\nAt the end of the day, however, we had to create some support code to ease the migration.</p>\n\n<p>The original application database access code was in the form of reactive repositories:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.springframework.data.annotation.Id</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.springframework.data.mongodb.core.mapping.Document</span>\n\n<span class=\"nd\">@Document</span><span class=\"p\">(</span><span class=\"n\">collection</span> <span class=\"p\">=</span> <span class=\"s\">\"users\"</span><span class=\"p\">)</span>\n<span class=\"kd\">data class</span> <span class=\"nc\">User</span><span class=\"p\">(</span>\n    <span class=\"nd\">@Id</span> <span class=\"kd\">val</span> <span class=\"py\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">name</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">UserRepository</span><span class=\"p\">:</span> <span class=\"nc\">ReactiveMongoRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">,</span> <span class=\"nc\">String</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">findFirstByTypeOrderByNameDesc</span><span class=\"p\">(</span><span class=\"n\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>We wanted to preserve the interface and as much code as possible. Here is what we had to do to get this effect.</p>\n\n<p>First we decided that our components would use <code class=\"language-plaintext highlighter-rouge\">MongoDatabase</code> rather than <code class=\"language-plaintext highlighter-rouge\">MongoClient</code> offered by <em>Micronaut MongoDB</em>.\nWe had only one database so that was an obvious choice.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Factory</span>\n<span class=\"kd\">class</span> <span class=\"nc\">MongoConfig</span> <span class=\"p\">{</span>\n    <span class=\"nd\">@Singleton</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">mongoDatabase</span><span class=\"p\">(</span><span class=\"n\">mongoClient</span><span class=\"p\">:</span> <span class=\"nc\">MongoClient</span><span class=\"p\">,</span> <span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"nc\">DefaultMongoConfiguration</span><span class=\"p\">):</span> <span class=\"nc\">MongoDatabase</span> <span class=\"p\">=</span>\n        <span class=\"n\">mongoClient</span><span class=\"p\">.</span><span class=\"nf\">getDatabase</span><span class=\"p\">(</span><span class=\"n\">configuration</span><span class=\"p\">.</span><span class=\"n\">connectionString</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">().</span><span class=\"n\">database</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Then there was the question of configuring <em>kmongo</em>. It wasn’t as straightforward as we’d thought it would be. Let’s take a look at the\nfinal code.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Factory</span>\n<span class=\"kd\">class</span> <span class=\"nc\">KMongoFactory</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@Singleton</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">kCodecRegistry</span><span class=\"p\">():</span> <span class=\"nc\">CodecRegistry</span> <span class=\"p\">{</span>\n        <span class=\"nc\">ObjectMappingConfiguration</span><span class=\"p\">.</span><span class=\"nf\">addCustomCodec</span><span class=\"p\">(</span><span class=\"nc\">JodaDateSerializationCodec</span><span class=\"p\">)</span> <span class=\"c1\">// 1 - custom Joda DateTime coded</span>\n        <span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"nf\">registerBsonModule</span><span class=\"p\">(</span><span class=\"nc\">JodaModule</span><span class=\"p\">())</span>                  <span class=\"c1\">// 2 - register default Joda module</span>\n        <span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"nf\">registerBsonModule</span><span class=\"p\">(</span><span class=\"nc\">JodaDateSerializationModule</span><span class=\"p\">)</span>   <span class=\"c1\">// 3 - register custom Joda module</span>\n        <span class=\"nf\">with</span><span class=\"p\">(</span><span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"n\">bsonMapper</span><span class=\"p\">.</span><span class=\"n\">factory</span> <span class=\"k\">as</span> <span class=\"nc\">BsonFactory</span><span class=\"p\">)</span> <span class=\"p\">{</span>         <span class=\"c1\">// 4 - change BigDecimal handling</span>\n            <span class=\"nf\">disable</span><span class=\"p\">(</span><span class=\"nc\">BsonGenerator</span><span class=\"p\">.</span><span class=\"nc\">Feature</span><span class=\"p\">.</span><span class=\"nc\">WRITE_BIGDECIMALS_AS_DECIMAL128</span><span class=\"p\">)</span>\n            <span class=\"nf\">enable</span><span class=\"p\">(</span><span class=\"nc\">BsonGenerator</span><span class=\"p\">.</span><span class=\"nc\">Feature</span><span class=\"p\">.</span><span class=\"nc\">WRITE_BIGDECIMALS_AS_STRINGS</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n        <span class=\"k\">return</span> <span class=\"nc\">ClassMappingType</span><span class=\"p\">.</span><span class=\"nf\">codecRegistry</span><span class=\"p\">(</span><span class=\"nc\">MongoClientSettings</span><span class=\"p\">.</span><span class=\"nf\">getDefaultCodecRegistry</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p><code class=\"language-plaintext highlighter-rouge\">MongoDB</code> driver expects a <code class=\"language-plaintext highlighter-rouge\">CodecRegistry</code> which defines how to encode a Java object into Mongo <code class=\"language-plaintext highlighter-rouge\">BSON</code>, so that it can be persisted in a database. By default\n<em>kmongo</em> supports a simple, <a href=\"https://github.com/FasterXML/jackson\">Jackson</a> based converter. However, there were a few issues in\nour application which forced us to create some customizations:</p>\n\n<ul>\n  <li><a href=\"https://www.joda.org/joda-time/\">Joda</a> date types in entity classes - our app has a long history and it still uses <em>Joda</em> date types.\nUnfortunately they do not work with <em>kmongo</em>, so we had to teach it how to handle them. It required a few steps.\n    <ul>\n      <li>\n        <p>(1) <em>kmongo</em> had to know how to serialize a <em>Joda</em> date type to a <code class=\"language-plaintext highlighter-rouge\">MongoDB</code> date type:</p>\n\n        <div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">object</span> <span class=\"nc\">JodaDateSerializationCodec</span> <span class=\"p\">:</span> <span class=\"nc\">Codec</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">encode</span><span class=\"p\">(</span><span class=\"n\">writer</span><span class=\"p\">:</span> <span class=\"nc\">BsonWriter</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">DateTime</span><span class=\"p\">?,</span> <span class=\"n\">encoderContext</span><span class=\"p\">:</span> <span class=\"nc\">EncoderContext</span><span class=\"p\">?)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">value</span> <span class=\"p\">==</span> <span class=\"k\">null</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"n\">writer</span><span class=\"p\">.</span><span class=\"nf\">writeNull</span><span class=\"p\">()</span>\n        <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n            <span class=\"n\">writer</span><span class=\"p\">.</span><span class=\"nf\">writeDateTime</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">.</span><span class=\"n\">millis</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">getEncoderClass</span><span class=\"p\">():</span> <span class=\"nc\">Class</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">:</span> <span class=\"nc\">BsonReader</span><span class=\"p\">,</span> <span class=\"n\">decoderContext</span><span class=\"p\">:</span> <span class=\"nc\">DecoderContext</span><span class=\"p\">?):</span> <span class=\"nc\">DateTime</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nc\">DateTime</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">.</span><span class=\"nf\">readDateTime</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>(2) <em>Jackson</em> used by <em>kmongo</em> also had to know how to handle <em>Joda</em> date types,</li>\n      <li>\n        <p>(3) to make things harder sometimes we stored a datetime as a long value, therefore we had to add support for that as well:</p>\n\n        <div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">object</span> <span class=\"nc\">JodaDateSerializationModule</span> <span class=\"p\">:</span> <span class=\"nc\">SimpleModule</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"nf\">init</span> <span class=\"p\">{</span>\n        <span class=\"nf\">addSerializer</span><span class=\"p\">(</span><span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"nc\">JodaDateSerializer</span><span class=\"p\">())</span>\n        <span class=\"nf\">addDeserializer</span><span class=\"p\">(</span><span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"nc\">JodaDateDeserializer</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">JodaDateSerializer</span> <span class=\"p\">:</span> <span class=\"nc\">JsonSerializer</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;()</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">DateTime</span><span class=\"p\">,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?)</span> <span class=\"p\">{</span>\n        <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeObject</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">.</span><span class=\"nf\">toDate</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">JodaDateDeserializer</span> <span class=\"p\">:</span> <span class=\"nc\">JsonDeserializer</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;()</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">deserialize</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">:</span> <span class=\"nc\">JsonParser</span><span class=\"p\">,</span> <span class=\"n\">ctxt</span><span class=\"p\">:</span> <span class=\"nc\">DeserializationContext</span><span class=\"p\">?):</span> <span class=\"nc\">DateTime</span> <span class=\"p\">=</span>\n        <span class=\"k\">when</span> <span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"n\">currentToken</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"nc\">JsonToken</span><span class=\"p\">.</span><span class=\"nc\">VALUE_NUMBER_INT</span> <span class=\"p\">-&gt;</span> <span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"nf\">readValueAs</span><span class=\"p\">(</span><span class=\"nc\">Long</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">).</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"o\">::</span><span class=\"nc\">DateTime</span><span class=\"p\">)</span>\n            <span class=\"k\">else</span> <span class=\"p\">-&gt;</span> <span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"nf\">readValueAs</span><span class=\"p\">(</span><span class=\"nc\">Date</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">).</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"o\">::</span><span class=\"nc\">DateTime</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>(4) finally we stored <code class=\"language-plaintext highlighter-rouge\">BigDecimal</code> values as plain <code class=\"language-plaintext highlighter-rouge\">String</code>, which is not a default behaviour of <em>kmongo</em>, so we had to change it.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>As you can see some of the problems we had to face came from using either old technologies or not using them properly. It turned out there were more issues.</p>\n\n<p>For our entity IDs we usually used an artificial <code class=\"language-plaintext highlighter-rouge\">String</code> value. <code class=\"language-plaintext highlighter-rouge\">MongoDB</code> has special support for it in a form of <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/\"><code class=\"language-plaintext highlighter-rouge\">ObjectId</code></a>\ntype, which we gladly used in our application. But, here a new issue came up - in order to make our integration tests easier to read and write we used <code class=\"language-plaintext highlighter-rouge\">String</code>-type IDs\nnot conformant to <code class=\"language-plaintext highlighter-rouge\">ObjectId</code> restrictions (so for example our user IDs were <code class=\"language-plaintext highlighter-rouge\">user-1</code>, <code class=\"language-plaintext highlighter-rouge\">user-2</code>, etc.).\n<em>Spring Data</em> handles this transparently, but here we had to introduce one more customization. Our entity classes now\nhad to contain a special annotation indicating what serializer to use for our ID fields:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.bson.codecs.pojo.annotations.BsonId</span>\n\n<span class=\"kd\">data class</span> <span class=\"nc\">User</span><span class=\"p\">(</span>\n    <span class=\"nd\">@BsonId</span> <span class=\"nd\">@JsonSerialize</span><span class=\"p\">(</span><span class=\"n\">using</span> <span class=\"p\">=</span> <span class=\"nc\">CustomIdJsonSerializer</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">)</span> <span class=\"kd\">val</span> <span class=\"py\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">name</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">CustomIdJsonSerializer</span> <span class=\"p\">:</span> <span class=\"nc\">StdScalarSerializer</span><span class=\"p\">&lt;</span><span class=\"nc\">String</span><span class=\"p\">&gt;(</span><span class=\"nc\">String</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"k\">false</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">?,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?)</span> <span class=\"p\">=</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">value</span> <span class=\"p\">!=</span> <span class=\"k\">null</span> <span class=\"p\">&amp;&amp;</span> <span class=\"nc\">ObjectId</span><span class=\"p\">.</span><span class=\"nf\">isValid</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">))</span> <span class=\"p\">{</span> <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeObjectId</span><span class=\"p\">(</span><span class=\"nc\">ObjectId</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">))</span> <span class=\"p\">}</span>\n        <span class=\"k\">else</span> <span class=\"p\">{</span> <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeString</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">)</span> <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serializeWithType</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">?,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?,</span> <span class=\"n\">typeSer</span><span class=\"p\">:</span> <span class=\"nc\">TypeSerializer</span><span class=\"p\">?)</span> <span class=\"p\">=</span>\n        <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">gen</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">isEmpty</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Boolean</span> <span class=\"p\">=</span> <span class=\"n\">value</span><span class=\"p\">.</span><span class=\"nf\">isEmpty</span><span class=\"p\">()</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">acceptJsonFormatVisitor</span><span class=\"p\">(</span><span class=\"n\">visitor</span><span class=\"p\">:</span> <span class=\"nc\">JsonFormatVisitorWrapper</span><span class=\"p\">?,</span> <span class=\"n\">typeHint</span><span class=\"p\">:</span> <span class=\"nc\">JavaType</span><span class=\"p\">?)</span> <span class=\"p\">=</span> <span class=\"nf\">visitStringFormat</span><span class=\"p\">(</span><span class=\"n\">visitor</span><span class=\"p\">,</span> <span class=\"n\">typeHint</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>With the basics set up we could now focus on how to make the <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes work with as little effort as possible. We decided to create a base <code class=\"language-plaintext highlighter-rouge\">BaseRepository</code>\nclass letting us write concrete <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes easier:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">abstract</span> <span class=\"kd\">class</span> <span class=\"nc\">BaseRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;(</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">mongoDatabase</span><span class=\"p\">:</span> <span class=\"nc\">MongoDatabase</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">collectionName</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">clazz</span><span class=\"p\">:</span> <span class=\"nc\">Class</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span>\n<span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">open</span> <span class=\"k\">fun</span> <span class=\"nf\">findById</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">findOne</span><span class=\"p\">(</span><span class=\"nf\">eq</span><span class=\"p\">(</span><span class=\"s\">\"_id\"</span><span class=\"p\">,</span> <span class=\"n\">id</span><span class=\"p\">.</span><span class=\"nf\">maybeObjectId</span><span class=\"p\">()))</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">findOne</span><span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">:</span> <span class=\"nc\">Bson</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">withCollection</span> <span class=\"p\">{</span>\n        <span class=\"nf\">find</span><span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">).</span><span class=\"nf\">toMono</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">R</span><span class=\"p\">&gt;</span> <span class=\"nf\">withCollection</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">:</span> <span class=\"nc\">MongoCollection</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">R</span><span class=\"p\">):</span> <span class=\"nc\">R</span> <span class=\"p\">=</span>\n                <span class=\"n\">mongoDatabase</span>\n                    <span class=\"p\">.</span><span class=\"nf\">getCollection</span><span class=\"p\">(</span><span class=\"n\">collectionName</span><span class=\"p\">,</span> <span class=\"n\">clazz</span><span class=\"p\">)</span>\n                    <span class=\"p\">.</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Finally we wrote the <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes. A rewritten version of the <code class=\"language-plaintext highlighter-rouge\">UserRepository</code> mentioned at the beginning of this section looked like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Context</span>\n<span class=\"kd\">class</span> <span class=\"nc\">UserRepository</span><span class=\"p\">(</span>\n    <span class=\"n\">mongoDatabase</span><span class=\"p\">:</span> <span class=\"nc\">MongoDatabase</span>\n<span class=\"p\">):</span> <span class=\"nc\">BaseRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;(</span><span class=\"n\">mongoDatabase</span><span class=\"p\">,</span> <span class=\"s\">\"users\"</span><span class=\"p\">,</span> <span class=\"nc\">User</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">findFirstByTypeOrderByNameDesc</span><span class=\"p\">(</span><span class=\"n\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nf\">withCollection</span> <span class=\"p\">{</span>\n            <span class=\"nf\">find</span><span class=\"p\">(</span><span class=\"nf\">and</span><span class=\"p\">(</span><span class=\"nf\">eq</span><span class=\"p\">(</span><span class=\"s\">\"type\"</span><span class=\"p\">,</span> <span class=\"n\">type</span><span class=\"p\">)))</span>\n                <span class=\"p\">.</span><span class=\"nf\">sort</span><span class=\"p\">(</span><span class=\"nf\">descending</span><span class=\"p\">(</span><span class=\"s\">\"name\"</span><span class=\"p\">))</span>\n                <span class=\"p\">.</span><span class=\"nf\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"p\">.</span><span class=\"nf\">toMono</span><span class=\"p\">()</span>\n        <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"fear-of-the-dark\">Fear of the Dark</h3>\n\n<p><a href=\"https://spockframework.org/spock/docs/2.0/index.html\">Spock</a> is our framework of choice for writing tests. We still tend to use it even in <code class=\"language-plaintext highlighter-rouge\">Kotlin</code> applications,\nalthough sometimes the resulting code is not as clear as it’d be if not for <code class=\"language-plaintext highlighter-rouge\">Groovy</code> (`coroutines!). So how does\n<em>Micronaut</em> work with <em>Spock</em>? Actually, quite well.</p>\n\n<p>For testing there is a <a href=\"https://github.com/micronaut-projects/micronaut-test\">micronaut-test</a> project, which provides testing extensions for <em>Spock</em>\nand many other testing libraries. <a href=\"https://docs.spring.io/spring-framework/docs/current/reference/html/testing.html\">The general approach to writing test cases with Spring</a> which we were familiar with,\nis very similar in <em>micronaut-test</em>. Let’s have a look at a simple test case:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@MicronautTest</span> <span class=\"c1\">// 1</span>\n<span class=\"kd\">class</span> <span class=\"nc\">SimpleIntSpec</span> <span class=\"kd\">extends</span> <span class=\"n\">Specification</span> <span class=\"o\">{</span>\n    <span class=\"nd\">@Inject</span> <span class=\"c1\">// 2</span>\n    <span class=\"n\">UserService</span> <span class=\"n\">userService</span>\n\n    <span class=\"kt\">def</span> <span class=\"s2\">\"should persist a user\"</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n        <span class=\"nl\">given:</span>\n        <span class=\"n\">userService</span><span class=\"o\">.</span><span class=\"na\">createUser</span><span class=\"o\">(</span><span class=\"s2\">\"user-1\"</span><span class=\"o\">,</span> <span class=\"s2\">\"John\"</span><span class=\"o\">,</span> <span class=\"s2\">\"Doe\"</span><span class=\"o\">)</span>\n\n        <span class=\"nl\">when:</span>\n        <span class=\"kt\">def</span> <span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"n\">userService</span><span class=\"o\">.</span><span class=\"na\">getUser</span><span class=\"o\">(</span><span class=\"s2\">\"user-1\"</span><span class=\"o\">)</span>\n\n        <span class=\"nl\">then:</span>\n        <span class=\"n\">user</span><span class=\"o\">.</span><span class=\"na\">firstName</span> <span class=\"o\">==</span> <span class=\"s2\">\"John\"</span>\n        <span class=\"n\">user</span><span class=\"o\">.</span><span class=\"na\">lastName</span> <span class=\"o\">==</span> <span class=\"s2\">\"Doe\"</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<p>There are two interesting things in this test case:</p>\n<ul>\n  <li>(1) <code class=\"language-plaintext highlighter-rouge\">@MicronautTest</code> is an annotation you have to put in your test classes to start <em>Micronaut</em> application,</li>\n  <li>(2) <code class=\"language-plaintext highlighter-rouge\">@Inject</code> is <a href=\"https://micronaut.io/\">Micronaut</a>’s version of <code class=\"language-plaintext highlighter-rouge\">@Autowired</code> (or… <code class=\"language-plaintext highlighter-rouge\">@Inject</code>, which is also supported by <code class=\"language-plaintext highlighter-rouge\">Spring</code>). Be aware that since\n<em>Micronaut</em> <code class=\"language-plaintext highlighter-rouge\">3.0.0</code> you should use <code class=\"language-plaintext highlighter-rouge\">@jakarta.inject.Inject</code> annotation instead of the former <code class=\"language-plaintext highlighter-rouge\">@javax.inject.Inject</code>.</li>\n</ul>\n\n<p>If your tests make API calls to your application via REST endpoints, and you run your web container on a random port (which is common), then the way to retrieve it\nis through the use of the injected <code class=\"language-plaintext highlighter-rouge\">EmbeddedServer</code>:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@MicronautTest</span>\n<span class=\"kd\">class</span> <span class=\"nc\">ApiIntSpec</span> <span class=\"kd\">extends</span> <span class=\"n\">Specification</span> <span class=\"o\">{</span>\n    <span class=\"nd\">@Inject</span>\n    <span class=\"n\">EmbeddedServer</span> <span class=\"n\">server</span>\n\n    <span class=\"kt\">def</span> <span class=\"s2\">\"should create a user using API call\"</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n        <span class=\"nl\">given:</span>\n        <span class=\"kt\">def</span> <span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://localhost:{$server.port}/users\"</span>\n\n        <span class=\"nl\">when:</span>\n        <span class=\"c1\">// here goes your test...</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"money\">Money</h2>\n\n<p>As a side effect, an additional benefit you get when you use <em>Micronaut</em> is a faster development cycle. As stated at the\nbeginning of this post, one of the main features of this framework is faster startup. Therefore, when writing test cases and then running tests,\ntheir execution time is lower than their Spring equivalent. This may not be significant if your tests are few, but sooner or later\ntheir number will grow and then the speed will become more visible and important. For large codebases time savings can be really impressive.</p>\n\n<h2 id=\"should-i-stay-or-should-i-go\">Should I stay or should I go</h2>\n\n<p>The experience we gained during migration to <em>Micronaut</em> gave us more courage and assurance. So when the time came to decide what technology\nto use for a quite large greenfield project, we didn’t hesitate (well, we actually did, but not for long).\nSix months later with the system running in the production environment we’re happy we started that long journey. And if you’re considering\n<em>Micronaut</em> for one of your projects, I can wholeheartedly recommend: go for it.</p>\n","contentSnippet":"Micronaut is one of the new application frameworks that have recently sprang up. It promises\nlow memory usage and faster application startup. At Allegro we decided to give it a try. In this article we’ll learn what\ncame out of it and if it’s worth considering when creating microservices-based systems.\nParadise city\nAt Allegro we run a few hundred microservices, most of which use Spring Framework. We also have services created in other technologies.\nAnd to make things more complicated we run them on a few different types of clouds - our own Mesos-based as well as private and public k8s-based ones.\nTherefore in order for all of it to work consistently and smoothly we created a number of supporting libraries and at the same time we defined a kind of\ncontract for all services. This way, if there is ever a need or will to create a service with a new shiny technology, it should be feasible with as little\nwork as possible. You can read more about this approach in a great article by Piotr Betkier.\n(You Gotta) Fight for Your Right (To Party!)\nIn order to be up to date with current technologies, at Allegro we run hackathons where we try out the “trendy” solutions. Over a year ago\nwe decided to taste Micronaut. The framework represents one of the new approaches to some of the inherent problems of existing solutions:\nit steers clear of using Java Reflection and does as much as it can at compile or rather build time. Major things achieved this way are:\nlower memory usage - Java Reflection in most current JDK implementations is a memory hog; Micronaut has its own implementation of Java Reflection-like API\nwhich doesn’t suffer from that problem,\nfaster startup - Java Reflection is also not a speed daemon; doing things ahead of time means less has to be done at runtime,\nability to create native apps - GraalVM, another new kid on the block, allows creating native binaries out of a JVM-based application; however, there\nare some caveats and one of them is… Java Reflection (basically if your application uses it, it has to provide some metadata for the native compiler). Since\nMicronaut has its own implementation, the problem is simply non-existent.\nWe wanted to see how difficult it is to create a new microservice with Micronaut that would run on our on-premise cloud and do something meaningful. So during\nour hackathon we defined the following goals for our simple app:\nit should be possible to deploy the app on our on-premise cloud,\nthe app should provide and use basic functionalities, such as:\n    \ntelemetry,\nconfiguration management,\nREST endpoints,\nability to call other microservices,\nability to send messages to Hermes,\ndatabase access (we voted for MongoDB),\n(optionally) ability to be compiled into a native binary with GraalVM.\nAfter a very satisfying hackathon we carried the day. Our microservice had all the above-mentioned functionalities - some of them obviously\nin a makeshift form, but that didn’t matter. We achieved all the goals.\nHighway to Hell\nThe result of the hackathon pushed us forward to make something even bolder. We wanted to have a real Micronaut-based application in our production environment.\nTo make things harder - we wanted to convert an existing Spring-based system to a Micronaut-based one. Though we reached our destination, the road\nwas quite bumpy. Let’s see what awaits those who take that path.\nParanoid\nTo ease a migration from Spring a special micronaut-spring project has been created.\nIt supports a limited selection of Spring annotations and functionality so that in theory one can just replace Spring dependencies with Micronaut ones.\nSpecifically among the most interesting features are:\nstandard inversion of control annotations: @Component, @Service, @Repository,@Bean, @Autowired, @Configuration, @Primary and many others\nare converted into their Micronaut counterparts,\nstandard Spring interfaces: @Environment, @ApplicationEventPublisher, @ApplicationContext, @BeanFactory and their @*Aware versions are also\nadapted to their Micronaut counterparts,\nMVC controller annotations: @RestController, @GetMapping, @PostMapping and many others are converted into their Micronaut counterparts.\nThis makes the whole exercise simpler, but unfortunately it also comes at a price. Not all features are supported (e.g. Spring’s @PathVariable is not)\nand for those which are, they sometimes have subtle differences. For this reason oftentimes you simply have to revert to the regular manual code\nconversion. The problem is that this kind of approach will lead you to a mixed solution - you’ll have both Micronaut and Spring annotations in your code.\nAnd then a question arises: which annotations should I use for the newly created code? Do we stick with the old Spring annotations if there is even\none instance of it in the current codebase? Or maybe treat this old code as a “necessary evil” and always put Micronaut annotations for the added functionality?\nWe came to the conclusion that we did not want to use micronaut-spring at all. This\nof course led to more work, but in the end we think it was worth it. The converted application does not have any Spring dependencies, no “technical debt”.\nSad but True\nOne of the things not covered at all by micronaut-spring is exception handling in MVC.\nIn Spring our handlers looked something like this:\n\nimport org.springframework.http.HttpStatus.BAD_REQUEST\n\n@ControllerAdvice\n@Order(Ordered.HIGHEST_PRECEDENCE)\nclass DefaultExceptionHandler {\n\n    @ExceptionHandler(SomeException::class)\n    @ResponseBody\n    fun handleSomeException(e: SomeException): ResponseEntity<*> = ResponseEntity(ApiError(e.message), BAD_REQUEST)\n}\n\n\nIn Micronaut exception handling can be done locally (i.e. functions handling exception will only be used for the exceptions thrown by the controller the\nfunctions are defined in) or globally. Since our Spring handlers acted globally, the equivalent Micronaut code is as follows:\n\nimport io.micronaut.http.HttpStatus.BAD_REQUEST\nimport io.micronaut.http.annotation.Error as HttpError\n\n@Controller\nclass DefaultExceptionHandler {\n\n    @Error(global = true)\n    fun handleSomeException(e: SomeException): HttpResponse<*> =\n        HttpResponseFactory.INSTANCE.status(BAD_REQUEST, ApiError(e.message))\n}\n\n\nDirty Deeds Done Dirt Cheap\nAt Allegro we use many different types of databases. The application of this exercise used MongoDB. As it turned out we couldn’t have chosen worse. Don’t get me\nwrong - Micronaut supports most of the databases out there, but not all are treated equally well.\nSince our system used Spring Data, we tried to find something similar from the Micronaut world. Micronaut Data\nis - as its authors say - “inspired by GORM and Spring Data”. Unfortunately the inspiration doesn’t go too far. And in case of MongoDB it actually doesn’t even\ntake a step. Instead, we used Micronaut MongoDB library. This simple project will provide\nyour services only with either a blocking MongoClient or a\nreactive MongoClient\nalong with some healthchecks. Not enough even for a modest application.\nFortunately some good people created kmongo - a little library that helped us a lot in converting the database access part of our app.\nAt the end of the day, however, we had to create some support code to ease the migration.\nThe original application database access code was in the form of reactive repositories:\n\nimport org.springframework.data.annotation.Id\nimport org.springframework.data.mongodb.core.mapping.Document\n\n@Document(collection = \"users\")\ndata class User(\n    @Id val id: String,\n    val name: String,\n    val type: String\n)\n\nclass UserRepository: ReactiveMongoRepository<User, String> {\n    fun findFirstByTypeOrderByNameDesc(type: String): Mono<User>\n}\n\n\nWe wanted to preserve the interface and as much code as possible. Here is what we had to do to get this effect.\nFirst we decided that our components would use MongoDatabase rather than MongoClient offered by Micronaut MongoDB.\nWe had only one database so that was an obvious choice.\n\n@Factory\nclass MongoConfig {\n    @Singleton\n    fun mongoDatabase(mongoClient: MongoClient, configuration: DefaultMongoConfiguration): MongoDatabase =\n        mongoClient.getDatabase(configuration.connectionString.get().database)\n}\n\n\nThen there was the question of configuring kmongo. It wasn’t as straightforward as we’d thought it would be. Let’s take a look at the\nfinal code.\n\n@Factory\nclass KMongoFactory {\n\n    @Singleton\n    fun kCodecRegistry(): CodecRegistry {\n        ObjectMappingConfiguration.addCustomCodec(JodaDateSerializationCodec) // 1 - custom Joda DateTime coded\n        KMongoConfiguration.registerBsonModule(JodaModule())                  // 2 - register default Joda module\n        KMongoConfiguration.registerBsonModule(JodaDateSerializationModule)   // 3 - register custom Joda module\n        with(KMongoConfiguration.bsonMapper.factory as BsonFactory) {         // 4 - change BigDecimal handling\n            disable(BsonGenerator.Feature.WRITE_BIGDECIMALS_AS_DECIMAL128)\n            enable(BsonGenerator.Feature.WRITE_BIGDECIMALS_AS_STRINGS)\n        }\n        return ClassMappingType.codecRegistry(MongoClientSettings.getDefaultCodecRegistry())\n    }\n}\n\n\nMongoDB driver expects a CodecRegistry which defines how to encode a Java object into Mongo BSON, so that it can be persisted in a database. By default\nkmongo supports a simple, Jackson based converter. However, there were a few issues in\nour application which forced us to create some customizations:\nJoda date types in entity classes - our app has a long history and it still uses Joda date types.\nUnfortunately they do not work with kmongo, so we had to teach it how to handle them. It required a few steps.\n    \n(1) kmongo had to know how to serialize a Joda date type to a MongoDB date type:\n\nobject JodaDateSerializationCodec : Codec<DateTime> {\n    override fun encode(writer: BsonWriter, value: DateTime?, encoderContext: EncoderContext?) {\n        if (value == null) {\n            writer.writeNull()\n        } else {\n            writer.writeDateTime(value.millis)\n        }\n    }\n\n    override fun getEncoderClass(): Class<DateTime> {\n        return DateTime::class.java\n    }\n\n    override fun decode(reader: BsonReader, decoderContext: DecoderContext?): DateTime {\n        return DateTime(reader.readDateTime())\n    }\n}\n\n        \n(2) Jackson used by kmongo also had to know how to handle Joda date types,\n(3) to make things harder sometimes we stored a datetime as a long value, therefore we had to add support for that as well:\n\nobject JodaDateSerializationModule : SimpleModule() {\n    init {\n        addSerializer(DateTime::class.java, JodaDateSerializer())\n        addDeserializer(DateTime::class.java, JodaDateDeserializer())\n    }\n}\n\nclass JodaDateSerializer : JsonSerializer<DateTime>() {\n    override fun serialize(value: DateTime, gen: JsonGenerator, serializers: SerializerProvider?) {\n        gen.writeObject(value.toDate())\n    }\n}\n\nclass JodaDateDeserializer : JsonDeserializer<DateTime>() {\n    override fun deserialize(parser: JsonParser, ctxt: DeserializationContext?): DateTime =\n        when (parser.currentToken) {\n            JsonToken.VALUE_NUMBER_INT -> parser.readValueAs(Long::class.java).let(::DateTime)\n            else -> parser.readValueAs(Date::class.java).let(::DateTime)\n        }\n}\n\n        \n(4) finally we stored BigDecimal values as plain String, which is not a default behaviour of kmongo, so we had to change it.\nAs you can see some of the problems we had to face came from using either old technologies or not using them properly. It turned out there were more issues.\nFor our entity IDs we usually used an artificial String value. MongoDB has special support for it in a form of ObjectId\ntype, which we gladly used in our application. But, here a new issue came up - in order to make our integration tests easier to read and write we used String-type IDs\nnot conformant to ObjectId restrictions (so for example our user IDs were user-1, user-2, etc.).\nSpring Data handles this transparently, but here we had to introduce one more customization. Our entity classes now\nhad to contain a special annotation indicating what serializer to use for our ID fields:\n\nimport org.bson.codecs.pojo.annotations.BsonId\n\ndata class User(\n    @BsonId @JsonSerialize(using = CustomIdJsonSerializer::class) val id: String,\n    val name: String,\n    val type: String\n)\n\nclass CustomIdJsonSerializer : StdScalarSerializer<String>(String::class.java, false) {\n    override fun serialize(value: String?, gen: JsonGenerator, serializers: SerializerProvider?) =\n        if (value != null && ObjectId.isValid(value)) { gen.writeObjectId(ObjectId(value)) }\n        else { gen.writeString(value) }\n\n    override fun serializeWithType(value: String?, gen: JsonGenerator, serializers: SerializerProvider?, typeSer: TypeSerializer?) =\n        serialize(value, gen, serializers)\n\n    override fun isEmpty(value: String): Boolean = value.isEmpty()\n\n    override fun acceptJsonFormatVisitor(visitor: JsonFormatVisitorWrapper?, typeHint: JavaType?) = visitStringFormat(visitor, typeHint)\n}\n\n\nWith the basics set up we could now focus on how to make the *Repository classes work with as little effort as possible. We decided to create a base BaseRepository\nclass letting us write concrete *Repository classes easier:\n\nabstract class BaseRepository<T>(\n    private val mongoDatabase: MongoDatabase,\n    private val collectionName: String,\n    private val clazz: Class<T>\n) {\n    open fun findById(id: String): Mono<T> = findOne(eq(\"_id\", id.maybeObjectId()))\n\n    fun findOne(filter: Bson): Mono<T> = withCollection {\n        find(filter).toMono()\n    }\n\n    fun <R> withCollection(fn: MongoCollection<T>.() -> R): R =\n                mongoDatabase\n                    .getCollection(collectionName, clazz)\n                    .let(fn)\n}\n\n\nFinally we wrote the *Repository classes. A rewritten version of the UserRepository mentioned at the beginning of this section looked like this:\n\n@Context\nclass UserRepository(\n    mongoDatabase: MongoDatabase\n): BaseRepository<User>(mongoDatabase, \"users\", User::class.java) {\n\n    fun findFirstByTypeOrderByNameDesc(type: String): Mono<User> =\n        withCollection {\n            find(and(eq(\"type\", type)))\n                .sort(descending(\"name\"))\n                .limit(1)\n                .toMono()\n        }\n}\n\n\nFear of the Dark\nSpock is our framework of choice for writing tests. We still tend to use it even in Kotlin applications,\nalthough sometimes the resulting code is not as clear as it’d be if not for Groovy (`coroutines!). So how does\nMicronaut work with Spock? Actually, quite well.\nFor testing there is a micronaut-test project, which provides testing extensions for Spock\nand many other testing libraries. The general approach to writing test cases with Spring which we were familiar with,\nis very similar in micronaut-test. Let’s have a look at a simple test case:\n\n@MicronautTest // 1\nclass SimpleIntSpec extends Specification {\n    @Inject // 2\n    UserService userService\n\n    def \"should persist a user\"() {\n        given:\n        userService.createUser(\"user-1\", \"John\", \"Doe\")\n\n        when:\n        def user = userService.getUser(\"user-1\")\n\n        then:\n        user.firstName == \"John\"\n        user.lastName == \"Doe\"\n    }\n}\n\n\nThere are two interesting things in this test case:\n(1) @MicronautTest is an annotation you have to put in your test classes to start Micronaut application,\n(2) @Inject is Micronaut’s version of @Autowired (or… @Inject, which is also supported by Spring). Be aware that since\nMicronaut 3.0.0 you should use @jakarta.inject.Inject annotation instead of the former @javax.inject.Inject.\nIf your tests make API calls to your application via REST endpoints, and you run your web container on a random port (which is common), then the way to retrieve it\nis through the use of the injected EmbeddedServer:\n\n@MicronautTest\nclass ApiIntSpec extends Specification {\n    @Inject\n    EmbeddedServer server\n\n    def \"should create a user using API call\"() {\n        given:\n        def url = \"http://localhost:{$server.port}/users\"\n\n        when:\n        // here goes your test...\n    }\n}\n\n\nMoney\nAs a side effect, an additional benefit you get when you use Micronaut is a faster development cycle. As stated at the\nbeginning of this post, one of the main features of this framework is faster startup. Therefore, when writing test cases and then running tests,\ntheir execution time is lower than their Spring equivalent. This may not be significant if your tests are few, but sooner or later\ntheir number will grow and then the speed will become more visible and important. For large codebases time savings can be really impressive.\nShould I stay or should I go\nThe experience we gained during migration to Micronaut gave us more courage and assurance. So when the time came to decide what technology\nto use for a quite large greenfield project, we didn’t hesitate (well, we actually did, but not for long).\nSix months later with the system running in the production environment we’re happy we started that long journey. And if you’re considering\nMicronaut for one of your projects, I can wholeheartedly recommend: go for it.","guid":"https://blog.allegro.tech/2021/11/micronaut.html","categories":["tech","backend","performance","micronaut","kotlin","graalvm"],"isoDate":"2021-11-21T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"OAuth rate-limiting","link":"https://blog.allegro.tech/2021/11/oauth-rate-limiting.html","pubDate":"Tue, 09 Nov 2021 00:00:00 +0100","authors":{"author":[{"name":["Marek Walkowiak"],"photo":["https://blog.allegro.tech/img/authors/marek.walkowiak.jpg"],"url":["https://blog.allegro.tech/authors/marek.walkowiak"]},{"name":["Daniel Faderski"],"photo":["https://blog.allegro.tech/img/authors/daniel.faderski.jpg"],"url":["https://blog.allegro.tech/authors/daniel.faderski"]}]},"content":"<p>Every e-commerce platform needs some kind of central authorization system. At <a href=\"https://allegro.tech/\">Allegro</a> we use\nOAuth and have our own implementation that stays true to the\n<a href=\"https://datatracker.ietf.org/doc/html/rfc6749\">RFC</a>. Allegro has millions of users. There are also a lot of requests\nthat go through OAuth services. At some point there comes a need to have better control over how much traffic we want to\nallow in a certain time window, while maintaining full performance of the platform. Here is where the idea of\nrate-limiting comes in handy.</p>\n\n<h2 id=\"prologue\">Prologue</h2>\n\n<p>According to OAuth RFC, to use OAuth you need to be a registered client. Some clients are very small external\nintegrators (simple shops), while others are in a different league and can produce millions of requests per day (Allegro\nmobile and other large partner apps). Every user can create their own OAuth client and use it to integrate with\nDevelopers API. Unfortunately, not all of them do that correctly as per RFC.</p>\n\n<p>Normally such clients are not a big issue, but in certain cases they can generate a lot of unwanted and unneeded\ntraffic. This traffic includes, but is not limited to, creating huge numbers of new access tokens that are then thrown\naway instead of being reused.</p>\n\n<p>According to the RFC, the access tokens generated with most grant types (e.g. authorization code grant) should be reused\nup until their expiration period. When they expire, the\nprovided <a href=\"https://datatracker.ietf.org/doc/html/rfc6749#section-1.5\">refresh token</a> should be used to receive a new\naccess token via refresh token grant.</p>\n\n<p>Some of the clients rarely refresh tokens or even don’t reuse them at all. This causes a lot of unnecessary traffic (\noften in the form of sudden spikes)\nthat can lead to potential issues on both sides. We had pretty good monitoring of this issue, but we needed better tools\nto deal with that problem as well as to educate the clients to make proper use of OAuth.</p>\n\n<h2 id=\"planning-the-solution\">Planning the solution</h2>\n\n<p>Before tackling the problem directly we needed a little more information and careful planning. Firstly, we wanted to\nmake sure that our solution would solve the problem. Secondly, blocking too many clients could end up in disaster. To be\ncertain that our solution was error-free, we started by making sure that we knew what we want to achieve. Here are the\nproperties we expected from our solution:</p>\n\n<ul>\n  <li>It cannot block trusted clients such as Allegro apps.</li>\n  <li>It should not negatively affect performance.</li>\n  <li>It should be configurable per client since different clients have different traffic characteristics.</li>\n  <li>It should be able to distinguish between user and non-user use of OAuth. Client credentials grant used without context\nof a user, for example should be treated differently than user-based authorization code grant. In effect, the limiting\n“per user” should be introduced in the second case.</li>\n  <li>It should directly cause an improvement in traffic spikes.</li>\n  <li>It should work well in a highly distributed environment with dozens of service instances and database nodes.</li>\n  <li>It cannot be too costly or require too much additional infrastructure like additional databases, external systems,\netc.</li>\n</ul>\n\n<h2 id=\"tackling-the-problem\">Tackling the problem</h2>\n\n<p>To meet those needs we needed a robust solution. Since RFC leaves a lot of room for implementation by end-users, it does\nnot specify how such rate-limiting should work.</p>\n\n<p>As with every problem of such kind, it’s worth starting with in-depth research of existing solutions. There are various\nstrategies and approaches to this problem in many different scenarios, but only a few of them were applicable to our\ncase and enabled us to fulfill our goals.</p>\n\n<p>As usual, we also explored the existing implementations of such solutions in the form of enterprise or open-source\nlibraries and frameworks. Unfortunately, we did not find any that would meet all of our needs and at the same time be\nflexible enough to easily integrate into our ecosystem. It’s also worth noting that we were limited by certain\nconstraints such as long-term costs of additional resources.</p>\n\n<p>In the end, we decided to go with implementing our own solution. There were several algorithms to choose from that could\nserve as its base:</p>\n\n<ol>\n  <li>Precise query-based per-user counter — query the database for a token count for each request. Not suitable, because\nit would cause way too much database traffic.</li>\n  <li>Fixed window based on TTL documents — uses a rate-limit counter that is cleared every fixed period. While less\nchallenging for the database, it shares a common vulnerability with the first algorithm: sudden TTL-caused spikes in\nallowed requests that consequently cause all of the following requests to be blocked (rate-limit exhaustion).</li>\n  <li>Sliding window log — stores all requests as a timestamped log entry in a database, that are later aggregated when\nneeded. Too costly, because again it would cause too heavy a load on the database.</li>\n  <li>Sliding window — uses a rate-limit counter that is a time-based weighted moving window. It has the advantage of\nrelatively low database load of the fixed window algorithm without its spike-related flaws.</li>\n</ol>\n\n<p>We carefully weighed the pros and cons of each of those options and finally decided to make a PoC of the fourth option:\nthe sliding window algorithm.</p>\n\n<p>The algorithm is based on counters that each hold a count of requests in their respective time frames. At each point of\ntime, an abstract window is calculated from two neighboring frames: current and previous. The further away the window is\nfrom the previous frame, the more important the counter from the current frame is. The weight is based strictly on\nproportions. If, for example, the span of the current window is 75% of the previous frame and 25% of the current frame,\nthen the value of the current rate-limit counter is a sum of 75% of the previous frame counter and 25% of the current\nframe counter. The results are put into a hashmap with users as keys and counters as values, which acts as a cache.</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/sliding-window-algorithm.png\" alt=\"Sliding window algorithm\" />\nIn the picture above the current counter value would be:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>value = previousCounter * partOfPreviousFrame + currentCounter * partOfCurrentFrame\nvalue = 12 * 0.75 + 5 * 0.25 = 10.25\n</code></pre></div></div>\n\n<p>The time complexity of the sliding window algorithm is O(1) since it’s a simple hashmap get operation, while its space\ncomplexity is O(n) where n is the number of user counters held by an instance. It works well in distributed systems\nbecause it’s based on simple counters that are relatively easy to synchronize. Its cost is low since we only need to\nstore simple counters and can easily minimize database queries, as well as improve performance with caching in a\nstraightforward manner. Thanks to the fact that the sliding window takes into account the time relative to the current\ntimestamp, we can flatten the token creation spikes. Its main disadvantage is that it indirectly relies on the even\ndistribution of requests in time, which makes it less sensitive to spike traffic within one time window.</p>\n\n<h2 id=\"following-the-plan\">Following the plan</h2>\n\n<h3 id=\"remote-state-synchronization\">Remote state synchronization</h3>\n\n<p>There are two kinds of states that need to be tracked for the rate-limiting to work. The first one is the global state.\nIt’s basically a global counter of all requests made by a client to rate-limited OAuth endpoints within a certain\nperiod. This state is persisted in the database and from its perspective represents the most recent rate-limit status.</p>\n\n<p>The second type is the local state of each instance, one part of which is the local counter of incoming requests for a\nclient within a time window. Since instances are not directly connected to the client and every one of them can make\nrequests to different instances concurrently, the local counter of each instance for that client will most probably have\na different value. Rate-limiting is a global functionality in the sense that we are interested in the value of global\ncounters and not of individual counters. Consider the following scenario: we would like to set a rate limit of 50 RPS\nfor a client while having 10 instances. If we relied only on local counters to do the rate-limiting, it would be\npossible for the traffic of 500 RPS to be split evenly between instances and not trigger rate-limiting - and that’s not\nthe result we are aiming for.</p>\n\n<p>To decide whether to reject an incoming request or not, the local instance needs a second part of the state - a global\ncounter. When making a decision, it sums both the local and the global ones to check whether new requests would go\nbeyond the rate-limit boundaries.</p>\n\n<p>Consequently, the instances need to be aware of the requests made to the others and for that to work, the global counter\nneeds to be updated regularly. Each instance will periodically try to flush the sum of global and local counters from\ntheir state to the database. Here is where things get tricky. If several instances are trying to save different global\ncounters, there is a risk that one of them will attempt to overwrite the changes that were just saved by the other. To\ncounter that, we use a mechanism called optimistic locking that is\ndescribed <a href=\"#resolving-the-conflicts\">later in this post</a>. Once the state is persisted successfully, the local state is\ncleared. At this point, it consists of the incoming requests counter that is equal to 0 and the global snapshot counter\nthat is equal to the value persisted in the database for that client.</p>\n\n<h3 id=\"caching-clients-global-state\">Caching clients’ global state</h3>\n\n<p>Any instance should be aware of the rate limit state for the whole cluster. It represents the global number of requests\nmade by a particular client and its users. We keep a mapping between a pair (client_id, username) and the number of\nrequests made in memory, which makes our algorithm efficient. There is one caveat: keeping all the clients and their\nusers would take too much space, so we only keep those clients and users who are actively making requests to the OAuth\nserver. As soon as they stop calling our servers we delete them from the instance’s cache.</p>\n\n<h3 id=\"sharing-the-state\">Sharing the state</h3>\n\n<p>Our internal OAuth service works in a distributed manner. There are many instances of the OAuth servers and we should\nhave mechanisms to coordinate rate-limiting between them. In practice, it means that if a client is making a request to\nserver instance A, the server instance B should consider it when calculating the allowed number of requests in the\ncurrent time window. Below are the key points that sum up this description:</p>\n\n<ul>\n  <li>We have a global request counter per each client/user.</li>\n  <li>The counter is stored in the Mongo database.</li>\n  <li>Each instance shares the counter - it reads and writes its current value.</li>\n  <li>The counter depicts the window in the current timestamp.</li>\n</ul>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"clientId\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"some-client\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"username\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"some-user\"</span><span class=\"w\">\n  </span><span class=\"p\">},</span><span class=\"w\">\n  </span><span class=\"nl\">\"version\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">NumberLong(</span><span class=\"mi\">405</span><span class=\"err\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"expire\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">ISODate(</span><span class=\"s2\">\"2021-05-29T14:24:01.376Z\"</span><span class=\"err\">)</span><span class=\"p\">,</span><span class=\"w\">\n  </span><span class=\"nl\">\"requestCounters\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"1622211780\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"1622211840\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>The counter consists of a few fields:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">_id</code> - to uniquely identify the client and the user connected with the request.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">version</code> - for the optimistic locking purposes.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">expire</code> - used to remove the counter if it has no updates, which means that the client is not currently creating new\ntokens.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">requestCounters</code> - mapping of the number of requests at consecutive points in time.</li>\n</ul>\n\n<p>The counter is cached so we don’t have to fetch it each time the request from the client is made, since it would kill\nperformance. Each instance refreshes the counter asynchronously - reads and writes to the Mongo database are made in a\ndedicated thread. Each one also holds the counters only for the clients they have to handle. A particular instance\nremoves the client’s state after it stops sending requests.</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/sharing-the-state.png\" alt=\"Sharing the state example\" />\n<img src=\"/img/articles/2021-11-09-oauth-rate-limiting/sharing-the-state-table.png\" alt=\"Sharing the state example table\" /></p>\n\n<p>This table depicts an example flow of counters on two instances (A, B) of our OAuth service. Each of them has its own\ncounter\n(accordingly <strong>cnt1</strong> and <strong>cnt2</strong>), <strong>mng</strong> is a state in the sharded Mongo database and <strong>total req</strong> is the sum of\nall requests made to all instances.</p>\n\n<p>The state of each instance consists of two counters (<code class=\"language-plaintext highlighter-rouge\">g</code> / <code class=\"language-plaintext highlighter-rouge\">i</code>). The first one (<code class=\"language-plaintext highlighter-rouge\">g</code> for “global”) describes how many\nrequests have been globally made and are persisted in the database. The second one tracks how many requests have come to\nthat instance since the last flush (<code class=\"language-plaintext highlighter-rouge\">i</code> for “in-flight”). The total number of requests from its perspective is the sum\nof <code class=\"language-plaintext highlighter-rouge\">g</code> and <code class=\"language-plaintext highlighter-rouge\">i</code> and it is the value that is going to be persisted.</p>\n\n<p>Below is the description of what happens in this scenario, step by step:</p>\n\n<ol>\n  <li>There is an initial state with no requests made to either instance.</li>\n  <li>One request is made to instance A and its i<sub>A</sub> counter is increased to 1.</li>\n  <li>Next, instance A pushes its total counter to the database, setting its <code class=\"language-plaintext highlighter-rouge\">g</code> counter to 1 and its <code class=\"language-plaintext highlighter-rouge\">i</code> counter to 0.</li>\n  <li>Instance B periodically pulls the <code class=\"language-plaintext highlighter-rouge\">g</code> counter from the database. Now, the new counter is pulled and the instance’s\ncurrent state is <code class=\"language-plaintext highlighter-rouge\">g</code> = 1, i<sub>B</sub> = 0.</li>\n  <li>Next, instance A receives one request, and at the same time, instance B receives three requests. At this time\ninstance A knows of 2 requests, 1 of which came since the last flush. Instance B knows of 4 requests, 3 of them came\nsince the last flush.</li>\n  <li>Instance B pushes its total counter (3+1=4) to Mongo. As a consequence, its state is set to (4 / 0). Instance A has\nnot yet flushed its counter to persistent storage. Remember that both instances do it independently whenever a fixed\ninterval passes.</li>\n  <li>Now, Instance A tries to push its state (1+1=2) to the database. If the push was successful, it would overwrite the\nstate previously written by the instance B (in step 5), resulting in data loss. We want our counters to be precise so\nwe need to use optimistic locking here. It causes the push to fail, if the version of its state differs from the\nversion persisted in the database. If that scenario occurs, the instance knows that it should refresh its global\ncounter before trying to save it again.</li>\n  <li>Instance A refreshed its state. It is now equal to (4/1), which means 4 requests are persisted in the database and 1\nthat has not been flushed yet. Now it can safely push its total counter (4+1=5) to the database and set its state\nto (5/0).</li>\n  <li>At this point both instances have pushed their state to the database. Notice, however, that instance B has not yet\npulled the counter written by instance B in the previous step.</li>\n  <li>After the last pull, the counter states on both instances are consistent with the database state and reflect the\nglobal number of requests made by a client.</li>\n</ol>\n\n<h3 id=\"persisting-the-state\">Persisting the state</h3>\n\n<p>To properly persist the state in a distributed environment with minimal impact on application performance, we needed to\ntake several strategies into consideration.</p>\n\n<h4 id=\"resolving-the-conflicts\">Resolving the conflicts</h4>\n\n<p>As we’ve already mentioned we use optimistic locking to prevent the state from being overwritten by the different\ninstances. It’s quite a common problem in a the world of distributed systems. It works by using version numbers kept in\nMongoDB, which designates how many updates have been made from the beginning of its creation. After each update the\nversion increases by one.</p>\n\n<p>Before save to the database:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n   </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"err\">...</span><span class=\"p\">},</span><span class=\"w\">\n   </span><span class=\"nl\">\"version\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"nl\">\"requestCount\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"err\">...</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>After the save:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n   </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"err\">...</span><span class=\"p\">},</span><span class=\"w\">\n   </span><span class=\"nl\">\"version\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"nl\">\"requestCount\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"w\">\n   </span><span class=\"err\">...</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>But how does a particular instance save the document atomically? How does it know that there was an update made by\nanother instance in the meantime? For this purpose, we use a Mongo query to do\nthe <a href=\"https://en.wikipedia.org/wiki/Compare-and-swap\">CAS</a> update that looks like:</p>\n\n<div class=\"language-js highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">ratelimits</span><span class=\"p\">.</span><span class=\"nx\">update</span><span class=\"p\">(</span>\n <span class=\"o\">&lt;</span><span class=\"nx\">filter</span> <span class=\"nx\">query</span><span class=\"o\">&gt;</span>\n <span class=\"o\">&lt;</span><span class=\"nx\">update</span> <span class=\"nx\">operation</span><span class=\"o\">&gt;</span>\n<span class=\"p\">)</span>\n\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">ratelimits</span><span class=\"p\">.</span><span class=\"nx\">update</span><span class=\"p\">(</span>\n <span class=\"p\">{</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"p\">{...},</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">version</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"mi\">1</span>\n <span class=\"p\">},</span>\n <span class=\"p\">{</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">version</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"mi\">2</span><span class=\"p\">,</span>\n   <span class=\"dl\">\"</span><span class=\"s2\">requestCount</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"mi\">10</span>\n <span class=\"p\">}</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If the query returns 0 elements updated we know there was a collision and there was a concurrent save which changed the\nstate. If this happens we need to:</p>\n\n<ol>\n  <li>Update state from database to local instance,</li>\n  <li>Apply inflight recorded changes (the ones not yet persisted to the database),</li>\n  <li>Save the state,</li>\n  <li>If the query returns 1 element updated, the save was successful without any collisions,</li>\n</ol>\n\n<h4 id=\"saving-in-batches\">Saving in batches</h4>\n\n<p>As we already mentioned, a single instance of the OAuth server handles many clients. So it has to synchronize the state\nfor each of them. Doing the above save operation for each client individually would cause a massive number of queries\nand would quickly saturate our resources. That’s why we\nuse <a href=\"https://docs.mongodb.com/manual/core/bulk-write-operations/#bulk-write-operations\">Mongo bulk operations</a>. They\nallow defining many operations in a single query.</p>\n\n<h2 id=\"going-into-production\">Going into production</h2>\n\n<h3 id=\"dry-run\">Dry-run</h3>\n\n<p>Before actually blocking the clients we needed a way to check real outputs from our solution. We implemented the dry-run\nmode, which enabled us do this without affecting any clients, while also giving us metrics depicting which of them would\nbe blocked given a particular limit and time window. We took the following steps:</p>\n\n<ol>\n  <li>Set high rate limit up to the point when no clients would be blocked (running in dry-run mode all the time).</li>\n  <li>Lowered the threshold to the point that showed us the outstanding clients, while not blocking the others.</li>\n  <li>Communicated with clients abusing our future rate limit policy and gave them a chance to optimize the way they use\nthe OAuth server.</li>\n  <li>Switched modes from dry-run to actively blocking the clients.</li>\n</ol>\n\n<h3 id=\"canary-deployment\">Canary deployment</h3>\n\n<p>It would be risky to deploy this kind of feature to the whole OAuth cluster without ensuring it’s properly working on a\nfew instances. We used canary deployment which allows deploying a version of a service to only a few production\ninstances. During this deployment, we monitored CPU usage and response time metrics. After ensuring there was meaningful\ndisparity we rolled out the full feature to all production instances.</p>\n\n<h3 id=\"observability\">Observability</h3>\n\n<p>To monitor and verify our solution we needed a bunch of metrics telling us how many clients were affected by rate limit\npolicy and which of them are getting closer to being blocked. In practice, two charts shown below were enough to monitor\ntheir behaviour:</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/ratelimit-denied-rate.png\" alt=\"Ratelimit Denied Rate\" /></p>\n\n<p>This chart shows clients and their blocked rate. Each color depicts a different blocked client.</p>\n\n<p><img src=\"/img/articles/2021-11-09-oauth-rate-limiting/ratelimit-allowed-rate.png\" alt=\"Ratelimit Allowed Rate\" /></p>\n\n<p>This one on the other hand depicts the allowed rate. The limit line is helpful to see if any client is getting closer to\nit and will be blocked soon. It’s worth mentioning that the default rate limit policy isn’t sufficient for all of the\nclients. Some of them require special treatment and for them we can configure different thresholds, which is why a few\nof them go beyond the actual default limit (the red line).</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>Rate-limiting is a common problem, but surprisingly not so trivial to solve, especially in a high-scale, distributed\nenvironment. Plenty of popular solutions can be found, but most of them deal with it only from a perspective of a single\nmachine and were not well-suited for our system. Coming up with the above approach took quite a bit of research and\nplanning, but in the end, its deployment allowed us to effectively achieve our goal. We are pretty content with the\nfinal product, as it is both effective and fast, but are constantly tweaking it and looking for new ways to optimize\nthat process.</p>\n","contentSnippet":"Every e-commerce platform needs some kind of central authorization system. At Allegro we use\nOAuth and have our own implementation that stays true to the\nRFC. Allegro has millions of users. There are also a lot of requests\nthat go through OAuth services. At some point there comes a need to have better control over how much traffic we want to\nallow in a certain time window, while maintaining full performance of the platform. Here is where the idea of\nrate-limiting comes in handy.\nPrologue\nAccording to OAuth RFC, to use OAuth you need to be a registered client. Some clients are very small external\nintegrators (simple shops), while others are in a different league and can produce millions of requests per day (Allegro\nmobile and other large partner apps). Every user can create their own OAuth client and use it to integrate with\nDevelopers API. Unfortunately, not all of them do that correctly as per RFC.\nNormally such clients are not a big issue, but in certain cases they can generate a lot of unwanted and unneeded\ntraffic. This traffic includes, but is not limited to, creating huge numbers of new access tokens that are then thrown\naway instead of being reused.\nAccording to the RFC, the access tokens generated with most grant types (e.g. authorization code grant) should be reused\nup until their expiration period. When they expire, the\nprovided refresh token should be used to receive a new\naccess token via refresh token grant.\nSome of the clients rarely refresh tokens or even don’t reuse them at all. This causes a lot of unnecessary traffic (\noften in the form of sudden spikes)\nthat can lead to potential issues on both sides. We had pretty good monitoring of this issue, but we needed better tools\nto deal with that problem as well as to educate the clients to make proper use of OAuth.\nPlanning the solution\nBefore tackling the problem directly we needed a little more information and careful planning. Firstly, we wanted to\nmake sure that our solution would solve the problem. Secondly, blocking too many clients could end up in disaster. To be\ncertain that our solution was error-free, we started by making sure that we knew what we want to achieve. Here are the\nproperties we expected from our solution:\nIt cannot block trusted clients such as Allegro apps.\nIt should not negatively affect performance.\nIt should be configurable per client since different clients have different traffic characteristics.\nIt should be able to distinguish between user and non-user use of OAuth. Client credentials grant used without context\nof a user, for example should be treated differently than user-based authorization code grant. In effect, the limiting\n“per user” should be introduced in the second case.\nIt should directly cause an improvement in traffic spikes.\nIt should work well in a highly distributed environment with dozens of service instances and database nodes.\nIt cannot be too costly or require too much additional infrastructure like additional databases, external systems,\netc.\nTackling the problem\nTo meet those needs we needed a robust solution. Since RFC leaves a lot of room for implementation by end-users, it does\nnot specify how such rate-limiting should work.\nAs with every problem of such kind, it’s worth starting with in-depth research of existing solutions. There are various\nstrategies and approaches to this problem in many different scenarios, but only a few of them were applicable to our\ncase and enabled us to fulfill our goals.\nAs usual, we also explored the existing implementations of such solutions in the form of enterprise or open-source\nlibraries and frameworks. Unfortunately, we did not find any that would meet all of our needs and at the same time be\nflexible enough to easily integrate into our ecosystem. It’s also worth noting that we were limited by certain\nconstraints such as long-term costs of additional resources.\nIn the end, we decided to go with implementing our own solution. There were several algorithms to choose from that could\nserve as its base:\nPrecise query-based per-user counter — query the database for a token count for each request. Not suitable, because\nit would cause way too much database traffic.\nFixed window based on TTL documents — uses a rate-limit counter that is cleared every fixed period. While less\nchallenging for the database, it shares a common vulnerability with the first algorithm: sudden TTL-caused spikes in\nallowed requests that consequently cause all of the following requests to be blocked (rate-limit exhaustion).\nSliding window log — stores all requests as a timestamped log entry in a database, that are later aggregated when\nneeded. Too costly, because again it would cause too heavy a load on the database.\nSliding window — uses a rate-limit counter that is a time-based weighted moving window. It has the advantage of\nrelatively low database load of the fixed window algorithm without its spike-related flaws.\nWe carefully weighed the pros and cons of each of those options and finally decided to make a PoC of the fourth option:\nthe sliding window algorithm.\nThe algorithm is based on counters that each hold a count of requests in their respective time frames. At each point of\ntime, an abstract window is calculated from two neighboring frames: current and previous. The further away the window is\nfrom the previous frame, the more important the counter from the current frame is. The weight is based strictly on\nproportions. If, for example, the span of the current window is 75% of the previous frame and 25% of the current frame,\nthen the value of the current rate-limit counter is a sum of 75% of the previous frame counter and 25% of the current\nframe counter. The results are put into a hashmap with users as keys and counters as values, which acts as a cache.\n\nIn the picture above the current counter value would be:\n\nvalue = previousCounter * partOfPreviousFrame + currentCounter * partOfCurrentFrame\nvalue = 12 * 0.75 + 5 * 0.25 = 10.25\n\n\nThe time complexity of the sliding window algorithm is O(1) since it’s a simple hashmap get operation, while its space\ncomplexity is O(n) where n is the number of user counters held by an instance. It works well in distributed systems\nbecause it’s based on simple counters that are relatively easy to synchronize. Its cost is low since we only need to\nstore simple counters and can easily minimize database queries, as well as improve performance with caching in a\nstraightforward manner. Thanks to the fact that the sliding window takes into account the time relative to the current\ntimestamp, we can flatten the token creation spikes. Its main disadvantage is that it indirectly relies on the even\ndistribution of requests in time, which makes it less sensitive to spike traffic within one time window.\nFollowing the plan\nRemote state synchronization\nThere are two kinds of states that need to be tracked for the rate-limiting to work. The first one is the global state.\nIt’s basically a global counter of all requests made by a client to rate-limited OAuth endpoints within a certain\nperiod. This state is persisted in the database and from its perspective represents the most recent rate-limit status.\nThe second type is the local state of each instance, one part of which is the local counter of incoming requests for a\nclient within a time window. Since instances are not directly connected to the client and every one of them can make\nrequests to different instances concurrently, the local counter of each instance for that client will most probably have\na different value. Rate-limiting is a global functionality in the sense that we are interested in the value of global\ncounters and not of individual counters. Consider the following scenario: we would like to set a rate limit of 50 RPS\nfor a client while having 10 instances. If we relied only on local counters to do the rate-limiting, it would be\npossible for the traffic of 500 RPS to be split evenly between instances and not trigger rate-limiting - and that’s not\nthe result we are aiming for.\nTo decide whether to reject an incoming request or not, the local instance needs a second part of the state - a global\ncounter. When making a decision, it sums both the local and the global ones to check whether new requests would go\nbeyond the rate-limit boundaries.\nConsequently, the instances need to be aware of the requests made to the others and for that to work, the global counter\nneeds to be updated regularly. Each instance will periodically try to flush the sum of global and local counters from\ntheir state to the database. Here is where things get tricky. If several instances are trying to save different global\ncounters, there is a risk that one of them will attempt to overwrite the changes that were just saved by the other. To\ncounter that, we use a mechanism called optimistic locking that is\ndescribed later in this post. Once the state is persisted successfully, the local state is\ncleared. At this point, it consists of the incoming requests counter that is equal to 0 and the global snapshot counter\nthat is equal to the value persisted in the database for that client.\nCaching clients’ global state\nAny instance should be aware of the rate limit state for the whole cluster. It represents the global number of requests\nmade by a particular client and its users. We keep a mapping between a pair (client_id, username) and the number of\nrequests made in memory, which makes our algorithm efficient. There is one caveat: keeping all the clients and their\nusers would take too much space, so we only keep those clients and users who are actively making requests to the OAuth\nserver. As soon as they stop calling our servers we delete them from the instance’s cache.\nSharing the state\nOur internal OAuth service works in a distributed manner. There are many instances of the OAuth servers and we should\nhave mechanisms to coordinate rate-limiting between them. In practice, it means that if a client is making a request to\nserver instance A, the server instance B should consider it when calculating the allowed number of requests in the\ncurrent time window. Below are the key points that sum up this description:\nWe have a global request counter per each client/user.\nThe counter is stored in the Mongo database.\nEach instance shares the counter - it reads and writes its current value.\nThe counter depicts the window in the current timestamp.\n\n{\n  \"_id\" : {\n    \"clientId\" : \"some-client\",\n    \"username\" : \"some-user\"\n  },\n  \"version\" : NumberLong(405),\n  \"expire\" : ISODate(\"2021-05-29T14:24:01.376Z\"),\n  \"requestCounters\" : {\n    \"1622211780\" : 1,\n    \"1622211840\" : 1\n  }\n}\n\n\nThe counter consists of a few fields:\n_id - to uniquely identify the client and the user connected with the request.\nversion - for the optimistic locking purposes.\nexpire - used to remove the counter if it has no updates, which means that the client is not currently creating new\ntokens.\nrequestCounters - mapping of the number of requests at consecutive points in time.\nThe counter is cached so we don’t have to fetch it each time the request from the client is made, since it would kill\nperformance. Each instance refreshes the counter asynchronously - reads and writes to the Mongo database are made in a\ndedicated thread. Each one also holds the counters only for the clients they have to handle. A particular instance\nremoves the client’s state after it stops sending requests.\n\n\nThis table depicts an example flow of counters on two instances (A, B) of our OAuth service. Each of them has its own\ncounter\n(accordingly cnt1 and cnt2), mng is a state in the sharded Mongo database and total req is the sum of\nall requests made to all instances.\nThe state of each instance consists of two counters (g / i). The first one (g for “global”) describes how many\nrequests have been globally made and are persisted in the database. The second one tracks how many requests have come to\nthat instance since the last flush (i for “in-flight”). The total number of requests from its perspective is the sum\nof g and i and it is the value that is going to be persisted.\nBelow is the description of what happens in this scenario, step by step:\nThere is an initial state with no requests made to either instance.\nOne request is made to instance A and its iA counter is increased to 1.\nNext, instance A pushes its total counter to the database, setting its g counter to 1 and its i counter to 0.\nInstance B periodically pulls the g counter from the database. Now, the new counter is pulled and the instance’s\ncurrent state is g = 1, iB = 0.\nNext, instance A receives one request, and at the same time, instance B receives three requests. At this time\ninstance A knows of 2 requests, 1 of which came since the last flush. Instance B knows of 4 requests, 3 of them came\nsince the last flush.\nInstance B pushes its total counter (3+1=4) to Mongo. As a consequence, its state is set to (4 / 0). Instance A has\nnot yet flushed its counter to persistent storage. Remember that both instances do it independently whenever a fixed\ninterval passes.\nNow, Instance A tries to push its state (1+1=2) to the database. If the push was successful, it would overwrite the\nstate previously written by the instance B (in step 5), resulting in data loss. We want our counters to be precise so\nwe need to use optimistic locking here. It causes the push to fail, if the version of its state differs from the\nversion persisted in the database. If that scenario occurs, the instance knows that it should refresh its global\ncounter before trying to save it again.\nInstance A refreshed its state. It is now equal to (4/1), which means 4 requests are persisted in the database and 1\nthat has not been flushed yet. Now it can safely push its total counter (4+1=5) to the database and set its state\nto (5/0).\nAt this point both instances have pushed their state to the database. Notice, however, that instance B has not yet\npulled the counter written by instance B in the previous step.\nAfter the last pull, the counter states on both instances are consistent with the database state and reflect the\nglobal number of requests made by a client.\nPersisting the state\nTo properly persist the state in a distributed environment with minimal impact on application performance, we needed to\ntake several strategies into consideration.\nResolving the conflicts\nAs we’ve already mentioned we use optimistic locking to prevent the state from being overwritten by the different\ninstances. It’s quite a common problem in a the world of distributed systems. It works by using version numbers kept in\nMongoDB, which designates how many updates have been made from the beginning of its creation. After each update the\nversion increases by one.\nBefore save to the database:\n\n{\n   \"_id\" : {...},\n   \"version\" : 0,\n   \"requestCount\": 0,\n   ...\n}\n\n\nAfter the save:\n\n{\n   \"_id\" : {...},\n   \"version\" : 1,\n   \"requestCount\": 5,\n   ...\n}\n\n\nBut how does a particular instance save the document atomically? How does it know that there was an update made by\nanother instance in the meantime? For this purpose, we use a Mongo query to do\nthe CAS update that looks like:\n\n\ndb.ratelimits.update(\n <filter query>\n <update operation>\n)\n\ndb.ratelimits.update(\n {\n   \"_id\": {...},\n   \"version\": 1\n },\n {\n   \"version\": 2,\n   \"requestCount\": 10\n }\n)\n\n\nIf the query returns 0 elements updated we know there was a collision and there was a concurrent save which changed the\nstate. If this happens we need to:\nUpdate state from database to local instance,\nApply inflight recorded changes (the ones not yet persisted to the database),\nSave the state,\nIf the query returns 1 element updated, the save was successful without any collisions,\nSaving in batches\nAs we already mentioned, a single instance of the OAuth server handles many clients. So it has to synchronize the state\nfor each of them. Doing the above save operation for each client individually would cause a massive number of queries\nand would quickly saturate our resources. That’s why we\nuse Mongo bulk operations. They\nallow defining many operations in a single query.\nGoing into production\nDry-run\nBefore actually blocking the clients we needed a way to check real outputs from our solution. We implemented the dry-run\nmode, which enabled us do this without affecting any clients, while also giving us metrics depicting which of them would\nbe blocked given a particular limit and time window. We took the following steps:\nSet high rate limit up to the point when no clients would be blocked (running in dry-run mode all the time).\nLowered the threshold to the point that showed us the outstanding clients, while not blocking the others.\nCommunicated with clients abusing our future rate limit policy and gave them a chance to optimize the way they use\nthe OAuth server.\nSwitched modes from dry-run to actively blocking the clients.\nCanary deployment\nIt would be risky to deploy this kind of feature to the whole OAuth cluster without ensuring it’s properly working on a\nfew instances. We used canary deployment which allows deploying a version of a service to only a few production\ninstances. During this deployment, we monitored CPU usage and response time metrics. After ensuring there was meaningful\ndisparity we rolled out the full feature to all production instances.\nObservability\nTo monitor and verify our solution we needed a bunch of metrics telling us how many clients were affected by rate limit\npolicy and which of them are getting closer to being blocked. In practice, two charts shown below were enough to monitor\ntheir behaviour:\n\nThis chart shows clients and their blocked rate. Each color depicts a different blocked client.\n\nThis one on the other hand depicts the allowed rate. The limit line is helpful to see if any client is getting closer to\nit and will be blocked soon. It’s worth mentioning that the default rate limit policy isn’t sufficient for all of the\nclients. Some of them require special treatment and for them we can configure different thresholds, which is why a few\nof them go beyond the actual default limit (the red line).\nConclusion\nRate-limiting is a common problem, but surprisingly not so trivial to solve, especially in a high-scale, distributed\nenvironment. Plenty of popular solutions can be found, but most of them deal with it only from a perspective of a single\nmachine and were not well-suited for our system. Coming up with the above approach took quite a bit of research and\nplanning, but in the end, its deployment allowed us to effectively achieve our goal. We are pretty content with the\nfinal product, as it is both effective and fast, but are constantly tweaking it and looking for new ways to optimize\nthat process.","guid":"https://blog.allegro.tech/2021/11/oauth-rate-limiting.html","categories":["tech","architecture","oauth","microservices"],"isoDate":"2021-11-08T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How we refactored the search form UI component","link":"https://blog.allegro.tech/2021/10/refactoring-opbox-search.html","pubDate":"Tue, 26 Oct 2021 00:00:00 +0200","authors":{"author":[{"name":["Volodymyr Khytskyi"],"photo":["https://blog.allegro.tech/img/authors/volodymyr.khytskyi.jpg"],"url":["https://blog.allegro.tech/authors/volodymyr.khytskyi"]}]},"content":"<p>This article describes a classic case of refactoring a search form UI component, a critical part of every e-commerce\nplatform. In it I’ll explain the precursor of change, analysis process, as well as aspects to pay attention to and\nprinciples to apply while designing a new solution. If you are planning to conduct refactoring of a codebase or just\ncurious to learn more about frontend internals at <a href=\"https://allegro.tech\">Allegro</a>, you might learn a thing or two from\nthis article. Sounds interesting? Hop on!</p>\n\n<h2 id=\"how-does-the-search-form-function-at-allegro\">How does the search form function at Allegro?</h2>\n\n<p>For starters, so that we all are on the same page, let me briefly explain how the search form at Allegro works and what\nfunctionality it is responsible for. Under the hood, it is one of our many\n<a href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">OpBox</a> components and its\ntechnical name is opbox-search.</p>\n\n<p>From UI standpoint it consists of four parts:</p>\n\n<ul>\n  <li>an input field</li>\n  <li>a scope selector</li>\n  <li>a submit button</li>\n  <li>a dropdown with a list of suggestions</li>\n</ul>\n\n<p><img src=\"/img/articles/2021-10-26-refactoring-opbox-search/component-breakdown.png\" alt=\"Component Breakdown\" title=\"Component Breakdown\" /></p>\n\n<p>Functionality-wise, whenever a user clicks/taps into the input or types a search phrase, a dropdown with a list of\nsuggestions shows up and the user can navigate through by using keyboard/mouse/touchscreen. The suggestion list\nitself, at most, consists of two sections: phrases searched in the past and popular/matching suggestions. Those are\nfetched in real time as the user interacts with the form. Additionally, there is also a preconfigured option to search\nthe phrase in products’ descriptions.</p>\n\n<p>The form also provides a possibility to narrow down the search scope to a particular department, a certain\nuser, a charity organization, etc. Depending on the selected scope, when user click the submission button, they are\nredirected to an appropriate product/user/charity listing page and the search phrase is sent as a URL query parameter.</p>\n\n<p>At this point you might be wondering: this sounds quite easy, how come you messed up such a simple component?</p>\n\n<h2 id=\"a-bit-of-history-and-the-precursor-of-refactoring\">A bit of history and the precursor of refactoring</h2>\n\n<p>The initial commit took place back in 2017 and up until the point of refactoring the project, there were 2292 commits\nspread across 189 pull requests merged to the main branch. All those contributions were made by different independent\nteams. Over time the component evolved, some external APIs changed, some features became deprecated and new ones were\nadded. At one point it also changed its ownership to another development team. As expected, all those factors left some\nmarks on the codebase.</p>\n\n<p>One of ample examples of troublesome conditions was the store entity that is responsible for handling the runtime state.\nIn reality, besides performing its primary function it also handled network calls and contained pieces of business logic\nnon-relevant for search scoping and suggestions listing.</p>\n\n<p>To make matters worse, the internals of the entity were publicly exposed and therefore any dependent, e.g. the search\ninput or the scope selector, was free to manipulate the store arbitrarily. Not hard to imagine, using such “shortcuts”\nhas lead to degradation of codebase readability and maintainability.</p>\n\n<p>At one point in time it reached its critical mass and we started considering the cost of maintaining the existing\ncodebase vs. the cost of redeveloping it from scratch.</p>\n\n<h2 id=\"refactoring-expectations\">Refactoring expectations</h2>\n\n<p>Refactoring itself doesn’t provide any business value. Instead it is an investment that should save engineers’ time and\neffort. That is why the primary goal was to gain back the confidence in further development of the component by:</p>\n\n<ul>\n  <li>streamlining the maintenance of existing features</li>\n  <li>streamlining and simplifying the development of new business features</li>\n  <li>using tools, design patterns and principles to help engineers develop stable and correctly functioning features</li>\n</ul>\n\n<p>Subsequently, achieving the goal would:</p>\n\n<ul>\n  <li>shorten the delivery time</li>\n  <li>create, from an architectural standpoint, a new solution resilient to multiple future contributions from different teams</li>\n</ul>\n\n<h2 id=\"into-the-technical-analysis\">Into the technical analysis</h2>\n\n<p>So by this point we learned what the functional requirements are and identified the issues we have with the current\nsolution. We also outlined refactoring expectations, hence we could start laying down the conceptual foundation of a new\nsolution. We began by asking ourselves a few questions that could help us formalize our technical end goals.</p>\n\n<h3 id=\"what-issues-did-we-want-to-avoid-this-time-what-did-we-want-the-new-solution-to-improve-upon\">What issues did we want to avoid this time? What did we want the new solution to improve upon?</h3>\n\n<p>Going back to the store example, we clearly didn’t want to allow any dependent to mutate its data in any abritrary way\nnor did we want the store to contain pieces of logic that are not of its concern, e.g. networking. Instead we wanted to\nmove these irrelevant pieces into more suitable locations, decrease entanglement of different parts of the codebase,\nstructure it into logical entities that are more human readable, draw boundaries between those entities, define rules\nof communication and make sure we expose to the public API only what we intended to.</p>\n\n<h3 id=\"what-development-principles-and-design-patterns-could-we-have-incorporated\">What development principles and design patterns could we have incorporated?</h3>\n\n<h4 id=\"the-single-responsibility-principle-srp\">The single-responsibility principle (SRP)</h4>\n\n<p>The “S” of the “SOLID” acronym. As the name hints, a class (or a unit of code) should be responsible only for a single\npiece of functionality. A simple and powerful principle, yet quite often overlooked. Say, if we build a piece of logic\nassociated with an input HTML element, its only responsibility should be to tightly interact with the element, e.g.\nlisten to its DOM events and update its value if needed. At the same time, you don’t want this logic to affect the\nsubmission behavior of a form element inside of which the input element is placed.</p>\n\n<h4 id=\"the-separation-of-concerns-principle-soc\">The separation of concerns principle (SoC)</h4>\n\n<p>SoC goes well in pair with SRP and states that one should not place functionalities of different domains under the same\nlogical entity (say, an object, a class or a module). For example, we need to render a piece of information on the\nscreen but beforehand the information needs to be fetched over the network. Since the view and network layers have\ndifferent concerns we don’t want to place both of them under a single logical entity. Let these two be separate ones\nwith established dependency relation to one another via a public API.</p>\n\n<h4 id=\"the-loose-coupling-principle\">The loose coupling principle</h4>\n\n<p>Loose coupling means that a single logical entity knows as little as possible about other entities and communication\nbetween them follows strict rules. One important characteristic we wanted to achieve here is to minimize negative\neffects on application’s runtime in case an entity malfunctions. As an analogy, we could imagine a graph of airports\nthat are connected to each other via a set of flight routes. Say, there are direct routes from airport A to B, C and D.\nIf the airport C gets closed due to renovation of a runway, the routes to B and D are not affected in any way. Moreover,\nsome passengers might not even know that C is not operating at that moment.</p>\n\n<h4 id=\"event-driven-dataflow\">Event-driven dataflow</h4>\n\n<p>Since we are dealing with a UI component that has several moving parts, applying event-driven techniques come in handy\nbecause of their asynchronous nature and because messaging channels are subscription-based. Here is another analogy: you\nare at your place waiting for a hand-to-hand package delivery. Instead of opening the door every now and then to check\nif there is a courier behind it you would probably wait first for a doorbell to ring, right? Only once it rings (an\nevent occurs), you would open the door to obtain the package.</p>\n\n<h3 id=\"which-development-constraints-if-any-did-we-want-to-introduce-intentionally\">Which development constraints (if any) did we want to introduce intentionally?</h3>\n\n<p>We wanted the architecture of the new solution to follow a certain set of rules in order to maintain its structure. We\nalso wanted to make it harder for engineers to break those rules. In the short term it might be a drawback, but we are\ninterested in keeping the codebase well-maintained in the long run. By carefully designing the APIs and applying static\ntype analysis we were not only able to meet the requirement but also lifted some complexity from engineers’ shoulders,\nand this is where TypeScript shines brightly.</p>\n\n<h2 id=\"applying-all-of-the-above-in-a-new-store-solution\">Applying all of the above in a new store solution</h2>\n\n<p>Based on the conclusions reached above we were ready to start the actual work and started it from the core, that is the\nstore entity. As with any store solutions, let us list functional characteristics that one should provide.</p>\n\n<p>The store should:</p>\n<ul>\n  <li>hold current runtime state</li>\n  <li>be initialized with a default state</li>\n  <li>provide a public API that allows dependents to update its state during runtime in a <em>controlled way</em></li>\n  <li>expose information channels to propagate the state change to every subscriber</li>\n  <li>not expose any implementation details</li>\n</ul>\n\n<p>Defining the structure of the store’s data is as simple as declaring a TypeScript interface, but we need to be able to\nexpose information that the state has mutated in a particular way. That is, we need to build event-driven communication\nbetween the store and its dependents. For this purpose we could use an event emitter and define as many topics as needed.\nIn our case, having a dedicated topic per state property turned out to work perfectly as we didn’t have that many\nproperties in the first place. And since we wanted to have the state and event emitter under the same umbrella, we\nencapsulated them into the following class declaration:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kr\">interface</span> <span class=\"nx\">State</span> <span class=\"p\">{</span>\n  <span class=\"nl\">foo</span><span class=\"p\">:</span> <span class=\"nx\">boolean</span><span class=\"p\">;</span>\n  <span class=\"nl\">bar</span><span class=\"p\">:</span> <span class=\"kr\">string</span><span class=\"p\">;</span>\n  <span class=\"c1\">// ...</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">type</span> <span class=\"nx\">Topic</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n  <span class=\"p\">[</span><span class=\"nx\">K</span> <span class=\"k\">in</span> <span class=\"nx\">Capitalize</span><span class=\"o\">&lt;</span><span class=\"kr\">keyof</span> <span class=\"nx\">State</span><span class=\"o\">&gt;</span><span class=\"p\">]:</span> <span class=\"nx\">Uncapitalize</span><span class=\"o\">&lt;</span><span class=\"nx\">K</span><span class=\"o\">&gt;</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nx\">Store</span> <span class=\"p\">{</span>\n  <span class=\"kd\">constructor</span><span class=\"p\">(</span>\n    <span class=\"k\">private</span> <span class=\"nx\">state</span><span class=\"p\">:</span> <span class=\"nx\">State</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">readonly</span> <span class=\"nx\">emitter</span><span class=\"p\">:</span> <span class=\"nx\">Emitter</span><span class=\"o\">&lt;</span><span class=\"nb\">Record</span><span class=\"o\">&lt;</span>\n       <span class=\"kr\">keyof</span> <span class=\"nx\">State</span><span class=\"p\">,</span>\n       <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">:</span> <span class=\"nx\">State</span><span class=\"p\">)</span> <span class=\"o\">=&gt;</span> <span class=\"k\">void</span>\n    <span class=\"o\">&gt;&gt;</span><span class=\"p\">,</span>\n  <span class=\"p\">)</span> <span class=\"p\">{}</span>\n\n  <span class=\"k\">public</span> <span class=\"nx\">on</span><span class=\"p\">(...</span><span class=\"nx\">args</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"k\">return</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(...</span><span class=\"nx\">args</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>The last piece of the puzzle is that we needed to automate event emission triggering. At the same time, we aimed to\nminimize the size of the boilerplate code needed to set up this behavior for each state property. Since every property\nhas a corresponding topic, that is where we were able to leverage the power of JavaScript’s accessor descriptor and\nwithin a setter we could trigger the emission:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">Object</span>\n  <span class=\"p\">.</span><span class=\"nx\">values</span><span class=\"p\">(</span><span class=\"nx\">Topic</span><span class=\"p\">)</span>\n  <span class=\"p\">.</span><span class=\"nx\">forEach</span><span class=\"p\">(</span><span class=\"nx\">key</span> <span class=\"o\">=&gt;</span> <span class=\"nb\">Object</span><span class=\"p\">.</span><span class=\"nx\">defineProperty</span><span class=\"p\">(</span><span class=\"nx\">Store</span><span class=\"p\">.</span><span class=\"nx\">prototype</span><span class=\"p\">,</span> <span class=\"nx\">key</span><span class=\"p\">,</span> <span class=\"p\">{</span>\n    <span class=\"kd\">get</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n      <span class=\"k\">return</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span><span class=\"p\">[</span><span class=\"nx\">key</span><span class=\"p\">];</span>\n    <span class=\"p\">},</span>\n    <span class=\"kd\">set</span><span class=\"p\">(</span><span class=\"nx\">value</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span> <span class=\"o\">=</span> <span class=\"p\">{</span> <span class=\"p\">...</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"nx\">key</span><span class=\"p\">]:</span> <span class=\"nx\">value</span> <span class=\"p\">};</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"nx\">key</span><span class=\"p\">,</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">}));</span>\n</code></pre></div></div>\n\n<p>After putting it all together, not only did we achieve the above-mentioned characteristics but also a simple way to\nwork with the store:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">foo</span> <span class=\"o\">=</span> <span class=\"kc\">true</span><span class=\"p\">;</span>\n<span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"nx\">Topic</span><span class=\"p\">.</span><span class=\"nx\">Foo</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">:</span> <span class=\"nx\">State</span><span class=\"p\">)</span> <span class=\"o\">=&gt;</span> <span class=\"p\">...);</span>\n</code></pre></div></div>\n\n<p>At this point the store is ready and we would like to test how well the solution performs in reality.</p>\n\n<h2 id=\"event-driven-communication-between-the-store-and-ui-parts\">Event-driven communication between the store and UI parts</h2>\n\n<p>Now we could focus on developing the UI parts of our component. Luckily, business requirements provide enough hints where\nto place each piece of functionality. Let’s take a look at how we shaped the search input and the suggestion list, and\nhow these UI parts cooperate with each other and the store.</p>\n\n<p>Recall that functionality-wise the suggestion list is a dropdown that should be rendered whenever a user types a search\nphrase or clicks/taps into the input element. We also need to fetch best matching suggestions whenever the input value\nchanges.</p>\n\n<p>We started with the search input as it doesn’t have any dependencies besides the store. With functional requirements in\nmind, we aimed it to be good at doing just two things:</p>\n\n<ul>\n  <li>Proxying DOM events such as focus, blur, click, keydown, etc.</li>\n  <li>Notifying the store whenever the input value changes</li>\n</ul>\n\n<p>Since updating the store is pretty much straightforward, let’s take a look at how we handled DOM events. Events such as\n<code class=\"language-plaintext highlighter-rouge\">click</code>, <code class=\"language-plaintext highlighter-rouge\">focus</code>, <code class=\"language-plaintext highlighter-rouge\">blur</code> convey the fact that there was some sort of interaction with the input HTML element. Unlike\n<code class=\"language-plaintext highlighter-rouge\">input</code> event, where one is interested in knowing what the current value is, the above-mentioned ones don’t include any\nrelated information. We only need to be able to communicate to the dependents the fact that such an event took place\nand that is why, similarly to the store, the search input has an event emitter of its own. Now, you might be thinking:\nwhy would you want to have multiple sources of information given you already have the event-driven store solution? There\nare a couple of reasons:</p>\n\n<ul>\n  <li>Since those DOM events don’t contain additional information, there is nothing we need to put into our store. It\naligns well with the single-responsibility principle, according to which the store only fulfills its primary function\nand has no hint of existence of the search input.</li>\n  <li>By bypassing the store we shortened an event message travel distance from the source to a subscriber.</li>\n  <li>It also aligns well with a mental model, where if one wants to react to an event that happened in the search input,\none subscribes to its publicly available communication channels.</li>\n</ul>\n\n<p>The gist of this approach and binding with the store can be achieved in just a few lines of code. Here we add several\nDOM event listeners and, depending on the event type, decide whether we need to proxy them into the event emitter or\nupdate the store’s state:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nx\">SearchInput</span> <span class=\"p\">{</span>\n  <span class=\"kd\">constructor</span><span class=\"p\">(</span>\n    <span class=\"k\">private</span> <span class=\"k\">readonly</span> <span class=\"nx\">inputNode</span><span class=\"p\">:</span> <span class=\"nx\">HTMLInputElement</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"nx\">store</span><span class=\"p\">:</span> <span class=\"nx\">Store</span><span class=\"p\">,</span>\n    <span class=\"k\">public</span> <span class=\"k\">readonly</span> <span class=\"nx\">emitter</span><span class=\"p\">:</span> <span class=\"nx\">Emitter</span><span class=\"o\">&lt;</span><span class=\"nb\">Record</span><span class=\"o\">&lt;</span>\n      <span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span> <span class=\"o\">|</span> <span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span> <span class=\"o\">|</span> <span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n      <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">void</span><span class=\"p\">,</span>\n    <span class=\"o\">&gt;&gt;</span><span class=\"p\">,</span>\n  <span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span><span class=\"p\">));</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span><span class=\"p\">));</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">emit</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">));</span>\n    <span class=\"nx\">inputNode</span><span class=\"p\">.</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">input</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">({</span> <span class=\"na\">currentTarget</span><span class=\"p\">:</span> <span class=\"p\">{</span> <span class=\"nx\">value</span> <span class=\"p\">}})</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">input</span> <span class=\"o\">=</span> <span class=\"nx\">value</span><span class=\"p\">);</span>\n  <span class=\"p\">}</span>\n  <span class=\"c1\">// ...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Now, we can focus our attention on the suggestions list. Communication-wise, all it needs to do is to subscribe to\nseveral topics provided by the store and the search input:</p>\n\n<div class=\"language-ts highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"p\">{</span> <span class=\"nx\">fetchSuggestions</span> <span class=\"p\">}</span> <span class=\"k\">from</span> <span class=\"dl\">'</span><span class=\"s1\">./network</span><span class=\"dl\">'</span><span class=\"p\">;</span>\n\n<span class=\"kd\">class</span> <span class=\"nx\">SuggestionsList</span> <span class=\"p\">{</span>\n  <span class=\"kd\">constructor</span><span class=\"p\">(</span>\n    <span class=\"k\">private</span> <span class=\"nx\">store</span><span class=\"p\">:</span> <span class=\"nx\">Store</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">readonly</span> <span class=\"nx\">searchInput</span><span class=\"p\">:</span> <span class=\"nx\">SearchInput</span><span class=\"p\">,</span>\n    <span class=\"c1\">// ...</span>\n  <span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">searchInput</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">click</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderVisible</span><span class=\"p\">());</span>\n    <span class=\"nx\">searchInput</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">focus</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderVisible</span><span class=\"p\">());</span>\n    <span class=\"nx\">searchInput</span><span class=\"p\">.</span><span class=\"nx\">emitter</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">blur</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderHidden</span><span class=\"p\">());</span>\n    <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">input</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">fetchItemsList</span><span class=\"p\">());</span>\n    <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">on</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">suggestionsListData</span><span class=\"dl\">'</span><span class=\"p\">,</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderItemsList</span><span class=\"p\">();</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">renderVisible</span><span class=\"p\">();</span>\n    <span class=\"p\">});</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"k\">private</span> <span class=\"k\">async</span> <span class=\"nx\">fetchItemsList</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">suggesiontsListData</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"nx\">fetchSuggestions</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nx\">input</span><span class=\"p\">);</span>\n  <span class=\"p\">}</span>\n  <span class=\"c1\">// ...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>When a <code class=\"language-plaintext highlighter-rouge\">click</code> or <code class=\"language-plaintext highlighter-rouge\">focus</code> event message arrives from the search input, <code class=\"language-plaintext highlighter-rouge\">SuggestionsList</code> renders a dropdown UI element.\nOn the other hand, <code class=\"language-plaintext highlighter-rouge\">blur</code> event occurrence hides it. A change of input value in the store leads to fetching suggestions\nover the network and lastly, whenever suggestions data is available, <code class=\"language-plaintext highlighter-rouge\">SuggestionsList</code> renders the item list and makes\nthe dropdown visible.</p>\n\n<p>Note that because we apply the separation of concerns principle, the <code class=\"language-plaintext highlighter-rouge\">fetchItemsList</code> method only delegates a job to an\nexternal dependency responsible for network communication. Upon successful response, <code class=\"language-plaintext highlighter-rouge\">SuggestionsList</code> doesn’t\nimmediately start rendering the data, instead the data is put into the store and <code class=\"language-plaintext highlighter-rouge\">SuggestionList</code> listens to that data\nchange. With such data circulation we ensure that:</p>\n\n<ul>\n  <li>The store is a single source of truth (and data)</li>\n  <li>Suggestions’ data is propagated to every dependent</li>\n  <li>We avoid possible redundant rendering cycles</li>\n</ul>\n\n<p>With that, our functional requirement is implemented.</p>\n\n<h2 id=\"the-final-solution\">The final solution</h2>\n\n<p>Reapplying the above principles and techniques to develop the remaining functional requirements, we ended up with a\nsolution that can be illustrated as follows:</p>\n\n<p><img src=\"/img/articles/2021-10-26-refactoring-opbox-search/architecture-diagram.png\" alt=\"Architecture Diagram\" title=\"Architecture Diagram\" /></p>\n\n<p>Were we able to meet our expectations? At the end of the day, after careful problem analysis, testing out POCs and\ndevelopment preparations, the implementation process itself went quite smoothly. Multiple people participated and\nwe are quite satisfied with the end result. Will it withstand future challenges? Only time will tell.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>At Allegro we value our customer experience and that is why we pay a lot of attention to performance of our frontend\nsolutions. At the same time, as software engineers, we want to stay productive and, therefore, we also care about our\ndevelopment experience. Achieving good results in both worlds is where the real challenge lies.</p>\n","contentSnippet":"This article describes a classic case of refactoring a search form UI component, a critical part of every e-commerce\nplatform. In it I’ll explain the precursor of change, analysis process, as well as aspects to pay attention to and\nprinciples to apply while designing a new solution. If you are planning to conduct refactoring of a codebase or just\ncurious to learn more about frontend internals at Allegro, you might learn a thing or two from\nthis article. Sounds interesting? Hop on!\nHow does the search form function at Allegro?\nFor starters, so that we all are on the same page, let me briefly explain how the search form at Allegro works and what\nfunctionality it is responsible for. Under the hood, it is one of our many\nOpBox components and its\ntechnical name is opbox-search.\nFrom UI standpoint it consists of four parts:\nan input field\na scope selector\na submit button\na dropdown with a list of suggestions\n\nFunctionality-wise, whenever a user clicks/taps into the input or types a search phrase, a dropdown with a list of\nsuggestions shows up and the user can navigate through by using keyboard/mouse/touchscreen. The suggestion list\nitself, at most, consists of two sections: phrases searched in the past and popular/matching suggestions. Those are\nfetched in real time as the user interacts with the form. Additionally, there is also a preconfigured option to search\nthe phrase in products’ descriptions.\nThe form also provides a possibility to narrow down the search scope to a particular department, a certain\nuser, a charity organization, etc. Depending on the selected scope, when user click the submission button, they are\nredirected to an appropriate product/user/charity listing page and the search phrase is sent as a URL query parameter.\nAt this point you might be wondering: this sounds quite easy, how come you messed up such a simple component?\nA bit of history and the precursor of refactoring\nThe initial commit took place back in 2017 and up until the point of refactoring the project, there were 2292 commits\nspread across 189 pull requests merged to the main branch. All those contributions were made by different independent\nteams. Over time the component evolved, some external APIs changed, some features became deprecated and new ones were\nadded. At one point it also changed its ownership to another development team. As expected, all those factors left some\nmarks on the codebase.\nOne of ample examples of troublesome conditions was the store entity that is responsible for handling the runtime state.\nIn reality, besides performing its primary function it also handled network calls and contained pieces of business logic\nnon-relevant for search scoping and suggestions listing.\nTo make matters worse, the internals of the entity were publicly exposed and therefore any dependent, e.g. the search\ninput or the scope selector, was free to manipulate the store arbitrarily. Not hard to imagine, using such “shortcuts”\nhas lead to degradation of codebase readability and maintainability.\nAt one point in time it reached its critical mass and we started considering the cost of maintaining the existing\ncodebase vs. the cost of redeveloping it from scratch.\nRefactoring expectations\nRefactoring itself doesn’t provide any business value. Instead it is an investment that should save engineers’ time and\neffort. That is why the primary goal was to gain back the confidence in further development of the component by:\nstreamlining the maintenance of existing features\nstreamlining and simplifying the development of new business features\nusing tools, design patterns and principles to help engineers develop stable and correctly functioning features\nSubsequently, achieving the goal would:\nshorten the delivery time\ncreate, from an architectural standpoint, a new solution resilient to multiple future contributions from different teams\nInto the technical analysis\nSo by this point we learned what the functional requirements are and identified the issues we have with the current\nsolution. We also outlined refactoring expectations, hence we could start laying down the conceptual foundation of a new\nsolution. We began by asking ourselves a few questions that could help us formalize our technical end goals.\nWhat issues did we want to avoid this time? What did we want the new solution to improve upon?\nGoing back to the store example, we clearly didn’t want to allow any dependent to mutate its data in any abritrary way\nnor did we want the store to contain pieces of logic that are not of its concern, e.g. networking. Instead we wanted to\nmove these irrelevant pieces into more suitable locations, decrease entanglement of different parts of the codebase,\nstructure it into logical entities that are more human readable, draw boundaries between those entities, define rules\nof communication and make sure we expose to the public API only what we intended to.\nWhat development principles and design patterns could we have incorporated?\nThe single-responsibility principle (SRP)\nThe “S” of the “SOLID” acronym. As the name hints, a class (or a unit of code) should be responsible only for a single\npiece of functionality. A simple and powerful principle, yet quite often overlooked. Say, if we build a piece of logic\nassociated with an input HTML element, its only responsibility should be to tightly interact with the element, e.g.\nlisten to its DOM events and update its value if needed. At the same time, you don’t want this logic to affect the\nsubmission behavior of a form element inside of which the input element is placed.\nThe separation of concerns principle (SoC)\nSoC goes well in pair with SRP and states that one should not place functionalities of different domains under the same\nlogical entity (say, an object, a class or a module). For example, we need to render a piece of information on the\nscreen but beforehand the information needs to be fetched over the network. Since the view and network layers have\ndifferent concerns we don’t want to place both of them under a single logical entity. Let these two be separate ones\nwith established dependency relation to one another via a public API.\nThe loose coupling principle\nLoose coupling means that a single logical entity knows as little as possible about other entities and communication\nbetween them follows strict rules. One important characteristic we wanted to achieve here is to minimize negative\neffects on application’s runtime in case an entity malfunctions. As an analogy, we could imagine a graph of airports\nthat are connected to each other via a set of flight routes. Say, there are direct routes from airport A to B, C and D.\nIf the airport C gets closed due to renovation of a runway, the routes to B and D are not affected in any way. Moreover,\nsome passengers might not even know that C is not operating at that moment.\nEvent-driven dataflow\nSince we are dealing with a UI component that has several moving parts, applying event-driven techniques come in handy\nbecause of their asynchronous nature and because messaging channels are subscription-based. Here is another analogy: you\nare at your place waiting for a hand-to-hand package delivery. Instead of opening the door every now and then to check\nif there is a courier behind it you would probably wait first for a doorbell to ring, right? Only once it rings (an\nevent occurs), you would open the door to obtain the package.\nWhich development constraints (if any) did we want to introduce intentionally?\nWe wanted the architecture of the new solution to follow a certain set of rules in order to maintain its structure. We\nalso wanted to make it harder for engineers to break those rules. In the short term it might be a drawback, but we are\ninterested in keeping the codebase well-maintained in the long run. By carefully designing the APIs and applying static\ntype analysis we were not only able to meet the requirement but also lifted some complexity from engineers’ shoulders,\nand this is where TypeScript shines brightly.\nApplying all of the above in a new store solution\nBased on the conclusions reached above we were ready to start the actual work and started it from the core, that is the\nstore entity. As with any store solutions, let us list functional characteristics that one should provide.\nThe store should:\nhold current runtime state\nbe initialized with a default state\nprovide a public API that allows dependents to update its state during runtime in a controlled way\nexpose information channels to propagate the state change to every subscriber\nnot expose any implementation details\nDefining the structure of the store’s data is as simple as declaring a TypeScript interface, but we need to be able to\nexpose information that the state has mutated in a particular way. That is, we need to build event-driven communication\nbetween the store and its dependents. For this purpose we could use an event emitter and define as many topics as needed.\nIn our case, having a dedicated topic per state property turned out to work perfectly as we didn’t have that many\nproperties in the first place. And since we wanted to have the state and event emitter under the same umbrella, we\nencapsulated them into the following class declaration:\n\ninterface State {\n  foo: boolean;\n  bar: string;\n  // ...\n}\n\ntype Topic = {\n  [K in Capitalize<keyof State>]: Uncapitalize<K>;\n}\n\nclass Store {\n  constructor(\n    private state: State,\n    private readonly emitter: Emitter<Record<\n       keyof State,\n       (state: State) => void\n    >>,\n  ) {}\n\n  public on(...args) { return this.emitter.on(...args);\n}\n\n\nThe last piece of the puzzle is that we needed to automate event emission triggering. At the same time, we aimed to\nminimize the size of the boilerplate code needed to set up this behavior for each state property. Since every property\nhas a corresponding topic, that is where we were able to leverage the power of JavaScript’s accessor descriptor and\nwithin a setter we could trigger the emission:\n\nObject\n  .values(Topic)\n  .forEach(key => Object.defineProperty(Store.prototype, key, {\n    get() {\n      return this.state[key];\n    },\n    set(value) {\n      this.state = { ...this.state, [key]: value };\n      this.emitter.emit(key, this.state);\n    }\n  }));\n\n\nAfter putting it all together, not only did we achieve the above-mentioned characteristics but also a simple way to\nwork with the store:\n\nstore.foo = true;\nstore.on(Topic.Foo, (state: State) => ...);\n\n\nAt this point the store is ready and we would like to test how well the solution performs in reality.\nEvent-driven communication between the store and UI parts\nNow we could focus on developing the UI parts of our component. Luckily, business requirements provide enough hints where\nto place each piece of functionality. Let’s take a look at how we shaped the search input and the suggestion list, and\nhow these UI parts cooperate with each other and the store.\nRecall that functionality-wise the suggestion list is a dropdown that should be rendered whenever a user types a search\nphrase or clicks/taps into the input element. We also need to fetch best matching suggestions whenever the input value\nchanges.\nWe started with the search input as it doesn’t have any dependencies besides the store. With functional requirements in\nmind, we aimed it to be good at doing just two things:\nProxying DOM events such as focus, blur, click, keydown, etc.\nNotifying the store whenever the input value changes\nSince updating the store is pretty much straightforward, let’s take a look at how we handled DOM events. Events such as\nclick, focus, blur convey the fact that there was some sort of interaction with the input HTML element. Unlike\ninput event, where one is interested in knowing what the current value is, the above-mentioned ones don’t include any\nrelated information. We only need to be able to communicate to the dependents the fact that such an event took place\nand that is why, similarly to the store, the search input has an event emitter of its own. Now, you might be thinking:\nwhy would you want to have multiple sources of information given you already have the event-driven store solution? There\nare a couple of reasons:\nSince those DOM events don’t contain additional information, there is nothing we need to put into our store. It\naligns well with the single-responsibility principle, according to which the store only fulfills its primary function\nand has no hint of existence of the search input.\nBy bypassing the store we shortened an event message travel distance from the source to a subscriber.\nIt also aligns well with a mental model, where if one wants to react to an event that happened in the search input,\none subscribes to its publicly available communication channels.\nThe gist of this approach and binding with the store can be achieved in just a few lines of code. Here we add several\nDOM event listeners and, depending on the event type, decide whether we need to proxy them into the event emitter or\nupdate the store’s state:\n\nclass SearchInput {\n  constructor(\n    private readonly inputNode: HTMLInputElement,\n    private store: Store,\n    public readonly emitter: Emitter<Record<\n      'click' | 'focus' | 'blur',\n      () => void,\n    >>,\n  ) {\n    inputNode.addEventListener('click', () => emitter.emit('click'));\n    inputNode.addEventListener('focus', () => emitter.emit('focus'));\n    inputNode.addEventListener('blur', () => emitter.emit('blur'));\n    inputNode.addEventListener('input', ({ currentTarget: { value }}) => store.input = value);\n  }\n  // ...\n}\n\n\nNow, we can focus our attention on the suggestions list. Communication-wise, all it needs to do is to subscribe to\nseveral topics provided by the store and the search input:\n\nimport { fetchSuggestions } from './network';\n\nclass SuggestionsList {\n  constructor(\n    private store: Store,\n    private readonly searchInput: SearchInput,\n    // ...\n  ) {\n    searchInput.emitter.on('click', () => this.renderVisible());\n    searchInput.emitter.on('focus', () => this.renderVisible());\n    searchInput.emitter.on('blur', () => this.renderHidden());\n    store.on('input', () => this.fetchItemsList());\n    store.on('suggestionsListData', () => {\n      this.renderItemsList();\n      this.renderVisible();\n    });\n  }\n\n  private async fetchItemsList() {\n    this.store.suggesiontsListData = await fetchSuggestions(this.store.input);\n  }\n  // ...\n}\n\n\nWhen a click or focus event message arrives from the search input, SuggestionsList renders a dropdown UI element.\nOn the other hand, blur event occurrence hides it. A change of input value in the store leads to fetching suggestions\nover the network and lastly, whenever suggestions data is available, SuggestionsList renders the item list and makes\nthe dropdown visible.\nNote that because we apply the separation of concerns principle, the fetchItemsList method only delegates a job to an\nexternal dependency responsible for network communication. Upon successful response, SuggestionsList doesn’t\nimmediately start rendering the data, instead the data is put into the store and SuggestionList listens to that data\nchange. With such data circulation we ensure that:\nThe store is a single source of truth (and data)\nSuggestions’ data is propagated to every dependent\nWe avoid possible redundant rendering cycles\nWith that, our functional requirement is implemented.\nThe final solution\nReapplying the above principles and techniques to develop the remaining functional requirements, we ended up with a\nsolution that can be illustrated as follows:\n\nWere we able to meet our expectations? At the end of the day, after careful problem analysis, testing out POCs and\ndevelopment preparations, the implementation process itself went quite smoothly. Multiple people participated and\nwe are quite satisfied with the end result. Will it withstand future challenges? Only time will tell.\nSummary\nAt Allegro we value our customer experience and that is why we pay a lot of attention to performance of our frontend\nsolutions. At the same time, as software engineers, we want to stay productive and, therefore, we also care about our\ndevelopment experience. Achieving good results in both worlds is where the real challenge lies.","guid":"https://blog.allegro.tech/2021/10/refactoring-opbox-search.html","categories":["tech","frontend","architecture","refactoring","developmentexperience","typescript"],"isoDate":"2021-10-25T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Comparing MongoDB composite indexes","link":"https://blog.allegro.tech/2021/10/comparing-mongodb-composite-indexes.html","pubDate":"Mon, 18 Oct 2021 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>One of the key elements ensuring efficient operation of the services we work on every day at\n<a href=\"https://allegro.tech/\">Allegro</a> is fast responses from the database.\nWe spend a lot of time to properly model the data so that storing and querying take as little time as possible.\nYou can read more about why good schema design is important in one of my earlier\n<a href=\"/2021/01/impact-of-the-data-model-on-the-MongoDB-database-size.html\">posts</a>.\nIt’s also equally important to make sure that all queries are covered with indexes of the correct type whenever\npossible. Indexes are used to quickly search the database and under certain conditions even allow results to be\nreturned directly from the index, without the need to access the data itself. However, indexes are not\nall the same and it’s important to learn more about their different types in order to make the right choices later on.</p>\n\n<h2 id=\"whats-the-difference\">What’s the difference?</h2>\n<p>I’ve had a conversation with a colleague of mine the other day, about the point of using composite keys in a MongoDB\ndatabase. I’ve always been a firm believer that it’s a good idea to use a composite key wherever possible because\nsearching this way is very fast. My colleague, on the other hand, advocates using artificial keys and creating\nseparate <a href=\"https://docs.mongodb.com/manual/core/index-compound/\">composite indexes</a> on fields for\nwhich I would use a composite key.\nAfter a brief disagreement, I realized that other than my intuition, I had no arguments to defend my beliefs.\nI decided to see how indexes on composite keys differ from composite indexes created on regular fields in practice.</p>\n\n<p>As an example for our considerations, we will use an entity describing a person by first and last name, let’s also\nassume that this pair is unique.</p>\n\n<p>Such data can be stored in a collection (let’s call it <code class=\"language-plaintext highlighter-rouge\">composite</code>) with a composite key that contains both fields.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"John\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"surname\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Doe\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>A unique index will be automatically created on pair of both fields along with the collection.</p>\n\n<p>Many developers would most likely prefer to use an artificial key, and index the <code class=\"language-plaintext highlighter-rouge\">name</code> and <code class=\"language-plaintext highlighter-rouge\">surname</code> fields\nseparately, as shown in collection <code class=\"language-plaintext highlighter-rouge\">artificial</code>:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"_id\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"err\">ObjectId(</span><span class=\"s2\">\"615eb18b76e172647f7462e2\"</span><span class=\"err\">)</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"name\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"John\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"surname\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Doe\"</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>When it comes to the second model, only the artificial key will be automatically covered by the index. To be\nable to efficiently search the collection by first and last name, we need to manually create a composite index on both\nfields. To maintain consistency with the first collection, of course, uniqueness also needs to be enforced:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">createIndex</span><span class=\"p\">({</span><span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"na\">unique</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>At first glance, we can clearly see that the first way of storing data is more compact, since we only store two fields\nof interest, and only one index is required in addition to the data. For the second collection, in addition to the data\nitself, we need to store an artificial key, moreover two indexes are required here: on the artificial key and on\nthe <code class=\"language-plaintext highlighter-rouge\">name</code> and <code class=\"language-plaintext highlighter-rouge\">surname</code> fields.</p>\n\n<p>We can now move on to comparing the execution plans of queries to two collections, so let’s take a look at the result\nof the <code class=\"language-plaintext highlighter-rouge\">explain</code> commands:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">composite</span><span class=\"p\">.</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>and:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Let’s start with the second result first. We can see that the optimiser chose to use the index we manually created.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"winningPlan\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"FETCH\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"inputStage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IXSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"keyPattern\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"w\">\n      </span><span class=\"p\">},</span><span class=\"w\">\n      </span><span class=\"nl\">\"indexName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"name_1_surname_1\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>[…]</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"executionStats\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionSuccess\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>In the case of a collection with a composite key, however, the plan is different; it contains the word <code class=\"language-plaintext highlighter-rouge\">IDHACK</code>:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"winningPlan\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IDHACK\"</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>[…]</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"executionStats\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionSuccess\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>It means that the optimiser skipped the index selection phase (although in our case there were no other indexes, but\nit doesn’t matter) and decided to use the key index. This operation is considered to be the fastest one possible.\nWhenever there is a key in the query, its index will be used (while ignoring conditions on other fields).</p>\n\n<p>Let’s also take a look at the notation <code class=\"language-plaintext highlighter-rouge\">\"nReturned\" : 1</code>, which means that both queries returned a single document.</p>\n\n<p>We already know that queries to both collections will be handled with an index. However, I’ve been wondering if there\nare any differences between these indexes?</p>\n\n<p>The first should be search time: since whenever there is a key in the condition list, its index will be used,\ntheoretically, key’s index should be the fastest. We’ll get to that topic in a moment. For now, let’s see what happens\nif we only want to search one field at a time:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">composite</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">artificial</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>In the case of the first query, the index was admittedly used, but we got no results, as evidenced by the notation:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionStages\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IDHACK\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>This happens because a composite key requires all its components to be used by the query, so it is impossible to search\nby a single field.</p>\n\n<p>The situation is different when querying the second collection, here the index was also used, however this time the\ndocument was found:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"winningPlan\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"FETCH\"</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"inputStage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IXSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"keyPattern\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"w\">\n      </span><span class=\"p\">},</span><span class=\"w\">\n      </span><span class=\"nl\">\"indexName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"name_1_surname_1\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>[…]</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"executionStats\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionSuccess\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">1</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>What if we decide to search by <code class=\"language-plaintext highlighter-rouge\">surname</code> only?</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">composite</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">artificial</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>In a collection with a composite key, we have a situation similar to the previous one - the index was used, but we\ndidn’t receive any document. The reason, of course, is the same: we didn’t use all the key fields.</p>\n\n<p>By querying the collection with a separate composite index, we got the document we were looking for, but it turns out\nthat this time the index was not used, and instead the database had to search through the entire collection:</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"winningPlan\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"stage\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"COLLSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"filter\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n            </span><span class=\"nl\">\"surname\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n                </span><span class=\"nl\">\"$eq\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Doe\"</span><span class=\"w\">\n            </span><span class=\"p\">}</span><span class=\"w\">\n        </span><span class=\"p\">},</span><span class=\"w\">\n        </span><span class=\"nl\">\"direction\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"forward\"</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>This is because with composite indexes, while it is possible not to use all indexed\nfields, it is only permissible to skip values in the reverse order, that is, reading indexed fields from the right.\nOur index was created on the fields in the following order:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">createIndex</span><span class=\"p\">({</span><span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"na\">unique</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>It is therefore possible to omit the condition on the <code class=\"language-plaintext highlighter-rouge\">surname</code> field and search only by <code class=\"language-plaintext highlighter-rouge\">name</code>, but it’s not possible\nthe other way around.</p>\n\n<p>We managed to find out the first difference between two types of indexes: composite key indexes are less flexible,\nrequire all values to be specified, while in regular composite indexes we can omit values from the right.</p>\n\n<p>Let’s also check if the order of conditions matter?</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">composite</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">_id</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"p\">{</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">artificial</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">surname</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Doe</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"dl\">\"</span><span class=\"s2\">name</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">John</span><span class=\"dl\">\"</span><span class=\"p\">}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">executionStats</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>And here’s another surprise: even though all the components of the key were provided, but in reverse order, the\nfirst query did not find the document.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"executionStages\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IDHACK\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"nReturned\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">0</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>With a regular composite index, the optimiser was able to reverse the field order itself and use the appropriate\nindex to find the document: <code class=\"language-plaintext highlighter-rouge\">\"nReturned\" : 1</code></p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"winningPlan\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"stage\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"FETCH\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"inputStage\"</span><span class=\"w\"> </span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"stage\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"IXSCAN\"</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"keyPattern\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n            </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"w\">\n            </span><span class=\"nl\">\"surname\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mf\">1.0</span><span class=\"w\">\n          </span><span class=\"p\">},</span><span class=\"w\">\n          </span><span class=\"nl\">\"indexName\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"name_1_surname_1\"</span><span class=\"w\">\n        </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>For the time being, indexes on composite keys are losing against regular ones. This is a good time to get back to the\nquestion about the search time of the two indexes. Now that we’ve established that indexes on composite keys are less\nflexible, it’s a good idea to figure out what we gain in return for such limitations. We already know that <code class=\"language-plaintext highlighter-rouge\">IDHACK</code>\nskips all indexes and always uses a key, so one might think that this is the fastest available way to get to the\ndocument. I decided to check this on my own.</p>\n\n<h2 id=\"its-time-to-experiment\">It’s time to experiment</h2>\n\n<p>I filled both previously used collections with 10 million documents. I used the following scripts for this purpose:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">composite</span><span class=\"p\">.</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span><span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">name_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">),</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">surname_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)}})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">artificial</span><span class=\"p\">.</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span><span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"k\">new</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(),</span> <span class=\"na\">name</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">name_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">),</span> <span class=\"na\">surname</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">surname_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>It is worth noting here that I’m adding documents in batches. This is definitely faster than a list of single inserts\nand is useful when we need to generate a large amount of data quickly. Also note that I am using existing collections\nso my index on <code class=\"language-plaintext highlighter-rouge\">name</code> and <code class=\"language-plaintext highlighter-rouge\">surname</code> fields already exists.</p>\n\n<p>I measured the execution time of both scripts using following commands (to avoid network latency I performed the\nmeasurements on my laptop, with a local instance of MongoDB version 4.4.0 running):</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill2.js\n</code></pre></div></div>\n\n<p>Filling collections with documents took <strong>58,95s</strong> and <strong>76,48s</strong> respectively. So when it comes to <code class=\"language-plaintext highlighter-rouge\">insert</code> operation time,\nthe composite key collection clearly wins. The reason for this, of course, is a simpler document structure and only one\nindex, instead of two.</p>\n\n<p>I was more interested in read times, because in a typical case, data is usually read more often than written. For each collection,\nI prepared a script containing a list of <code class=\"language-plaintext highlighter-rouge\">find</code> commands for 1 million documents in random order. Of course, the order of\nqueries in both scripts is the same:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.composite.find({_id: {name: 'name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'}})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.artificial.find({name: 'name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find2.js\n</code></pre></div></div>\n\n<p>Finally, I measured the execution time of both scripts using the commands:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find2.js\n</code></pre></div></div>\n\n<p>The results surprised me. The script running on the first collection took <strong>11.19s</strong> and the second took <strong>10.15s</strong>.\nAlthough the differences are small, I was sure that using a composite key would be much faster than searching\nthrough regular fields and would make up for all the inconvenience of less flexible keys. Meanwhile, it turns\nout that a collection built with an artificial key and a separate index on two fields wins in terms of search speed.</p>\n\n<p>I had one more idea. Maybe searching a document by a key that doesn’t exist would be faster?\nTo test this, I generated another two scripts, this time searching for non-existent documents:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.composite.find({_id: {name: 'missing_name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'missing_surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'}})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find-missing1.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.artificial.find({name: 'missing_name_</span><span class=\"nv\">$x</span><span class=\"s2\">', surname: 'missing_surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find-missing2.js\n</code></pre></div></div>\n\n<p>Unfortunately, the composite key collection also lost in this case: <strong>12.44s</strong> vs. <strong>10.26s</strong>.</p>\n\n<p>Finally, I decided to run one more test. Since when using composite key we have to pass the entire key to the query\nand cannot search by its fragment, I decided to create a third collection, this time its key being the concatenation of\nfirst and last name:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">concatenation</span><span class=\"p\">.</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span><span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">name_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"dl\">'</span><span class=\"s1\">-</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"dl\">'</span><span class=\"s1\">surname_</span><span class=\"dl\">'</span> <span class=\"o\">+</span> <span class=\"nx\">NumberInt</span><span class=\"p\">(</span><span class=\"nx\">i</span><span class=\"p\">)})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>I then prepared another script containing 1 million search operations (in the same order as the previous attempts,\nof course):</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for </span>i <span class=\"k\">in</span> <span class=\"o\">{</span>1..1000000<span class=\"o\">}</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n    <span class=\"nb\">echo</span> <span class=\"s2\">\"db.concatenation.find({_id: 'name_</span><span class=\"nv\">$x</span><span class=\"s2\">-surname_</span><span class=\"nv\">$x</span><span class=\"s2\">'})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find3.js\n</code></pre></div></div>\n\n<p>This time I was finally able to get a result better than the one achieved with the artificial key approach: <strong>9.71s</strong>.\nAll results are summarized in the chart below:</p>\n\n<p><img src=\"/img/articles/2021-10-18-comparing-mongodb-composite-indexes/final-results.png\" alt=\"Collection stats\" /></p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p>It appears that the <code class=\"language-plaintext highlighter-rouge\">IDHACK</code> operation is indeed the fastest possible way to access a document, but only for a simple\nkey. As soon as we add an additional field into the key, the benefits of the <code class=\"language-plaintext highlighter-rouge\">IDHACK</code> operation disappear and the\nregular index works better.</p>\n\n<p>In general, my experiments did not confirm the advantages of using a composite key. The artificial key model is more\nflexible in querying and returns results faster. However, if you want to search even faster, you may consider using a\nsimple key created by concatenating selected fields.</p>\n\n<p>I realize, of course, that I did not use the latest available version of the mongo server. For this reason, I am\nposting all the scripts I used in my attempts and encourage you to experiment on your own. Perhaps composite keys\nperform better in newer versions of the database?</p>\n","contentSnippet":"One of the key elements ensuring efficient operation of the services we work on every day at\nAllegro is fast responses from the database.\nWe spend a lot of time to properly model the data so that storing and querying take as little time as possible.\nYou can read more about why good schema design is important in one of my earlier\nposts.\nIt’s also equally important to make sure that all queries are covered with indexes of the correct type whenever\npossible. Indexes are used to quickly search the database and under certain conditions even allow results to be\nreturned directly from the index, without the need to access the data itself. However, indexes are not\nall the same and it’s important to learn more about their different types in order to make the right choices later on.\nWhat’s the difference?\nI’ve had a conversation with a colleague of mine the other day, about the point of using composite keys in a MongoDB\ndatabase. I’ve always been a firm believer that it’s a good idea to use a composite key wherever possible because\nsearching this way is very fast. My colleague, on the other hand, advocates using artificial keys and creating\nseparate composite indexes on fields for\nwhich I would use a composite key.\nAfter a brief disagreement, I realized that other than my intuition, I had no arguments to defend my beliefs.\nI decided to see how indexes on composite keys differ from composite indexes created on regular fields in practice.\nAs an example for our considerations, we will use an entity describing a person by first and last name, let’s also\nassume that this pair is unique.\nSuch data can be stored in a collection (let’s call it composite) with a composite key that contains both fields.\n\n{\n    \"_id\" : {\n        \"name\" : \"John\",\n        \"surname\" : \"Doe\"\n    }\n}\n\n\nA unique index will be automatically created on pair of both fields along with the collection.\nMany developers would most likely prefer to use an artificial key, and index the name and surname fields\nseparately, as shown in collection artificial:\n\n{\n    \"_id\" : ObjectId(\"615eb18b76e172647f7462e2\"),\n    \"name\" : \"John\",\n    \"surname\" : \"Doe\"\n}\n\n\nWhen it comes to the second model, only the artificial key will be automatically covered by the index. To be\nable to efficiently search the collection by first and last name, we need to manually create a composite index on both\nfields. To maintain consistency with the first collection, of course, uniqueness also needs to be enforced:\n\ndb.artificial.createIndex({name: 1, surname: 1}, {unique: true})\n\n\nAt first glance, we can clearly see that the first way of storing data is more compact, since we only store two fields\nof interest, and only one index is required in addition to the data. For the second collection, in addition to the data\nitself, we need to store an artificial key, moreover two indexes are required here: on the artificial key and on\nthe name and surname fields.\nWe can now move on to comparing the execution plans of queries to two collections, so let’s take a look at the result\nof the explain commands:\n\ndb.composite.find({\n    \"_id\" : {\n        \"name\" : \"John\",\n        \"surname\" : \"Doe\"\n    }\n}).explain(\"executionStats\")\n\n\nand:\n\ndb.artificial.find({\"name\" : \"John\", \"surname\" : \"Doe\"}).explain(\"executionStats\")\n\n\nLet’s start with the second result first. We can see that the optimiser chose to use the index we manually created.\n\n{\n  \"winningPlan\": {\n    \"stage\": \"FETCH\",\n    \"inputStage\": {\n      \"stage\": \"IXSCAN\",\n      \"keyPattern\": {\n        \"name\": 1.0,\n        \"surname\": 1.0\n      },\n      \"indexName\": \"name_1_surname_1\"\n    }\n  }\n}\n\n\n[…]\n\n{\n  \"executionStats\" : {\n    \"executionSuccess\": true,\n    \"nReturned\": 1\n  }\n}\n\n\nIn the case of a collection with a composite key, however, the plan is different; it contains the word IDHACK:\n\n{\n  \"winningPlan\": {\n    \"stage\": \"IDHACK\"\n  }\n}\n\n\n[…]\n\n{\n  \"executionStats\" : {\n    \"executionSuccess\": true,\n    \"nReturned\": 1\n  }\n}\n\n\nIt means that the optimiser skipped the index selection phase (although in our case there were no other indexes, but\nit doesn’t matter) and decided to use the key index. This operation is considered to be the fastest one possible.\nWhenever there is a key in the query, its index will be used (while ignoring conditions on other fields).\nLet’s also take a look at the notation \"nReturned\" : 1, which means that both queries returned a single document.\nWe already know that queries to both collections will be handled with an index. However, I’ve been wondering if there\nare any differences between these indexes?\nThe first should be search time: since whenever there is a key in the condition list, its index will be used,\ntheoretically, key’s index should be the fastest. We’ll get to that topic in a moment. For now, let’s see what happens\nif we only want to search one field at a time:\n\ndb.getCollection('composite').find({\n    \"_id\" : {\n        \"name\" : \"John\"\n    }\n}).explain(\"executionStats\")\n\n\n\ndb.getCollection('artificial').find({\"name\" : \"John\"}).explain(\"executionStats\")\n\n\nIn the case of the first query, the index was admittedly used, but we got no results, as evidenced by the notation:\n\n{\n    \"executionStages\": {\n        \"stage\": \"IDHACK\",\n        \"nReturned\": 0\n    }\n}\n\n\nThis happens because a composite key requires all its components to be used by the query, so it is impossible to search\nby a single field.\nThe situation is different when querying the second collection, here the index was also used, however this time the\ndocument was found:\n\n{\n  \"winningPlan\": {\n    \"stage\": \"FETCH\",\n    \"inputStage\": {\n      \"stage\": \"IXSCAN\",\n      \"keyPattern\": {\n        \"name\": 1.0,\n        \"surname\": 1.0\n      },\n      \"indexName\": \"name_1_surname_1\"\n    }\n  }\n}\n\n\n[…]\n\n{\n  \"executionStats\" : {\n    \"executionSuccess\": true,\n    \"nReturned\": 1\n  }\n}\n\n\nWhat if we decide to search by surname only?\n\ndb.getCollection('composite').find({\n    \"_id\" : {\n        \"surname\" : \"Doe\"\n    }\n}).explain(\"executionStats\")\n\n\n\ndb.getCollection('artificial').find({\"surname\" : \"Doe\"}).explain(\"executionStats\")\n\n\nIn a collection with a composite key, we have a situation similar to the previous one - the index was used, but we\ndidn’t receive any document. The reason, of course, is the same: we didn’t use all the key fields.\nBy querying the collection with a separate composite index, we got the document we were looking for, but it turns out\nthat this time the index was not used, and instead the database had to search through the entire collection:\n\n{\n    \"winningPlan\" : {\n        \"stage\" : \"COLLSCAN\",\n        \"filter\" : {\n            \"surname\" : {\n                \"$eq\" : \"Doe\"\n            }\n        },\n        \"direction\" : \"forward\"\n    }\n}\n\n\nThis is because with composite indexes, while it is possible not to use all indexed\nfields, it is only permissible to skip values in the reverse order, that is, reading indexed fields from the right.\nOur index was created on the fields in the following order:\n\ndb.artificial.createIndex({name: 1, surname: 1}, {unique: true})\n\n\nIt is therefore possible to omit the condition on the surname field and search only by name, but it’s not possible\nthe other way around.\nWe managed to find out the first difference between two types of indexes: composite key indexes are less flexible,\nrequire all values to be specified, while in regular composite indexes we can omit values from the right.\nLet’s also check if the order of conditions matter?\n\ndb.getCollection('composite').find({\n    \"_id\" : {\n        \"surname\" : \"Doe\",\n        \"name\" : \"John\"\n    }\n}).explain(\"executionStats\")\n\n\n\ndb.getCollection('artificial').find({\"surname\" : \"Doe\", \"name\" : \"John\"}).explain(\"executionStats\")\n\n\nAnd here’s another surprise: even though all the components of the key were provided, but in reverse order, the\nfirst query did not find the document.\n\n{\n    \"executionStages\" : {\n      \"stage\": \"IDHACK\",\n      \"nReturned\": 0\n    }\n}\n\n\nWith a regular composite index, the optimiser was able to reverse the field order itself and use the appropriate\nindex to find the document: \"nReturned\" : 1\n\n{\n    \"winningPlan\" : {\n        \"stage\" : \"FETCH\",\n        \"inputStage\" : {\n          \"stage\": \"IXSCAN\",\n          \"keyPattern\": {\n            \"name\": 1.0,\n            \"surname\": 1.0\n          },\n          \"indexName\": \"name_1_surname_1\"\n        }\n    }\n}\n\n\nFor the time being, indexes on composite keys are losing against regular ones. This is a good time to get back to the\nquestion about the search time of the two indexes. Now that we’ve established that indexes on composite keys are less\nflexible, it’s a good idea to figure out what we gain in return for such limitations. We already know that IDHACK\nskips all indexes and always uses a key, so one might think that this is the fastest available way to get to the\ndocument. I decided to check this on my own.\nIt’s time to experiment\nI filled both previously used collections with 10 million documents. I used the following scripts for this purpose:\n\nvar bulk = db.composite.initializeUnorderedBulkOp();\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({_id: {name: 'name_' + NumberInt(i), surname: 'surname_' + NumberInt(i)}})\n}\nbulk.execute();\n\n\n\nvar bulk = db.artificial.initializeUnorderedBulkOp();\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({_id: new ObjectId(), name: 'name_' + NumberInt(i), surname: 'surname_' + NumberInt(i)})\n}\nbulk.execute();\n\n\nIt is worth noting here that I’m adding documents in batches. This is definitely faster than a list of single inserts\nand is useful when we need to generate a large amount of data quickly. Also note that I am using existing collections\nso my index on name and surname fields already exists.\nI measured the execution time of both scripts using following commands (to avoid network latency I performed the\nmeasurements on my laptop, with a local instance of MongoDB version 4.4.0 running):\n\ntime mongo test fill1.js\n\n\n\ntime mongo test fill2.js\n\n\nFilling collections with documents took 58,95s and 76,48s respectively. So when it comes to insert operation time,\nthe composite key collection clearly wins. The reason for this, of course, is a simpler document structure and only one\nindex, instead of two.\nI was more interested in read times, because in a typical case, data is usually read more often than written. For each collection,\nI prepared a script containing a list of find commands for 1 million documents in random order. Of course, the order of\nqueries in both scripts is the same:\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.composite.find({_id: {name: 'name_$x', surname: 'surname_$x'}})\"\ndone >> find1.js\n\n\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.artificial.find({name: 'name_$x', surname: 'surname_$x'})\"\ndone >> find2.js\n\n\nFinally, I measured the execution time of both scripts using the commands:\n\ntime mongo test find1.js\n\n\n\ntime mongo test find2.js\n\n\nThe results surprised me. The script running on the first collection took 11.19s and the second took 10.15s.\nAlthough the differences are small, I was sure that using a composite key would be much faster than searching\nthrough regular fields and would make up for all the inconvenience of less flexible keys. Meanwhile, it turns\nout that a collection built with an artificial key and a separate index on two fields wins in terms of search speed.\nI had one more idea. Maybe searching a document by a key that doesn’t exist would be faster?\nTo test this, I generated another two scripts, this time searching for non-existent documents:\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.composite.find({_id: {name: 'missing_name_$x', surname: 'missing_surname_$x'}})\"\ndone >> find-missing1.js\n\n\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.artificial.find({name: 'missing_name_$x', surname: 'missing_surname_$x'})\"\ndone >> find-missing2.js\n\n\nUnfortunately, the composite key collection also lost in this case: 12.44s vs. 10.26s.\nFinally, I decided to run one more test. Since when using composite key we have to pass the entire key to the query\nand cannot search by its fragment, I decided to create a third collection, this time its key being the concatenation of\nfirst and last name:\n\nvar bulk = db.concatenation.initializeUnorderedBulkOp();\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({_id: 'name_' + NumberInt(i) + '-' + 'surname_' + NumberInt(i)})\n}\nbulk.execute();\n\n\nI then prepared another script containing 1 million search operations (in the same order as the previous attempts,\nof course):\n\n#!/bin/bash\nRANDOM=42\nfor i in {1..1000000}\ndo\n  x=$(( $RANDOM % 10000000))\n    echo \"db.concatenation.find({_id: 'name_$x-surname_$x'})\"\ndone >> find3.js\n\n\nThis time I was finally able to get a result better than the one achieved with the artificial key approach: 9.71s.\nAll results are summarized in the chart below:\n\nConclusion\nIt appears that the IDHACK operation is indeed the fastest possible way to access a document, but only for a simple\nkey. As soon as we add an additional field into the key, the benefits of the IDHACK operation disappear and the\nregular index works better.\nIn general, my experiments did not confirm the advantages of using a composite key. The artificial key model is more\nflexible in querying and returns results faster. However, if you want to search even faster, you may consider using a\nsimple key created by concatenating selected fields.\nI realize, of course, that I did not use the latest available version of the mongo server. For this reason, I am\nposting all the scripts I used in my attempts and encourage you to experiment on your own. Perhaps composite keys\nperform better in newer versions of the database?","guid":"https://blog.allegro.tech/2021/10/comparing-mongodb-composite-indexes.html","categories":["tech","mongodb","index","performance","query tuning"],"isoDate":"2021-10-17T22:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"}],"jobs":[{"id":"743999785422127","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"229d607a-333b-431b-9abe-78137730f5fd","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:56:17.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785422127","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999785421861","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"a6b2b59e-28e3-4bfa-89ab-b13ab97f06c8","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:54:52.000Z","location":{"city":"Warszawa, Poznań, Kraków, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785421861","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999782019147","name":"IT Security Specialist","uuid":"3c61af42-98a1-4b48-ae1f-dc9b2c60a0fa","refNumber":"REF2548W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-27T10:32:45.000Z","location":{"city":"Warszawa, Poznań, Kraków, Wrocław, Toruń, Gdańsk, Lublin, Łódź, Katowice","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"faf591d7-af03-4d05-b67e-dba247924756","valueLabel":"IT - Other"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"SEC, RED, BLUE, Security"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999782019147","creator":{"name":"Adriana Gesiarz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779485268","name":"Chief Architect","uuid":"5ea8adaa-e9ae-4cb2-a1dc-b27600247ffb","refNumber":"REF2835R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T13:53:22.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Gdańsk, Łódź, Katowice, Lublin","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"chief architect, architekt, architect, platform, architektura"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779485268","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779448775","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"c8e577cc-c93a-43e7-8e73-e430989798d7","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:36.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448775","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1635344914000,"duration":7200000,"id":"281692274","name":"Allegro Tech Live #23 - Przygody backendowców w C#","date_in_series_pattern":false,"status":"past","time":1636045200000,"local_date":"2021-11-04","local_time":"18:00","updated":1636056125000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":29,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281692274/","description":"------ Rejestracja: https://app.evenea.pl/event/allegro-tech-live-23/------- Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Kiedyś spotykaliśmy się w naszych biurach, a teraz…","visibility":"public","member_pay_fee":false},{"created":1634290537000,"duration":5400000,"id":"281441586","name":"Allegro Tech Live #22 - Jak wygląda codzienność lidera w Allegro?","date_in_series_pattern":false,"status":"past","time":1634832000000,"local_date":"2021-10-21","local_time":"18:00","updated":1634841007000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":67,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281441586/","description":"!!!! Rejestracja: https://app.evenea.pl/event/allegro-tech-live-22/ !!!! Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale…","visibility":"public","member_pay_fee":false},{"created":1633336352000,"duration":7200000,"id":"281199452","name":"Allegro Tech Live #21 - Jak zautomatyzować bezpieczeństwo IT?","date_in_series_pattern":false,"status":"past","time":1633622400000,"local_date":"2021-10-07","local_time":"18:00","updated":1633634017000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281199452/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false},{"created":1629120828000,"duration":93600000,"id":"280149404","name":"Allegro Tech Meeting 2021","date_in_series_pattern":false,"status":"past","time":1632927600000,"local_date":"2021-09-29","local_time":"17:00","updated":1633024970000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":144,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/280149404/","description":"❗Uwaga, aby wziąć udział w wydarzeniu zarejestruj się tutaj: https://app.evenea.pl/event/atm-2021/ ❗ Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to…","how_to_find_us":"https://www.youtube.com/c/AllegroTechBlog","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}