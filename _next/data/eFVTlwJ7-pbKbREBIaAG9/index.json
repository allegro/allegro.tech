{"pageProps":{"posts":[{"title":"Probabilistic Data Structures and Algorithms in NoSQL databases","link":"https://blog.allegro.tech/2022/10/probabilistic-algorithms.html","pubDate":"Tue, 04 Oct 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>One of the <a href=\"https://en.wikipedia.org/wiki/ACID\">four fundamental</a> features of transactional databases is durability. It says that once a\ntransaction is committed, the stored data remains available even if the database crashes. If we upload some information\ninto the database, we must be able to read it later, no matter what happens.</p>\n\n<p>It is so elementary that we frequently don’t even think about it: if we save a record with the ’42’\nvalue in a database, we will get ’42’ every time we read that\nrecord, until the next modification. The durability concept can be generalized somewhat, by considering not only transactional\ndatabases but those that do not provide transactions. After all, in each of them, after a\ncorrect write we can be sure that the stored information is in the database and we have access\nto it.</p>\n\n<p>But it turns out that there are databases that provide us with solutions making that the concept of durability —\neven in this generalized form — no longer so obvious. What would you say if we stored\n1 000 records in a database, and the database claimed that there were only 998 of them? Or, if we\ncreated a database storing sets of values and in some cases the database would claim that an\nelement was in that set, while in fact it was not? Seeing such a behavior many would probably start\nlooking for an error. However, behavior like this is not necessarily an error, as long as we use a database\nthat implements probabilistic algorithms and data structures. Solutions based on these methods allow some\ninaccuracy in the results, but in return they are able to provide us with great savings in the resources\nused. More interesting is that there is a good chance that you are already using such a DB.</p>\n\n<p>In this post we will learn about two probability-based techniques, perform some experiments and\nconsider when it is worth using a database that lies to us a bit.</p>\n\n<h2 id=\"fast-cardinality-aggregation\">Fast cardinality aggregation</h2>\n\n<p>Some time ago I had the opportunity to work on a service based on Elasticsearch. This service collects\nhuge amounts of data, which is later analyzed by our customer care specialists. One of the key elements to be analyzed\nis a simple aggregate — the number of unique occurrences of certain values. In mathematics, this\nquantity is called the power of the set or the cardinal number.</p>\n\n<p>The easiest way to understand this is to use an example: imagine that I take out all the banknotes\nfrom my wallet and it turns out that I have 10 of them, with the following nominal values:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>If we arranged them by value, we would end up collecting these 10\nbanknotes in four piles with values: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 100]</code>, so the cardinal number of the set containing my 10\nbanknotes equals: 4.</p>\n\n<p>Elasticsearch has a special function: <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html\">cardinality</a>, which is used to determine the power of the set and we use\nthis function specifically to count unique occurrences that I mentioned earlier.</p>\n\n<p>It may seem that counting unique occurrences of values is a trivial task.\nLet’s go back to our example with the banknotes. You can think of many ways to check how many\nunique values there are in this list, probably one of the simplest is to use the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class. One of its main features is\nthat it de-duplicates the elements added to it, thus it stores only one occurrence of each.</p>\n\n<p>After adding 10 values of banknotes: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]</code> to an instance of the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>\nclass, it will ultimately only store the values <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 100]</code> (not necessarily in that order, but it\ndoesn’t matter it this case). So all we need to do is check the size of this set and we have the result we were\nlooking for: 4.</p>\n\n<p>This solution is simple and looks tempting, yet it has a certain drawback: the more unique elements the set stores,\nthe more memory our program needs. In an extreme case, when each added element is different from\nthe others, the memory complexity of this approach will be linear. This is bad news when we\nwant to operate on a large volume of data, because we will immediately use all available memory.\nIf, additionally, requests for the cardinal number come from\nclients with high intensity, and the input set contains billions of elements, it is easy to imagine that the\napproach described above has no chance of success.</p>\n\n<p>How to address this issue? In such a situation we can switch to one of ingenious probabilistic algorithms. Their\nmain feature is that they give approximate rather than exact results. The huge advantage, on the\nother hand, is that they are much less resource-intensive.</p>\n\n<h2 id=\"near-optimal-cardinality-estimator\">Near-optimal cardinality estimator</h2>\n\n<p>One such algorithm — HyperLogLog (HLL) — has been implemented in the aforementioned\nElasticsearch to build the cardinality function. It is used to count the unique values of a given field of\nan indexed document, and it does so with a certain approximation, using very little memory.\nInterestingly, you can control the accuracy of this approximation with a special parameter. This is\nbecause in addition to the field to be counted, the cardinality function also accepts a\n<code class=\"language-plaintext highlighter-rouge\">precision_threshold</code> argument, due to which we can specify how much inaccuracy we agree to, in\nexchange for less or more memory usage.</p>\n\n<p>Obviously, in some cases even a small error is unacceptable. We must then abandon the probabilistic\napproach and look for another solution. However, for a sizable class of problems, certain\napproximation is completely sufficient. Imagine a video clip uploaded to a popular streaming service.\nIf the author of the clip has a bit of luck, the counter of unique views of his/her work starts spinning\nvery quickly. In case of very high popularity, when displaying the current number of visits, full\naccuracy will not matter so much; we can reconcile with displaying a value that differs from the\nactual one by a few percent. It is completely sufficient that the accurate data — e.g. for monetization\npurposes — is available the next day, when we calculate it accurately using, for example, Apache Spark.</p>\n\n<p>Implementing such a counter of unique visitors into a site operating on huge data sets, we could\ntherefore consider using the HLL algorithm.</p>\n\n<p>Readers interested in a detailed description of the HLL algorithm are referred to a great article on\n<a href=\"http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation\">Damn Cool Algorithms post</a>.\nHowever, its most important features are worth noting here:</p>\n<ul>\n  <li>the results, although approximate, are deterministic,</li>\n  <li>the maximum possible error is known,</li>\n  <li>amount of memory used is fixed.</li>\n</ul>\n\n<p>The last two features are closely related and can be controlled: we can decrease the error level by increasing\nthe available memory limit and vice versa.\nThere are many ready-made implementations of the HLL algorithm available, so it’s worth reaching\nfor one of them and doing some experiments. I will use <a href=\"https://datasketches.apache.org/docs/HLL/HLL.html\">datasketches</a>\nand compare the memory consumption with the classic approach using the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. Moreover, I will add a third variant based\non a <code class=\"language-plaintext highlighter-rouge\">distinct</code> method from the Kotlin language, which — like the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> constructor — de-duplicates\nelements from the list.</p>\n\n<p>Below there is a code snippet of a simple program that determines the cardinal number of a set of numbers using <code class=\"language-plaintext highlighter-rouge\">HashSet</code>\nclass from Java language. In order to be able to run some trials, I’ve introduced a couple of basic parameters. The\ninput list consists of <code class=\"language-plaintext highlighter-rouge\">n</code> numbers, while using the <code class=\"language-plaintext highlighter-rouge\">f</code> parameter and the <code class=\"language-plaintext highlighter-rouge\">modulo</code> function I decide what\npart of the input list is unique. For example, for n=1 000 000 and f=0.1, the result will be a cardinal\nnumber equal to 100 000.</p>\n\n<p>Please note the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> constructor parameter. By default, when the constructor is empty - this class is\n<a href=\"https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/HashSet.html#%3Cinit%3E()\">initialized with the value 16</a>,\nwhich means that before adding the 17th element, memory reallocation must occur for next portion of elements, which takes time.\nTo eliminate this extra time I allocate in advance as much memory as needed.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">mod</span> <span class=\"p\">=</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">*</span> <span class=\"n\">f</span><span class=\"p\">).</span><span class=\"nf\">toLong</span><span class=\"p\">()</span>\n<span class=\"kd\">val</span> <span class=\"py\">set</span> <span class=\"p\">=</span> <span class=\"nc\">HashSet</span><span class=\"p\">&lt;</span><span class=\"nc\">Long</span><span class=\"p\">&gt;(</span><span class=\"n\">mod</span><span class=\"p\">.</span><span class=\"nf\">toInt</span><span class=\"p\">())</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">elapsed</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"mi\">0</span> <span class=\"n\">until</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">set</span><span class=\"p\">.</span><span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"p\">%</span> <span class=\"n\">mod</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"n\">cardinality</span> <span class=\"p\">=</span> <span class=\"k\">set</span><span class=\"p\">.</span><span class=\"n\">size</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Two other programs do exactly the same thing: determine the cardinal number of a set of numbers, but one uses Kotlin\n<code class=\"language-plaintext highlighter-rouge\">distinct</code> method and the second one uses HLL algorithm. You can find full code of all three applications\non this <a href=\"https://github.com/mknasiecki/prob-alg-post\">repository</a>.</p>\n\n<p>All three programs, in addition to the result, also measure total execution time. Moreover, using\n<a href=\"https://openjdk.java.net/tools/svc/jconsole/\">jConsole</a> I am also able to measure the amount of memory used. I decided\nto measure the total memory used by the\nprograms, because measuring the size of the data structures is not a trivial task.</p>\n\n<p>We start by checking the variant n=1 000 000/f=0.25 as a result of which we should get a power of set\nequal 250 000. Let’s take a look at the results:</p>\n\n<p><em>n=1 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>250 000</td>\n      <td>250 000</td>\n      <td>249 979.9</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>71</td>\n      <td>106</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>42</td>\n      <td>73</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>In case of such a small set the deviation of the result of the HLL variant from the true value is far less than\n1%, while in this case you can already see the benefits of this method; the amount of memory used is\nhalf compared to the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> version and as much as 3 times less when compared to the\nversion using the Kotlin language function.</p>\n\n<p>It is worth pausing here for a moment to consider what is the reason for such a big difference in consumed memory.\nThe first two programs are based on collections of objects, thus storing in memory entire instances along with their references.\nThe HLL method, on the other hand, uses memory-efficient bit arrays that store data based on object hashes. This makes\nit insensitive to the original size of the processed data. It means that the benefits of using HLL increase with the\nmemory needed to store the objects you want to count. The results presented above would be even more spectacular if we\nused, for example, email addresses or IP addresses instead of numbers.</p>\n\n<p>During the next attempt we increase the value of the <code class=\"language-plaintext highlighter-rouge\">n</code> parameter tenfold:</p>\n\n<p><em>n=10 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>2 500 000</td>\n      <td>2 500 000</td>\n      <td>2 484 301.4</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>483</td>\n      <td>863</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>233</td>\n      <td>574</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The error value has increased slightly, while the difference in memory usage and the performance\ntime is even greater than before. Therefore, it is worthwhile to increase the size of the set again:</p>\n\n<p><em>n=100 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>25 000 000</td>\n      <td>25 000 000</td>\n      <td>25 301 157.2</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.2</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>3857</td>\n      <td>7718</td>\n      <td>1538</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>1800</td>\n      <td>5300</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Deviation from the correct result exceeded 1%; the times also went up, although they are still many\ntimes shorter compared to other variants. It’s worth noting that the amount of memory used has practically not changed.</p>\n\n<p>Now let’s see what happens when we change the second parameter, which determines the number of\nunique elements in the input set:</p>\n\n<p><em>n=10 000 000/f=0.5</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>5 000 000</td>\n      <td>5 000 000</td>\n      <td>5 067 045.2</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.34</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>467</td>\n      <td>914</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>420</td>\n      <td>753</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>n=10 000 000/f=0.75</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>7 500 000</td>\n      <td>7 500 000</td>\n      <td>7 619 136.7</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.59</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>589</td>\n      <td>1187</td>\n      <td>191</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>616</td>\n      <td>843</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Again, the results clearly show the advantages of the HLL algorithm. With a relatively low error we\nsignificantly reduced the amount of memory used and the time required for calculations.\nAs you can see and as expected, the classical approach gives accurate results but it consumes a lot of\nmemory, while the solution using HLL brings results characterized by approx. 1% error, but in return\nwe use much less memory. A certain surprise for me is the poor result of the Kotlin <code class=\"language-plaintext highlighter-rouge\">distinct</code> function; I\nexpected results more similar to the variant based on the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. Presumably the key difference is that it returns an instance\nof the <code class=\"language-plaintext highlighter-rouge\">List</code> class rather than <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. This requires further investigation, which is beyond the scope of my considerations.</p>\n\n<p>The HLL algorithm is implemented in several solutions, including the aforementioned Elasticsearch,\nas well as in e.g. <a href=\"https://redis.com/redis-best-practices/counting/hyperloglog/\">Redis</a> and <a href=\"https://prestodb.io/docs/current/functions/hyperloglog.html\">Presto</a>. The above experiments clearly show that the approximate method, in case\nwe need to process huge amounts of data, is a good idea provided that we allow a result with a small\nerror.</p>\n\n<h2 id=\"memory-efficient-presence-test\">Memory-efficient presence test</h2>\n\n<p>It turns out that the HLL is not the only probabilistic algorithm available in popular databases —\nanother example of this approach is the Bloom Filter. This is an implementation of a memory-saving structure that is\nused in the so-called presence test. Let’s go back to our example with my cash: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]</code>.\nImagine that we want to test whether there is a 100 value banknote in my wallet. In this case the answer is positive, but the test\nfor the 200 value banknote should be false, since there is no such a banknote in the wallet.</p>\n\n<p>Of course, we are able again to implement a solution to this problem by simply using the properties\nof the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class and the <code class=\"language-plaintext highlighter-rouge\">contains</code> method. However, similarly as in case of determining the\ncardinality — the memory requirement increases with the size of the dataset.\nAgain, the solution for this problem may be an approximate method.</p>\n\n<p>Similarly as in case of the HLL algorithm the Bloom Filter allows for some inaccuracy, and in this case\nthis means false positive results. This is because it can happen that the Bloom Filter finds that an element\nbelongs to a given set, while in fact it is not there. However, the opposite situation is not possible\nso if the Bloom Filter states that an element is not part of the set, it is certainly true. Referring this to\nour example with the content of my wallet, the Bloom Filter could therefore assure me that there was a\n200 value banknote in it, while standing at the checkout in a store it would turn out that,\nunfortunately, it is not there. What a pity…</p>\n\n<p>Before we move on to examine how this algorithm works, let’s consider where it could be useful. A\ntypical example is a recommendation system. Imagine we are designing a system intended to suggest\narticles for users to read, a feature common on social media sites. Such a system needs to store a\nlist of articles read by each user so that it does not suggest them again. It is easy to imagine that\nstoring these articles with each user in the classic way would quickly exhaust memory resources. If we\ndon’t use any data removal mechanism, the database will grow indefinitely. The discussed Bloom\nFilter fits perfectly here as it will allow us to save a lot of memory, although, one must consider\nconsequences of its limitations related to possible false results. It may happen that we will get false\ninformation that a certain article has already been read by someone, while in fact this is not true.\nConsequently, we will not offer that user to read the material. On the other hand, the opposite\nsituation is not possible: we will never display to a user a recommendation of an article that he/she\nhas already read.</p>\n\n<p>At this point it is worth checking how much we gain by accepting the inconvenience described\nabove. I have prepared two implementations of a program that adds to a set of values and then\nchecks if they are there.\nThe first program uses the classic approach — the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class, while the second uses the Bloom\nFilter available in the popular <a href=\"https://guava.dev/releases/20.0/api/docs/com/google/common/hash/BloomFilter.html\">guava</a> library.\nAgain, using jConsole we register for both programs the amount of memory used, and additionally — for the version with\nthe Bloom Filter we also check the\nnumber of false positives. This value can be easily controlled, as the maximum allowed false positive\nrate can be set in the API; for needs of the following tests we will set it to 1%.</p>\n\n<p>Moreover, we will measure the total time of adding values to the set and the total time of querying\nwhether there are values in the set.</p>\n\n<p>Same as before we will perform a number of tests using the following parameters: <code class=\"language-plaintext highlighter-rouge\">n</code> — the size of the set of\nnumbers, and <code class=\"language-plaintext highlighter-rouge\">f</code> — what part of it should be added to the set. The configuration n=1 000 000 and f=0.1 means\nthat the first 100 000 numbers out of 1 000 000 will be added to the set. So, in the first part, the program will\nadd 100 000 numbers to the set and then — in the second stage — it will perform a presence test\nby checking whether the numbers above 100 000 belong to the set. There is no point in checking the\nnumbers added to the set beforehand, because we know that Bloom Filters do not give false\nnegative results. On the other hand, if any number above 100 000 is found according to the Bloom Filter in\nthe set, we will consider it a false positive.</p>\n\n<p>Following code snippet presents fragment of the Bloom Filter variant:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">insertions</span> <span class=\"p\">=</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">*</span> <span class=\"n\">f</span><span class=\"p\">).</span><span class=\"nf\">toInt</span><span class=\"p\">()</span>\n<span class=\"kd\">val</span> <span class=\"py\">filter</span> <span class=\"p\">=</span> <span class=\"nc\">BloomFilter</span><span class=\"p\">.</span><span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"nc\">Funnels</span><span class=\"p\">.</span><span class=\"nf\">integerFunnel</span><span class=\"p\">(),</span> <span class=\"n\">insertions</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"kd\">var</span> <span class=\"py\">falsePositives</span> <span class=\"p\">=</span> <span class=\"mi\">0</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">insertTime</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"mi\">0</span> <span class=\"n\">until</span> <span class=\"n\">insertions</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"n\">filter</span><span class=\"p\">.</span><span class=\"nf\">put</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">queryTime</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"n\">insertions</span> <span class=\"n\">until</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">.</span><span class=\"nf\">mightContain</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">))</span> <span class=\"p\">{</span>\n            <span class=\"n\">falsePositives</span><span class=\"p\">++;</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">fpRatio</span> <span class=\"p\">=</span> <span class=\"n\">falsePositives</span><span class=\"p\">/</span><span class=\"n\">n</span><span class=\"p\">.</span><span class=\"nf\">toDouble</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>Again — you can find full code of both programs on aforementioned <a href=\"https://github.com/mknasiecki/prob-alg-post\">repository</a>.</p>\n\n<p>Let’s start with the following configuration: n=10 000 000/f=0.1:</p>\n\n<p><em>n=10 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>81</td>\n      <td>293</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>82</td>\n      <td>846</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>94</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>As you can see, the Bloom Filter returned less than 1% false results, but — at the same time — it used three times\nless memory than HashSet variant. Unfortunately, the times of Bloom Filter’s version are significantly higher.\nLet’s check what happens when we increase the size of the input set:</p>\n\n<p><em>n=100 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>593</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>988</td>\n      <td>944</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>876</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>n=500 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>1975</td>\n      <td>1372</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>4115</td>\n      <td>4923</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>4400</td>\n      <td>81</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The number of false positives is still below the preset 1%, the amount of memory used is still lower\nthan the classic implementation, and interestingly also the times of the probabilistic variant are\nlower, at least for inserting. Thus, it can be seen that along with the increase in the size of the data the benefit of this\nmethod increases.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>The above results clearly show that by accepting a small share of false answers, we can gain significant savings in memory usage.\nSimilarly to the HLL algorithm, the structure based on the Bloom Filters is available in many popular\ndatabases like <a href=\"https://redis.com/redis-best-practices/bloom-filter-pattern/\">Redis</a>,\n<a href=\"https://hbase.apache.org/2.2/devapidocs/org/apache/hadoop/hbase/util/BloomFilter.html\">HBase</a>\nor <a href=\"https://cassandra.apache.org/doc/latest/cassandra/operating/bloom_filters.html\">Cassandra</a>.</p>\n\n<p>The simple experiments we conducted showed that probabilistic algorithms can save a lot\nof memory, which is especially important if our database stores huge amounts of data. In such cases it\nis sometimes worth letting your database lie to you a little.</p>\n","contentSnippet":"One of the four fundamental features of transactional databases is durability. It says that once a\ntransaction is committed, the stored data remains available even if the database crashes. If we upload some information\ninto the database, we must be able to read it later, no matter what happens.\nIt is so elementary that we frequently don’t even think about it: if we save a record with the ’42’\nvalue in a database, we will get ’42’ every time we read that\nrecord, until the next modification. The durability concept can be generalized somewhat, by considering not only transactional\ndatabases but those that do not provide transactions. After all, in each of them, after a\ncorrect write we can be sure that the stored information is in the database and we have access\nto it.\nBut it turns out that there are databases that provide us with solutions making that the concept of durability —\neven in this generalized form — no longer so obvious. What would you say if we stored\n1 000 records in a database, and the database claimed that there were only 998 of them? Or, if we\ncreated a database storing sets of values and in some cases the database would claim that an\nelement was in that set, while in fact it was not? Seeing such a behavior many would probably start\nlooking for an error. However, behavior like this is not necessarily an error, as long as we use a database\nthat implements probabilistic algorithms and data structures. Solutions based on these methods allow some\ninaccuracy in the results, but in return they are able to provide us with great savings in the resources\nused. More interesting is that there is a good chance that you are already using such a DB.\nIn this post we will learn about two probability-based techniques, perform some experiments and\nconsider when it is worth using a database that lies to us a bit.\nFast cardinality aggregation\nSome time ago I had the opportunity to work on a service based on Elasticsearch. This service collects\nhuge amounts of data, which is later analyzed by our customer care specialists. One of the key elements to be analyzed\nis a simple aggregate — the number of unique occurrences of certain values. In mathematics, this\nquantity is called the power of the set or the cardinal number.\nThe easiest way to understand this is to use an example: imagine that I take out all the banknotes\nfrom my wallet and it turns out that I have 10 of them, with the following nominal values:\n\n[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]\n\n\nIf we arranged them by value, we would end up collecting these 10\nbanknotes in four piles with values: [10, 20, 50, 100], so the cardinal number of the set containing my 10\nbanknotes equals: 4.\nElasticsearch has a special function: cardinality, which is used to determine the power of the set and we use\nthis function specifically to count unique occurrences that I mentioned earlier.\nIt may seem that counting unique occurrences of values is a trivial task.\nLet’s go back to our example with the banknotes. You can think of many ways to check how many\nunique values there are in this list, probably one of the simplest is to use the HashSet class. One of its main features is\nthat it de-duplicates the elements added to it, thus it stores only one occurrence of each.\nAfter adding 10 values of banknotes: [10, 20, 50, 20, 50, 100, 50, 20, 10, 10] to an instance of the HashSet\nclass, it will ultimately only store the values [10, 20, 50, 100] (not necessarily in that order, but it\ndoesn’t matter it this case). So all we need to do is check the size of this set and we have the result we were\nlooking for: 4.\nThis solution is simple and looks tempting, yet it has a certain drawback: the more unique elements the set stores,\nthe more memory our program needs. In an extreme case, when each added element is different from\nthe others, the memory complexity of this approach will be linear. This is bad news when we\nwant to operate on a large volume of data, because we will immediately use all available memory.\nIf, additionally, requests for the cardinal number come from\nclients with high intensity, and the input set contains billions of elements, it is easy to imagine that the\napproach described above has no chance of success.\nHow to address this issue? In such a situation we can switch to one of ingenious probabilistic algorithms. Their\nmain feature is that they give approximate rather than exact results. The huge advantage, on the\nother hand, is that they are much less resource-intensive.\nNear-optimal cardinality estimator\nOne such algorithm — HyperLogLog (HLL) — has been implemented in the aforementioned\nElasticsearch to build the cardinality function. It is used to count the unique values of a given field of\nan indexed document, and it does so with a certain approximation, using very little memory.\nInterestingly, you can control the accuracy of this approximation with a special parameter. This is\nbecause in addition to the field to be counted, the cardinality function also accepts a\nprecision_threshold argument, due to which we can specify how much inaccuracy we agree to, in\nexchange for less or more memory usage.\nObviously, in some cases even a small error is unacceptable. We must then abandon the probabilistic\napproach and look for another solution. However, for a sizable class of problems, certain\napproximation is completely sufficient. Imagine a video clip uploaded to a popular streaming service.\nIf the author of the clip has a bit of luck, the counter of unique views of his/her work starts spinning\nvery quickly. In case of very high popularity, when displaying the current number of visits, full\naccuracy will not matter so much; we can reconcile with displaying a value that differs from the\nactual one by a few percent. It is completely sufficient that the accurate data — e.g. for monetization\npurposes — is available the next day, when we calculate it accurately using, for example, Apache Spark.\nImplementing such a counter of unique visitors into a site operating on huge data sets, we could\ntherefore consider using the HLL algorithm.\nReaders interested in a detailed description of the HLL algorithm are referred to a great article on\nDamn Cool Algorithms post.\nHowever, its most important features are worth noting here:\nthe results, although approximate, are deterministic,\nthe maximum possible error is known,\namount of memory used is fixed.\nThe last two features are closely related and can be controlled: we can decrease the error level by increasing\nthe available memory limit and vice versa.\nThere are many ready-made implementations of the HLL algorithm available, so it’s worth reaching\nfor one of them and doing some experiments. I will use datasketches\nand compare the memory consumption with the classic approach using the HashSet. Moreover, I will add a third variant based\non a distinct method from the Kotlin language, which — like the HashSet constructor — de-duplicates\nelements from the list.\nBelow there is a code snippet of a simple program that determines the cardinal number of a set of numbers using HashSet\nclass from Java language. In order to be able to run some trials, I’ve introduced a couple of basic parameters. The\ninput list consists of n numbers, while using the f parameter and the modulo function I decide what\npart of the input list is unique. For example, for n=1 000 000 and f=0.1, the result will be a cardinal\nnumber equal to 100 000.\nPlease note the HashSet constructor parameter. By default, when the constructor is empty - this class is\ninitialized with the value 16,\nwhich means that before adding the 17th element, memory reallocation must occur for next portion of elements, which takes time.\nTo eliminate this extra time I allocate in advance as much memory as needed.\n\nval mod = (n * f).toLong()\nval set = HashSet<Long>(mod.toInt())\n\nval elapsed = measureTimeMillis {\n    for (i in 0 until n) {\n        set.add(i % mod)\n    }\n\n    cardinality = set.size\n}\n\n\nTwo other programs do exactly the same thing: determine the cardinal number of a set of numbers, but one uses Kotlin\ndistinct method and the second one uses HLL algorithm. You can find full code of all three applications\non this repository.\nAll three programs, in addition to the result, also measure total execution time. Moreover, using\njConsole I am also able to measure the amount of memory used. I decided\nto measure the total memory used by the\nprograms, because measuring the size of the data structures is not a trivial task.\nWe start by checking the variant n=1 000 000/f=0.25 as a result of which we should get a power of set\nequal 250 000. Let’s take a look at the results:\nn=1 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      250 000\n      250 000\n      249 979.9\n    \nerror [%]\n      0\n      0\n      0.01\n    \ntime [ms]\n      71\n      106\n      53\n    \nmemory [MB]\n      42\n      73\n      21\n    \nIn case of such a small set the deviation of the result of the HLL variant from the true value is far less than\n1%, while in this case you can already see the benefits of this method; the amount of memory used is\nhalf compared to the HashSet version and as much as 3 times less when compared to the\nversion using the Kotlin language function.\nIt is worth pausing here for a moment to consider what is the reason for such a big difference in consumed memory.\nThe first two programs are based on collections of objects, thus storing in memory entire instances along with their references.\nThe HLL method, on the other hand, uses memory-efficient bit arrays that store data based on object hashes. This makes\nit insensitive to the original size of the processed data. It means that the benefits of using HLL increase with the\nmemory needed to store the objects you want to count. The results presented above would be even more spectacular if we\nused, for example, email addresses or IP addresses instead of numbers.\nDuring the next attempt we increase the value of the n parameter tenfold:\nn=10 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      2 500 000\n      2 500 000\n      2 484 301.4\n    \nerror [%]\n      0\n      0\n      0.63\n    \ntime [ms]\n      483\n      863\n      189\n    \nmemory [MB]\n      233\n      574\n      21\n    \nThe error value has increased slightly, while the difference in memory usage and the performance\ntime is even greater than before. Therefore, it is worthwhile to increase the size of the set again:\nn=100 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      25 000 000\n      25 000 000\n      25 301 157.2\n    \nerror [%]\n      0\n      0\n      1.2\n    \ntime [ms]\n      3857\n      7718\n      1538\n    \nmemory [MB]\n      1800\n      5300\n      21\n    \nDeviation from the correct result exceeded 1%; the times also went up, although they are still many\ntimes shorter compared to other variants. It’s worth noting that the amount of memory used has practically not changed.\nNow let’s see what happens when we change the second parameter, which determines the number of\nunique elements in the input set:\nn=10 000 000/f=0.5\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      5 000 000\n      5 000 000\n      5 067 045.2\n    \nerror [%]\n      0\n      0\n      1.34\n    \ntime [ms]\n      467\n      914\n      183\n    \nmemory [MB]\n      420\n      753\n      21\n    \nn=10 000 000/f=0.75\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      7 500 000\n      7 500 000\n      7 619 136.7\n    \nerror [%]\n      0\n      0\n      1.59\n    \ntime [ms]\n      589\n      1187\n      191\n    \nmemory [MB]\n      616\n      843\n      26\n    \nAgain, the results clearly show the advantages of the HLL algorithm. With a relatively low error we\nsignificantly reduced the amount of memory used and the time required for calculations.\nAs you can see and as expected, the classical approach gives accurate results but it consumes a lot of\nmemory, while the solution using HLL brings results characterized by approx. 1% error, but in return\nwe use much less memory. A certain surprise for me is the poor result of the Kotlin distinct function; I\nexpected results more similar to the variant based on the HashSet. Presumably the key difference is that it returns an instance\nof the List class rather than HashSet. This requires further investigation, which is beyond the scope of my considerations.\nThe HLL algorithm is implemented in several solutions, including the aforementioned Elasticsearch,\nas well as in e.g. Redis and Presto. The above experiments clearly show that the approximate method, in case\nwe need to process huge amounts of data, is a good idea provided that we allow a result with a small\nerror.\nMemory-efficient presence test\nIt turns out that the HLL is not the only probabilistic algorithm available in popular databases —\nanother example of this approach is the Bloom Filter. This is an implementation of a memory-saving structure that is\nused in the so-called presence test. Let’s go back to our example with my cash: [10, 20, 50, 20, 50, 100, 50, 20, 10, 10].\nImagine that we want to test whether there is a 100 value banknote in my wallet. In this case the answer is positive, but the test\nfor the 200 value banknote should be false, since there is no such a banknote in the wallet.\nOf course, we are able again to implement a solution to this problem by simply using the properties\nof the HashSet class and the contains method. However, similarly as in case of determining the\ncardinality — the memory requirement increases with the size of the dataset.\nAgain, the solution for this problem may be an approximate method.\nSimilarly as in case of the HLL algorithm the Bloom Filter allows for some inaccuracy, and in this case\nthis means false positive results. This is because it can happen that the Bloom Filter finds that an element\nbelongs to a given set, while in fact it is not there. However, the opposite situation is not possible\nso if the Bloom Filter states that an element is not part of the set, it is certainly true. Referring this to\nour example with the content of my wallet, the Bloom Filter could therefore assure me that there was a\n200 value banknote in it, while standing at the checkout in a store it would turn out that,\nunfortunately, it is not there. What a pity…\nBefore we move on to examine how this algorithm works, let’s consider where it could be useful. A\ntypical example is a recommendation system. Imagine we are designing a system intended to suggest\narticles for users to read, a feature common on social media sites. Such a system needs to store a\nlist of articles read by each user so that it does not suggest them again. It is easy to imagine that\nstoring these articles with each user in the classic way would quickly exhaust memory resources. If we\ndon’t use any data removal mechanism, the database will grow indefinitely. The discussed Bloom\nFilter fits perfectly here as it will allow us to save a lot of memory, although, one must consider\nconsequences of its limitations related to possible false results. It may happen that we will get false\ninformation that a certain article has already been read by someone, while in fact this is not true.\nConsequently, we will not offer that user to read the material. On the other hand, the opposite\nsituation is not possible: we will never display to a user a recommendation of an article that he/she\nhas already read.\nAt this point it is worth checking how much we gain by accepting the inconvenience described\nabove. I have prepared two implementations of a program that adds to a set of values and then\nchecks if they are there.\nThe first program uses the classic approach — the HashSet class, while the second uses the Bloom\nFilter available in the popular guava library.\nAgain, using jConsole we register for both programs the amount of memory used, and additionally — for the version with\nthe Bloom Filter we also check the\nnumber of false positives. This value can be easily controlled, as the maximum allowed false positive\nrate can be set in the API; for needs of the following tests we will set it to 1%.\nMoreover, we will measure the total time of adding values to the set and the total time of querying\nwhether there are values in the set.\nSame as before we will perform a number of tests using the following parameters: n — the size of the set of\nnumbers, and f — what part of it should be added to the set. The configuration n=1 000 000 and f=0.1 means\nthat the first 100 000 numbers out of 1 000 000 will be added to the set. So, in the first part, the program will\nadd 100 000 numbers to the set and then — in the second stage — it will perform a presence test\nby checking whether the numbers above 100 000 belong to the set. There is no point in checking the\nnumbers added to the set beforehand, because we know that Bloom Filters do not give false\nnegative results. On the other hand, if any number above 100 000 is found according to the Bloom Filter in\nthe set, we will consider it a false positive.\nFollowing code snippet presents fragment of the Bloom Filter variant:\n\nval insertions = (n * f).toInt()\nval filter = BloomFilter.create(Funnels.integerFunnel(), insertions, 0.01)\nvar falsePositives = 0\n\nval insertTime = measureTimeMillis {\n    for (i in 0 until insertions) {\n        filter.put(i)\n    }\n}\n\nval queryTime = measureTimeMillis {\n    for (i in insertions until n) {\n        if (filter.mightContain(i)) {\n            falsePositives++;\n        }\n    }\n}\n\nval fpRatio = falsePositives/n.toDouble()\n\n\nAgain — you can find full code of both programs on aforementioned repository.\nLet’s start with the following configuration: n=10 000 000/f=0.1:\nn=10 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      81\n      293\n    \nquery time [ms]\n      82\n      846\n    \nmemory [MB]\n      94\n      30\n    \nAs you can see, the Bloom Filter returned less than 1% false results, but — at the same time — it used three times\nless memory than HashSet variant. Unfortunately, the times of Bloom Filter’s version are significantly higher.\nLet’s check what happens when we increase the size of the input set:\nn=100 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      593\n      318\n    \nquery time [ms]\n      988\n      944\n    \nmemory [MB]\n      876\n      29\n    \nn=500 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      1975\n      1372\n    \nquery time [ms]\n      4115\n      4923\n    \nmemory [MB]\n      4400\n      81\n    \nThe number of false positives is still below the preset 1%, the amount of memory used is still lower\nthan the classic implementation, and interestingly also the times of the probabilistic variant are\nlower, at least for inserting. Thus, it can be seen that along with the increase in the size of the data the benefit of this\nmethod increases.\nSummary\nThe above results clearly show that by accepting a small share of false answers, we can gain significant savings in memory usage.\nSimilarly to the HLL algorithm, the structure based on the Bloom Filters is available in many popular\ndatabases like Redis,\nHBase\nor Cassandra.\nThe simple experiments we conducted showed that probabilistic algorithms can save a lot\nof memory, which is especially important if our database stores huge amounts of data. In such cases it\nis sometimes worth letting your database lie to you a little.","guid":"https://blog.allegro.tech/2022/10/probabilistic-algorithms.html","categories":["tech","performance","NoSQL"],"isoDate":"2022-10-03T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Example of modularization in Allegro Pay Android application","link":"https://blog.allegro.tech/2022/09/example-of-modularization-in-allegro-pay-android-application.html","pubDate":"Mon, 26 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Kwiatek"],"photo":["https://blog.allegro.tech/img/authors/michal.kwiatek.jpg"],"url":["https://blog.allegro.tech/authors/michal.kwiatek"]}]},"content":"<p>Currently, in the Android world, the topic of modularization is very popular. Many bloggers describe their experiences\nwith it and analyze what <a href=\"https://developer.android.com/topic/modularization/patterns\">Google recommends</a>. Our team\nstarted the modularization process before it was hot. I will describe our reasons, decisions, problems and give you some\nadvice. We will see if modularization makes sense and what it brings to the table. I will also post some statistics\nshowing what it looked like before and after the modularization process.</p>\n\n<h2 id=\"some-theory\">Some theory</h2>\n\n<h3 id=\"module\">Module</h3>\n\n<blockquote>\n  <p>A <a href=\"https://developer.android.com/studio/projects#:~:text=inside%20your%20project.-,Modules,test%2C%20and%20debug%20each%20module\">module</a>\nis a collection of source files and build settings that allow you to divide your project into discrete units of\nfunctionality. Your project can have one or many modules, and one module may use another module as a dependency. You can\nindependently build, test, and debug each module.</p>\n</blockquote>\n\n<h3 id=\"background\">Background</h3>\n\n<p>Allegro Pay is a payment method on Allegro that allows you to postpone the payment by 30 days or divide it into smaller\nparts. People who use Allegro Pay know how many functionalities it has, those who don’t use it yet will know after\nreading this article. It started from 3 modules. At the time of writing this article the Allegro application for the\nAndroid platform consists of over 120 modules, 9 of which are maintained by Allegro Pay Team. In this quarter, we\nfocused on extracting several domains (features) from the main Allegro Pay module into separate, smaller and specialized\nmodules.</p>\n\n<h2 id=\"what-made-us-start-the-modularization-process\">What made us start the modularization process?</h2>\n\n<p>The main reason for the modularization process was the build time of one of these 3 modules — containing the entire\nAllegro Pay domain. Our internal monitoring tools showed that build times started to average 100 seconds, and at their\nworst point grew to just over 120 seconds. The module contains over 40k LoC (lines of code). In addition, we faced\nproblems when introducing changes, such as conflicts or the possibility of accidental modification of another\nfunctionality.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/before_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h3 id=\"cheat\">Cheat</h3>\n\n<p>I mention the build time for a reason. In our case, in a multi-module project, we use some Gradle instructions. Our\n<code class=\"language-plaintext highlighter-rouge\">gradle.properties</code> file looks something like this:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// some instructions</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">parallel</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">configureondemand</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">caching</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"c1\">// more instructions</span>\n</code></pre></div></div>\n\n<p>The first instruction\nenables <a href=\"https://docs.gradle.org/current/userguide/performance.html#parallel_execution\">parallelization</a>\nso that Gradle can perform more than one task at a time as long as the tasks are in different modules. The second one\nallows you to\n<a href=\"https://docs.gradle.org/current/userguide/multi_project_configuration_and_execution.html#sec:configuration_on_demand\">configure modules</a>\nthat are relevant only to the task you want, rather than configuring them all, which is the default behavior.\nImportantly, this instruction should be used for multi-module projects. And the last one is\n<a href=\"https://docs.gradle.org/current/userguide/build_cache.html\">caching</a>. It is „a cache mechanism that aims to save time\nby reusing outputs produced by other builds. The build cache works by storing (locally or remotely) build outputs and\nallowing builds to fetch these outputs from the cache when it is determined that inputs have not changed, avoiding the\nexpensive work of regenerating them.” By default, the build cache is disabled.</p>\n\n<h2 id=\"refinement-decisions-and-plans\">Refinement, decisions and plans</h2>\n\n<p>At one of the weekly meetings, we discussed how to solve the problem of the growing module and the increasing number of\ndependencies and functionalities. We decided that the best way would be to extract several domains (features) into\nseparate modules. Every new module should contain the implemented part of the domain that it represents according to the\nname and a small contract module that can be attached to other modules in order to provide them with the implemented\nfunctionality. So, we have planned the following modules:</p>\n\n<ol>\n  <li>ais (a banking service that isn’t relevant in the context of this article) with contract module,</li>\n  <li>common,</li>\n  <li>consolidation with contract module,</li>\n  <li>onboarding with contract module,</li>\n  <li>overpayment with contract module,</li>\n  <li>repayment with contract module.</li>\n</ol>\n\n<h2 id=\"contract\">Contract</h2>\n\n<p>The contract is a special module containing all the necessary interfaces, classes and methods that allow you to use the\nfunctionality in other places in an easy way. It is defined inside the module containing the functionality\nimplementation. It should be emphasized here that the implementation module can only be based on a contract. This\nsolution means that every developer working on the project knows where to find the necessary information and interfaces\nto run any feature.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">interface</span> <span class=\"nc\">AllegroPaySomeProcessHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">createSomeIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nc\">Context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">:</span> <span class=\"nc\">OtherData</span><span class=\"p\">):</span> <span class=\"nc\">Intent</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">observeSomeResult</span><span class=\"p\">():</span> <span class=\"nc\">Observable</span><span class=\"p\">&lt;</span><span class=\"nc\">SomeResultEvent</span><span class=\"p\">&gt;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">internal</span> <span class=\"kd\">class</span> <span class=\"nc\">AllegroPaySomeProcessHandlerImpl</span> <span class=\"p\">:</span> <span class=\"nc\">AllegroPaySomeProcessHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">createSomeIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nc\">Context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">:</span> <span class=\"nc\">OtherData</span><span class=\"p\">):</span> <span class=\"nc\">Intent</span> <span class=\"p\">=</span>\n        <span class=\"nc\">SomeActivity</span><span class=\"p\">.</span><span class=\"nf\">getIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">observeSomeResult</span><span class=\"p\">():</span> <span class=\"nc\">Observable</span><span class=\"p\">&lt;</span><span class=\"nc\">SomeResultEvent</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nc\">DataBus</span><span class=\"p\">.</span><span class=\"nf\">listen</span><span class=\"p\">(</span><span class=\"nc\">SomeResultEvent</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>The above example shows what one of the assumptions of object-oriented programming — encapsulation — looks like in\npractice. The <em>AllegroPaySomeProcessHandler</em> interface provides two methods, one of them creates\nthe <a href=\"https://developer.android.com/reference/android/content/Intent\">Intent</a> necessary to run the process, and the other\nobserves its result. The exact implementation is hidden in an internal class, not accessible from the contract module.\nEvery change of interface implementation is transparent to contract clients. Example of how to declare a dependency on a\ncontract:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nf\">dependencies</span> <span class=\"p\">{</span>\n    <span class=\"n\">implementation</span> <span class=\"nf\">project</span> <span class=\"p\">(</span><span class=\"err\">'</span><span class=\"p\">:</span><span class=\"n\">allegropay-some</span><span class=\"p\">:</span><span class=\"n\">contract</span><span class=\"err\">'</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"tool\">Tool</h2>\n\n<p>The Allegro application consists of many modules and it is important to provide programmers with the right tools to work\neffectively. In the organization, the delivery of this type of tools is handled by the core team. A tool that allows us\nto check whether our module meets the requirement set for it is\nthe <a href=\"https://github.com/jraska/modules-graph-assert\">Module Graph Assert</a>. It is a Gradle plugin which „helps keep your\nmodule graph healthy and lean.” This tool defines the types of modules that are allowed in the application, the\ndependencies between them and the height of the dependency tree. The following types are defined in the Allegro\napplication: <em>App</em>, <em>Feature</em>, <em>Contract</em>, <em>Library</em>, <em>Util</em> and <em>NeedsMigration</em>. The last type tells us that the\nmodule still requires work from its owners and appropriate adaptation to one of the other types. We can also define\nallowed and restricted dependencies between modules, e.g. a contract may depend only on another contract or a module\nmarked as a feature depends only on the contract or library. Allegro app configuration:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">moduleGraphAssert</span> <span class=\"o\">{</span>\n    <span class=\"n\">maxHeight</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n    <span class=\"n\">allowed</span> <span class=\"o\">=</span> <span class=\"o\">[</span>\n        <span class=\"s1\">'App -&gt; Feature'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Library'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Util'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Library'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Util'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Contract -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'NeedsMigration -&gt; .*'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'.* -&gt; NeedsMigration'</span><span class=\"o\">,</span>\n    <span class=\"o\">]</span>\n    <span class=\"n\">restricted</span> <span class=\"o\">=</span> <span class=\"o\">[</span>\n        <span class=\"s1\">'Contract -X&gt; NeedsMigration'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Library -X&gt; .*'</span>\n    <span class=\"o\">]</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"initial-modules\">Initial modules</h2>\n\n<p>The first separated feature module was overpayment. We immediately prepared a common module containing functionalities\nused in more than one Allegro Pay module. The contract that is shown earlier contains one method returning an Intent\nneeded to run the overpayment process. The feature module includes user-visible screens, use cases and network\ncommunication. Several thousand lines of code were added to this module and the time needed to build the main Allegro\nPay module was shortened. At that time, the build time of the main module was around 87.5 seconds, common and\noverpayment modules around 10.5 seconds.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/first_modules_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"following-modules\">Following modules</h2>\n\n<p>In the next stages, we separated the ais, consolidation and repayment modules. The current values of the build times of\nindividual modules are around 33.7 seconds for the Allegro Pay main module, 13.4 seconds for the ais, 12 seconds for the\nconsolidation, 10.6 seconds for the repayment. The extraction process was analogous to that of the first module.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/few_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"onboarding-module\">Onboarding module</h2>\n\n<p>This module was the most challenging and possibly the most time consuming. This was due to the combination of the\nprocess being available from multiple screens in different modules and ensuring unchanged functionality. During this\nmodularization process, we discovered the possibility of optimizing and reducing the amount of code. This module\ncontains approximately 10k LoC and the build time is less than 20 seconds. It is a really huge module.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/onboarding_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"other-two-modules\">Other two modules</h2>\n\n<p>If you remember, I mentioned three modules at the beginning of this text. So far, I have described the division of the\nlargest module. Let me now describe others in more detail. The first is the special analytical module. Includes an\nexternal library and a small contract. It was created at the same time as the main Allegro Pay module. The current value\nof the build time is 3 seconds and the module has more than 150 lines of code.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/sms_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<p>The second is the SMS verification module. It contains a functionality that allows users to authorize operations by\nproviding SMS code. Currently, it is used in the processes of buying, consolidation, onboarding and overpayment. We only\nwrote a contract here, which provides a universal and easy interface. The build time is approximately 9 seconds and the\nmodule contains almost 2k lines of code.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/sa_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"fin\">Fin</h2>\n\n<p>Probably for some of you, the division method used may be associated with the Latin term <em>divide et impera</em>. This\nparadigm of algorithm design could also be used in the modularization process by dividing one large module into several\nsmaller ones, each specialized in one task. The use of the concept of this paradigm, encapsulation by creating a\ncontract and Gradle configuration allowed to significantly reduce the build time and speed up the development of the\napplication. This solution introduces consistency in the module and decreases the possibility of introducing a\nregression by encapsulating each individual domain. Also the problem with the redundant conflicts has been minimalized.\nAfter the implementation of the modules described above, the main module containing the Allegro Pay responsibilities has\nshrunk significantly, and now contains around 18.4k LoC (which means it was reduced by half). In addition,\nmodularization will allow us to add new features and extend the existing ones in an easier and safer way. It was an\ninteresting challenge from a technical point of view.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/after_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n","contentSnippet":"Currently, in the Android world, the topic of modularization is very popular. Many bloggers describe their experiences\nwith it and analyze what Google recommends. Our team\nstarted the modularization process before it was hot. I will describe our reasons, decisions, problems and give you some\nadvice. We will see if modularization makes sense and what it brings to the table. I will also post some statistics\nshowing what it looked like before and after the modularization process.\nSome theory\nModule\nA module\nis a collection of source files and build settings that allow you to divide your project into discrete units of\nfunctionality. Your project can have one or many modules, and one module may use another module as a dependency. You can\nindependently build, test, and debug each module.\nBackground\nAllegro Pay is a payment method on Allegro that allows you to postpone the payment by 30 days or divide it into smaller\nparts. People who use Allegro Pay know how many functionalities it has, those who don’t use it yet will know after\nreading this article. It started from 3 modules. At the time of writing this article the Allegro application for the\nAndroid platform consists of over 120 modules, 9 of which are maintained by Allegro Pay Team. In this quarter, we\nfocused on extracting several domains (features) from the main Allegro Pay module into separate, smaller and specialized\nmodules.\nWhat made us start the modularization process?\nThe main reason for the modularization process was the build time of one of these 3 modules — containing the entire\nAllegro Pay domain. Our internal monitoring tools showed that build times started to average 100 seconds, and at their\nworst point grew to just over 120 seconds. The module contains over 40k LoC (lines of code). In addition, we faced\nproblems when introducing changes, such as conflicts or the possibility of accidental modification of another\nfunctionality.\n\nCheat\nI mention the build time for a reason. In our case, in a multi-module project, we use some Gradle instructions. Our\ngradle.properties file looks something like this:\n\n// some instructions\norg.gradle.parallel = true\norg.gradle.configureondemand = true\norg.gradle.caching = true\n// more instructions\n\n\nThe first instruction\nenables parallelization\nso that Gradle can perform more than one task at a time as long as the tasks are in different modules. The second one\nallows you to\nconfigure modules\nthat are relevant only to the task you want, rather than configuring them all, which is the default behavior.\nImportantly, this instruction should be used for multi-module projects. And the last one is\ncaching. It is „a cache mechanism that aims to save time\nby reusing outputs produced by other builds. The build cache works by storing (locally or remotely) build outputs and\nallowing builds to fetch these outputs from the cache when it is determined that inputs have not changed, avoiding the\nexpensive work of regenerating them.” By default, the build cache is disabled.\nRefinement, decisions and plans\nAt one of the weekly meetings, we discussed how to solve the problem of the growing module and the increasing number of\ndependencies and functionalities. We decided that the best way would be to extract several domains (features) into\nseparate modules. Every new module should contain the implemented part of the domain that it represents according to the\nname and a small contract module that can be attached to other modules in order to provide them with the implemented\nfunctionality. So, we have planned the following modules:\nais (a banking service that isn’t relevant in the context of this article) with contract module,\ncommon,\nconsolidation with contract module,\nonboarding with contract module,\noverpayment with contract module,\nrepayment with contract module.\nContract\nThe contract is a special module containing all the necessary interfaces, classes and methods that allow you to use the\nfunctionality in other places in an easy way. It is defined inside the module containing the functionality\nimplementation. It should be emphasized here that the implementation module can only be based on a contract. This\nsolution means that every developer working on the project knows where to find the necessary information and interfaces\nto run any feature.\n\ninterface AllegroPaySomeProcessHandler {\n\n    fun createSomeIntent(context: Context, someId: String, otherData: OtherData): Intent\n\n    fun observeSomeResult(): Observable<SomeResultEvent>\n}\n\n\n\ninternal class AllegroPaySomeProcessHandlerImpl : AllegroPaySomeProcessHandler {\n\n    override fun createSomeIntent(context: Context, someId: String, otherData: OtherData): Intent =\n        SomeActivity.getIntent(context, someId, otherData)\n\n    override fun observeSomeResult(): Observable<SomeResultEvent> =\n        DataBus.listen(SomeResultEvent::class.java)\n}\n\n\nThe above example shows what one of the assumptions of object-oriented programming — encapsulation — looks like in\npractice. The AllegroPaySomeProcessHandler interface provides two methods, one of them creates\nthe Intent necessary to run the process, and the other\nobserves its result. The exact implementation is hidden in an internal class, not accessible from the contract module.\nEvery change of interface implementation is transparent to contract clients. Example of how to declare a dependency on a\ncontract:\n\ndependencies {\n    implementation project (':allegropay-some:contract')\n}\n\n\nTool\nThe Allegro application consists of many modules and it is important to provide programmers with the right tools to work\neffectively. In the organization, the delivery of this type of tools is handled by the core team. A tool that allows us\nto check whether our module meets the requirement set for it is\nthe Module Graph Assert. It is a Gradle plugin which „helps keep your\nmodule graph healthy and lean.” This tool defines the types of modules that are allowed in the application, the\ndependencies between them and the height of the dependency tree. The following types are defined in the Allegro\napplication: App, Feature, Contract, Library, Util and NeedsMigration. The last type tells us that the\nmodule still requires work from its owners and appropriate adaptation to one of the other types. We can also define\nallowed and restricted dependencies between modules, e.g. a contract may depend only on another contract or a module\nmarked as a feature depends only on the contract or library. Allegro app configuration:\n\nmoduleGraphAssert {\n    maxHeight = 5\n    allowed = [\n        'App -> Feature',\n        'App -> Library',\n        'App -> Util',\n        'App -> Contract',\n        'Feature -> Library',\n        'Feature -> Util',\n        'Feature -> Contract',\n        'Contract -> Contract',\n        'NeedsMigration -> .*',\n        '.* -> NeedsMigration',\n    ]\n    restricted = [\n        'Contract -X> NeedsMigration',\n        'Library -X> .*'\n    ]\n}\n\n\nInitial modules\nThe first separated feature module was overpayment. We immediately prepared a common module containing functionalities\nused in more than one Allegro Pay module. The contract that is shown earlier contains one method returning an Intent\nneeded to run the overpayment process. The feature module includes user-visible screens, use cases and network\ncommunication. Several thousand lines of code were added to this module and the time needed to build the main Allegro\nPay module was shortened. At that time, the build time of the main module was around 87.5 seconds, common and\noverpayment modules around 10.5 seconds.\n\nFollowing modules\nIn the next stages, we separated the ais, consolidation and repayment modules. The current values of the build times of\nindividual modules are around 33.7 seconds for the Allegro Pay main module, 13.4 seconds for the ais, 12 seconds for the\nconsolidation, 10.6 seconds for the repayment. The extraction process was analogous to that of the first module.\n\nOnboarding module\nThis module was the most challenging and possibly the most time consuming. This was due to the combination of the\nprocess being available from multiple screens in different modules and ensuring unchanged functionality. During this\nmodularization process, we discovered the possibility of optimizing and reducing the amount of code. This module\ncontains approximately 10k LoC and the build time is less than 20 seconds. It is a really huge module.\n\nOther two modules\nIf you remember, I mentioned three modules at the beginning of this text. So far, I have described the division of the\nlargest module. Let me now describe others in more detail. The first is the special analytical module. Includes an\nexternal library and a small contract. It was created at the same time as the main Allegro Pay module. The current value\nof the build time is 3 seconds and the module has more than 150 lines of code.\n\nThe second is the SMS verification module. It contains a functionality that allows users to authorize operations by\nproviding SMS code. Currently, it is used in the processes of buying, consolidation, onboarding and overpayment. We only\nwrote a contract here, which provides a universal and easy interface. The build time is approximately 9 seconds and the\nmodule contains almost 2k lines of code.\n\nFin\nProbably for some of you, the division method used may be associated with the Latin term divide et impera. This\nparadigm of algorithm design could also be used in the modularization process by dividing one large module into several\nsmaller ones, each specialized in one task. The use of the concept of this paradigm, encapsulation by creating a\ncontract and Gradle configuration allowed to significantly reduce the build time and speed up the development of the\napplication. This solution introduces consistency in the module and decreases the possibility of introducing a\nregression by encapsulating each individual domain. Also the problem with the redundant conflicts has been minimalized.\nAfter the implementation of the modules described above, the main module containing the Allegro Pay responsibilities has\nshrunk significantly, and now contains around 18.4k LoC (which means it was reduced by half). In addition,\nmodularization will allow us to add new features and extend the existing ones in an easier and safer way. It was an\ninteresting challenge from a technical point of view.","guid":"https://blog.allegro.tech/2022/09/example-of-modularization-in-allegro-pay-android-application.html","categories":["tech","kotlin","mobile","android","modularization","gradle","allegro-pay"],"isoDate":"2022-09-25T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study","link":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","pubDate":"Tue, 13 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Kamil Starczak"],"photo":["https://blog.allegro.tech/img/authors/kamil.starczak.jpg"],"url":["https://blog.allegro.tech/authors/kamil.starczak"]}]},"content":"<p>Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with <a href=\"https://azure.microsoft.com/services/cosmos-db/\">Azure\nCosmos DB</a> and batch processing.</p>\n\n<h2 id=\"our-story\">Our story</h2>\n\n<p>At Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.</p>\n\n<p>In this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:</p>\n\n<ul>\n  <li>The overall time of such a batch operation cannot exceed 5 minutes per 1 million records.</li>\n  <li>The processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.</li>\n  <li>The solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.</li>\n  <li>The solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.</li>\n</ul>\n\n<p>The outcomes of this case study are published as an open source repository (see <a href=\"#our-library\">Our library</a>).</p>\n\n<h2 id=\"cosmos-db--the-basics\">Cosmos DB — the basics</h2>\n\n<p>Before going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the <a href=\"#database-utilization\">Database utilization</a> chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (<a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/#features\">source</a>). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.</p>\n\n<p>How does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>foreach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n</code></pre></div></div>\n\n<p>For each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.</p>\n\n<p>What’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.</p>\n\n<h2 id=\"cosmos-db--scaling-and-provisioning\">Cosmos DB — scaling and provisioning</h2>\n\n<p>Cosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img01.png\" alt=\"Cosmos DB Request Units overview\" /></p>\n\n<p>Source: <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\">https://docs.microsoft.com/en-us/azure/cosmos-db/request-units</a></p>\n\n<p>But what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.</p>\n\n<p>At this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.</p>\n\n<h3 id=\"manual\">Manual</h3>\n\n<p>In the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.</p>\n\n<p>What happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img02.png\" alt=\"Manual mode visualized\" /></p>\n\n<h3 id=\"autoscale\">Autoscale</h3>\n\n<p>The second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img03.png\" alt=\"Autoscale mode visualized\" /></p>\n\n<p>What’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.</p>\n\n<p>Nevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.</p>\n\n<h3 id=\"serverless\">Serverless</h3>\n\n<p>The last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img04.png\" alt=\"Serverless mode visualized\" /></p>\n\n<p>Unfortunately, if we read the docs, we can find some additional information:</p>\n\n<ul>\n  <li>The maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.</li>\n  <li>The guarantees for the operation latencies are worse — 30ms instead of 10ms.</li>\n  <li>Serverless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.</li>\n  <li>Moreover, the maximum throughput during a single second is 5000 RUs.</li>\n</ul>\n\n<p>As we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.</p>\n\n<p>To sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.</p>\n\n<h2 id=\"database-utilization\">Database utilization</h2>\n\n<p>Let’s go back to the sample code I was running.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Foreach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n</code></pre></div></div>\n\n<p>It processed 50k records in about 10 minutes. How loaded was the database?</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img05.png\" alt=\"Normalized RU Consumption metric\" /></p>\n\n<p>Normalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.</p>\n\n<p>If we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.</p>\n\n<p>This time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.</p>\n\n<h3 id=\"ru-limiter\">RU limiter</h3>\n\n<p>Is there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img06.png\" alt=\"RU limiter visualized\" /></p>\n\n<p>The mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img07.png\" alt=\"Total Request Units metric\" /></p>\n\n<h3 id=\"partition-key-ranges\">Partition key ranges</h3>\n\n<p>It almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img08.png\" alt=\"Normalized RU Consumption metric\" /></p>\n\n<p>Something is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled <code class=\"language-plaintext highlighter-rouge\">PartitionKeyRangeId</code>. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img09.png\" alt=\"Normalized RU Consumption metric split by Partition Key ranges\" /></p>\n\n<p>If we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.</p>\n\n<h3 id=\"a-bit-of-reverse-engineering\">A bit of reverse engineering</h3>\n\n<p>Is there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.</p>\n\n<h2 id=\"the-autoscaler-auto-scaler\">The “Autoscaler auto scaler”</h2>\n\n<p>The problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.</p>\n\n<p>The one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.</p>\n\n<p>But what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img10.png\" alt=\"Autoscaler visualized during batch processing\" /></p>\n\n<p>With this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.</p>\n\n<p>This way, we have fulfilled all the requirements that were set at the beginning:</p>\n\n<ul>\n  <li>Batch processing time — 5 minutes per 1 million records.</li>\n  <li>No risk of starving other processes, thanks to the RU limiter.</li>\n  <li>Cost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.</li>\n  <li>Scalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.</li>\n</ul>\n\n<h2 id=\"our-library\">Our library</h2>\n\n<p>The outcomes of this work have been published as an opensource .NET library on our GitHub page:\n<a href=\"https://github.com/allegro/cosmosdb-utils/tree/main/src/Allegro.CosmosDb.BatchUtilities\">Allegro.CosmosDb.BatchUtilities</a>.\nFeel free to use it or even contribute new features.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n\n<p>And here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:</p>\n\n<ul>\n  <li>Cosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.</li>\n  <li>Cosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.</li>\n  <li>You must pay close attention to the costs, as it is very easy to get high bills by misusing the service.</li>\n  <li>It’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.</li>\n</ul>\n","contentSnippet":"Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with Azure\nCosmos DB and batch processing.\nOur story\nAt Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.\nIn this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:\nThe overall time of such a batch operation cannot exceed 5 minutes per 1 million records.\nThe processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.\nThe solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.\nThe solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.\nThe outcomes of this case study are published as an open source repository (see Our library).\nCosmos DB — the basics\nBefore going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the Database utilization chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (source). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.\nHow does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:\n\nforeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nFor each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.\nWhat’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.\nCosmos DB — scaling and provisioning\nCosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.\n\nSource: https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\nBut what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.\nAt this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.\nManual\nIn the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.\nWhat happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.\n\nAutoscale\nThe second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.\n\nWhat’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.\nNevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.\nServerless\nThe last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.\n\nUnfortunately, if we read the docs, we can find some additional information:\nThe maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.\nThe guarantees for the operation latencies are worse — 30ms instead of 10ms.\nServerless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.\nMoreover, the maximum throughput during a single second is 5000 RUs.\nAs we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.\nTo sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.\nDatabase utilization\nLet’s go back to the sample code I was running.\n\nForeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nIt processed 50k records in about 10 minutes. How loaded was the database?\n\nNormalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.\nIf we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.\nThis time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.\nRU limiter\nIs there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.\n\nThe mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.\n\nPartition key ranges\nIt almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.\n\nSomething is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled PartitionKeyRangeId. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.\n\nIf we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.\nA bit of reverse engineering\nIs there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.\nThe “Autoscaler auto scaler”\nThe problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.\nThe one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.\nBut what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).\n\nWith this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.\nThis way, we have fulfilled all the requirements that were set at the beginning:\nBatch processing time — 5 minutes per 1 million records.\nNo risk of starving other processes, thanks to the RU limiter.\nCost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.\nScalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.\nOur library\nThe outcomes of this work have been published as an opensource .NET library on our GitHub page:\nAllegro.CosmosDb.BatchUtilities.\nFeel free to use it or even contribute new features.\nConclusions\nAnd here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:\nCosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.\nCosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.\nYou must pay close attention to the costs, as it is very easy to get high bills by misusing the service.\nIt’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.","guid":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","categories":["tech","cloud","azure","cosmosdb"],"isoDate":"2022-09-12T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"MBox: server-driven UI for mobile apps","link":"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html","pubDate":"Wed, 03 Aug 2022 00:00:00 +0200","authors":{"author":[{"name":["Paulina Sadowska"],"photo":["https://blog.allegro.tech/img/authors/paulina.sadowska.jpg"],"url":["https://blog.allegro.tech/authors/paulina.sadowska"]}]},"content":"<p>In this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the\nfirst version of the in-house server-driven rendering tool called MBox and used it to render the\nhomepage in the Allegro app on <a href=\"https://play.google.com/store/apps/details?id=pl.allegro\">Android</a>\nand <a href=\"https://apps.apple.com/pl/app/allegro/id305659772\">iOS</a>. We have come a long way since then, and now we use this\ntool to render more and more screens in the Allegro app.</p>\n\n<p>After over three years of working on MBox, we want to share how it works and the key advantages and challenges of using this approach.</p>\n\n<h2 id=\"why-server-driven-ui\">Why server-driven UI?</h2>\n\n<p>The idea behind MBox was to make mobile development faster without compromising the app quality. Implementing a\nfeature twice for Android and iOS takes a lot of time and requires two people with unique skill sets (knowledge of\nAndroid and iOS frameworks). There is also the risk that both apps will not behave consistently because each person may\ninterpret the requirements slightly differently.</p>\n\n<p>Using a server-driven UI solves that problem because each business feature is implemented only once on the backend.\nThat gives us consistency out of the box and shortens the time needed to implement the feature.\nAlso, developers don’t need to know mobile frameworks to develop for mobile anymore.</p>\n\n<p>Another advantage of server-driven UI is that it allows releasing features independently of the release train. We\ncan deploy changes multiple times a day and when something goes wrong — roll back to the previous version immediately.\nIt gives teams a lot more flexibility and allows them to experiment and iterate much faster. What’s more, deployed\nchanges are visible to all clients, no matter which app version they use.</p>\n\n<h2 id=\"how-does-mbox-work\">How does MBox work?</h2>\n\n<h3 id=\"defining-the-screen-layout\">Defining the screen layout</h3>\n\n<p>While designing MBox, we wanted to create a tool that would give developers total flexibility to implement any layout\nthey need — as long as it’s consistent with our design system, Metrum.</p>\n\n<p>That’s why MBox screens are built using primitive components, which our rendering libraries map to native views.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/1_MBox_SG.png\" alt=\"MessageWidget structure\" /></p>\n\n<p>Developers can arrange MBox components freely using different types of containers that MBox supports (<code class=\"language-plaintext highlighter-rouge\">flex-container</code>,\n<code class=\"language-plaintext highlighter-rouge\">stack-container</code>, <code class=\"language-plaintext highlighter-rouge\">absolute-container</code>, <code class=\"language-plaintext highlighter-rouge\">list-container</code>, etc.). Those components can be styled and configured to match\ndifferent business scenarios.</p>\n\n<p>MBox renders components on mobile apps consistently, but it also respects slight differences unique to Android and\niOS platforms.\nFor example, dialog action in MBox supports the same functionalities on both platforms, but the dialog itself looks\ndifferent on Android and iOS:</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/6_alert.png\" alt=\"MBox dialog action on Android and iOS\" /></p>\n\n<p>That gives MBox screens a native look and feel and perfectly blends in with parts of the app developed\nnatively, without MBox. We had to add a label that shows which parts of the app are rendered by MBox, because\neven mobile developers couldn’t tell where native screens ended and MBox started.</p>\n\n<h3 id=\"what-about-more-complex-views\">What about more complex views?</h3>\n\n<p>Creating more complex, reusable views is also possible. For example, our design system specifies something called the\nmessage: an element with a vertical line, an icon, and some texts and buttons. However, because this element is complex\nand its requirements may change over time, it’s defined on the backend service as a widget — the element that developers\ncan reuse across different screens.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/2_MessageWidget.png\" alt=\"MessageWidget structure\" /></p>\n\n<p>If the requirements for the message widget change, we can easily modify it on the backend side without the need to\nrelease the app. That’s because it’s not defined directly in the MBox libraries included in the mobile apps, but\nspecified on the backend using MBox components.</p>\n\n<h3 id=\"unified-tracking\">Unified tracking</h3>\n\n<p>Besides defining layouts, MBox also allows us to specify tracking events on the backend. For tracking events,\nconsistency is crucial. If events are not triggered under the same scenarios and with the same data\non both platforms, it’s hard to compare the data and make business decisions.</p>\n\n<p>MBox solves that problem. All events tracked on MBox screens are defined on the backend, meaning unified tracking\nbetween Android and iOS and across different app versions.</p>\n\n<h3 id=\"testing\">Testing</h3>\n\n<p>Since the MBox rendering engine is a core of more and more screens in the app, it had to be thoroughly covered by unit\ntests and integration tests. We also have screenshot tests that ensure that MBox components render correctly. That\nallows us to find out early about possible regressions.</p>\n\n<p>Teams that develop screens using MBox also have various tools that allow them to test their features. They can write\nunit tests in the MBox backend service and check if correct MBox components are created for the given data.\nThey can also add a URL of their page to Visual Regression — the tool that creates a screenshot of\nthe page whenever someone commits anything to the MBox backend and if any change in the page is detected, the author is\nautomatically notified in their pull request.</p>\n\n<p>Feature teams can also write UI tests for the native apps to test how their page integrates with the rest of the app and\nif all interactions work as expected. However, those tests have to be written on both platforms by the mobile developers\nand should take into account that the content of the page under tests can be changed on the backend.</p>\n\n<h2 id=\"the-journey-to-make-mbox-interactive\">The journey to make MBox interactive</h2>\n\n<p>When we started working on MBox, we were focused mainly on pages that contain a lot of frequently changing content but\nnot many interactions with users. In the first version of MBox, it was possible to define only basic actions like\nopening a new screen or adding an offer to the cart. That changed gradually when new teams started using MBox.</p>\n\n<p>To make MBox more interactive, we used the same atomic approach we adopted when designing MBox layout components. We\ngradually added generic actions that were not custom-made to serve specific business features but were reusable across\ndifferent use cases.</p>\n\n<h3 id=\"for-example\">For example:</h3>\n\n<p>One of the first challenges that we faced was allowing the implementation of an “add to watchlist” star in MBox. We\ncould’ve just added the ”watchlist star” component that checks if a user is logged in (redirects to the login page if it’s\nnot), adds an offer to the watchlist, and changes the star icon from empty to full. In the short term, it should have\nbeen easier. But it’s not a way that would allow MBox to scale.</p>\n\n<p>Instead, we designed a couple of atomic mechanisms that allow building this feature on the backend and could be reused\nin the future in different use cases.</p>\n\n<p>We added a logic component called <code class=\"language-plaintext highlighter-rouge\">multivariant</code> that allows changing one component into another thanks to the\n<code class=\"language-plaintext highlighter-rouge\">changeVariant</code> action. That enabled us to switch the star icon from empty to full. Next, we added the <code class=\"language-plaintext highlighter-rouge\">sendRequest</code>\naction\nthat sends requests with given URL, headers, and other data to our services. That allows adding and removing an offer to\nand from the watchlist. Lastly, we added the <code class=\"language-plaintext highlighter-rouge\">loginIfNeeded</code> action that allows checking if a user is logged in and\nredirecting to the login screen if needed. That allows ensuring the user is logged in before making the request.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/3_add_to_watched.png\" alt=\"Add to watchlist: scheme\" /></p>\n\n<p>Of course, doing it this way took much more time than just implementing the ”add to watchlist” component in MBox libraries\nnatively. But this is the way that scales and gives us flexibility.</p>\n\n<p>Over time mechanisms that we designed earlier were reused on other screens. And more and more often, when the new team\nwanted to use MBox on their screen, most of the building blocks they needed were already there. It definitely\nwouldn’t be the case if not for our atomic approach.</p>\n\n<h2 id=\"the-challenges\">The challenges</h2>\n\n<p>We also encountered many challenges while working on MBox.</p>\n\n<h3 id=\"consistency-of-the-engines\">Consistency of the engines</h3>\n\n<p>We create two separate rendering engines for mobile platforms, so we must be extra cautious to ensure everything works consistently.\nEven a tiny inconsistency in the behavior of the engines may be hugely problematic for the developers that use MBox.\nIt may force them to, for example, define different layouts for each mobile platform.</p>\n\n<p>To make sure the engines are consistent, each feature in MBox is implemented synchronously by a pair of developers\n(Android and iOS) who consult with each other regularly. During the work, they make sure that they interpret the\nrequirements and cover edge cases in the same way.\nThe new features are ready to merge only after thorough tests on both platforms that check both correctness and\nconsistency.</p>\n\n<h3 id=\"versioning\">Versioning</h3>\n\n<p>On MBox, we also have to pay close attention to versioning. We use semantic versioning in the engines. Each new feature\nhas to be marked with the same minor and major version on both platforms. We also prepare changelogs containing\ninformation about what\nfunctionalities are available in which version.</p>\n\n<p>On the backend, we allow checking the version of the MBox engine that the user has and serve different content depending\non it.\nFor example, when the screen contains the <code class=\"language-plaintext highlighter-rouge\">switch</code> component, supported since version <code class=\"language-plaintext highlighter-rouge\">1.21</code>,\nwe can define that for users who have the app with the older versions of MBox, <code class=\"language-plaintext highlighter-rouge\">checkbox</code> will be displayed instead.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/5_fallback.png\" alt=\"Fallback mechanism\" /></p>\n\n<h3 id=\"testing-changes-introduced-to-the-engines\">Testing changes introduced to the engines</h3>\n\n<p>And last but not least: testing. Because MBox is used to render various screens in Allegro mobile apps, we must be\ncautious whenever we introduce engine changes to avoid negatively impacting existing MBox screens.\nThe screenshot and UI tests cover every MBox component and action. We’re also encouraging feature teams to add their\nscreens to the Visual Regression and cover their screens with UI tests in the mobile repositories. All those things\nallow us to minimize the risk of introducing a regression.</p>\n\n<h2 id=\"how-does-mbox-connect-to-other-parts-of-the-allegro-ecosystem\">How does MBox connect to other parts of the Allegro ecosystem?</h2>\n\n<p>Consistency across mobile platforms is not everything. Another important aspect of our work is making sure mobile and\nweb platforms are as consistent as possible, respecting native differences that make each platform unique.</p>\n\n<p>MBox integrates with our content management system, also used for the web (<a href=\"https://blog.allegro.tech/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">Opbox</a> Page Manager). The screen’s content\nconfigured in the Opbox admin panel is sent through the Opbox services to the MBox backend service. The MBox service\nmaps the\ndata into MBox components that make up the MBox screen. Then the screen definition in JSON format is sent to apps and is\nrendered using native views.</p>\n\n<p>The same data from Opbox is also used to render the web equivalent of the same screen. Opbox defines its own mappings\nfor the web: Opbox Components, which describe how to map the data into HTML elements that make up the Allegro web pages.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/4_architecture.png\" alt=\"Add to watchlist: scheme\" /></p>\n\n<p>Integration with Opbox gives us a lot of advantages. Very often, to change the content in the app and web, you don’t\nneed to change the code at all — all you need to do is change the content in the Opbox admin panel.</p>\n\n<p>Another huge advantage is that we have unified tracking between all platforms and can use the same tools for A/B testing\nthat are used for the web. Previously, code for A/B tests had to be written for each mobile platform separately in\nnative\ncode and then cleaned up after the finished experiment. Now, some experiments work out of the box since Opbox sends\ndifferent data to MBox depending on the experiment variant the user falls into. Sometimes a little bit of code in the\nMBox backend is required to conduct an experiment, but it’s not comparable to the amount of work A/B tests take when\nthey’re performed in the native code without MBox.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n\n<p>MBox is a tool that changed how we work on mobile apps in Allegro. It allowed us to shorten the development time without\ncompromising the quality and stability of the app and without losing the native look and feel of the Allegro apps.</p>\n\n<p>We have come a long way during those three years since we started working on MBox. At first, our ambition was to create\na tool that would be used on content screens with very few interactions. Over time, we pushed the boundaries of what\nMBox\nis capable of and entered screens with more and more interactions with the user.</p>\n\n<p>Currently MBox is used in over 25 screens in Allegro mobile apps and the number is still growing. In the first half of\n2022 alone, 27 teams made changes to the app using MBox and created about 300 pull requests. We deployed changes over\n100 times which means ~4.15 releases a week.</p>\n\n<p>We’re confident that it’s not the end of the possibilities ahead of us. We still see how we can make MBox even more\npowerful. We’d love to shorten development time even more by providing tools that allow defining MBox screens in\nTypeScript. That’ll enable developers to reuse some parts of the code between mobile and web and take advantage of\nbetter tools such as hot reload. Another thing we’re currently focused on is adding the binding mechanism to MBox and\nthe client-side logic to allow defining the business logic on the backend. Implementing those mechanisms will allow\nintroducing even more interactivity into MBox screens.</p>\n\n<p>But that is the topic for the next articles. Stay tuned!</p>\n","contentSnippet":"In this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the\nfirst version of the in-house server-driven rendering tool called MBox and used it to render the\nhomepage in the Allegro app on Android\nand iOS. We have come a long way since then, and now we use this\ntool to render more and more screens in the Allegro app.\nAfter over three years of working on MBox, we want to share how it works and the key advantages and challenges of using this approach.\nWhy server-driven UI?\nThe idea behind MBox was to make mobile development faster without compromising the app quality. Implementing a\nfeature twice for Android and iOS takes a lot of time and requires two people with unique skill sets (knowledge of\nAndroid and iOS frameworks). There is also the risk that both apps will not behave consistently because each person may\ninterpret the requirements slightly differently.\nUsing a server-driven UI solves that problem because each business feature is implemented only once on the backend.\nThat gives us consistency out of the box and shortens the time needed to implement the feature.\nAlso, developers don’t need to know mobile frameworks to develop for mobile anymore.\nAnother advantage of server-driven UI is that it allows releasing features independently of the release train. We\ncan deploy changes multiple times a day and when something goes wrong — roll back to the previous version immediately.\nIt gives teams a lot more flexibility and allows them to experiment and iterate much faster. What’s more, deployed\nchanges are visible to all clients, no matter which app version they use.\nHow does MBox work?\nDefining the screen layout\nWhile designing MBox, we wanted to create a tool that would give developers total flexibility to implement any layout\nthey need — as long as it’s consistent with our design system, Metrum.\nThat’s why MBox screens are built using primitive components, which our rendering libraries map to native views.\n\nDevelopers can arrange MBox components freely using different types of containers that MBox supports (flex-container,\nstack-container, absolute-container, list-container, etc.). Those components can be styled and configured to match\ndifferent business scenarios.\nMBox renders components on mobile apps consistently, but it also respects slight differences unique to Android and\niOS platforms.\nFor example, dialog action in MBox supports the same functionalities on both platforms, but the dialog itself looks\ndifferent on Android and iOS:\n\nThat gives MBox screens a native look and feel and perfectly blends in with parts of the app developed\nnatively, without MBox. We had to add a label that shows which parts of the app are rendered by MBox, because\neven mobile developers couldn’t tell where native screens ended and MBox started.\nWhat about more complex views?\nCreating more complex, reusable views is also possible. For example, our design system specifies something called the\nmessage: an element with a vertical line, an icon, and some texts and buttons. However, because this element is complex\nand its requirements may change over time, it’s defined on the backend service as a widget — the element that developers\ncan reuse across different screens.\n\nIf the requirements for the message widget change, we can easily modify it on the backend side without the need to\nrelease the app. That’s because it’s not defined directly in the MBox libraries included in the mobile apps, but\nspecified on the backend using MBox components.\nUnified tracking\nBesides defining layouts, MBox also allows us to specify tracking events on the backend. For tracking events,\nconsistency is crucial. If events are not triggered under the same scenarios and with the same data\non both platforms, it’s hard to compare the data and make business decisions.\nMBox solves that problem. All events tracked on MBox screens are defined on the backend, meaning unified tracking\nbetween Android and iOS and across different app versions.\nTesting\nSince the MBox rendering engine is a core of more and more screens in the app, it had to be thoroughly covered by unit\ntests and integration tests. We also have screenshot tests that ensure that MBox components render correctly. That\nallows us to find out early about possible regressions.\nTeams that develop screens using MBox also have various tools that allow them to test their features. They can write\nunit tests in the MBox backend service and check if correct MBox components are created for the given data.\nThey can also add a URL of their page to Visual Regression — the tool that creates a screenshot of\nthe page whenever someone commits anything to the MBox backend and if any change in the page is detected, the author is\nautomatically notified in their pull request.\nFeature teams can also write UI tests for the native apps to test how their page integrates with the rest of the app and\nif all interactions work as expected. However, those tests have to be written on both platforms by the mobile developers\nand should take into account that the content of the page under tests can be changed on the backend.\nThe journey to make MBox interactive\nWhen we started working on MBox, we were focused mainly on pages that contain a lot of frequently changing content but\nnot many interactions with users. In the first version of MBox, it was possible to define only basic actions like\nopening a new screen or adding an offer to the cart. That changed gradually when new teams started using MBox.\nTo make MBox more interactive, we used the same atomic approach we adopted when designing MBox layout components. We\ngradually added generic actions that were not custom-made to serve specific business features but were reusable across\ndifferent use cases.\nFor example:\nOne of the first challenges that we faced was allowing the implementation of an “add to watchlist” star in MBox. We\ncould’ve just added the ”watchlist star” component that checks if a user is logged in (redirects to the login page if it’s\nnot), adds an offer to the watchlist, and changes the star icon from empty to full. In the short term, it should have\nbeen easier. But it’s not a way that would allow MBox to scale.\nInstead, we designed a couple of atomic mechanisms that allow building this feature on the backend and could be reused\nin the future in different use cases.\nWe added a logic component called multivariant that allows changing one component into another thanks to the\nchangeVariant action. That enabled us to switch the star icon from empty to full. Next, we added the sendRequest\naction\nthat sends requests with given URL, headers, and other data to our services. That allows adding and removing an offer to\nand from the watchlist. Lastly, we added the loginIfNeeded action that allows checking if a user is logged in and\nredirecting to the login screen if needed. That allows ensuring the user is logged in before making the request.\n\nOf course, doing it this way took much more time than just implementing the ”add to watchlist” component in MBox libraries\nnatively. But this is the way that scales and gives us flexibility.\nOver time mechanisms that we designed earlier were reused on other screens. And more and more often, when the new team\nwanted to use MBox on their screen, most of the building blocks they needed were already there. It definitely\nwouldn’t be the case if not for our atomic approach.\nThe challenges\nWe also encountered many challenges while working on MBox.\nConsistency of the engines\nWe create two separate rendering engines for mobile platforms, so we must be extra cautious to ensure everything works consistently.\nEven a tiny inconsistency in the behavior of the engines may be hugely problematic for the developers that use MBox.\nIt may force them to, for example, define different layouts for each mobile platform.\nTo make sure the engines are consistent, each feature in MBox is implemented synchronously by a pair of developers\n(Android and iOS) who consult with each other regularly. During the work, they make sure that they interpret the\nrequirements and cover edge cases in the same way.\nThe new features are ready to merge only after thorough tests on both platforms that check both correctness and\nconsistency.\nVersioning\nOn MBox, we also have to pay close attention to versioning. We use semantic versioning in the engines. Each new feature\nhas to be marked with the same minor and major version on both platforms. We also prepare changelogs containing\ninformation about what\nfunctionalities are available in which version.\nOn the backend, we allow checking the version of the MBox engine that the user has and serve different content depending\non it.\nFor example, when the screen contains the switch component, supported since version 1.21,\nwe can define that for users who have the app with the older versions of MBox, checkbox will be displayed instead.\n\nTesting changes introduced to the engines\nAnd last but not least: testing. Because MBox is used to render various screens in Allegro mobile apps, we must be\ncautious whenever we introduce engine changes to avoid negatively impacting existing MBox screens.\nThe screenshot and UI tests cover every MBox component and action. We’re also encouraging feature teams to add their\nscreens to the Visual Regression and cover their screens with UI tests in the mobile repositories. All those things\nallow us to minimize the risk of introducing a regression.\nHow does MBox connect to other parts of the Allegro ecosystem?\nConsistency across mobile platforms is not everything. Another important aspect of our work is making sure mobile and\nweb platforms are as consistent as possible, respecting native differences that make each platform unique.\nMBox integrates with our content management system, also used for the web (Opbox Page Manager). The screen’s content\nconfigured in the Opbox admin panel is sent through the Opbox services to the MBox backend service. The MBox service\nmaps the\ndata into MBox components that make up the MBox screen. Then the screen definition in JSON format is sent to apps and is\nrendered using native views.\nThe same data from Opbox is also used to render the web equivalent of the same screen. Opbox defines its own mappings\nfor the web: Opbox Components, which describe how to map the data into HTML elements that make up the Allegro web pages.\n\nIntegration with Opbox gives us a lot of advantages. Very often, to change the content in the app and web, you don’t\nneed to change the code at all — all you need to do is change the content in the Opbox admin panel.\nAnother huge advantage is that we have unified tracking between all platforms and can use the same tools for A/B testing\nthat are used for the web. Previously, code for A/B tests had to be written for each mobile platform separately in\nnative\ncode and then cleaned up after the finished experiment. Now, some experiments work out of the box since Opbox sends\ndifferent data to MBox depending on the experiment variant the user falls into. Sometimes a little bit of code in the\nMBox backend is required to conduct an experiment, but it’s not comparable to the amount of work A/B tests take when\nthey’re performed in the native code without MBox.\nConclusions\nMBox is a tool that changed how we work on mobile apps in Allegro. It allowed us to shorten the development time without\ncompromising the quality and stability of the app and without losing the native look and feel of the Allegro apps.\nWe have come a long way during those three years since we started working on MBox. At first, our ambition was to create\na tool that would be used on content screens with very few interactions. Over time, we pushed the boundaries of what\nMBox\nis capable of and entered screens with more and more interactions with the user.\nCurrently MBox is used in over 25 screens in Allegro mobile apps and the number is still growing. In the first half of\n2022 alone, 27 teams made changes to the app using MBox and created about 300 pull requests. We deployed changes over\n100 times which means ~4.15 releases a week.\nWe’re confident that it’s not the end of the possibilities ahead of us. We still see how we can make MBox even more\npowerful. We’d love to shorten development time even more by providing tools that allow defining MBox screens in\nTypeScript. That’ll enable developers to reuse some parts of the code between mobile and web and take advantage of\nbetter tools such as hot reload. Another thing we’re currently focused on is adding the binding mechanism to MBox and\nthe client-side logic to allow defining the business logic on the backend. Implementing those mechanisms will allow\nintroducing even more interactivity into MBox screens.\nBut that is the topic for the next articles. Stay tuned!","guid":"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html","categories":["tech","Server-driven UI","mobile","mbox"],"isoDate":"2022-08-02T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[],"events":[{"created":1664275530000,"duration":5400000,"id":"288748190","name":"Allegro Tech Live #30 - Dług technologiczny - jak go spłacić i nie zbankrutować","date_in_series_pattern":false,"status":"past","time":1665676800000,"local_date":"2022-10-13","local_time":"18:00","updated":1665684962000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/288748190/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-30](https://app.evenea.pl/event/allegro-tech-talk-30) UWAGA: Z różnych powodów (mniej lub bardziej zależnych od nas) czwartkowe spotkanie Allegro Tech Talk przenosimy w pełni do świata online. Chociaż…","visibility":"public","member_pay_fee":false},{"created":1657193453000,"duration":7200000,"id":"287035383","name":"Allegro Tech Labs #10 Online: Poskromić stan w React","date_in_series_pattern":false,"status":"past","time":1658934000000,"local_date":"2022-07-27","local_time":"17:00","updated":1658944632000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":26,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/287035383/","description":"❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: [https://app.evenea.pl/event/allegro-tech-labs-10/](https://app.evenea.pl/event/allegro-tech-labs-10/?fbclid=IwAR1Zj3sIcfx3WEWiFfS_hgiW6BJQD6stYouSGuSqfxDq9YVeom8fTFcrE1Q) ❗ **Allegro Tech Labs** to w 100% zdalna odsłona naszych stacjonarnych spotkań warsztatowych. Zazwyczaj spotykaliśmy się…","visibility":"public","member_pay_fee":false},{"created":1655131243000,"duration":5400000,"id":"286545395","name":"Allegro Tech Live #29 - Wyzwania Product Managera","date_in_series_pattern":false,"status":"past","time":1656604800000,"local_date":"2022-06-30","local_time":"18:00","updated":1656612323000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":88,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/286545395/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":"https://app.evenea.pl/event/allegro-tech-live-29/","visibility":"public","member_pay_fee":false},{"created":1650552918000,"duration":100800000,"id":"285416318","name":"UX Research Confetti - II edycja","date_in_series_pattern":false,"status":"past","time":1653562800000,"local_date":"2022-05-26","local_time":"13:00","updated":1653666063000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/285416318/","description":"REJESTRACJA NA WYDARZENIE -&gt; https://app.evenea.pl/event/ux-research-confetti-2/ 🎉 Niech ponownie rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy……","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"S03E04 - Jakub Westfalewski, Paweł Wolak - O rekrutacji developerów w Allegro","link":"https://podcast.allegro.tech/o-rekrutacji-developer%C3%B3w-w-allegro/","pubDate":"Thu, 06 Oct 2022 00:00:00 GMT","content":"Jak zacząć pracę w Allegro i dlaczego #dobrzetubyć? Jak wygląda proces rekrutacji na stanowiska developerskie? Jak przygotować się do udziału w naszej rekrutacji i kim są “finiszujący perfekcjoniści”? Czym jest i za co odpowiada Hiring Squad? Jakie wymagania trzeba spełnić, aby być senior developerem, a jakie, żeby zostać juniorem? Na te i inne pytania odpowiadają Jakub Westfalewski - Team Manager zespołu zajmującego się procesem zakupowym i pozakupowym oraz Paweł Wolak - Senior Front-end Software Engineer w Allegro.","contentSnippet":"Jak zacząć pracę w Allegro i dlaczego #dobrzetubyć? Jak wygląda proces rekrutacji na stanowiska developerskie? Jak przygotować się do udziału w naszej rekrutacji i kim są “finiszujący perfekcjoniści”? Czym jest i za co odpowiada Hiring Squad? Jakie wymagania trzeba spełnić, aby być senior developerem, a jakie, żeby zostać juniorem? Na te i inne pytania odpowiadają Jakub Westfalewski - Team Manager zespołu zajmującego się procesem zakupowym i pozakupowym oraz Paweł Wolak - Senior Front-end Software Engineer w Allegro.","guid":"https://podcast.allegro.tech/o-rekrutacji-developer%C3%B3w-w-allegro/","isoDate":"2022-10-06T00:00:00.000Z"},{"title":"S03E03 - Paweł Marcinkowski - O Data & AI w Allegro Pay","link":"https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/","pubDate":"Thu, 22 Sep 2022 00:00:00 GMT","content":"Jak zbudowany jest obszar Data & AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data & AI w Allegro Pay.","contentSnippet":"Jak zbudowany jest obszar Data & AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data & AI w Allegro Pay.","guid":"https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/","isoDate":"2022-09-22T00:00:00.000Z"},{"title":"S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box","link":"https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/","pubDate":"Thu, 08 Sep 2022 00:00:00 GMT","content":"Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.","contentSnippet":"Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.","guid":"https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/","isoDate":"2022-09-08T00:00:00.000Z"},{"title":"S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro","link":"https://podcast.allegro.tech/o-quality-assurance-w-allegro/","pubDate":"Thu, 25 Aug 2022 00:00:00 GMT","content":"Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro. ","contentSnippet":"Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro.","guid":"https://podcast.allegro.tech/o-quality-assurance-w-allegro/","isoDate":"2022-08-25T00:00:00.000Z"}]},"__N_SSG":true}