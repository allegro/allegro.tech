{"pageProps":{"posts":[{"title":"How to turn on TypeScript strict mode in specific files","link":"https://blog.allegro.tech/2021/09/How-to-turn-on-TypeScript-strict-mode-in-specific-files.html","pubDate":"Mon, 06 Sep 2021 00:00:00 +0200","authors":{"author":[{"name":["Kamil Krysiak"],"photo":["https://blog.allegro.tech/img/authors/kamil.krysiak.jpg"],"url":["https://blog.allegro.tech/authors/kamil.krysiak"]},{"name":["Jarosław Glegoła"],"photo":["https://blog.allegro.tech/img/authors/jaroslaw.glegola.jpg"],"url":["https://blog.allegro.tech/authors/jaroslaw.glegola"]}]},"content":"<p>Imagine you have to migrate your JavaScript project to TypeScript. It’s fairly simple to convert one file from JS to TS, but if\nyou want to take type checking to the next level (going for TypeScript’s strict mode) it is not that easy. The only solution you\nhave is turning on strict mode for the whole project which may result in thousands of errors. For most projects that are not strict yet,\nit would take quite a bit of time and effort to fix all the strict errors at once.</p>\n\n<h2 id=\"turning-strict-mode-on-in-development-only\">Turning strict-mode on in development only?</h2>\n\n<p>You could think of turning on strict mode during development, catching strict errors that way, and then turning it off before\npushing your changes, but this approach has a few downsides.</p>\n\n<ol>\n  <li>You’ve got to remember to change <code class=\"language-plaintext highlighter-rouge\">tsconfig.json</code> every time you make changes — without automation, this could get tedious.</li>\n  <li>It won’t work in your CI pipeline</li>\n  <li>It will show errors in files you don’t want to make strict yet</li>\n</ol>\n\n<p>Ok, so what can we do to improve this workflow?</p>\n\n<h2 id=\"introducing-typescript-strict-plugin\">Introducing typescript-strict-plugin</h2>\n\n<p><a href=\"https://github.com/allegro/typescript-strict-plugin\">typescript-strict-plugin</a> eliminates all the above problems by allowing you to specify exactly what files you want to be strictly\nchecked. You can do that by simply putting a single comment at the top of the file and typescript will strictly check it. Now\nevery member of your team will have strict errors shown to them in the editor of their choosing (yes, this plugin works with\nwebstorm, vscode, vim, and more).</p>\n\n<p>Unfortunately, typescript plugins do not work at compilation time, they work only in IDEs. Another nice feature that comes in the\npackage is a compile-time tool that allows you to connect the strict plugin to your CI pipeline, or a pre-commit hook. It checks\nmarked files with strict mode and prints to the console all strict errors found. If a single strict error is found, the tool\nexits with an error, so you can be sure that all specified files are really strict (strict, strict, strict… ahh).</p>\n\n<h2 id=\"how-to-use-it\">How to use it?</h2>\n\n<h3 id=\"install-the-typescript-strict-plugin-package\">Install the <code class=\"language-plaintext highlighter-rouge\">typescript-strict-plugin</code> package</h3>\n\n<p>with npm:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>npm i <span class=\"nt\">-D</span> typescript-strict-plugin\n</code></pre></div></div>\n\n<p>or yarn:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>yarn add <span class=\"nt\">-D</span> typescript-strict-plugin\n</code></pre></div></div>\n\n<h3 id=\"add-the-plugin-to-your-tsconfigjson\">Add the plugin to your <code class=\"language-plaintext highlighter-rouge\">tsconfig.json</code></h3>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"compilerOptions\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"err\">//...</span><span class=\"w\">\n    </span><span class=\"nl\">\"strict\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"plugins\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n      </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"typescript-strict-plugin\"</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">]</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<h3 id=\"mark-strict-files-with-ts-strict-comment\">Mark strict files with <code class=\"language-plaintext highlighter-rouge\">//@ts-strict</code> comment</h3>\n\n<p>Before:</p>\n\n<div class=\"language-typescript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">const</span> <span class=\"nx\">name</span><span class=\"p\">:</span> <span class=\"kr\">string</span> <span class=\"o\">=</span> <span class=\"kc\">null</span><span class=\"p\">;</span> <span class=\"c1\">// no error here</span>\n</code></pre></div></div>\n\n<p>After:</p>\n\n<div class=\"language-typescript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// @ts-strict</span>\n<span class=\"p\">...</span>\n<span class=\"kd\">const</span> <span class=\"nx\">name</span><span class=\"p\">:</span> <span class=\"kr\">string</span> <span class=\"o\">=</span> <span class=\"kc\">null</span><span class=\"p\">;</span> <span class=\"c1\">// TS2322: Type ‘null’ is not assignable to type ‘string’.</span>\n</code></pre></div></div>\n\n<p>You can also directly specify directories you want to be strict. In the following example, every file in <code class=\"language-plaintext highlighter-rouge\">src</code> and <code class=\"language-plaintext highlighter-rouge\">test</code>\ndirectories will be strictly checked.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"compilerOptions\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"err\">//...</span><span class=\"w\">\n    </span><span class=\"nl\">\"strict\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"plugins\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n      </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"typescript-strict-plugin\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"path\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">\"./src\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"./test\"</span><span class=\"p\">]</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">]</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<h3 id=\"add-tsc-strict-to-your-type-checking-packagejson-scripts\">Add <code class=\"language-plaintext highlighter-rouge\">tsc-strict</code> to your type checking package.json scripts</h3>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"err\">//</span><span class=\"w\"> </span><span class=\"err\">package.json</span><span class=\"w\">\n</span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"scripts\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"typecheck\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"tsc &amp;&amp; tsc-strict\"</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>Otherwise, the plugin will not work outside your IDE.</p>\n\n<p><strong>Note:</strong> <code class=\"language-plaintext highlighter-rouge\">tsc-strict</code> script uses TypeScript’s <code class=\"language-plaintext highlighter-rouge\">tsc</code> under the hood, so the full type checking time in this scenario would double.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p><code class=\"language-plaintext highlighter-rouge\">typescript-strict-plugin</code> can improve your app’s reliability and type safety. And all that without any disadvantages except for\ncompilation time and a few comments.</p>\n\n<p>If you’re interested in how this works under the hood, we are working on a separate post on making your own TS plugin, so stay\ntuned!</p>\n","contentSnippet":"Imagine you have to migrate your JavaScript project to TypeScript. It’s fairly simple to convert one file from JS to TS, but if\nyou want to take type checking to the next level (going for TypeScript’s strict mode) it is not that easy. The only solution you\nhave is turning on strict mode for the whole project which may result in thousands of errors. For most projects that are not strict yet,\nit would take quite a bit of time and effort to fix all the strict errors at once.\nTurning strict-mode on in development only?\nYou could think of turning on strict mode during development, catching strict errors that way, and then turning it off before\npushing your changes, but this approach has a few downsides.\nYou’ve got to remember to change tsconfig.json every time you make changes — without automation, this could get tedious.\nIt won’t work in your CI pipeline\nIt will show errors in files you don’t want to make strict yet\nOk, so what can we do to improve this workflow?\nIntroducing typescript-strict-plugin\ntypescript-strict-plugin eliminates all the above problems by allowing you to specify exactly what files you want to be strictly\nchecked. You can do that by simply putting a single comment at the top of the file and typescript will strictly check it. Now\nevery member of your team will have strict errors shown to them in the editor of their choosing (yes, this plugin works with\nwebstorm, vscode, vim, and more).\nUnfortunately, typescript plugins do not work at compilation time, they work only in IDEs. Another nice feature that comes in the\npackage is a compile-time tool that allows you to connect the strict plugin to your CI pipeline, or a pre-commit hook. It checks\nmarked files with strict mode and prints to the console all strict errors found. If a single strict error is found, the tool\nexits with an error, so you can be sure that all specified files are really strict (strict, strict, strict… ahh).\nHow to use it?\nInstall the typescript-strict-plugin package\nwith npm:\n\nnpm i -D typescript-strict-plugin\n\n\nor yarn:\n\nyarn add -D typescript-strict-plugin\n\n\nAdd the plugin to your tsconfig.json\n\n{\n  \"compilerOptions\": {\n    //...\n    \"strict\": false,\n    \"plugins\": [\n      {\n        \"name\": \"typescript-strict-plugin\"\n      }\n    ]\n  }\n}\n\n\nMark strict files with //@ts-strict comment\nBefore:\n\nconst name: string = null; // no error here\n\n\nAfter:\n\n// @ts-strict\n...\nconst name: string = null; // TS2322: Type ‘null’ is not assignable to type ‘string’.\n\n\nYou can also directly specify directories you want to be strict. In the following example, every file in src and test\ndirectories will be strictly checked.\n\n{\n  \"compilerOptions\": {\n    //...\n    \"strict\": false,\n    \"plugins\": [\n      {\n        \"name\": \"typescript-strict-plugin\",\n        \"path\": [\"./src\", \"./test\"]\n      }\n    ]\n  }\n}\n\n\nAdd tsc-strict to your type checking package.json scripts\n\n// package.json\n{\n  \"scripts\": {\n    \"typecheck\": \"tsc && tsc-strict\"\n  }\n}\n\n\nOtherwise, the plugin will not work outside your IDE.\nNote: tsc-strict script uses TypeScript’s tsc under the hood, so the full type checking time in this scenario would double.\nConclusion\ntypescript-strict-plugin can improve your app’s reliability and type safety. And all that without any disadvantages except for\ncompilation time and a few comments.\nIf you’re interested in how this works under the hood, we are working on a separate post on making your own TS plugin, so stay\ntuned!","guid":"https://blog.allegro.tech/2021/09/How-to-turn-on-TypeScript-strict-mode-in-specific-files.html","categories":["typescript","scrict mode","typescript plugin","code quality"],"isoDate":"2021-09-05T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Splitting data that does not fit on one machine using data partitioning","link":"https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html","pubDate":"Tue, 10 Aug 2021 00:00:00 +0200","authors":{"author":[{"name":["Tomasz Lelek"],"photo":["https://blog.allegro.tech/img/authors/tomasz.lelek.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.lelek"]},{"name":["Jon Skeet"],"photo":["https://blog.allegro.tech/img/authors/jon.skeet.jpg"],"url":["https://blog.allegro.tech/authors/jon.skeet"]}]},"content":"<p>The following article is an excerpt from <a href=\"https://www.manning.com/books/software-mistakes-and-tradeoffs\">Software Mistakes and Trade-offs</a> book.\nIn real-world big data applications, the amount of data that we need to store and process can be often counted in the hundreds of terabytes or petabytes. It is not feasible to store such an amount of data on one physical node. We need a way to split that data into N data nodes.</p>\n\n<p>The technique for splitting the data is called data partitioning. There are a lot of techniques to partition your data.</p>\n\n<p>For online processing sources (like a database), you may pick some ID, for example, user-ID, and store a range of users on a dedicated node. For example, assuming that you have 1000 user IDs and 5 data nodes, the first node can store IDs from 0 to 200, the second node can store data from 201 to 400, and so on. When picking the partitioning scheme, you need to be careful not to introduce the data skew. Such a situation can occur when most of the data is produced by one or a group of IDs that belongs to the same data node. For example, let’s assume that the user ID 10 is responsible for 80% of our traffic and generates 80% of the data. Therefore, it will mean that 80% of the data is stored on the first data node, and our partitioning will not be optimal. In the worst case, this user’s amount of data may be too big to store on the given data node. It is important to note that for online processing, the partitioning is optimized for reading or writing data access patterns.</p>\n\n<h2 id=\"offline-big-data-partitioning\">Offline big data partitioning</h2>\n\n<p>We will focus now on the offline, big data processing partitioning.</p>\n\n<p>For Big Data systems, we often need to store the historical data (cold data) for an “indefinite” amount of time. It is crucial to store the data for as long as we can. When the data is produced, we may not be aware of the business value that it can bring in the future. For example, we may save all user’s request data with all the HTTP headers. When the data is saved, there may be no use case for these HTTP headers. In the future, however, we may decide to build a tool that profiles our users by the type of device (Android, iOS) that they use. Such information is propagated in the HTTP headers. We can execute our new profiling logic based on the historical data because we stored it in the raw data. It is important to note here that the data was not needed for a long period.</p>\n\n<p>On the other hand, we needed to store a lot of information and save it for later. Thus, our storage needs to contain a lot of data stored in cold storage. In Big Data applications, it often means that data is saved to a Hadoop distributed file system (HDFS). It also means that the data should be partitioned in a fairly generic way. We cannot optimize for read patterns because we cannot anticipate how those read patterns will look like.</p>\n\n<p>Because of these reasons, the most often used data partitioning scheme for big data offline processing is based on dates. Let’s assume that we have a system that saves user’s data on the /users file system path and clickstream data in the /clicks file system path. We will analyze the first data set that stores the user’s data. We are assuming that the number of records that we store is equal to 10 billion. We started collecting the data in the year 2017, and it’s been collected since then.</p>\n\n<p>The partitioning scheme that we pick is based on the date. It means that our partition identifier starts with the year. We will have 2017, 2018, 2019, and 2020 partitions. If we would have smaller data requirements, partitioning by year may be enough. In such a scenario, the file system path for our user’s data would be /users/2017, /users/2018, and so on. It will be analogical for clicks: /clicks/2017, /clicks/2018, and so on.</p>\n\n<p><img src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img1.png\" alt=\"Figure 1\" /></p>\n\n<h2 id=\"four-data-partitions\">Four data partitions</h2>\n\n<p>By using this partitioning, the user’s data will have 4 partitions. It means that we can split the data into up to four physical data nodes. The first node will store the data for the year 2017, the second node for 2018, etc. Nothing prevents us from keeping all of those partitions on the same physical node when having four partitions. We may be ok with storing the data on one physical node as long as we have enough disk space. Once the disk space runs out, we can create a new physical node and move some of the partitions to the new node.</p>\n\n<p>In practice, such a partitioning scheme is too coarse-grained. Having one big partition for all year’s data is hard from both a read and write perspective. When you read such data and are interested only in events from a particular date, you need to scan the whole year’s data. It’s very inefficient and time-consuming. It is also problematic from the writing perspective because if your disk space runs out, there is no easy way to split the data further. You won’t be able to perform a successful write.</p>\n\n<p>Because of that reason, offline big data systems tend to partition the data in a more fine-grained fashion. The data is partitioned by year, month, and even day. For example, if you are writing data for the 2nd of January 2020, you will save the event into a /users/2020/01/02 partition. Such a partitioning gives you a lot of flexibility at the read side as well. If you wish to analyze events for a specific day, you can directly read the data from the partition. If you want to perform some higher-level analysis, for example, analyze the whole month’s data, you can read all partitions within a given month. The same pattern applies if you want to analyze a whole year’s data.</p>\n\n<p>To sum up, our 10 billion records will be partitioned in the following way:</p>\n\n<p><img src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img2.png\" alt=\"Figure 2\" /></p>\n\n<h2 id=\"date-based-data-partitioning\">Date based data partitioning</h2>\n\n<p>You can see that the initial 10 billion records are partitioned into year/month, and finally, a specific date of the month. In the end, each day’s partition contains a hundred thousand records. Such an amount of data can easily fit into one machine disk space. It also means that we have 365/366 partitions per year. The upper number of data nodes on which we can partition the data is equal to the number of days * the number of years we store data. If your one-day data does not fit into one machine disk space, you can easily partition your data further by hours of the day, minutes, seconds, and so on.</p>\n\n<h2 id=\"partitioning-vs-sharding\">Partitioning vs sharding</h2>\n\n<p>Assuming that we have our data partitioned by the date, we can split that data into multiple nodes. In such a scenario, we are putting a subset of all partition keys in a physical node.</p>\n\n<p>Our user’s data is partitioned into N partitions (logical shards). Let’s assume that our partition granularity is a month. In that case, the data for the year 2020 has 12 partitions that can be split horizontally into N physical nodes (physical shards). It is important to note that N is less than or equal to 12. In other words, the maximum level of physical shards is 12. This architecture pattern is called sharding.</p>\n\n<p>Let’s assume that we have three physical nodes. In that case, we can say that our user’s data for the year 2020 is partitioned into 12 partitions. Next, they are assigned to 3 shards (nodes). Each of the nodes stores 4 partitions for 2020 (12 partitions / 3 nodes = 4 partitions/node).</p>\n\n<p><img src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img1.png\" alt=\"Figure 8.6. Sharding\" /></p>\n\n<p>In our diagram, the physical shard is the same as the physical node. The partition keys (logical shards) are distributed evenly to physical shards. In case a new node is added to a cluster, each physical shard needs to re-assign one of its logical shards to a new physical node.</p>\n\n<p>There are a variety of algorithms for shards assignments. They also need to handle shards re-distribution in case of adding or removing a node (failure or scale down). This technique is used by most big data technologies and data stores such as HDFS, Cassandra, Kafka, Elastic, etc., and they vary depending on the implementation.</p>\n\n<p>At this point, we know the basics of data partitioning. Next, we need to understand how the partitioning algorithms work in depth. This is essential to understand if we want to reason about the big data tools we are using to get business value. This is discussed at length in the book.</p>\n\n<p>This is discussed at length in the book. You can purchase it with a special discount <a href=\"https://www.manning.com/books/software-mistakes-and-tradeoffs\">here</a> using the code <strong>allegrotech35</strong>.</p>\n","contentSnippet":"The following article is an excerpt from Software Mistakes and Trade-offs book.\nIn real-world big data applications, the amount of data that we need to store and process can be often counted in the hundreds of terabytes or petabytes. It is not feasible to store such an amount of data on one physical node. We need a way to split that data into N data nodes.\nThe technique for splitting the data is called data partitioning. There are a lot of techniques to partition your data.\nFor online processing sources (like a database), you may pick some ID, for example, user-ID, and store a range of users on a dedicated node. For example, assuming that you have 1000 user IDs and 5 data nodes, the first node can store IDs from 0 to 200, the second node can store data from 201 to 400, and so on. When picking the partitioning scheme, you need to be careful not to introduce the data skew. Such a situation can occur when most of the data is produced by one or a group of IDs that belongs to the same data node. For example, let’s assume that the user ID 10 is responsible for 80% of our traffic and generates 80% of the data. Therefore, it will mean that 80% of the data is stored on the first data node, and our partitioning will not be optimal. In the worst case, this user’s amount of data may be too big to store on the given data node. It is important to note that for online processing, the partitioning is optimized for reading or writing data access patterns.\nOffline big data partitioning\nWe will focus now on the offline, big data processing partitioning.\nFor Big Data systems, we often need to store the historical data (cold data) for an “indefinite” amount of time. It is crucial to store the data for as long as we can. When the data is produced, we may not be aware of the business value that it can bring in the future. For example, we may save all user’s request data with all the HTTP headers. When the data is saved, there may be no use case for these HTTP headers. In the future, however, we may decide to build a tool that profiles our users by the type of device (Android, iOS) that they use. Such information is propagated in the HTTP headers. We can execute our new profiling logic based on the historical data because we stored it in the raw data. It is important to note here that the data was not needed for a long period.\nOn the other hand, we needed to store a lot of information and save it for later. Thus, our storage needs to contain a lot of data stored in cold storage. In Big Data applications, it often means that data is saved to a Hadoop distributed file system (HDFS). It also means that the data should be partitioned in a fairly generic way. We cannot optimize for read patterns because we cannot anticipate how those read patterns will look like.\nBecause of these reasons, the most often used data partitioning scheme for big data offline processing is based on dates. Let’s assume that we have a system that saves user’s data on the /users file system path and clickstream data in the /clicks file system path. We will analyze the first data set that stores the user’s data. We are assuming that the number of records that we store is equal to 10 billion. We started collecting the data in the year 2017, and it’s been collected since then.\nThe partitioning scheme that we pick is based on the date. It means that our partition identifier starts with the year. We will have 2017, 2018, 2019, and 2020 partitions. If we would have smaller data requirements, partitioning by year may be enough. In such a scenario, the file system path for our user’s data would be /users/2017, /users/2018, and so on. It will be analogical for clicks: /clicks/2017, /clicks/2018, and so on.\n\nFour data partitions\nBy using this partitioning, the user’s data will have 4 partitions. It means that we can split the data into up to four physical data nodes. The first node will store the data for the year 2017, the second node for 2018, etc. Nothing prevents us from keeping all of those partitions on the same physical node when having four partitions. We may be ok with storing the data on one physical node as long as we have enough disk space. Once the disk space runs out, we can create a new physical node and move some of the partitions to the new node.\nIn practice, such a partitioning scheme is too coarse-grained. Having one big partition for all year’s data is hard from both a read and write perspective. When you read such data and are interested only in events from a particular date, you need to scan the whole year’s data. It’s very inefficient and time-consuming. It is also problematic from the writing perspective because if your disk space runs out, there is no easy way to split the data further. You won’t be able to perform a successful write.\nBecause of that reason, offline big data systems tend to partition the data in a more fine-grained fashion. The data is partitioned by year, month, and even day. For example, if you are writing data for the 2nd of January 2020, you will save the event into a /users/2020/01/02 partition. Such a partitioning gives you a lot of flexibility at the read side as well. If you wish to analyze events for a specific day, you can directly read the data from the partition. If you want to perform some higher-level analysis, for example, analyze the whole month’s data, you can read all partitions within a given month. The same pattern applies if you want to analyze a whole year’s data.\nTo sum up, our 10 billion records will be partitioned in the following way:\n\nDate based data partitioning\nYou can see that the initial 10 billion records are partitioned into year/month, and finally, a specific date of the month. In the end, each day’s partition contains a hundred thousand records. Such an amount of data can easily fit into one machine disk space. It also means that we have 365/366 partitions per year. The upper number of data nodes on which we can partition the data is equal to the number of days * the number of years we store data. If your one-day data does not fit into one machine disk space, you can easily partition your data further by hours of the day, minutes, seconds, and so on.\nPartitioning vs sharding\nAssuming that we have our data partitioned by the date, we can split that data into multiple nodes. In such a scenario, we are putting a subset of all partition keys in a physical node.\nOur user’s data is partitioned into N partitions (logical shards). Let’s assume that our partition granularity is a month. In that case, the data for the year 2020 has 12 partitions that can be split horizontally into N physical nodes (physical shards). It is important to note that N is less than or equal to 12. In other words, the maximum level of physical shards is 12. This architecture pattern is called sharding.\nLet’s assume that we have three physical nodes. In that case, we can say that our user’s data for the year 2020 is partitioned into 12 partitions. Next, they are assigned to 3 shards (nodes). Each of the nodes stores 4 partitions for 2020 (12 partitions / 3 nodes = 4 partitions/node).\n\nIn our diagram, the physical shard is the same as the physical node. The partition keys (logical shards) are distributed evenly to physical shards. In case a new node is added to a cluster, each physical shard needs to re-assign one of its logical shards to a new physical node.\nThere are a variety of algorithms for shards assignments. They also need to handle shards re-distribution in case of adding or removing a node (failure or scale down). This technique is used by most big data technologies and data stores such as HDFS, Cassandra, Kafka, Elastic, etc., and they vary depending on the implementation.\nAt this point, we know the basics of data partitioning. Next, we need to understand how the partitioning algorithms work in depth. This is essential to understand if we want to reason about the big data tools we are using to get business value. This is discussed at length in the book.\nThis is discussed at length in the book. You can purchase it with a special discount here using the code allegrotech35.","guid":"https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html","categories":["tech","performance","sharding","bigdata","dba","distributed"],"isoDate":"2021-08-09T22:00:00.000Z","thumbnail":"images/post-headers/bigdata.png"},{"title":"CSS Architecture and Performance in Micro Frontends","link":"https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html","pubDate":"Thu, 29 Jul 2021 00:00:00 +0200","authors":{"author":[{"name":["Mateusz Krzeszowiak"],"photo":["https://blog.allegro.tech/img/authors/mateusz.krzeszowiak.jpg"],"url":["https://blog.allegro.tech/authors/mateusz.krzeszowiak"]}]},"content":"<p>It’s been over 5 years since the introduction of the <a href=\"https://blog.allegro.tech/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">article describing the ongoing transformation of Allegro’s frontend architecture</a> — an approach that was later formalized by the industry under the name of Micro Frontends. I think that after all this time we can safely say that this direction was correct and remained almost entirely unchanged in relation to the original idea. Still, some of the challenges foreseen in the publication soon became the reality. In this article I would like to focus on the CSS part of the whole adventure to tell you about how we manage consistency and frontend performance across over half a thousand components, and what it took us to get to where we stand today.</p>\n\n<h3 id=\"new-approach--new-challenges\">New Approach — New Challenges</h3>\n\n<p>Handling all the dependencies, libraries and visual compatibility when the entire website resides in a single repository is a challenge by itself. The level of difficulty increases even more, when there are hundreds of said repositories, each managed by a different team and tooling. When in such situation, one of the things that quickly become apparent is the need for some kind of guidelines around the look of various aspects of components being developed like color scheme, spacing, fonts etc. — those are exactly the reasons why the Metrum Design System came to life.</p>\n\n<p><img src=\"/img/articles/2021-07-29-css-architecture-and-performance-of-micro-frontends/metrum-design-system.jpg\" alt=\"Metrum Design System\" title=\"Metrum Design System\" /></p>\n\n<p>In its initial form — apart from visual examples and design resources — Metrum was providing reusable PostCSS mixins that every developer could install via separate npm packages and include in the component they were working on.</p>\n\n<div class=\"language-scss highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">@import</span> <span class=\"s1\">'node_modules/@metrum/button/css/mixins.css'</span><span class=\"p\">;</span>\n\n<span class=\"nc\">.button</span> <span class=\"p\">{</span>\n    <span class=\"k\">@mixin</span> <span class=\"nf\">m-button</span><span class=\"p\">;</span>\n    <span class=\"nl\">background-color</span><span class=\"p\">:</span> <span class=\"no\">black</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nt\">&lt;button</span> <span class=\"na\">class=</span><span class=\"s\">\"button\"</span><span class=\"nt\">&gt;&lt;/button&gt;</span>\n</code></pre></div></div>\n\n<p>If we try to evaluate that approach we could come up with following pros and cons:</p>\n\n<p><strong>Pros</strong></p>\n\n<ul>\n  <li>Easy to use — install mixin and include in component’s selector;</li>\n  <li>Mixins allow for sharing visual identity between components;</li>\n  <li>Developers can use mixins in any version without depending on other parts of the page;</li>\n  <li>Every component ships with a complete set of styles.</li>\n</ul>\n\n<p><strong>Cons</strong></p>\n\n<ul>\n  <li>Including mixins introduces duplication of CSS rules between components used on the same page;</li>\n  <li>More files — every component brings at least one request for its styles;</li>\n  <li>No sharing of CSS — no cache reuse between pages built from different components;</li>\n  <li>Clashing of class names within the global namespace.</li>\n</ul>\n\n<p>In summary, while being very flexible and easy to use, mixins-based approach was not ideal from a performance point of view. Every time when somebody wanted to use a button, input, link etc., they would have to include a mixin for it pulling the entire set of CSS rules to their stylesheet. This resulted in our users downloading unnecessary kilobytes during the first visit while bringing no caching benefit when navigating through other pages which in turn increased rendering times. We knew we could do better.</p>\n\n<h3 id=\"enter-css-modules\">Enter CSS Modules</h3>\n\n<p>After a lot of brainstorming, a decision was made that the next step should involve Metrum making use of CSS Modules. While the technical aspects and usage were changing as the adoption grew, the main principles stayed the same up to this day. Currently, whenever any developer wants to assemble a new component out of Metrum building blocks, they can install desired packages, compose styles from them and declare used classes in their markup:</p>\n\n<div class=\"language-scss highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nc\">.button</span> <span class=\"p\">{</span>\n    <span class=\"na\">composes</span><span class=\"p\">:</span> <span class=\"n\">m-button</span> <span class=\"n\">from</span> <span class=\"s1\">'@metrum/button'</span><span class=\"p\">;</span>\n    <span class=\"na\">composes</span><span class=\"p\">:</span> <span class=\"n\">m-background-color-black</span> <span class=\"n\">from</span> <span class=\"s1\">'@metrum/color'</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"o\">*</span> <span class=\"k\">as</span> <span class=\"nx\">styles</span> <span class=\"k\">from</span> <span class=\"dl\">'</span><span class=\"s1\">./styles.css</span><span class=\"dl\">'</span><span class=\"p\">;</span>\n\n<span class=\"k\">export</span> <span class=\"k\">default</span> <span class=\"kd\">function</span> <span class=\"nx\">render</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"s2\">`\n        &lt;button class=\"</span><span class=\"p\">${</span><span class=\"nx\">styles</span><span class=\"p\">.</span><span class=\"nx\">button</span><span class=\"p\">}</span><span class=\"s2\">\"&gt;...&lt;/button&gt;\n    `</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Thanks to the fact that all of our micro frontends run on Node.js, this approach can be used quite easily with the majority of tooling available. The only thing left to do is to collect all of the required Metrum stylesheets during render in our facade server called opbox-web and embed them on the page with the correct order. Ordering requirement is important, because we follow atomic design and more complicated components (molecules, organisms) are built using simpler ones (atoms). Lets see what all of those changes did to our list of tradeoffs:</p>\n\n<p><strong>Pros</strong></p>\n\n<ul>\n  <li>Still easy to use — install package and compose desired classes in your component;</li>\n  <li>Sharing classes means sharing visual traits which was one of our goals;</li>\n  <li>Styles for certain module only appear once per page if used in multiple components;</li>\n  <li>Each Metrum stylesheet can be cached by the browser separately and reused on different pages;</li>\n  <li>Developers can still use packages in any version without depending on other parts of the page.</li>\n</ul>\n\n<p><strong>Cons</strong></p>\n\n<ul>\n  <li>Additional logic has to be maintained that extracts needed Metrum stylesheets from components and adds them to the page once;</li>\n  <li>Above logic has to also take care of sorting so the order of styles is correct and we don’t run into problems with cascade;</li>\n  <li>Multiple versions of the same Metrum component may be needed on the page;</li>\n  <li>More and more requests have to be made as components transition to the new approach.</li>\n</ul>\n\n<p>Judging from the upsides the transition was worth it: despite higher maintenance effort we were finally able to share common CSS code between components, the amount of downloaded data as well as render times started decreasing. Unfortunately, after some time we started to see a worrying trend related to the number of embedded stylesheets. Prior to this change, it was roughly equal to the number of components used on the page. Afterwards, with additional Metrum modules, plus the fact that multiple versions of them may be needed, we ended up with as much as around 100 requests for render-blocking CSS.</p>\n\n<p>Usually, when we bring up the issue of excessive number of requests people respond with “So what? You have HTTP/2, right?” and yes, we do. It’s true that the user agent will reuse existing connections for multiple files but the limit of concurrent streams does exist, latency is still going to affect each one of them and the compression efficiency will be worse especially for those relatively small files like ours. We had to come up with yet another idea for improvement.</p>\n\n<h3 id=\"let-the-bundle-begin\">Let the Bundle Begin</h3>\n\n<p>As I touched briefly earlier, we have the opbox-web — a place that’s already responsible for extracting, sorting and embedding Metrum dependencies. We figured that instead of adding each of them separately, we could prepare predefined bundles that would serve as replacements. We did as planned and, after deployment on 6th of July 2020, achieved 15% improvement in FCP metric time, which means that our users saw the first render of content faster by almost half a second.</p>\n\n<p><img src=\"/img/articles/2021-07-29-css-architecture-and-performance-of-micro-frontends/fcp-after-metrum-bundle.png\" alt=\"FCP metric chart before and after deployment of Metrum bundle\" title=\"FCP metric chart before and after deployment of Metrum bundle\" /></p>\n\n<p>Improvement was satisfactory, but it came at a certain cost. From that time on we had to make sure all of the components used on a certain page share the same versions of Metrum modules supported by the bundle and I assure you, it was bothersome to say the least. Monitoring that nobody updated their dependency by accident was one thing (especially that we managed to automate it) but undergoing a process of actually wanting to do this was another. In addition, every time we failed within that area we had to bail and serve every stylesheet separately, preventing incorrect order of CSS and bringing performance to the previous low.</p>\n\n<p><strong>Pros</strong></p>\n\n<ul>\n  <li>CSS Modules usage stayed the same;</li>\n  <li>Fewer requests for critical resources resulted in noticeable improvement in FCP metric.</li>\n</ul>\n\n<p><strong>Cons</strong></p>\n\n<ul>\n  <li>Extra work is needed to keep Metrum packages versions aligned;</li>\n  <li>Updating Metrum dependencies becomes much harder as it requires synchronization between all of the components on a certain page;</li>\n  <li>All of the above meant that we only managed to enable this feature on the most popular of routes.</li>\n</ul>\n\n<p>We knew there was going to be additional effort to maintain this solution but the performance gains outweighed the cost at that time. It took almost a year of tedious work from multiple teams to keep the look and feel of Allegro up to date with newest changes, until we came up with another idea.</p>\n\n<h3 id=\"just-in-time-bundling\">Just In Time Bundling</h3>\n\n<p>In the beginning of 2021 another idea started to form. This time we wanted to unlock the agile nature of our Micro Frontends and their deployment. We came to the conclusion that it would be ideal if instead of serving bundles containing a predefined list of components, we could send one composed of just the files that were actually required to render a certain page. Collecting the list of CSS that’s needed was not the problem — we were generating the HEAD section after all — but generating these unique bundles, well that was something different.</p>\n\n<p>First option we had to verify was the possibility to prepare all of the bundles beforehand so they can be picked and served from CDN. Sadly, taking into account that there are around 500 components, any of which can either be used as a building block of a certain page or not, gives us 2<sup>500</sup> combinations which is way more than we can handle. Even if we optimistically assumed that each stylesheet required around 50ms to generate, it would take us roughly 5x10<sup>141</sup> years to cover everything. Additionally, it would not only be a waste of time and storage (some components have higher possibility to be used than others) but also at least a portion of the work would have to be redone every time a component is updated, what can happen multiple times a day.</p>\n\n<p>Finally, we went with a different approach by implementing a bundler microservice. Its operating principle can be explained in a few steps:</p>\n\n<ol>\n  <li>When user makes a request for a page, our server collects a list of CSS files that would normally be added to the HEAD section;</li>\n  <li>It sends this list of files to our microservice asking for an URL to the corresponding bundle that contains them;</li>\n  <li>The microservice checks if it has the bundle in its cache:\n    <ol>\n      <li>If it does, then bundle URL is immediately returned;</li>\n      <li>Otherwise, it also responds right away with empty result, triggering bundle generation in the background;</li>\n    </ol>\n  </li>\n  <li>Based on the response from the microservice, the server either embeds separate CSS files as usual or replaces them with a single bundle.</li>\n</ol>\n\n<p>This is where we are now – generating only what is actually needed and keeping the duration overhead minimal. A lot of thought and multiple iterations went into making it possible, so I think you can expect a completely separate article about this microservice in the future. Most important thing for us is that the trend of constant improvement for our users continues as confirmed by <a href=\"https://developers.google.com/web/tools/chrome-user-experience-report/\">Chrome UX Report</a>:</p>\n\n<p><img src=\"/img/articles/2021-07-29-css-architecture-and-performance-of-micro-frontends/fcp-in-crux.png\" alt=\"FCP according to CrUX over last 10 months\" title=\"FCP according to CrUX over last 10 months\" /></p>\n\n<h3 id=\"summary\">Summary</h3>\n\n<p>CSS architecture is one of the most important factors influencing performance, what makes ignoring it harder and harder as the page grows. Fortunately, our experience shows that even in higher-scale systems built using micro frontends, it is still possible to improve successfully. By solving problems of existing solutions and experimenting with new ideas we are able to constantly raise the bar of our metrics making browsing Allegro a better experience for our users month by month.</p>\n","contentSnippet":"It’s been over 5 years since the introduction of the article describing the ongoing transformation of Allegro’s frontend architecture — an approach that was later formalized by the industry under the name of Micro Frontends. I think that after all this time we can safely say that this direction was correct and remained almost entirely unchanged in relation to the original idea. Still, some of the challenges foreseen in the publication soon became the reality. In this article I would like to focus on the CSS part of the whole adventure to tell you about how we manage consistency and frontend performance across over half a thousand components, and what it took us to get to where we stand today.\nNew Approach — New Challenges\nHandling all the dependencies, libraries and visual compatibility when the entire website resides in a single repository is a challenge by itself. The level of difficulty increases even more, when there are hundreds of said repositories, each managed by a different team and tooling. When in such situation, one of the things that quickly become apparent is the need for some kind of guidelines around the look of various aspects of components being developed like color scheme, spacing, fonts etc. — those are exactly the reasons why the Metrum Design System came to life.\n\nIn its initial form — apart from visual examples and design resources — Metrum was providing reusable PostCSS mixins that every developer could install via separate npm packages and include in the component they were working on.\n\n@import 'node_modules/@metrum/button/css/mixins.css';\n\n.button {\n    @mixin m-button;\n    background-color: black;\n}\n\n\n\n<button class=\"button\"></button>\n\n\nIf we try to evaluate that approach we could come up with following pros and cons:\nPros\nEasy to use — install mixin and include in component’s selector;\nMixins allow for sharing visual identity between components;\nDevelopers can use mixins in any version without depending on other parts of the page;\nEvery component ships with a complete set of styles.\nCons\nIncluding mixins introduces duplication of CSS rules between components used on the same page;\nMore files — every component brings at least one request for its styles;\nNo sharing of CSS — no cache reuse between pages built from different components;\nClashing of class names within the global namespace.\nIn summary, while being very flexible and easy to use, mixins-based approach was not ideal from a performance point of view. Every time when somebody wanted to use a button, input, link etc., they would have to include a mixin for it pulling the entire set of CSS rules to their stylesheet. This resulted in our users downloading unnecessary kilobytes during the first visit while bringing no caching benefit when navigating through other pages which in turn increased rendering times. We knew we could do better.\nEnter CSS Modules\nAfter a lot of brainstorming, a decision was made that the next step should involve Metrum making use of CSS Modules. While the technical aspects and usage were changing as the adoption grew, the main principles stayed the same up to this day. Currently, whenever any developer wants to assemble a new component out of Metrum building blocks, they can install desired packages, compose styles from them and declare used classes in their markup:\n\n.button {\n    composes: m-button from '@metrum/button';\n    composes: m-background-color-black from '@metrum/color';\n}\n\n\n\nimport * as styles from './styles.css';\n\nexport default function render() {\n    return `\n        <button class=\"${styles.button}\">...</button>\n    `;\n}\n\n\nThanks to the fact that all of our micro frontends run on Node.js, this approach can be used quite easily with the majority of tooling available. The only thing left to do is to collect all of the required Metrum stylesheets during render in our facade server called opbox-web and embed them on the page with the correct order. Ordering requirement is important, because we follow atomic design and more complicated components (molecules, organisms) are built using simpler ones (atoms). Lets see what all of those changes did to our list of tradeoffs:\nPros\nStill easy to use — install package and compose desired classes in your component;\nSharing classes means sharing visual traits which was one of our goals;\nStyles for certain module only appear once per page if used in multiple components;\nEach Metrum stylesheet can be cached by the browser separately and reused on different pages;\nDevelopers can still use packages in any version without depending on other parts of the page.\nCons\nAdditional logic has to be maintained that extracts needed Metrum stylesheets from components and adds them to the page once;\nAbove logic has to also take care of sorting so the order of styles is correct and we don’t run into problems with cascade;\nMultiple versions of the same Metrum component may be needed on the page;\nMore and more requests have to be made as components transition to the new approach.\nJudging from the upsides the transition was worth it: despite higher maintenance effort we were finally able to share common CSS code between components, the amount of downloaded data as well as render times started decreasing. Unfortunately, after some time we started to see a worrying trend related to the number of embedded stylesheets. Prior to this change, it was roughly equal to the number of components used on the page. Afterwards, with additional Metrum modules, plus the fact that multiple versions of them may be needed, we ended up with as much as around 100 requests for render-blocking CSS.\nUsually, when we bring up the issue of excessive number of requests people respond with “So what? You have HTTP/2, right?” and yes, we do. It’s true that the user agent will reuse existing connections for multiple files but the limit of concurrent streams does exist, latency is still going to affect each one of them and the compression efficiency will be worse especially for those relatively small files like ours. We had to come up with yet another idea for improvement.\nLet the Bundle Begin\nAs I touched briefly earlier, we have the opbox-web — a place that’s already responsible for extracting, sorting and embedding Metrum dependencies. We figured that instead of adding each of them separately, we could prepare predefined bundles that would serve as replacements. We did as planned and, after deployment on 6th of July 2020, achieved 15% improvement in FCP metric time, which means that our users saw the first render of content faster by almost half a second.\n\nImprovement was satisfactory, but it came at a certain cost. From that time on we had to make sure all of the components used on a certain page share the same versions of Metrum modules supported by the bundle and I assure you, it was bothersome to say the least. Monitoring that nobody updated their dependency by accident was one thing (especially that we managed to automate it) but undergoing a process of actually wanting to do this was another. In addition, every time we failed within that area we had to bail and serve every stylesheet separately, preventing incorrect order of CSS and bringing performance to the previous low.\nPros\nCSS Modules usage stayed the same;\nFewer requests for critical resources resulted in noticeable improvement in FCP metric.\nCons\nExtra work is needed to keep Metrum packages versions aligned;\nUpdating Metrum dependencies becomes much harder as it requires synchronization between all of the components on a certain page;\nAll of the above meant that we only managed to enable this feature on the most popular of routes.\nWe knew there was going to be additional effort to maintain this solution but the performance gains outweighed the cost at that time. It took almost a year of tedious work from multiple teams to keep the look and feel of Allegro up to date with newest changes, until we came up with another idea.\nJust In Time Bundling\nIn the beginning of 2021 another idea started to form. This time we wanted to unlock the agile nature of our Micro Frontends and their deployment. We came to the conclusion that it would be ideal if instead of serving bundles containing a predefined list of components, we could send one composed of just the files that were actually required to render a certain page. Collecting the list of CSS that’s needed was not the problem — we were generating the HEAD section after all — but generating these unique bundles, well that was something different.\nFirst option we had to verify was the possibility to prepare all of the bundles beforehand so they can be picked and served from CDN. Sadly, taking into account that there are around 500 components, any of which can either be used as a building block of a certain page or not, gives us 2500 combinations which is way more than we can handle. Even if we optimistically assumed that each stylesheet required around 50ms to generate, it would take us roughly 5x10141 years to cover everything. Additionally, it would not only be a waste of time and storage (some components have higher possibility to be used than others) but also at least a portion of the work would have to be redone every time a component is updated, what can happen multiple times a day.\nFinally, we went with a different approach by implementing a bundler microservice. Its operating principle can be explained in a few steps:\nWhen user makes a request for a page, our server collects a list of CSS files that would normally be added to the HEAD section;\nIt sends this list of files to our microservice asking for an URL to the corresponding bundle that contains them;\nThe microservice checks if it has the bundle in its cache:\n    \nIf it does, then bundle URL is immediately returned;\nOtherwise, it also responds right away with empty result, triggering bundle generation in the background;\nBased on the response from the microservice, the server either embeds separate CSS files as usual or replaces them with a single bundle.\nThis is where we are now – generating only what is actually needed and keeping the duration overhead minimal. A lot of thought and multiple iterations went into making it possible, so I think you can expect a completely separate article about this microservice in the future. Most important thing for us is that the trend of constant improvement for our users continues as confirmed by Chrome UX Report:\n\nSummary\nCSS architecture is one of the most important factors influencing performance, what makes ignoring it harder and harder as the page grows. Fortunately, our experience shows that even in higher-scale systems built using micro frontends, it is still possible to improve successfully. By solving problems of existing solutions and experimenting with new ideas we are able to constantly raise the bar of our metrics making browsing Allegro a better experience for our users month by month.","guid":"https://blog.allegro.tech/2021/07/css-architecture-and-performance-of-micro-frontends.html","categories":["tech","webperf","frontend","performance","perfmatters","css"],"isoDate":"2021-07-28T22:00:00.000Z","thumbnail":"images/post-headers/webperf.png"},{"title":"Making API calls a seamless user experience","link":"https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html","pubDate":"Wed, 21 Jul 2021 00:00:00 +0200","authors":{"author":[{"name":["Paweł Wolak"],"photo":["https://blog.allegro.tech/img/authors/pawel.wolak.jpg"],"url":["https://blog.allegro.tech/authors/pawel.wolak"]}]},"content":"<p>Almost every modern web application somehow interacts with a backend - be it loading data, doing background sync, submitting a form, or publishing the metrics.\nMaking API requests is not an easy task - we have to consider multiple outcomes and handle them properly. Otherwise, we might end up with confused users and\ndecreased conversion. Although the stakes are high, it is still very likely to encounter an application designed with only a happy path scenario in\nmind. The question is - how can we improve it?</p>\n\n<h3 id=\"make-the-request-state-visible\">Make the request state visible</h3>\n<p>Back in the old days submitting a form would result in a full page reload. Until the page was ready, there was a clear sign that something was happening. If\nsomething went wrong typically there was an unstyled generic error message.</p>\n\n<p>This approach served its purpose very well - it was easy to tell that the page was still loading and it was easy to tell when there was an error. Then\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/Guide/AJAX\">AJAX</a> became popular, bringing with its benefits also certain drawbacks - it was up to the\nprogrammer to handle loading and error state indicators, which were often omitted. To prevent user’s confusion, you should always remember about\nclearly presenting the request state to the user.</p>\n\n<h3 id=\"retry-failed-requests\">Retry failed requests</h3>\n<p>As mentioned above, errors do happen sometimes. Usually, users are faced with a message and an option to retry the request. This approach is far better than\nfailing silently and not informing the user, but still can lead to abandoned actions. Maybe we can do better than that?</p>\n\n<p>What if instead of asking the user to retry the failed request, we could do it automatically? There is another follow-up question: is every request worth\nretrying? Imagine a server responding with status code <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\">403 (forbidden)</a>,\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422\">422 (unprocessable entity)</a>, or 4xx in general. These are the client’s faults and usually\nretrying those requests will yield the same result. Now let’s consider 5xx status codes, which are often caused by temporary database unavailability or server\nresource exhaustion. Especially in multi-server environments, chances are that the next request will be rerouted to a healthy instance, resulting in a\nsuccessful response.</p>\n\n<p>On the other hand, in case of increased traffic, repeating failed requests instantly could possibly make things worse. In order to prevent that, it is a good\npractice to introduce an exponential delay between consecutive retry attempts. Another solution, more common in inter-service communication, is a circuit breaker\nmechanism which prevents further requests until service becomes available.</p>\n\n<p>Also, keep in mind that network conditions might not be stable, particularly on mobile devices. If there is no connection, instead of making pointless API\ncalls you could queue the requests and observe\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/NavigatorOnLine/Online_and_offline_events\">online/offline events</a>.</p>\n\n<p>Finally, not every request is safe to retry. Sometimes when you receive 5xx, there is no guarantee that it has not been processed. Imagine retrying a request\nto make a money transfer from one account to another - handling it twice would be a disaster! In order to prevent these mistakes from happening, you have to\nmake sure your API is <a href=\"https://developer.mozilla.org/en-US/docs/Glossary/Idempotent\">idempotent</a>. This is usually achieved by using adequate HTTP methods\n(like <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods/PUT\">PUT</a>) or passing an additional identifier with the request.</p>\n\n<h3 id=\"do-not-wait-forever\">Do not wait forever</h3>\n<p>Have you ever wondered how long it takes before an API call times out due to no response from the server? If not, I have bad news for you -\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest/timeout\">the default timeout value in XMLHttpRequest is 0</a>, which basically means the browser will wait forever.\nWith <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API\">fetch</a> there is no parameter responsible for timeouts - it relies on browser defaults (which\nis 300 seconds for <a href=\"https://source.chromium.org/chromium/chromium/src/+/master:net/socket/client_socket_pool.cc;l=29\">Chrome</a> and\n<a href=\"https://searchfox.org/mozilla-central/source/netwerk/protocol/http/nsHttpHandler.cpp#219\">Firefox</a>.</p>\n\n<p>The good news is that you can actually implement timeout functionality using\n<a href=\"https://developer.mozilla.org/en-US/docs/Web/API/AbortController\">AbortController</a>:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">const</span> <span class=\"nx\">controller</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">AbortController</span><span class=\"p\">();</span>\n<span class=\"kd\">const</span> <span class=\"nx\">timeout</span> <span class=\"o\">=</span> <span class=\"nx\">setTimeout</span><span class=\"p\">(()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">controller</span><span class=\"p\">.</span><span class=\"nx\">abort</span><span class=\"p\">(),</span> <span class=\"mi\">3000</span><span class=\"p\">);</span> <span class=\"c1\">// 3s timeout</span>\n\n<span class=\"nx\">fetch</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">/api/something</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"na\">signal</span><span class=\"p\">:</span> <span class=\"nx\">controller</span><span class=\"p\">.</span><span class=\"nx\">signal</span> <span class=\"p\">})</span>\n  <span class=\"p\">.</span><span class=\"nx\">then</span><span class=\"p\">(</span><span class=\"nx\">response</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n    <span class=\"nx\">clearTimeout</span><span class=\"p\">(</span><span class=\"nx\">timeout</span><span class=\"p\">);</span>\n    <span class=\"c1\">// process the response</span>\n  <span class=\"p\">});</span>\n</code></pre></div></div>\n\n<p>Another useful application of AbortController is cancelling straggler requests (requests still in progress at a point when the response is no longer needed).\nThis is particularly useful in libraries like <a href=\"https://reactjs.org/\">React</a> where receiving a response after unmounting a component results in an error:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">useEffect</span><span class=\"p\">(()</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n  <span class=\"kd\">const</span> <span class=\"nx\">controller</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">AbortController</span><span class=\"p\">();</span>\n\n  <span class=\"nx\">fetch</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">/api/something</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"na\">signal</span><span class=\"p\">:</span> <span class=\"nx\">controller</span><span class=\"p\">.</span><span class=\"nx\">signal</span> <span class=\"p\">})</span>\n    <span class=\"p\">.</span><span class=\"nx\">then</span><span class=\"p\">(</span><span class=\"nx\">response</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n      <span class=\"c1\">// process the response</span>\n    <span class=\"p\">});</span>\n\n  <span class=\"k\">return</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nx\">controller</span><span class=\"p\">.</span><span class=\"nx\">abort</span><span class=\"p\">();</span> <span class=\"c1\">// cancel the request when un-mounting the component</span>\n<span class=\"p\">},</span> <span class=\"p\">[]);</span>\n</code></pre></div></div>\n\n<h3 id=\"be-optimistic\">Be optimistic</h3>\n<p>As funny as it may sound, there is an actual pattern called “optimistic UI”. The idea behind it is very straightforward: given that most of the time an API\nrequest will result in a successful response, we can skip the “loading” part and go straight to a result stage. In the unlikely event of failure, we can still\nrollback our changes and inform the user about the error.</p>\n\n<p>Let’s consider a popular example of a counter (eg. Facebook/Twitter like button). For the sake of simplicity I will skip the actual request and return a\npromise after 1 second so that approximately 25% of requests will fail:</p>\n\n<div class=\"language-html highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Counter value: <span class=\"nt\">&lt;span</span> <span class=\"na\">id=</span><span class=\"s\">\"counter\"</span><span class=\"nt\">&gt;</span>0<span class=\"nt\">&lt;/span&gt;</span> <span class=\"nt\">&lt;button</span> <span class=\"na\">id=</span><span class=\"s\">\"button\"</span><span class=\"nt\">&gt;</span>Increase<span class=\"nt\">&lt;/button&gt;</span>\n\n<span class=\"nt\">&lt;script&gt;</span>\n  <span class=\"kd\">let</span> <span class=\"nx\">counter</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n\n  <span class=\"kd\">const</span> <span class=\"nx\">updateView</span> <span class=\"o\">=</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"nb\">document</span><span class=\"p\">.</span><span class=\"nx\">querySelector</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">#counter</span><span class=\"dl\">\"</span><span class=\"p\">).</span><span class=\"nx\">innerText</span> <span class=\"o\">=</span> <span class=\"nx\">counter</span><span class=\"p\">;</span>\n\n  <span class=\"kd\">const</span> <span class=\"nx\">makeRequest</span> <span class=\"o\">=</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"k\">new</span> <span class=\"nb\">Promise</span><span class=\"p\">((</span><span class=\"nx\">resolve</span><span class=\"p\">,</span> <span class=\"nx\">reject</span><span class=\"p\">)</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n    <span class=\"kd\">const</span> <span class=\"nx\">outcome</span> <span class=\"o\">=</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">random</span><span class=\"p\">()</span> <span class=\"o\">&gt;</span> <span class=\"mf\">0.25</span> <span class=\"p\">?</span> <span class=\"nx\">resolve</span> <span class=\"p\">:</span> <span class=\"nx\">reject</span><span class=\"p\">;</span>\n    <span class=\"nx\">setTimeout</span><span class=\"p\">(</span><span class=\"nx\">outcome</span><span class=\"p\">,</span> <span class=\"mi\">1000</span><span class=\"p\">);</span>\n  <span class=\"p\">});</span>\n\n  <span class=\"nb\">document</span><span class=\"p\">.</span><span class=\"nx\">querySelector</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">#button</span><span class=\"dl\">\"</span><span class=\"p\">).</span><span class=\"nx\">addEventListener</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">click</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"k\">async</span> <span class=\"p\">()</span> <span class=\"o\">=&gt;</span> <span class=\"p\">{</span>\n    <span class=\"nx\">counter</span><span class=\"o\">++</span><span class=\"p\">;</span>\n    <span class=\"nx\">updateView</span><span class=\"p\">();</span>\n\n    <span class=\"k\">try</span> <span class=\"p\">{</span>\n      <span class=\"k\">await</span> <span class=\"nx\">makeRequest</span><span class=\"p\">();</span>\n    <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"nx\">counter</span><span class=\"o\">--</span><span class=\"p\">;</span>\n      <span class=\"nx\">updateView</span><span class=\"p\">();</span>\n    <span class=\"p\">}</span>\n  <span class=\"p\">});</span>\n<span class=\"nt\">&lt;/script&gt;</span>\n</code></pre></div></div>\n\n<p>By simply assuming the positive outcome we drastically reduce the amount of time spent on the interactions, making the whole experience more swift. However,\ndue to the risk of so-called <a href=\"https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_positive_error\">false positives</a> this pattern should not\nbe applied to critical actions (for example, making a reservation). If you are further interested I recommend reading a\n<a href=\"https://www.smashingmagazine.com/2016/11/true-lies-of-optimistic-user-interfaces/\">more comprehensive article about optimistic UI</a>.</p>\n\n<h3 id=\"summary\">Summary</h3>\n<p>In this article, I presented the basics of requests handling. Some of these ideas (like retries and timeouts) also apply to the backend service-to-service\ncommunication. Additionally, there are other interesting techniques which I encourage you to study (eg. preconnecting, batching). In the end, in my opinion,\neverything that improves UX is worth a try.</p>\n","contentSnippet":"Almost every modern web application somehow interacts with a backend - be it loading data, doing background sync, submitting a form, or publishing the metrics.\nMaking API requests is not an easy task - we have to consider multiple outcomes and handle them properly. Otherwise, we might end up with confused users and\ndecreased conversion. Although the stakes are high, it is still very likely to encounter an application designed with only a happy path scenario in\nmind. The question is - how can we improve it?\nMake the request state visible\nBack in the old days submitting a form would result in a full page reload. Until the page was ready, there was a clear sign that something was happening. If\nsomething went wrong typically there was an unstyled generic error message.\nThis approach served its purpose very well - it was easy to tell that the page was still loading and it was easy to tell when there was an error. Then\nAJAX became popular, bringing with its benefits also certain drawbacks - it was up to the\nprogrammer to handle loading and error state indicators, which were often omitted. To prevent user’s confusion, you should always remember about\nclearly presenting the request state to the user.\nRetry failed requests\nAs mentioned above, errors do happen sometimes. Usually, users are faced with a message and an option to retry the request. This approach is far better than\nfailing silently and not informing the user, but still can lead to abandoned actions. Maybe we can do better than that?\nWhat if instead of asking the user to retry the failed request, we could do it automatically? There is another follow-up question: is every request worth\nretrying? Imagine a server responding with status code 403 (forbidden),\n422 (unprocessable entity), or 4xx in general. These are the client’s faults and usually\nretrying those requests will yield the same result. Now let’s consider 5xx status codes, which are often caused by temporary database unavailability or server\nresource exhaustion. Especially in multi-server environments, chances are that the next request will be rerouted to a healthy instance, resulting in a\nsuccessful response.\nOn the other hand, in case of increased traffic, repeating failed requests instantly could possibly make things worse. In order to prevent that, it is a good\npractice to introduce an exponential delay between consecutive retry attempts. Another solution, more common in inter-service communication, is a circuit breaker\nmechanism which prevents further requests until service becomes available.\nAlso, keep in mind that network conditions might not be stable, particularly on mobile devices. If there is no connection, instead of making pointless API\ncalls you could queue the requests and observe\nonline/offline events.\nFinally, not every request is safe to retry. Sometimes when you receive 5xx, there is no guarantee that it has not been processed. Imagine retrying a request\nto make a money transfer from one account to another - handling it twice would be a disaster! In order to prevent these mistakes from happening, you have to\nmake sure your API is idempotent. This is usually achieved by using adequate HTTP methods\n(like PUT) or passing an additional identifier with the request.\nDo not wait forever\nHave you ever wondered how long it takes before an API call times out due to no response from the server? If not, I have bad news for you -\nthe default timeout value in XMLHttpRequest is 0, which basically means the browser will wait forever.\nWith fetch there is no parameter responsible for timeouts - it relies on browser defaults (which\nis 300 seconds for Chrome and\nFirefox.\nThe good news is that you can actually implement timeout functionality using\nAbortController:\n\nconst controller = new AbortController();\nconst timeout = setTimeout(() => controller.abort(), 3000); // 3s timeout\n\nfetch(\"/api/something\", { signal: controller.signal })\n  .then(response => {\n    clearTimeout(timeout);\n    // process the response\n  });\n\n\nAnother useful application of AbortController is cancelling straggler requests (requests still in progress at a point when the response is no longer needed).\nThis is particularly useful in libraries like React where receiving a response after unmounting a component results in an error:\n\nuseEffect(() => {\n  const controller = new AbortController();\n\n  fetch(\"/api/something\", { signal: controller.signal })\n    .then(response => {\n      // process the response\n    });\n\n  return () => controller.abort(); // cancel the request when un-mounting the component\n}, []);\n\n\nBe optimistic\nAs funny as it may sound, there is an actual pattern called “optimistic UI”. The idea behind it is very straightforward: given that most of the time an API\nrequest will result in a successful response, we can skip the “loading” part and go straight to a result stage. In the unlikely event of failure, we can still\nrollback our changes and inform the user about the error.\nLet’s consider a popular example of a counter (eg. Facebook/Twitter like button). For the sake of simplicity I will skip the actual request and return a\npromise after 1 second so that approximately 25% of requests will fail:\n\nCounter value: <span id=\"counter\">0</span> <button id=\"button\">Increase</button>\n\n<script>\n  let counter = 0;\n\n  const updateView = () => document.querySelector(\"#counter\").innerText = counter;\n\n  const makeRequest = () => new Promise((resolve, reject) => {\n    const outcome = Math.random() > 0.25 ? resolve : reject;\n    setTimeout(outcome, 1000);\n  });\n\n  document.querySelector(\"#button\").addEventListener(\"click\", async () => {\n    counter++;\n    updateView();\n\n    try {\n      await makeRequest();\n    } catch (e) {\n      counter--;\n      updateView();\n    }\n  });\n</script>\n\n\nBy simply assuming the positive outcome we drastically reduce the amount of time spent on the interactions, making the whole experience more swift. However,\ndue to the risk of so-called false positives this pattern should not\nbe applied to critical actions (for example, making a reservation). If you are further interested I recommend reading a\nmore comprehensive article about optimistic UI.\nSummary\nIn this article, I presented the basics of requests handling. Some of these ideas (like retries and timeouts) also apply to the backend service-to-service\ncommunication. Additionally, there are other interesting techniques which I encourage you to study (eg. preconnecting, batching). In the end, in my opinion,\neverything that improves UX is worth a try.","guid":"https://blog.allegro.tech/2021/07/making-api-calls-seamless-ux.html","categories":["javascript","frontend","ajax","restapi","http"],"isoDate":"2021-07-20T22:00:00.000Z","thumbnail":"images/post-headers/javascript.png"}],"jobs":[{"id":"743999771050127","name":"Senior Data Analyst (Delivery Experience)","uuid":"c972d145-569b-467c-a2ef-36c601626f1b","refNumber":"REF2388L","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-06T07:45:57.000Z","location":{"city":"Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572804","label":"IT - Analytics & Consulting"},"function":{"id":"analyst","label":"Analyst"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"743f6067-ce19-4a83-9a0d-10d49cd63004","valueLabel":"Delivery Experience"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572804","valueLabel":"IT - Analytics & Consulting"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"fa9e2c82-b44f-40df-a699-4dea59cb0c13","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Data analyst, SQL, analityk danych"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999771050127","creator":{"name":"Ada Latańska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999770710280","name":"Software Tester - Allegro Ads","uuid":"1a712ecd-cbb4-4ade-8397-53d4668e6632","refNumber":"REF2786Q","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-03T12:05:55.000Z","location":{"city":"Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572855","label":"IT - Quality Assurance"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572855","valueLabel":"IT - Quality Assurance"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999770710280","language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999770681225","name":"Front-end Software Engineer - Allegro Ads","uuid":"f78f39e5-2671-4d73-ad81-befd7eb5dcc7","refNumber":"REF3006D","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-03T09:16:10.000Z","location":{"city":"Kraków, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"javascript, typescript, react"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999770681225","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999770680972","name":"Software Engineer (Big Data team) - Allegro Ads","uuid":"7a622a2b-5877-4dc3-8d5c-e015f431db70","refNumber":"REF2774P","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-03T09:13:53.000Z","location":{"city":"Kraków, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Java, Kotlin, Groovy, Scala, Python, Spring, Reactive Programming, Spark, Hadoop, Mesos, TensorFlow"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999770680972","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999770678937","name":"Software Engineer (Java/Kotlin) - Allegro Ads","uuid":"06f7157f-0c03-4501-8738-6cc98b5ca44e","refNumber":"REF2578F","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-03T09:02:52.000Z","location":{"city":"Kraków, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, kotlin, scala"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999770678937","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1629120828000,"duration":93600000,"id":"280149404","name":"Allegro Tech Meeting 2021","date_in_series_pattern":false,"status":"upcoming","time":1632927600000,"local_date":"2021-09-29","local_time":"17:00","updated":1629798872000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":79,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/280149404/","description":"❗Uwaga, aby wziąć udział w wydarzeniu zarejestruj się tutaj: https://app.evenea.pl/event/atm-2021/ ❗ Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to…","how_to_find_us":"https://www.youtube.com/c/AllegroTechBlog","visibility":"public","member_pay_fee":false},{"created":1623957759000,"duration":7200000,"id":"278903176","name":"Allegro Tech Live #20: Wydajność Backendu","date_in_series_pattern":false,"status":"past","time":1624982400000,"local_date":"2021-06-29","local_time":"18:00","updated":1624994207000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":125,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278903176/","description":"Allegro Tech Live w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my zagościmy…","how_to_find_us":"https://youtu.be/VklKR_fO5OI","visibility":"public","member_pay_fee":false},{"created":1621842668000,"duration":100800000,"id":"278374635","name":"UX Research Confetti","date_in_series_pattern":false,"status":"past","time":1624456800000,"local_date":"2021-06-23","local_time":"16:00","updated":1624563213000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278374635/","description":"🎉 Niech rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy… Dlatego łączymy siły z ekspertami z…","visibility":"public","member_pay_fee":false},{"created":1622474681000,"duration":5400000,"id":"278528964","name":"Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?","date_in_series_pattern":false,"status":"past","time":1623340800000,"local_date":"2021-06-10","local_time":"18:00","updated":1623349290000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":52,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278528964/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":" https://www.youtube.com/watch?v=8sLX0ExSq7E","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}