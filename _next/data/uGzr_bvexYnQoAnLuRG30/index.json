{"pageProps":{"posts":[{"title":"How to ruin your Elasticsearch performance — Part I: Know your enemy","link":"https://blog.allegro.tech/2021/09/how-to-ruin-elasticsearch-performance-part-i.html","pubDate":"Thu, 30 Sep 2021 00:00:00 +0200","authors":{"author":[{"name":["Michał Kosmulski"],"photo":["https://blog.allegro.tech/img/authors/michal.kosmulski.jpg"],"url":["https://blog.allegro.tech/authors/michal.kosmulski"]}]},"content":"<p>It’s easy to find resources about <em>improving</em> <a href=\"https://www.elastic.co/elastic-stack\">Elasticsearch</a> performance, but what if you wanted to <em>reduce</em> it?\nThis is Part I of a two-post series, and will present some ES internals. In Part II we’ll deduce from them a collection of select tips which can help you ruin\nyour ES performance in no time. Most should also be applicable to <a href=\"https://solr.apache.org/\">Solr</a>, raw <a href=\"https://lucene.apache.org/\">Lucene</a>, or,\nfor that matter, to any other full-text search engine as well.</p>\n\n<p>Surprisingly, a number of people seem to have discovered these tactics already, and you may even find some of them used in your own production code.</p>\n\n<h2 id=\"know-your-enemy-know-your-battlefield\">Know your enemy, know your battlefield</h2>\n\n<p>In order to deal a truly devastating blow to Elastic’s performance, we first need to understand what goes on under the hood. Since full-text search is\na complex topic, consider this introduction both simplified and incomplete.</p>\n\n<h2 id=\"index-and-document-contents\">Index and document contents</h2>\n\n<p>In most full-text search engines data is split into two separate areas: the index, which makes it possible to find documents (represented by some sort of document ID)\nmatching specified criteria, and <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/mapping-store.html\">document storage</a> which makes it possible\nto retrieve the contents (values of all fields) of a document with specified ID.\nThis distinction improves performance, since usually document IDs will appear multiple times in the index, and it would not make much sense to duplicate all\ndocument contents. IDs can also fit into fixed-width fields which makes managing certain data structures easier. This separation also enables further\nspace savings: it is possible to specify that certain fields will never be searched, and therefore do not need to be in the index, while others might never\nneed to be returned in search results and thus can be omitted from document storage.</p>\n\n<p>For certain operations it may be necessary to <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/6.8/doc-values.html\">store field values within the index itself</a>,\nwhich is yet another approach.</p>\n\n<h2 id=\"inverted-index\">Inverted index</h2>\n\n<p>The basic data structure full-text search uses is the <a href=\"https://en.wikipedia.org/wiki/Inverted_index\">inverted index</a>. Basically, it is a map from keywords\nto sorted lists of document IDs, so-called postings lists. The specific data structures used to implement this mapping are many, but are not relevant here.\nWhat matters is that for a single-word query this index can find matching documents very fast: it actually contains a ready-to-use answer. The same\nstructure can, of course, be used not only for words: in a numeric index, for example, we may have a ready-to-use list with IDs of documents containing\nthe value 123 in a specific field.</p>\n\n<p><img src=\"/img/articles/2021-09-30-how-to-ruin-elasticsearch-performance/postings-lists.webp\" alt=\"Postings lists — lists of document IDs containing each individual word\" /></p>\n\n<h2 id=\"indexing\">Indexing</h2>\n\n<p>The mechanism for finding all documents containing a single word, described above, is very neat, but it can be so fast and simple only because\nthere is a ready answer for our query in the index. However, in order for it to end up in there, ES needs to perform a rather complex operation called <em>indexing</em>.\nWe won’t get into the details here, but suffice to say this process is both complex when it comes to the logic it implements, and resource-intensive, since\nit requires information about all documents to be gathered in a single place.</p>\n\n<p>This has far-reaching consequences. Adding a new document, which may contain hundreds or thousands of words, to the index, would mean that hundreds or thousands\nof postings lists would have to be updated. This would be prohibitively expensive in terms of performance. Therefore, full-text search engines usually employ\na different strategy: once built, an index is effectively immutable. When documents are added, removed, or modified, a new tiny index containing just the changes\nis created. At query time, results from the main and the incremental indices are merged. Any number of these incremental indices, called <a href=\"https://lucene.apache.org/core/8_9_0/core/org/apache/lucene/codecs/lucene87/package-summary.html#Segments\">segments</a> in\nElastic jargon, can be created, but the cost of merging results at search time grows quickly with their number. Therefore, a special process of segment merging\nmust be present in order to ensure that the number of segments (and thus also search latency) does not get out of control. This, obviously, further increases\ncomplexity of the whole system.</p>\n\n<h2 id=\"operations-on-postings-lists\">Operations on postings lists</h2>\n\n<p>So far we talked about the relatively simple case of searching for documents matching a single search term. But what if we wanted to find documents\ncontaining multiple search terms? This is where Elastic needs to combine several postings lists into a single one. Interestingly, the same issue arises\neven for a single search term if your index has multiple segments.</p>\n\n<p>A postings list represents a set of document IDs, and most ways of matching documents to search terms correspond to boolean operations on those sets.\nFor example, finding documents which contain both <code class=\"language-plaintext highlighter-rouge\">term1</code> and <code class=\"language-plaintext highlighter-rouge\">term2</code> corresponds to the logical operation <code class=\"language-plaintext highlighter-rouge\">set1 AND set2</code> (intersection of sets) where\n<code class=\"language-plaintext highlighter-rouge\">set1</code> and <code class=\"language-plaintext highlighter-rouge\">set2</code> are the sets of documents matching individual search terms. Likewise, finding documents containing any word out of several corresponds\nto the logical <code class=\"language-plaintext highlighter-rouge\">OR</code> operation (sum of sets) and documents which contain one term, but do not contain another, correspond to <code class=\"language-plaintext highlighter-rouge\">AND NOT</code> operator (difference\nof sets).</p>\n\n<p>There are many ways these operations can be implemented in practice, with search engines using lots of optimizations. However, some constraints on the\ncomplexity remain. Let’s take a look at one possible implementation and see what conclusions can be drawn.</p>\n\n<p>While the operations described below can be generalized to work on multiple lists at once, for simplicity we’ll just discuss operations which are binary,\ni.e. which take two arguments. Conclusions remain the same for higher arity.</p>\n\n<p>In the algorithms below, we’ll assume each postings list is an actual list of integers (doc IDs), sorted in ascending order, and that their sizes\nare <em>n</em> and <em>m</em>.</p>\n\n<h3 id=\"or-operator\">OR</h3>\n\n<p><img src=\"/img/articles/2021-09-30-how-to-ruin-elasticsearch-performance/list-merging-or.webp\" alt=\"Example algorithm for computing results of OR operation\" /></p>\n\n<p>The way to merge two sorted lists in an OR operation is straightforward (and it is also the reason for the lists to be sorted in the first place).\nFor each list we need to keep a pointer which will indicate the current position. Both pointers start at the beginning of their corresponding lists.\nIn each step we compare the values (integer IDs) indicated by the pointers, and add the smaller one to the result list. Then, we move that pointer forward. If the values\nare equal, we add the value to the result just once and move both pointers. When one pointer reaches the end of its list, we copy the remainder of the second\nlist to the end of result list, and we’re done. Like each of the input lists, the result is a sorted list without duplicates.</p>\n\n<p>If you are familiar with <a href=\"https://en.wikipedia.org/wiki/Merge_sort\">merge sort</a>, you will notice that this algorithm corresponds to its merge phase.</p>\n\n<p>Since the result is a sum of the two sets, its size is at least <em>max(m, n)</em> (in the case one set is a subset of the other) and at most\n<em>m + n</em> (in the case the two sets are disjoint). Due to the fact that the cursor has to go through all entries in each of the lists, the\nalgorithm’s complexity is O(n+m). Even for any other algorithm, since the size of the result list may reach <em>m + n</em>, and we have to generate that list,\ncomplexity of O(n+m) is expected.</p>\n\n<p>The result does not depend on the order of lists (OR operation is symmetric), and performance is not (much) affected by it, either.</p>\n\n<h3 id=\"and-and-and-not\">AND and AND NOT</h3>\n\n<p><img src=\"/img/articles/2021-09-30-how-to-ruin-elasticsearch-performance/list-merging-and.webp\" alt=\"Example algorithm for computing results of AND / AND NOT operations\" /></p>\n\n<p>Calculating the intersection of two sets (what corresponds to the logical AND operator) or their difference (AND NOT) are very similar operations.\nJust as when calculating the sum of sets, we need to maintain two pointers, one for each list. In each step of the iteration we look at the current value\nin the first list and then try to find that value in the second list, starting from the second list’s pointer’s position. If we find the value, we add it\nto the result list, and move the second list’s pointer to the corresponding position. If the value can’t be found, we advance the pointer to the first\nitem after which the searched-for value would be. Once the second list’s pointer reaches the end, we are done.</p>\n\n<p>The algorithmic complexity of these two operations differs quite a bit from that of OR operation. First of all, a new sub-operation of searching for a value\nin the second list is used. It can be implemented in a number of ways depending on the data structures (flat array, skip list, various trees, etc.). For a simple\nsorted list stored in an array, we could use binary search whose complexity is O(log(<em>m</em>)). Things get a little more complicated since we only search starting\nfrom current position rather than from the beginning, but let’s skim over this for now. What matters is that we perform a search in the second list\nfor each item from the first one: the complexity is no longer symmetric between the two lists. If we change the order of the lists, the result of AND operation\ndoes not change, but the cost of performing the calculation does.\nIt pays to have the shorter of the two list as the first in order, and the difference can be huge. The cost also depends very much on the data, i.e. how\nbig the intersection of the two lists is. If the intersection is empty, even looking for the first list’s first element in the second list will move the second\nlist’s pointer to the end, and the algorithm will finish very quickly. The upper limit on the size of the result list (which in turn puts a lower limit on\nalgorithmic complexity) is <em>min(m, n)</em>.</p>\n\n<p>If we’re performing the AND NOT rather than AND operation, the match condition has to be reversed from <em>found</em> to <em>not found</em>. While the result changes when\nwe exchange the two lists, algorithmic properties are the same as for AND, especially the asymmetry in how the first and second list’s size affects the\ncomputational cost.</p>\n\n<p>In order to improve performance, many search engines will automatically change the order in which operations are evaluated if it does not alter the end result.\nThis is an example of <em>query rewriting</em>. In particular, Lucene (and thus also Elasticsearch and Solr)\n<a href=\"https://github.com/apache/lucene/blob/5e0e7a5479bca798ccfe385629a0ca2ba5870bc0/lucene/core/src/java/org/apache/lucene/search/ConjunctionDISI.java#L153\">reorder lists passed to AND operator</a>\nso that they are sorted by length in ascending order.</p>\n\n<h2 id=\"beyond-the-basics\">Beyond the basics</h2>\n\n<p>There are lots of factors which affect Elasticsearch performance and can be exploited in order to make that performance worse, and which I haven’t mentioned\nhere. These include scoring, phrase search, run-time scripting, sharding and replication, hardware, and disk vs memory access just to name a few.\nThere are also lots of minor quirks which are too numerous to list here. Given that you can\n<a href=\"https://www.elastic.co/guide/en/elasticsearch/plugins/6.8/plugin-authors.html\">extend ES with custom plugins</a> that can execute arbitrary code, the\nopportunities for breaking things are endless.</p>\n\n<p>On the other hand, search engines employ <a href=\"https://www.elastic.co/blog/elasticsearch-query-execution-order\">a number of optimizations</a> which may counter\nsome of our efforts at achieving low performance.</p>\n\n<p>Then again, some of these optimizations may themselves lead to surprising results. For example, often there are two different ways of performing a task,\nand a heuristic is used to choose one or the other. Such heuristics are often simple threshold values: for example, if the number of sub-clauses in a\nquery is above 16, <a href=\"https://github.com/apache/lucene/blob/d5d6dc079395c47cd6d12dcce3bcfdd2c7d9dc63/lucene/core/src/java/org/apache/lucene/search/BooleanWeight.java#L358\">it will not be cached</a>.\nLikewise, certain pieces of data may be represented as lists and then suddenly switch to a bitset representation.\nSuch behaviors may be confusing since they make performance analysis more difficult.</p>\n\n<p>Anyway, even the basic knowledge presented above should allow you to deal some heavy damage to your search performance, so let’s get started.</p>\n\n<h2 id=\"summary\">Summary</h2>\n<p>I hope the first part of this post gave you some background on how Elastic works under the hood. In Part II, we’ll look at how to apply this knowledge in\npractice to making Elasticsearch performance as bad as possible.</p>\n","contentSnippet":"It’s easy to find resources about improving Elasticsearch performance, but what if you wanted to reduce it?\nThis is Part I of a two-post series, and will present some ES internals. In Part II we’ll deduce from them a collection of select tips which can help you ruin\nyour ES performance in no time. Most should also be applicable to Solr, raw Lucene, or,\nfor that matter, to any other full-text search engine as well.\nSurprisingly, a number of people seem to have discovered these tactics already, and you may even find some of them used in your own production code.\nKnow your enemy, know your battlefield\nIn order to deal a truly devastating blow to Elastic’s performance, we first need to understand what goes on under the hood. Since full-text search is\na complex topic, consider this introduction both simplified and incomplete.\nIndex and document contents\nIn most full-text search engines data is split into two separate areas: the index, which makes it possible to find documents (represented by some sort of document ID)\nmatching specified criteria, and document storage which makes it possible\nto retrieve the contents (values of all fields) of a document with specified ID.\nThis distinction improves performance, since usually document IDs will appear multiple times in the index, and it would not make much sense to duplicate all\ndocument contents. IDs can also fit into fixed-width fields which makes managing certain data structures easier. This separation also enables further\nspace savings: it is possible to specify that certain fields will never be searched, and therefore do not need to be in the index, while others might never\nneed to be returned in search results and thus can be omitted from document storage.\nFor certain operations it may be necessary to store field values within the index itself,\nwhich is yet another approach.\nInverted index\nThe basic data structure full-text search uses is the inverted index. Basically, it is a map from keywords\nto sorted lists of document IDs, so-called postings lists. The specific data structures used to implement this mapping are many, but are not relevant here.\nWhat matters is that for a single-word query this index can find matching documents very fast: it actually contains a ready-to-use answer. The same\nstructure can, of course, be used not only for words: in a numeric index, for example, we may have a ready-to-use list with IDs of documents containing\nthe value 123 in a specific field.\n\nIndexing\nThe mechanism for finding all documents containing a single word, described above, is very neat, but it can be so fast and simple only because\nthere is a ready answer for our query in the index. However, in order for it to end up in there, ES needs to perform a rather complex operation called indexing.\nWe won’t get into the details here, but suffice to say this process is both complex when it comes to the logic it implements, and resource-intensive, since\nit requires information about all documents to be gathered in a single place.\nThis has far-reaching consequences. Adding a new document, which may contain hundreds or thousands of words, to the index, would mean that hundreds or thousands\nof postings lists would have to be updated. This would be prohibitively expensive in terms of performance. Therefore, full-text search engines usually employ\na different strategy: once built, an index is effectively immutable. When documents are added, removed, or modified, a new tiny index containing just the changes\nis created. At query time, results from the main and the incremental indices are merged. Any number of these incremental indices, called segments in\nElastic jargon, can be created, but the cost of merging results at search time grows quickly with their number. Therefore, a special process of segment merging\nmust be present in order to ensure that the number of segments (and thus also search latency) does not get out of control. This, obviously, further increases\ncomplexity of the whole system.\nOperations on postings lists\nSo far we talked about the relatively simple case of searching for documents matching a single search term. But what if we wanted to find documents\ncontaining multiple search terms? This is where Elastic needs to combine several postings lists into a single one. Interestingly, the same issue arises\neven for a single search term if your index has multiple segments.\nA postings list represents a set of document IDs, and most ways of matching documents to search terms correspond to boolean operations on those sets.\nFor example, finding documents which contain both term1 and term2 corresponds to the logical operation set1 AND set2 (intersection of sets) where\nset1 and set2 are the sets of documents matching individual search terms. Likewise, finding documents containing any word out of several corresponds\nto the logical OR operation (sum of sets) and documents which contain one term, but do not contain another, correspond to AND NOT operator (difference\nof sets).\nThere are many ways these operations can be implemented in practice, with search engines using lots of optimizations. However, some constraints on the\ncomplexity remain. Let’s take a look at one possible implementation and see what conclusions can be drawn.\nWhile the operations described below can be generalized to work on multiple lists at once, for simplicity we’ll just discuss operations which are binary,\ni.e. which take two arguments. Conclusions remain the same for higher arity.\nIn the algorithms below, we’ll assume each postings list is an actual list of integers (doc IDs), sorted in ascending order, and that their sizes\nare n and m.\nOR\n\nThe way to merge two sorted lists in an OR operation is straightforward (and it is also the reason for the lists to be sorted in the first place).\nFor each list we need to keep a pointer which will indicate the current position. Both pointers start at the beginning of their corresponding lists.\nIn each step we compare the values (integer IDs) indicated by the pointers, and add the smaller one to the result list. Then, we move that pointer forward. If the values\nare equal, we add the value to the result just once and move both pointers. When one pointer reaches the end of its list, we copy the remainder of the second\nlist to the end of result list, and we’re done. Like each of the input lists, the result is a sorted list without duplicates.\nIf you are familiar with merge sort, you will notice that this algorithm corresponds to its merge phase.\nSince the result is a sum of the two sets, its size is at least max(m, n) (in the case one set is a subset of the other) and at most\nm + n (in the case the two sets are disjoint). Due to the fact that the cursor has to go through all entries in each of the lists, the\nalgorithm’s complexity is O(n+m). Even for any other algorithm, since the size of the result list may reach m + n, and we have to generate that list,\ncomplexity of O(n+m) is expected.\nThe result does not depend on the order of lists (OR operation is symmetric), and performance is not (much) affected by it, either.\nAND and AND NOT\n\nCalculating the intersection of two sets (what corresponds to the logical AND operator) or their difference (AND NOT) are very similar operations.\nJust as when calculating the sum of sets, we need to maintain two pointers, one for each list. In each step of the iteration we look at the current value\nin the first list and then try to find that value in the second list, starting from the second list’s pointer’s position. If we find the value, we add it\nto the result list, and move the second list’s pointer to the corresponding position. If the value can’t be found, we advance the pointer to the first\nitem after which the searched-for value would be. Once the second list’s pointer reaches the end, we are done.\nThe algorithmic complexity of these two operations differs quite a bit from that of OR operation. First of all, a new sub-operation of searching for a value\nin the second list is used. It can be implemented in a number of ways depending on the data structures (flat array, skip list, various trees, etc.). For a simple\nsorted list stored in an array, we could use binary search whose complexity is O(log(m)). Things get a little more complicated since we only search starting\nfrom current position rather than from the beginning, but let’s skim over this for now. What matters is that we perform a search in the second list\nfor each item from the first one: the complexity is no longer symmetric between the two lists. If we change the order of the lists, the result of AND operation\ndoes not change, but the cost of performing the calculation does.\nIt pays to have the shorter of the two list as the first in order, and the difference can be huge. The cost also depends very much on the data, i.e. how\nbig the intersection of the two lists is. If the intersection is empty, even looking for the first list’s first element in the second list will move the second\nlist’s pointer to the end, and the algorithm will finish very quickly. The upper limit on the size of the result list (which in turn puts a lower limit on\nalgorithmic complexity) is min(m, n).\nIf we’re performing the AND NOT rather than AND operation, the match condition has to be reversed from found to not found. While the result changes when\nwe exchange the two lists, algorithmic properties are the same as for AND, especially the asymmetry in how the first and second list’s size affects the\ncomputational cost.\nIn order to improve performance, many search engines will automatically change the order in which operations are evaluated if it does not alter the end result.\nThis is an example of query rewriting. In particular, Lucene (and thus also Elasticsearch and Solr)\nreorder lists passed to AND operator\nso that they are sorted by length in ascending order.\nBeyond the basics\nThere are lots of factors which affect Elasticsearch performance and can be exploited in order to make that performance worse, and which I haven’t mentioned\nhere. These include scoring, phrase search, run-time scripting, sharding and replication, hardware, and disk vs memory access just to name a few.\nThere are also lots of minor quirks which are too numerous to list here. Given that you can\nextend ES with custom plugins that can execute arbitrary code, the\nopportunities for breaking things are endless.\nOn the other hand, search engines employ a number of optimizations which may counter\nsome of our efforts at achieving low performance.\nThen again, some of these optimizations may themselves lead to surprising results. For example, often there are two different ways of performing a task,\nand a heuristic is used to choose one or the other. Such heuristics are often simple threshold values: for example, if the number of sub-clauses in a\nquery is above 16, it will not be cached.\nLikewise, certain pieces of data may be represented as lists and then suddenly switch to a bitset representation.\nSuch behaviors may be confusing since they make performance analysis more difficult.\nAnyway, even the basic knowledge presented above should allow you to deal some heavy damage to your search performance, so let’s get started.\nSummary\nI hope the first part of this post gave you some background on how Elastic works under the hood. In Part II, we’ll look at how to apply this knowledge in\npractice to making Elasticsearch performance as bad as possible.","guid":"https://blog.allegro.tech/2021/09/how-to-ruin-elasticsearch-performance-part-i.html","categories":["tech","full-text search","elasticsearch","elastic","es","performance"],"isoDate":"2021-09-29T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Evolution of web performance culture","link":"https://blog.allegro.tech/2021/09/evolution-of-web-performance-culture.html","pubDate":"Thu, 23 Sep 2021 00:00:00 +0200","authors":{"author":[{"name":["Jerzy Jelinek"],"photo":["https://blog.allegro.tech/img/authors/jerzy.jelinek.jpg"],"url":["https://blog.allegro.tech/authors/jerzy.jelinek"]}]},"content":"<p>The main goal of boosting website performance is to improve the user experience. In theory,\na satisfied customer is more likely to use a particular company’s services, which is then reflected in business results.\nHowever, from my own experience I can say that not every change can be easily converted into money.\nI would like to tell you how to reconcile these two worlds, how to convince the business that the benefits of\nbetter performance are a long-term investment, and how to streamline the development process during the design or code writing process.</p>\n\n<p>Web performance is a challenging and complex subject. It involves working at the intersection of content management,\nfrontend, backend and the network layer. Rarely does a single change make a dramatic difference in performance,\nonly the cumulative effort of small improvements in each of these areas produces noticeable results.</p>\n\n<p>As the team responsible for the performance of Allegro, we are responsible for implementing various optimizations,\nbut most of all we show other teams which modifications in their projects will positively affect the performance\nof the whole site. Our duty is to create the friendliest, performance-supporting work environment for developers\nand help non-technical people to understand the idea behind it.</p>\n\n<p>We would like to illustrate by our example how this can be achieved.</p>\n\n<h2 id=\"stage-one--measuring-the-web-performance\">Stage One — Measuring The Web Performance</h2>\n\n<p>At Allegro, we want to know if and how a given functionality affects the user as well as our business metrics.\nIn order to prove our hypothesis about performance impact we had to prepare a whole mechanism that allows us\nto track and analyze performance changes. I described it in greater detail in my first article titled\n<a href=\"/2021/06/measuring-web-performance.html\">Measuring The Web Performance</a>.</p>\n\n<p>Only then we could start optimizing our pages. Our actions brought the expected effect — we made progress,\nbut the pace was not sufficient. We lacked the support of business people, who would see profit in all of this.</p>\n\n<h2 id=\"stage-two--make-it-clear-that-performance-is-important\">Stage Two — Make it clear that performance is important</h2>\n\n<p>We have gone to great lengths to make the entire organization realize that the gains from web performance are long-term and important overall.</p>\n\n<p>Everyone subconsciously knows that a faster site improves user experience. The earlier users are able to see a page and use it,\nthe more often they are likely to do so and return. However, we had a problem proving that it truly makes our company earn more,\nin the end this is what every business is about.</p>\n\n<p>The first milestone turned out to be a test conducted together with the SEO team. It was an A/B test,\nwhere some users got an optimized offers list page that was loading faster, and the rest got the original page.\nIt turned out that all examined pages were added to Google cache (previously there were only a few of them),\nthe number of clicks, average ranking position and number of views increased from a few to several percent\nand the expected business profit from such a change was at 13% of the current GMV originating from organic traffic.\nAlthough being only a proof of concept, this experiment turned out to be an enabler for our next steps.\nIt helped us understand better what we were aiming for.</p>\n\n<p>This experiment has opened the way for us to make more optimizations, it also provided us with a solid argument\nthat convinced other product teams. However, we still felt unsatisfied — if performance affects\nGMV indirectly through SEO then are we able to prove a direct correlation as well? We had plenty of data,\nbut we lacked the analytical expertise to process it, therefore, we asked our business analysts to help us.\nBased on historical performance* and business data, they were able to confirm the impact on business metrics!</p>\n\n<blockquote>\n  <p>Each 100ms slowdown of First Input Delay results in an average drop in GMV of 3%.<br />\nEach 100ms slowdown of First Contentful Paint results in an average drop in GMV of 1.5%.<br />\nEach 100ms slowdown of Largest Contentful Paint results in an average drop in GMV of 0.5%.</p>\n</blockquote>\n\n<p>*The data comes from our real user measurements, not from synthetic tests.</p>\n\n<h2 id=\"stage-three--support-for-both-business-and-developers\">Stage Three — Support for both business and developers</h2>\n\n<p>After confirming our hypothesis, we had to implement a number of measures to ensure that web performance\nis taken into consideration throughout the entire process of delivery for all functionalities.</p>\n\n<h3 id=\"teaching\">Teaching</h3>\n\n<p>One of the main tasks of my team is to prepare the awareness campaign of our colleagues.\nTherefore, we periodically conduct two types of training:</p>\n\n<ul>\n  <li>For engineers, where we focus on the technical part including how to use the available tools to write optimal code.</li>\n  <li>For other employees (especially Product Managers), where we explain why performance is important and what benefits it brings.</li>\n</ul>\n\n<p>We assume that knowledge should be shared, that is why we describe each interesting case, experiment or bug fix on our internal blog.\nThanks to that we mitigate the risk that bad patterns will be repeated in the future.</p>\n\n<p>However, the best way to learn is to work with us directly, so we encourage everyone to visit us as part of the\n<a href=\"/2019/09/team-tourism-at-allegro.html\">team tourism</a> initiative.</p>\n\n<h3 id=\"supporting-technical-teams\">Supporting technical teams</h3>\n\n<p>In our team we believe that we should automate every task possible. We want our tools to support\nthe work of our engineers, so that they don’t have to remember to run performance tests,\ncheck the size of the resulting files, etc. That is why we have prepared several checks that apply to components development\nin our <a href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">Micro Frontends architecture</a>.</p>\n\n<h4 id=\"automatic-page-scanner\">Automatic page scanner</h4>\n\n<p>That’s where the whole automation story started. To detect problems with assets we had to check each page manually.\nThis was neither convenient nor scalable, so we created our first bot, which used PageSpeed Insights to check if:</p>\n\n<ul>\n  <li>assets are cached long enough,</li>\n  <li>their sizes on the page are appropriate,</li>\n  <li>there are any assets which are not minified,</li>\n  <li>images are in the right format and size,</li>\n  <li>some images should be loaded lazily.</li>\n</ul>\n\n<p>After detecting problems, we checked the owners of the asset or part of the page and notified them on Slack.</p>\n\n<p><img src=\"/img/articles/2021-09-23-evolution-of-web-performance-culture/brylant-bot.png\" alt=\"Brylant Bot\" title=\"Brylant Bot\" /></p>\n\n<h4 id=\"automatic-comments-in-pull-requests\">Automatic comments in pull requests</h4>\n\n<p>Two comments are generated while building the component. The first one presents a comparison of assets size\nwith the target branch and the estimated cost of the change.</p>\n\n<p><img src=\"/img/articles/2021-09-23-evolution-of-web-performance-culture/gh-sizes.png\" alt=\"Asset size comparison\" title=\"Asset size comparison\" /></p>\n\n<p>If the size exceeds the declared threshold, our entire team is automatically added as reviewers to the pull request.</p>\n\n<p>Additionally, to detect the culprit faster, a\n<a href=\"https://github.com/webpack-contrib/webpack-bundle-analyzer\">Webpack Bundle Analyzer</a> report is generated.</p>\n\n<p>In the second one, Lighthouse reports for target and feature branches are compared in order to catch performance metrics’ regressions at this early stage.\nEach component has a list of predefined presets (input data) and a server that displays them.\nThis functionality is used for: development, visual regression, snapshot and performance testing.\nLighthouse report is generated for one or more predefined states every time the component is built.</p>\n\n<p><img src=\"/img/articles/2021-09-23-evolution-of-web-performance-culture/lighthouse-report.png\" alt=\"Lighthouse report\" title=\"Lighthouse report\" /></p>\n\n<h4 id=\"automatic-notifications\">Automatic notifications</h4>\n\n<p>My team is notified on Slack every time a new dependency is added to any of the components.\nWe want to make sure that the libraries used are optimal and have no better (smaller, faster) replacements.</p>\n\n<p><img src=\"/img/articles/2021-09-23-evolution-of-web-performance-culture/bot-deps.png\" alt=\"Dependencies notification\" title=\"Dependencies notification\" /></p>\n\n<p>We get similar notifications when assets size changes by at least 5% compared to the target branch.\nWe want to make sure that, for example, treeshaking hasn’t broken down or some other change affecting the size has not occurred.</p>\n\n<p><img src=\"/img/articles/2021-09-23-evolution-of-web-performance-culture/bot-sizes.png\" alt=\"Size notification\" title=\"Size notification\" /></p>\n\n<h4 id=\"eslint\">ESlint</h4>\n\n<p>We use it not only for formatting but also for finding violations in our code using custom rules.\nWe have created several rules to support engineers in their daily work. You can read about a sample implementation\nin the post “<a href=\"/2020/08/using-eslint.html\">Using ESLint to improve your app’s performance</a>” on our blog.</p>\n\n<h4 id=\"analyses\">Analyses</h4>\n\n<p>We get requests from other teams to analyze their components or sites. Sometimes they lack the time budget\nfor such analysis, but would like to know what to improve.</p>\n\n<h3 id=\"research-and-development\">Research and development</h3>\n\n<p>Keep in mind that working on web performance is a continuous work. Every now and then new solutions,\nwhich may have a positive impact on the loading speed, appear on the market. Therefore, together with our team,\nwe run a series of tests to see if it makes sense to adopt a given solution.</p>\n\n<h3 id=\"appreciation-culture\">Appreciation culture</h3>\n\n<p>We believe that the carrot is better than the stick, so we praise other teams for the achievements\nrelated to improving performance. Some time ago we used to write it down in the form of a newsletter,\nnow we talk about it during our sprint summaries.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>We are constantly working on data collection, monitoring, awareness raising, optimization and research,\nwhich leads to a situation where more and more managers come to us for consultation.\nThey know that performance is important and needs to be taken care of. Allegro is constantly evolving,\nnew content and features are being created, without working on performance the site will slow down.\nHowever, we already have a whole arsenal of capabilities to help us deal with this.\nWe are no longer fighting alone as the Webperf team, but as an entire organization.</p>\n","contentSnippet":"The main goal of boosting website performance is to improve the user experience. In theory,\na satisfied customer is more likely to use a particular company’s services, which is then reflected in business results.\nHowever, from my own experience I can say that not every change can be easily converted into money.\nI would like to tell you how to reconcile these two worlds, how to convince the business that the benefits of\nbetter performance are a long-term investment, and how to streamline the development process during the design or code writing process.\nWeb performance is a challenging and complex subject. It involves working at the intersection of content management,\nfrontend, backend and the network layer. Rarely does a single change make a dramatic difference in performance,\nonly the cumulative effort of small improvements in each of these areas produces noticeable results.\nAs the team responsible for the performance of Allegro, we are responsible for implementing various optimizations,\nbut most of all we show other teams which modifications in their projects will positively affect the performance\nof the whole site. Our duty is to create the friendliest, performance-supporting work environment for developers\nand help non-technical people to understand the idea behind it.\nWe would like to illustrate by our example how this can be achieved.\nStage One — Measuring The Web Performance\nAt Allegro, we want to know if and how a given functionality affects the user as well as our business metrics.\nIn order to prove our hypothesis about performance impact we had to prepare a whole mechanism that allows us\nto track and analyze performance changes. I described it in greater detail in my first article titled\nMeasuring The Web Performance.\nOnly then we could start optimizing our pages. Our actions brought the expected effect — we made progress,\nbut the pace was not sufficient. We lacked the support of business people, who would see profit in all of this.\nStage Two — Make it clear that performance is important\nWe have gone to great lengths to make the entire organization realize that the gains from web performance are long-term and important overall.\nEveryone subconsciously knows that a faster site improves user experience. The earlier users are able to see a page and use it,\nthe more often they are likely to do so and return. However, we had a problem proving that it truly makes our company earn more,\nin the end this is what every business is about.\nThe first milestone turned out to be a test conducted together with the SEO team. It was an A/B test,\nwhere some users got an optimized offers list page that was loading faster, and the rest got the original page.\nIt turned out that all examined pages were added to Google cache (previously there were only a few of them),\nthe number of clicks, average ranking position and number of views increased from a few to several percent\nand the expected business profit from such a change was at 13% of the current GMV originating from organic traffic.\nAlthough being only a proof of concept, this experiment turned out to be an enabler for our next steps.\nIt helped us understand better what we were aiming for.\nThis experiment has opened the way for us to make more optimizations, it also provided us with a solid argument\nthat convinced other product teams. However, we still felt unsatisfied — if performance affects\nGMV indirectly through SEO then are we able to prove a direct correlation as well? We had plenty of data,\nbut we lacked the analytical expertise to process it, therefore, we asked our business analysts to help us.\nBased on historical performance* and business data, they were able to confirm the impact on business metrics!\nEach 100ms slowdown of First Input Delay results in an average drop in GMV of 3%.\n*The data comes from our real user measurements, not from synthetic tests.\nStage Three — Support for both business and developers\nAfter confirming our hypothesis, we had to implement a number of measures to ensure that web performance\nis taken into consideration throughout the entire process of delivery for all functionalities.\nTeaching\nOne of the main tasks of my team is to prepare the awareness campaign of our colleagues.\nTherefore, we periodically conduct two types of training:\nFor engineers, where we focus on the technical part including how to use the available tools to write optimal code.\nFor other employees (especially Product Managers), where we explain why performance is important and what benefits it brings.\nWe assume that knowledge should be shared, that is why we describe each interesting case, experiment or bug fix on our internal blog.\nThanks to that we mitigate the risk that bad patterns will be repeated in the future.\nHowever, the best way to learn is to work with us directly, so we encourage everyone to visit us as part of the\nteam tourism initiative.\nSupporting technical teams\nIn our team we believe that we should automate every task possible. We want our tools to support\nthe work of our engineers, so that they don’t have to remember to run performance tests,\ncheck the size of the resulting files, etc. That is why we have prepared several checks that apply to components development\nin our Micro Frontends architecture.\nAutomatic page scanner\nThat’s where the whole automation story started. To detect problems with assets we had to check each page manually.\nThis was neither convenient nor scalable, so we created our first bot, which used PageSpeed Insights to check if:\nassets are cached long enough,\ntheir sizes on the page are appropriate,\nthere are any assets which are not minified,\nimages are in the right format and size,\nsome images should be loaded lazily.\nAfter detecting problems, we checked the owners of the asset or part of the page and notified them on Slack.\n\nAutomatic comments in pull requests\nTwo comments are generated while building the component. The first one presents a comparison of assets size\nwith the target branch and the estimated cost of the change.\n\nIf the size exceeds the declared threshold, our entire team is automatically added as reviewers to the pull request.\nAdditionally, to detect the culprit faster, a\nWebpack Bundle Analyzer report is generated.\nIn the second one, Lighthouse reports for target and feature branches are compared in order to catch performance metrics’ regressions at this early stage.\nEach component has a list of predefined presets (input data) and a server that displays them.\nThis functionality is used for: development, visual regression, snapshot and performance testing.\nLighthouse report is generated for one or more predefined states every time the component is built.\n\nAutomatic notifications\nMy team is notified on Slack every time a new dependency is added to any of the components.\nWe want to make sure that the libraries used are optimal and have no better (smaller, faster) replacements.\n\nWe get similar notifications when assets size changes by at least 5% compared to the target branch.\nWe want to make sure that, for example, treeshaking hasn’t broken down or some other change affecting the size has not occurred.\n\nESlint\nWe use it not only for formatting but also for finding violations in our code using custom rules.\nWe have created several rules to support engineers in their daily work. You can read about a sample implementation\nin the post “Using ESLint to improve your app’s performance” on our blog.\nAnalyses\nWe get requests from other teams to analyze their components or sites. Sometimes they lack the time budget\nfor such analysis, but would like to know what to improve.\nResearch and development\nKeep in mind that working on web performance is a continuous work. Every now and then new solutions,\nwhich may have a positive impact on the loading speed, appear on the market. Therefore, together with our team,\nwe run a series of tests to see if it makes sense to adopt a given solution.\nAppreciation culture\nWe believe that the carrot is better than the stick, so we praise other teams for the achievements\nrelated to improving performance. Some time ago we used to write it down in the form of a newsletter,\nnow we talk about it during our sprint summaries.\nSummary\nWe are constantly working on data collection, monitoring, awareness raising, optimization and research,\nwhich leads to a situation where more and more managers come to us for consultation.\nThey know that performance is important and needs to be taken care of. Allegro is constantly evolving,\nnew content and features are being created, without working on performance the site will slow down.\nHowever, we already have a whole arsenal of capabilities to help us deal with this.\nWe are no longer fighting alone as the Webperf team, but as an entire organization.","guid":"https://blog.allegro.tech/2021/09/evolution-of-web-performance-culture.html","categories":["tech","webperf","frontend","performance","perfmatters","javascript"],"isoDate":"2021-09-22T22:00:00.000Z","thumbnail":"images/post-headers/javascript.png"},{"title":"How to turn on TypeScript strict mode in specific files","link":"https://blog.allegro.tech/2021/09/How-to-turn-on-TypeScript-strict-mode-in-specific-files.html","pubDate":"Mon, 06 Sep 2021 00:00:00 +0200","authors":{"author":[{"name":["Kamil Krysiak"],"photo":["https://blog.allegro.tech/img/authors/kamil.krysiak.jpg"],"url":["https://blog.allegro.tech/authors/kamil.krysiak"]},{"name":["Jarosław Glegoła"],"photo":["https://blog.allegro.tech/img/authors/jaroslaw.glegola.jpg"],"url":["https://blog.allegro.tech/authors/jaroslaw.glegola"]}]},"content":"<p>Imagine you have to migrate your JavaScript project to TypeScript. It’s fairly simple to convert one file from JS to TS, but if\nyou want to take type checking to the next level (going for TypeScript’s strict mode) it is not that easy. The only solution you\nhave is turning on strict mode for the whole project which may result in thousands of errors. For most projects that are not strict yet,\nit would take quite a bit of time and effort to fix all the strict errors at once.</p>\n\n<h2 id=\"turning-strict-mode-on-in-development-only\">Turning strict-mode on in development only?</h2>\n\n<p>You could think of turning on strict mode during development, catching strict errors that way, and then turning it off before\npushing your changes, but this approach has a few downsides.</p>\n\n<ol>\n  <li>You’ve got to remember to change <code class=\"language-plaintext highlighter-rouge\">tsconfig.json</code> every time you make changes — without automation, this could get tedious.</li>\n  <li>It won’t work in your CI pipeline</li>\n  <li>It will show errors in files you don’t want to make strict yet</li>\n</ol>\n\n<p>Ok, so what can we do to improve this workflow?</p>\n\n<h2 id=\"introducing-typescript-strict-plugin\">Introducing typescript-strict-plugin</h2>\n\n<p><a href=\"https://github.com/allegro/typescript-strict-plugin\">typescript-strict-plugin</a> eliminates all the above problems by allowing you to specify exactly what files you want to be strictly\nchecked. You can do that by simply putting a single comment at the top of the file and typescript will strictly check it. Now\nevery member of your team will have strict errors shown to them in the editor of their choosing (yes, this plugin works with\nwebstorm, vscode, vim, and more).</p>\n\n<p>Unfortunately, typescript plugins do not work at compilation time, they work only in IDEs. Another nice feature that comes in the\npackage is a compile-time tool that allows you to connect the strict plugin to your CI pipeline, or a pre-commit hook. It checks\nmarked files with strict mode and prints to the console all strict errors found. If a single strict error is found, the tool\nexits with an error, so you can be sure that all specified files are really strict (strict, strict, strict… ahh).</p>\n\n<h2 id=\"how-to-use-it\">How to use it?</h2>\n\n<h3 id=\"install-the-typescript-strict-plugin-package\">Install the <code class=\"language-plaintext highlighter-rouge\">typescript-strict-plugin</code> package</h3>\n\n<p>with npm:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>npm i <span class=\"nt\">-D</span> typescript-strict-plugin\n</code></pre></div></div>\n\n<p>or yarn:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>yarn add <span class=\"nt\">-D</span> typescript-strict-plugin\n</code></pre></div></div>\n\n<h3 id=\"add-the-plugin-to-your-tsconfigjson\">Add the plugin to your <code class=\"language-plaintext highlighter-rouge\">tsconfig.json</code></h3>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"compilerOptions\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"err\">//...</span><span class=\"w\">\n    </span><span class=\"nl\">\"strict\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"plugins\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n      </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"typescript-strict-plugin\"</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">]</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<h3 id=\"mark-strict-files-with-ts-strict-comment\">Mark strict files with <code class=\"language-plaintext highlighter-rouge\">//@ts-strict</code> comment</h3>\n\n<p>Before:</p>\n\n<div class=\"language-typescript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">const</span> <span class=\"nx\">name</span><span class=\"p\">:</span> <span class=\"kr\">string</span> <span class=\"o\">=</span> <span class=\"kc\">null</span><span class=\"p\">;</span> <span class=\"c1\">// no error here</span>\n</code></pre></div></div>\n\n<p>After:</p>\n\n<div class=\"language-typescript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// @ts-strict</span>\n<span class=\"p\">...</span>\n<span class=\"kd\">const</span> <span class=\"nx\">name</span><span class=\"p\">:</span> <span class=\"kr\">string</span> <span class=\"o\">=</span> <span class=\"kc\">null</span><span class=\"p\">;</span> <span class=\"c1\">// TS2322: Type ‘null’ is not assignable to type ‘string’.</span>\n</code></pre></div></div>\n\n<p>You can also directly specify directories you want to be strict. In the following example, every file in <code class=\"language-plaintext highlighter-rouge\">src</code> and <code class=\"language-plaintext highlighter-rouge\">test</code>\ndirectories will be strictly checked.</p>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"compilerOptions\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"err\">//...</span><span class=\"w\">\n    </span><span class=\"nl\">\"strict\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n    </span><span class=\"nl\">\"plugins\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"w\">\n      </span><span class=\"p\">{</span><span class=\"w\">\n        </span><span class=\"nl\">\"name\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"typescript-strict-plugin\"</span><span class=\"p\">,</span><span class=\"w\">\n        </span><span class=\"nl\">\"path\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">[</span><span class=\"s2\">\"./src\"</span><span class=\"p\">,</span><span class=\"w\"> </span><span class=\"s2\">\"./test\"</span><span class=\"p\">]</span><span class=\"w\">\n      </span><span class=\"p\">}</span><span class=\"w\">\n    </span><span class=\"p\">]</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<h3 id=\"add-tsc-strict-to-your-type-checking-packagejson-scripts\">Add <code class=\"language-plaintext highlighter-rouge\">tsc-strict</code> to your type checking package.json scripts</h3>\n\n<div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"err\">//</span><span class=\"w\"> </span><span class=\"err\">package.json</span><span class=\"w\">\n</span><span class=\"p\">{</span><span class=\"w\">\n  </span><span class=\"nl\">\"scripts\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n    </span><span class=\"nl\">\"typecheck\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"tsc &amp;&amp; tsc-strict\"</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div></div>\n\n<p>Otherwise, the plugin will not work outside your IDE.</p>\n\n<p><strong>Note:</strong> <code class=\"language-plaintext highlighter-rouge\">tsc-strict</code> script uses TypeScript’s <code class=\"language-plaintext highlighter-rouge\">tsc</code> under the hood, so the full type checking time in this scenario would double.</p>\n\n<h2 id=\"conclusion\">Conclusion</h2>\n\n<p><code class=\"language-plaintext highlighter-rouge\">typescript-strict-plugin</code> can improve your app’s reliability and type safety. And all that without any disadvantages except for\ncompilation time and a few comments.</p>\n\n<p>If you’re interested in how this works under the hood, we are working on a separate post on making your own TS plugin, so stay\ntuned!</p>\n","contentSnippet":"Imagine you have to migrate your JavaScript project to TypeScript. It’s fairly simple to convert one file from JS to TS, but if\nyou want to take type checking to the next level (going for TypeScript’s strict mode) it is not that easy. The only solution you\nhave is turning on strict mode for the whole project which may result in thousands of errors. For most projects that are not strict yet,\nit would take quite a bit of time and effort to fix all the strict errors at once.\nTurning strict-mode on in development only?\nYou could think of turning on strict mode during development, catching strict errors that way, and then turning it off before\npushing your changes, but this approach has a few downsides.\nYou’ve got to remember to change tsconfig.json every time you make changes — without automation, this could get tedious.\nIt won’t work in your CI pipeline\nIt will show errors in files you don’t want to make strict yet\nOk, so what can we do to improve this workflow?\nIntroducing typescript-strict-plugin\ntypescript-strict-plugin eliminates all the above problems by allowing you to specify exactly what files you want to be strictly\nchecked. You can do that by simply putting a single comment at the top of the file and typescript will strictly check it. Now\nevery member of your team will have strict errors shown to them in the editor of their choosing (yes, this plugin works with\nwebstorm, vscode, vim, and more).\nUnfortunately, typescript plugins do not work at compilation time, they work only in IDEs. Another nice feature that comes in the\npackage is a compile-time tool that allows you to connect the strict plugin to your CI pipeline, or a pre-commit hook. It checks\nmarked files with strict mode and prints to the console all strict errors found. If a single strict error is found, the tool\nexits with an error, so you can be sure that all specified files are really strict (strict, strict, strict… ahh).\nHow to use it?\nInstall the typescript-strict-plugin package\nwith npm:\n\nnpm i -D typescript-strict-plugin\n\n\nor yarn:\n\nyarn add -D typescript-strict-plugin\n\n\nAdd the plugin to your tsconfig.json\n\n{\n  \"compilerOptions\": {\n    //...\n    \"strict\": false,\n    \"plugins\": [\n      {\n        \"name\": \"typescript-strict-plugin\"\n      }\n    ]\n  }\n}\n\n\nMark strict files with //@ts-strict comment\nBefore:\n\nconst name: string = null; // no error here\n\n\nAfter:\n\n// @ts-strict\n...\nconst name: string = null; // TS2322: Type ‘null’ is not assignable to type ‘string’.\n\n\nYou can also directly specify directories you want to be strict. In the following example, every file in src and test\ndirectories will be strictly checked.\n\n{\n  \"compilerOptions\": {\n    //...\n    \"strict\": false,\n    \"plugins\": [\n      {\n        \"name\": \"typescript-strict-plugin\",\n        \"path\": [\"./src\", \"./test\"]\n      }\n    ]\n  }\n}\n\n\nAdd tsc-strict to your type checking package.json scripts\n\n// package.json\n{\n  \"scripts\": {\n    \"typecheck\": \"tsc && tsc-strict\"\n  }\n}\n\n\nOtherwise, the plugin will not work outside your IDE.\nNote: tsc-strict script uses TypeScript’s tsc under the hood, so the full type checking time in this scenario would double.\nConclusion\ntypescript-strict-plugin can improve your app’s reliability and type safety. And all that without any disadvantages except for\ncompilation time and a few comments.\nIf you’re interested in how this works under the hood, we are working on a separate post on making your own TS plugin, so stay\ntuned!","guid":"https://blog.allegro.tech/2021/09/How-to-turn-on-TypeScript-strict-mode-in-specific-files.html","categories":["typescript","scrict mode","typescript plugin","code quality"],"isoDate":"2021-09-05T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Splitting data that does not fit on one machine using data partitioning","link":"https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html","pubDate":"Tue, 10 Aug 2021 00:00:00 +0200","authors":{"author":[{"name":["Tomasz Lelek"],"photo":["https://blog.allegro.tech/img/authors/tomasz.lelek.jpg"],"url":["https://blog.allegro.tech/authors/tomasz.lelek"]},{"name":["Jon Skeet"],"photo":["https://blog.allegro.tech/img/authors/jon.skeet.jpg"],"url":["https://blog.allegro.tech/authors/jon.skeet"]}]},"content":"<p>The following article is an excerpt from <a href=\"https://www.manning.com/books/software-mistakes-and-tradeoffs\">Software Mistakes and Trade-offs</a> book.\nIn real-world big data applications, the amount of data that we need to store and process can be often counted in the hundreds of terabytes or petabytes. It is not feasible to store such an amount of data on one physical node. We need a way to split that data into N data nodes.</p>\n\n<p>The technique for splitting the data is called data partitioning. There are a lot of techniques to partition your data.</p>\n\n<p>For online processing sources (like a database), you may pick some ID, for example, user-ID, and store a range of users on a dedicated node. For example, assuming that you have 1000 user IDs and 5 data nodes, the first node can store IDs from 0 to 200, the second node can store data from 201 to 400, and so on. When picking the partitioning scheme, you need to be careful not to introduce the data skew. Such a situation can occur when most of the data is produced by one or a group of IDs that belongs to the same data node. For example, let’s assume that the user ID 10 is responsible for 80% of our traffic and generates 80% of the data. Therefore, it will mean that 80% of the data is stored on the first data node, and our partitioning will not be optimal. In the worst case, this user’s amount of data may be too big to store on the given data node. It is important to note that for online processing, the partitioning is optimized for reading or writing data access patterns.</p>\n\n<h2 id=\"offline-big-data-partitioning\">Offline big data partitioning</h2>\n\n<p>We will focus now on the offline, big data processing partitioning.</p>\n\n<p>For Big Data systems, we often need to store the historical data (cold data) for an “indefinite” amount of time. It is crucial to store the data for as long as we can. When the data is produced, we may not be aware of the business value that it can bring in the future. For example, we may save all user’s request data with all the HTTP headers. When the data is saved, there may be no use case for these HTTP headers. In the future, however, we may decide to build a tool that profiles our users by the type of device (Android, iOS) that they use. Such information is propagated in the HTTP headers. We can execute our new profiling logic based on the historical data because we stored it in the raw data. It is important to note here that the data was not needed for a long period.</p>\n\n<p>On the other hand, we needed to store a lot of information and save it for later. Thus, our storage needs to contain a lot of data stored in cold storage. In Big Data applications, it often means that data is saved to a Hadoop distributed file system (HDFS). It also means that the data should be partitioned in a fairly generic way. We cannot optimize for read patterns because we cannot anticipate how those read patterns will look like.</p>\n\n<p>Because of these reasons, the most often used data partitioning scheme for big data offline processing is based on dates. Let’s assume that we have a system that saves user’s data on the /users file system path and clickstream data in the /clicks file system path. We will analyze the first data set that stores the user’s data. We are assuming that the number of records that we store is equal to 10 billion. We started collecting the data in the year 2017, and it’s been collected since then.</p>\n\n<p>The partitioning scheme that we pick is based on the date. It means that our partition identifier starts with the year. We will have 2017, 2018, 2019, and 2020 partitions. If we would have smaller data requirements, partitioning by year may be enough. In such a scenario, the file system path for our user’s data would be /users/2017, /users/2018, and so on. It will be analogical for clicks: /clicks/2017, /clicks/2018, and so on.</p>\n\n<p><img src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img1.png\" alt=\"Figure 1\" /></p>\n\n<h2 id=\"four-data-partitions\">Four data partitions</h2>\n\n<p>By using this partitioning, the user’s data will have 4 partitions. It means that we can split the data into up to four physical data nodes. The first node will store the data for the year 2017, the second node for 2018, etc. Nothing prevents us from keeping all of those partitions on the same physical node when having four partitions. We may be ok with storing the data on one physical node as long as we have enough disk space. Once the disk space runs out, we can create a new physical node and move some of the partitions to the new node.</p>\n\n<p>In practice, such a partitioning scheme is too coarse-grained. Having one big partition for all year’s data is hard from both a read and write perspective. When you read such data and are interested only in events from a particular date, you need to scan the whole year’s data. It’s very inefficient and time-consuming. It is also problematic from the writing perspective because if your disk space runs out, there is no easy way to split the data further. You won’t be able to perform a successful write.</p>\n\n<p>Because of that reason, offline big data systems tend to partition the data in a more fine-grained fashion. The data is partitioned by year, month, and even day. For example, if you are writing data for the 2nd of January 2020, you will save the event into a /users/2020/01/02 partition. Such a partitioning gives you a lot of flexibility at the read side as well. If you wish to analyze events for a specific day, you can directly read the data from the partition. If you want to perform some higher-level analysis, for example, analyze the whole month’s data, you can read all partitions within a given month. The same pattern applies if you want to analyze a whole year’s data.</p>\n\n<p>To sum up, our 10 billion records will be partitioned in the following way:</p>\n\n<p><img src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img2.png\" alt=\"Figure 2\" /></p>\n\n<h2 id=\"date-based-data-partitioning\">Date based data partitioning</h2>\n\n<p>You can see that the initial 10 billion records are partitioned into year/month, and finally, a specific date of the month. In the end, each day’s partition contains a hundred thousand records. Such an amount of data can easily fit into one machine disk space. It also means that we have 365/366 partitions per year. The upper number of data nodes on which we can partition the data is equal to the number of days * the number of years we store data. If your one-day data does not fit into one machine disk space, you can easily partition your data further by hours of the day, minutes, seconds, and so on.</p>\n\n<h2 id=\"partitioning-vs-sharding\">Partitioning vs sharding</h2>\n\n<p>Assuming that we have our data partitioned by the date, we can split that data into multiple nodes. In such a scenario, we are putting a subset of all partition keys in a physical node.</p>\n\n<p>Our user’s data is partitioned into N partitions (logical shards). Let’s assume that our partition granularity is a month. In that case, the data for the year 2020 has 12 partitions that can be split horizontally into N physical nodes (physical shards). It is important to note that N is less than or equal to 12. In other words, the maximum level of physical shards is 12. This architecture pattern is called sharding.</p>\n\n<p>Let’s assume that we have three physical nodes. In that case, we can say that our user’s data for the year 2020 is partitioned into 12 partitions. Next, they are assigned to 3 shards (nodes). Each of the nodes stores 4 partitions for 2020 (12 partitions / 3 nodes = 4 partitions/node).</p>\n\n<p><img src=\"/img/articles/2021-08-10-splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning/img1.png\" alt=\"Figure 8.6. Sharding\" /></p>\n\n<p>In our diagram, the physical shard is the same as the physical node. The partition keys (logical shards) are distributed evenly to physical shards. In case a new node is added to a cluster, each physical shard needs to re-assign one of its logical shards to a new physical node.</p>\n\n<p>There are a variety of algorithms for shards assignments. They also need to handle shards re-distribution in case of adding or removing a node (failure or scale down). This technique is used by most big data technologies and data stores such as HDFS, Cassandra, Kafka, Elastic, etc., and they vary depending on the implementation.</p>\n\n<p>At this point, we know the basics of data partitioning. Next, we need to understand how the partitioning algorithms work in depth. This is essential to understand if we want to reason about the big data tools we are using to get business value. This is discussed at length in the book.</p>\n\n<p>This is discussed at length in the book. You can purchase it with a special discount <a href=\"https://www.manning.com/books/software-mistakes-and-tradeoffs\">here</a> using the code <strong>allegrotech35</strong>.</p>\n","contentSnippet":"The following article is an excerpt from Software Mistakes and Trade-offs book.\nIn real-world big data applications, the amount of data that we need to store and process can be often counted in the hundreds of terabytes or petabytes. It is not feasible to store such an amount of data on one physical node. We need a way to split that data into N data nodes.\nThe technique for splitting the data is called data partitioning. There are a lot of techniques to partition your data.\nFor online processing sources (like a database), you may pick some ID, for example, user-ID, and store a range of users on a dedicated node. For example, assuming that you have 1000 user IDs and 5 data nodes, the first node can store IDs from 0 to 200, the second node can store data from 201 to 400, and so on. When picking the partitioning scheme, you need to be careful not to introduce the data skew. Such a situation can occur when most of the data is produced by one or a group of IDs that belongs to the same data node. For example, let’s assume that the user ID 10 is responsible for 80% of our traffic and generates 80% of the data. Therefore, it will mean that 80% of the data is stored on the first data node, and our partitioning will not be optimal. In the worst case, this user’s amount of data may be too big to store on the given data node. It is important to note that for online processing, the partitioning is optimized for reading or writing data access patterns.\nOffline big data partitioning\nWe will focus now on the offline, big data processing partitioning.\nFor Big Data systems, we often need to store the historical data (cold data) for an “indefinite” amount of time. It is crucial to store the data for as long as we can. When the data is produced, we may not be aware of the business value that it can bring in the future. For example, we may save all user’s request data with all the HTTP headers. When the data is saved, there may be no use case for these HTTP headers. In the future, however, we may decide to build a tool that profiles our users by the type of device (Android, iOS) that they use. Such information is propagated in the HTTP headers. We can execute our new profiling logic based on the historical data because we stored it in the raw data. It is important to note here that the data was not needed for a long period.\nOn the other hand, we needed to store a lot of information and save it for later. Thus, our storage needs to contain a lot of data stored in cold storage. In Big Data applications, it often means that data is saved to a Hadoop distributed file system (HDFS). It also means that the data should be partitioned in a fairly generic way. We cannot optimize for read patterns because we cannot anticipate how those read patterns will look like.\nBecause of these reasons, the most often used data partitioning scheme for big data offline processing is based on dates. Let’s assume that we have a system that saves user’s data on the /users file system path and clickstream data in the /clicks file system path. We will analyze the first data set that stores the user’s data. We are assuming that the number of records that we store is equal to 10 billion. We started collecting the data in the year 2017, and it’s been collected since then.\nThe partitioning scheme that we pick is based on the date. It means that our partition identifier starts with the year. We will have 2017, 2018, 2019, and 2020 partitions. If we would have smaller data requirements, partitioning by year may be enough. In such a scenario, the file system path for our user’s data would be /users/2017, /users/2018, and so on. It will be analogical for clicks: /clicks/2017, /clicks/2018, and so on.\n\nFour data partitions\nBy using this partitioning, the user’s data will have 4 partitions. It means that we can split the data into up to four physical data nodes. The first node will store the data for the year 2017, the second node for 2018, etc. Nothing prevents us from keeping all of those partitions on the same physical node when having four partitions. We may be ok with storing the data on one physical node as long as we have enough disk space. Once the disk space runs out, we can create a new physical node and move some of the partitions to the new node.\nIn practice, such a partitioning scheme is too coarse-grained. Having one big partition for all year’s data is hard from both a read and write perspective. When you read such data and are interested only in events from a particular date, you need to scan the whole year’s data. It’s very inefficient and time-consuming. It is also problematic from the writing perspective because if your disk space runs out, there is no easy way to split the data further. You won’t be able to perform a successful write.\nBecause of that reason, offline big data systems tend to partition the data in a more fine-grained fashion. The data is partitioned by year, month, and even day. For example, if you are writing data for the 2nd of January 2020, you will save the event into a /users/2020/01/02 partition. Such a partitioning gives you a lot of flexibility at the read side as well. If you wish to analyze events for a specific day, you can directly read the data from the partition. If you want to perform some higher-level analysis, for example, analyze the whole month’s data, you can read all partitions within a given month. The same pattern applies if you want to analyze a whole year’s data.\nTo sum up, our 10 billion records will be partitioned in the following way:\n\nDate based data partitioning\nYou can see that the initial 10 billion records are partitioned into year/month, and finally, a specific date of the month. In the end, each day’s partition contains a hundred thousand records. Such an amount of data can easily fit into one machine disk space. It also means that we have 365/366 partitions per year. The upper number of data nodes on which we can partition the data is equal to the number of days * the number of years we store data. If your one-day data does not fit into one machine disk space, you can easily partition your data further by hours of the day, minutes, seconds, and so on.\nPartitioning vs sharding\nAssuming that we have our data partitioned by the date, we can split that data into multiple nodes. In such a scenario, we are putting a subset of all partition keys in a physical node.\nOur user’s data is partitioned into N partitions (logical shards). Let’s assume that our partition granularity is a month. In that case, the data for the year 2020 has 12 partitions that can be split horizontally into N physical nodes (physical shards). It is important to note that N is less than or equal to 12. In other words, the maximum level of physical shards is 12. This architecture pattern is called sharding.\nLet’s assume that we have three physical nodes. In that case, we can say that our user’s data for the year 2020 is partitioned into 12 partitions. Next, they are assigned to 3 shards (nodes). Each of the nodes stores 4 partitions for 2020 (12 partitions / 3 nodes = 4 partitions/node).\n\nIn our diagram, the physical shard is the same as the physical node. The partition keys (logical shards) are distributed evenly to physical shards. In case a new node is added to a cluster, each physical shard needs to re-assign one of its logical shards to a new physical node.\nThere are a variety of algorithms for shards assignments. They also need to handle shards re-distribution in case of adding or removing a node (failure or scale down). This technique is used by most big data technologies and data stores such as HDFS, Cassandra, Kafka, Elastic, etc., and they vary depending on the implementation.\nAt this point, we know the basics of data partitioning. Next, we need to understand how the partitioning algorithms work in depth. This is essential to understand if we want to reason about the big data tools we are using to get business value. This is discussed at length in the book.\nThis is discussed at length in the book. You can purchase it with a special discount here using the code allegrotech35.","guid":"https://blog.allegro.tech/2021/08/splitting-data-that-does-not-fit-on-one-machine-using-data-partitioning.html","categories":["tech","performance","sharding","bigdata","dba","distributed"],"isoDate":"2021-08-09T22:00:00.000Z","thumbnail":"images/post-headers/bigdata.png"}],"jobs":[{"id":"743999776643004","name":"Senior Data Analyst (Marketing)","uuid":"5511938f-deb3-4be8-b0e9-2261b070d8ad","refNumber":"REF2945Y","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-30T12:29:11.000Z","location":{"city":"Poznań, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572804","label":"IT - Analytics & Consulting"},"function":{"id":"analyst","label":"Analyst"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"f44b7e2c-00ec-4407-892a-ee95add8bd9a","valueLabel":"Marketing"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572804","valueLabel":"IT - Analytics & Consulting"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999776643004","creator":{"name":"Ada Latańska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999775932192","name":"Business Application Administrator (Cognos)","uuid":"b1cd3655-7601-4909-8372-e6448cef7b3b","refNumber":"REF2689W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-28T09:13:29.000Z","location":{"city":"Poznań, Warszawa","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"1013275","label":"IT Business Services"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"1013275","valueLabel":"IT Business Services"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Cognos"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999775932192","creator":{"name":"Paulina Tynecka"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999775257988","name":"Senior Software Engineer (Java/Kotlin) - Allegro Ads","uuid":"2c4ace41-625d-48c5-97f0-4939eecd2d39","refNumber":"REF2578F","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-24T12:44:25.000Z","location":{"city":"Kraków, Warszawa, Poznań, Wrocław","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, kotlin, scala"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999775257988","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999775257738","name":"Mid Software Engineer (Java/Kotlin) - Allegro Ads","uuid":"5d155f3c-c760-4dcc-bdd4-f9a5ed54b74d","refNumber":"REF2578F","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-24T12:41:58.000Z","location":{"city":"Kraków, Warszawa, Poznań, Wrocław","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"612f4406198fc8471029176c","fieldLabel":"Initiative","valueId":"3a0d7379-96c9-4a23-95fe-db3c6653e44b","valueLabel":"not  appilcable"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, kotlin, scala"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999775257738","creator":{"name":"Jagoda Rusiniak"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999775249967","name":"Senior Software Engineer (Java/Kotlin) - Consumer Technology Experience","uuid":"c41ec7e9-cfd3-46c2-a327-3e143e36915d","refNumber":"REF3061W","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-09-24T11:55:35.000Z","location":{"city":"Warszawa, Poznań, Toruń, Wrocław, Kraków","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"java, kotlin"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999775249967","creator":{"name":"Aleksandra Sotowska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1629120828000,"duration":93600000,"id":"280149404","name":"Allegro Tech Meeting 2021","date_in_series_pattern":false,"status":"past","time":1632927600000,"local_date":"2021-09-29","local_time":"17:00","updated":1633024970000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":144,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/280149404/","description":"❗Uwaga, aby wziąć udział w wydarzeniu zarejestruj się tutaj: https://app.evenea.pl/event/atm-2021/ ❗ Allegro to jedna z najbardziej zaawansowanych technologicznie firm w naszej części Europy. Allegro to…","how_to_find_us":"https://www.youtube.com/c/AllegroTechBlog","visibility":"public","member_pay_fee":false},{"created":1623957759000,"duration":7200000,"id":"278903176","name":"Allegro Tech Live #20: Wydajność Backendu","date_in_series_pattern":false,"status":"past","time":1624982400000,"local_date":"2021-06-29","local_time":"18:00","updated":1624994207000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":125,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278903176/","description":"Allegro Tech Live w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my zagościmy…","how_to_find_us":"https://youtu.be/VklKR_fO5OI","visibility":"public","member_pay_fee":false},{"created":1621842668000,"duration":100800000,"id":"278374635","name":"UX Research Confetti","date_in_series_pattern":false,"status":"past","time":1624456800000,"local_date":"2021-06-23","local_time":"16:00","updated":1624563213000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278374635/","description":"🎉 Niech rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy… Dlatego łączymy siły z ekspertami z…","visibility":"public","member_pay_fee":false},{"created":1622474681000,"duration":5400000,"id":"278528964","name":"Allegro Tech Live Odcinek: #19   Co to znaczy być liderem i jak nim zostać?","date_in_series_pattern":false,"status":"past","time":1623340800000,"local_date":"2021-06-10","local_time":"18:00","updated":1623349290000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":52,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/278528964/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":" https://www.youtube.com/watch?v=8sLX0ExSq7E","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}