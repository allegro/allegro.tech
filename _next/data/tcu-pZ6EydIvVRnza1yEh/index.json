{"pageProps":{"posts":[{"title":"Transactions in MongoDB","link":"https://blog.allegro.tech/2022/12/transactions-in-mongodb.html","pubDate":"Wed, 21 Dec 2022 00:00:00 +0100","authors":{"author":[{"name":["Piotr Kisielewicz"],"photo":["https://blog.allegro.tech/img/authors/piotr.kisielewicz.jpg"],"url":["https://blog.allegro.tech/authors/piotr.kisielewicz"]}]},"content":"<p>Since version 4.0, transactions have been introduced to the world of Mongo databases. However, the way they work differs greatly from the tried and true world of SQL.</p>\n\n<p>In databases like PostgreSQL or MySQL there are transactions based on tables and relations with isolation levels such as <code class=\"language-plaintext highlighter-rouge\">READ_UNCOMMITED</code>, <code class=\"language-plaintext highlighter-rouge\">READ_COMMITED</code>, <code class=\"language-plaintext highlighter-rouge\">REPEATABLE_READ</code> and <code class=\"language-plaintext highlighter-rouge\">SERIALIZABLE</code>. These help us to recognize if the record we’re working on is dirty, both when we want to access it or when someone else wants to do it after we’ve made some changes. In the world of documents, shards and replicas, the changes made to a single document are already atomic by design.</p>\n\n<p>Why do we need transactions then? I only talked about single documents, but after all, documents can be modified in bulk, and these operations are no longer atomic.</p>\n\n<p>That’s why transactions were introduced into MongoDB. In this blog post, we will check how they work and what their use cases are.</p>\n\n<h3 id=\"what-is-a-transaction\">What is a transaction?</h3>\n\n<p>A database transaction is a unit of work, designed to handle the changes of data in the database. It makes sure that the output of the data is consistent and doesn’t generate errors. It helps with concurrent changes to the database, and makes the database more scalable.</p>\n\n<p>By definition, database transactions are atomic, consistent, isolated and durable. Sounds familiar? Exactly, it’s <strong>ACID</strong>.</p>\n\n<h3 id=\"does-mongodb-implement-acid\">Does MongoDB implement ACID?</h3>\n\n<p>Before implementing transactions? Not really. Of course, there are some properties of ACID that are present (as stated earlier), but before version 4.0 they were impossible to achieve in a distributed ecosystem.</p>\n\n<p>Here’s an excerpt from the MongoDB website: <a href=\"https://www.mongodb.com/basics/acid-transactions\">we estimate that 80%-90% of applications that leverage the document model will not need to utilize transactions in MongoDB</a>.</p>\n\n<p>Alas, when you need to make changes to multiple documents, or worse, if they are split across multiple shards, then there is no guarantee that the changes will adhere to ACID properties. That’s why the need for ACID transactions arose.</p>\n\n<p>Fortunately, since version 4.0 transactions were added to MongoDB, first for multi-document changes, and then, in 4.2, for distributed data.</p>\n\n<h3 id=\"specifics-of-mongodb-transactions\">Specifics of MongoDB transactions</h3>\n\n<p>Having in mind the differences between MongoDB and SQL DB engines, we need to first take a look at how transactions in Mongo are specified. Since the problems that arise from having multiple documents, replica sets and shards differ from the issues of concurrent access to a table row, resolutions to these problems are also different in nature.</p>\n\n<p>The transaction in MongoDB is denoted by two properties: <strong>ReadConcern</strong> and <strong>WriteConcern</strong>.</p>\n\n<p>A <strong>ReadConcern</strong> property is used to control the consistency and isolation of the data we read from the database.</p>\n\n<p>Similarly, the <strong>WriteConcern</strong> property is us defining when we consider the data we write to be consistent in the database.</p>\n\n<p>(Side note: when we are talking about majority we are talking about calculated majority. You can read more about it in <a href=\"https://www.mongodb.com/docs/manual/reference/write-concern/#std-label-calculating-majority-count\">Calculating majority count</a>)</p>\n\n<p>We distinguish between three levels of <code class=\"language-plaintext highlighter-rouge\">ReadConcern</code> for transactions (others are unavailable to use in this case):</p>\n\n<ul>\n  <li>local, which reads the latest data from a node that has been queried. There are no guarantees that the data read is the most recent across the system,</li>\n  <li>majority, which reads the data at the point of majority-commit. Said point is calculated by the primary node. This ReadConcern doesn’t guarantee consistency unless WriteConcern of at least majority is also stated,</li>\n  <li>snapshot, which reads from a snapshot of majority-commited data. This ReadConcern level is mainly only available in transactions (so we can’t use it in single-document reads save for some outliers) and provides its benefits mainly with sharded transactions as it guarantees that the data is synchronized across shards</li>\n</ul>\n\n<p>As we can see, we can draw some parallels between MongoDB’s ReadConcern options and the SQL world, where local would be equal to READ UNCOMMITED, majority would be similar to REPEATABLE READ and snapshot has near likeness to SERIALIZABLE.</p>\n\n<p>What about WriteConcerns? There are three main descriptors:</p>\n<ul>\n  <li>1, which basically means that we are only interested in primary node committing the changes. Unfortunately, using this WriteConcern means that you will have no guarantees for any of the ReadConcerns stated above,</li>\n  <li>Any {number} greater than 1 states that we want to get the data committed by the primary and {number - 1} of secondary nodes. This WriteConcern depends on the number of nodes in the system, since it can denote more than the majority of the nodes and in this case would provide us with read guarantees,</li>\n  <li>majority, which means that majority of nodes acknowledge the changes in data. This WriteConcern provides us with read guarantees, and also gives us benefits of eventual consistency.</li>\n</ul>\n\n<h3 id=\"enough-theory-i-want-to-see-it-in-action\">Enough theory, I want to see it in action!</h3>\n\n<p>For this example I’m using <a href=\"https://www.mongodb.com/docs/manual/installation/\">MongoDB community edition 6.0 for macOS</a>.</p>\n\n<p>I’ve created a MongoDB server consisting of one config database and one shard replica set with three members. You can find the steps in the official MongoDB <a href=\"https://www.mongodb.com/docs/manual/tutorial/deploy-shard-cluster/\">documentation</a>.</p>\n\n<p>To start, we need some data that’s already present in the database, so that we can see the changes that are being made. First, let’s connect to one of our nodes in a replica set:</p>\n\n<p><code class=\"language-plaintext highlighter-rouge\">mongosh --host localhost --port 27027</code></p>\n\n<p>Then, let’s insert the following data:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">blog</span><span class=\"p\">.</span><span class=\"nx\">insertMany</span><span class=\"p\">([</span>\n    <span class=\"p\">{</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">title</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">GC, hands off my data!</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">author</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Allegro Blogperson</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">date</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-06-30</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">url</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html</span><span class=\"dl\">\"</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">title</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">How to facilitate EventStorming workshops</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">author</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Blog Stormer</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">date</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-07-19</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">url</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">https://blog.allegro.tech/2022/07/event-storming-workshops.html</span><span class=\"dl\">\"</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">title</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">MBox: server-driven UI for mobile apps</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">author</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Mobile Guru</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">date</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-08-03</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"dl\">\"</span><span class=\"s2\">url</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html</span><span class=\"dl\">\"</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">])</span>\n</code></pre></div></div>\n\n<p>Let’s also connect to a different node in our replica set:</p>\n\n<p><code class=\"language-plaintext highlighter-rouge\">mongosh --host localhost --port 27028</code></p>\n\n<p>We can verify here that the data is present:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>db.blog.find()\n</code></pre></div></div>\n\n<p>This should return all our data that we inserted earlier.</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d3</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">MBox: server-driven UI for mobile apps</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Mobile Guru</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-08-03</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d2</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">How to facilitate EventStorming workshops</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Blog Stormer</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-07-19</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/07/event-storming-workshops.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d1</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">GC, hands off my data!</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Allegro Blogperson</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-06-30</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>Time to add some data in a transaction. To do that, we first need to establish a transaction using the following command (in the first shell):</p>\n\n<p><code class=\"language-plaintext highlighter-rouge\">var session = db.getMongo().startSession()</code></p>\n\n<p>After we start the session, it’s time to open the transaction:</p>\n\n<p><code class=\"language-plaintext highlighter-rouge\">session.startTransaction({\"readConcern\": {\"level\": \"snapshot\"}, \"writeConcern\": {\"w\": \"majority\"}})</code></p>\n\n<p>Then, to make sure that we are using our collection in the context of a session, we need to run this:</p>\n\n<p><code class=\"language-plaintext highlighter-rouge\">var blog = session.getDatabase('test').getCollection('blog');</code></p>\n\n<p>We can now insert new data into our collection while transaction is active:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">blog</span><span class=\"p\">.</span><span class=\"nx\">insertOne</span><span class=\"p\">({</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">title</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Transactions in MongoDB</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">author</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">Piotr Kisielewicz</span><span class=\"dl\">\"</span><span class=\"p\">,</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">date</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-11-30</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n        <span class=\"dl\">\"</span><span class=\"s2\">url</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">https://blog.allegro.tech/2022/11/transactions-in-mongodb.html</span><span class=\"dl\">\"</span>\n    <span class=\"p\">})</span>\n</code></pre></div></div>\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{acknowledged: true, insertedId: ObjectId(\"6319e60accc51dfa32ca495a\")}\n</code></pre></div></div>\n\n<p>Before committing our transaction, let’s try to read the data from another replica (in the second shell which we opened previously):</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">blog</span><span class=\"p\">.</span><span class=\"nx\">find</span><span class=\"p\">()</span>\n<span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d3</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">MBox: server-driven UI for mobile apps</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Mobile Guru</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-08-03</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d2</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">How to facilitate EventStorming workshops</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Blog Stormer</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-07-19</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/07/event-storming-workshops.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d1</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">GC, hands off my data!</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Allegro Blogperson</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-06-30</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>As you can see, the new record is nowhere to be found. Let’s now commit the transaction (in the first shell):</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>session.commitTransaction();\n{\n    readOnly: false,\n    ok: 1,\n    lastCommittedOpTime: Timestamp({ t: 1662641697, i: 1 }),\n    '$clusterTime': {\n        clusterTime: Timestamp({ t: 1662641697, i: 1 }),\n        signature: {\n            hash: Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),\n            keyId: 0\n        }\n    },\n    operationTime: Timestamp({ t: 1662641697, i: 1 })\n}\n</code></pre></div></div>\n\n<p>And now, let’s run the same query (in the second shell):</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">blog</span><span class=\"p\">.</span><span class=\"nx\">find</span><span class=\"p\">()</span>\n<span class=\"p\">[</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d3</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">MBox: server-driven UI for mobile apps</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Mobile Guru</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-08-03</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d2</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">How to facilitate EventStorming workshops</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Blog Stormer</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-07-19</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/07/event-storming-workshops.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6311eebd6effda71326b35d1</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">GC, hands off my data!</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Allegro Blogperson</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-06-30</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">},</span>\n    <span class=\"p\">{</span>\n    <span class=\"na\">_id</span><span class=\"p\">:</span> <span class=\"nx\">ObjectId</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">6319e60accc51dfa32ca495a</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">title</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Transactions in MongoDB</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">author</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">Piotr Kisielewicz</span><span class=\"dl\">'</span><span class=\"p\">,</span>\n    <span class=\"na\">date</span><span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2022-09-30</span><span class=\"dl\">\"</span><span class=\"p\">),</span>\n    <span class=\"na\">url</span><span class=\"p\">:</span> <span class=\"dl\">'</span><span class=\"s1\">https://blog.allegro.tech/2022/09/transactions-in-mongodb.html</span><span class=\"dl\">'</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>Success! The data is now present after the commit, and we could see it being absent before committing the change.</p>\n\n<h3 id=\"it-was-all-in-cli-is-there-support-for-transactions-in-code\">It was all in CLI, is there support for transactions in code?</h3>\n\n<p>Yeah, the drivers are already there! For example, if you are using Spring, the only thing you need to do to get MongoDB transaction support is to annotate your methods with <code class=\"language-plaintext highlighter-rouge\">@Transactional</code>.</p>\n\n<p>If you’re using specific driver libraries, e.g. mongo-java-driver, then here’s a code snippet for you:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">client</span> <span class=\"p\">=</span> <span class=\"n\">new</span> <span class=\"nc\">MongoClient</span><span class=\"p\">(</span><span class=\"n\">uri</span><span class=\"p\">)</span>\n<span class=\"kd\">val</span> <span class=\"py\">db</span> <span class=\"p\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"nf\">getDatabase</span><span class=\"p\">(</span><span class=\"s\">\"blog\"</span><span class=\"p\">)</span>\n<span class=\"kd\">val</span> <span class=\"py\">blogCollection</span> <span class=\"p\">=</span> <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"nf\">getCollection</span><span class=\"p\">(</span><span class=\"s\">\"blogPost\"</span><span class=\"p\">,</span> <span class=\"nc\">BlogPost</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span>\n<span class=\"kd\">val</span> <span class=\"py\">session</span> <span class=\"p\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"nf\">startSession</span><span class=\"p\">()</span>\n<span class=\"k\">try</span> <span class=\"p\">{</span>\n    <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"nf\">startTransaction</span><span class=\"p\">(</span><span class=\"nc\">TransactionOptions</span><span class=\"p\">.</span><span class=\"nf\">builder</span><span class=\"p\">().</span><span class=\"nf\">writeConcern</span><span class=\"p\">(</span><span class=\"nc\">WriteConcern</span><span class=\"p\">.</span><span class=\"nc\">MAJORITY</span><span class=\"p\">).</span><span class=\"nf\">build</span><span class=\"p\">())</span>\n    <span class=\"n\">blogCollection</span><span class=\"p\">.</span><span class=\"nf\">insertOne</span><span class=\"p\">(</span><span class=\"n\">session</span><span class=\"p\">,</span> <span class=\"nc\">BlogPost</span><span class=\"p\">())</span>\n    <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"nf\">commitTransaction</span><span class=\"p\">()</span>\n<span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">MongoCommandException</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"nf\">abortTransaction</span><span class=\"p\">()</span>\n<span class=\"p\">}</span> <span class=\"k\">finally</span> <span class=\"p\">{</span>\n    <span class=\"n\">session</span><span class=\"p\">.</span><span class=\"nf\">close</span><span class=\"p\">()</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"do-we-use-mongodb-transactions-at-allegro\">Do we use MongoDB transactions at Allegro?</h3>\n\n<p>Not yet. We are evaluating transactions internally and are checking whether they would fit our use cases.</p>\n\n<h3 id=\"further-reading\">Further reading</h3>\n\n<p><a href=\"https://www.mongodb.com/basics/acid-transactions\">ACID transactions in MongoDB</a></p>\n\n<p><a href=\"https://www.mongodb.com/docs/manual/core/transactions/\">MongoDB transactions</a></p>\n\n<p><a href=\"https://www.mongodb.com/docs/manual/reference/read-concern/\">Read concern</a></p>\n\n<p><a href=\"https://www.mongodb.com/docs/manual/reference/write-concern/\">Write concern</a></p>\n\n<p><a href=\"https://spring.io/projects/spring-data-mongodb\">spring-data-mongodb</a></p>\n\n<p><a href=\"https://mongodb.github.io/mongo-java-driver/3.12/javadoc/com/mongodb/client/package-summary.html\">mongo-java-driver</a></p>\n\n","contentSnippet":"Since version 4.0, transactions have been introduced to the world of Mongo databases. However, the way they work differs greatly from the tried and true world of SQL.\nIn databases like PostgreSQL or MySQL there are transactions based on tables and relations with isolation levels such as READ_UNCOMMITED, READ_COMMITED, REPEATABLE_READ and SERIALIZABLE. These help us to recognize if the record we’re working on is dirty, both when we want to access it or when someone else wants to do it after we’ve made some changes. In the world of documents, shards and replicas, the changes made to a single document are already atomic by design.\nWhy do we need transactions then? I only talked about single documents, but after all, documents can be modified in bulk, and these operations are no longer atomic.\nThat’s why transactions were introduced into MongoDB. In this blog post, we will check how they work and what their use cases are.\nWhat is a transaction?\nA database transaction is a unit of work, designed to handle the changes of data in the database. It makes sure that the output of the data is consistent and doesn’t generate errors. It helps with concurrent changes to the database, and makes the database more scalable.\nBy definition, database transactions are atomic, consistent, isolated and durable. Sounds familiar? Exactly, it’s ACID.\nDoes MongoDB implement ACID?\nBefore implementing transactions? Not really. Of course, there are some properties of ACID that are present (as stated earlier), but before version 4.0 they were impossible to achieve in a distributed ecosystem.\nHere’s an excerpt from the MongoDB website: we estimate that 80%-90% of applications that leverage the document model will not need to utilize transactions in MongoDB.\nAlas, when you need to make changes to multiple documents, or worse, if they are split across multiple shards, then there is no guarantee that the changes will adhere to ACID properties. That’s why the need for ACID transactions arose.\nFortunately, since version 4.0 transactions were added to MongoDB, first for multi-document changes, and then, in 4.2, for distributed data.\nSpecifics of MongoDB transactions\nHaving in mind the differences between MongoDB and SQL DB engines, we need to first take a look at how transactions in Mongo are specified. Since the problems that arise from having multiple documents, replica sets and shards differ from the issues of concurrent access to a table row, resolutions to these problems are also different in nature.\nThe transaction in MongoDB is denoted by two properties: ReadConcern and WriteConcern.\nA ReadConcern property is used to control the consistency and isolation of the data we read from the database.\nSimilarly, the WriteConcern property is us defining when we consider the data we write to be consistent in the database.\n(Side note: when we are talking about majority we are talking about calculated majority. You can read more about it in Calculating majority count)\nWe distinguish between three levels of ReadConcern for transactions (others are unavailable to use in this case):\nlocal, which reads the latest data from a node that has been queried. There are no guarantees that the data read is the most recent across the system,\nmajority, which reads the data at the point of majority-commit. Said point is calculated by the primary node. This ReadConcern doesn’t guarantee consistency unless WriteConcern of at least majority is also stated,\nsnapshot, which reads from a snapshot of majority-commited data. This ReadConcern level is mainly only available in transactions (so we can’t use it in single-document reads save for some outliers) and provides its benefits mainly with sharded transactions as it guarantees that the data is synchronized across shards\nAs we can see, we can draw some parallels between MongoDB’s ReadConcern options and the SQL world, where local would be equal to READ UNCOMMITED, majority would be similar to REPEATABLE READ and snapshot has near likeness to SERIALIZABLE.\nWhat about WriteConcerns? There are three main descriptors:\n1, which basically means that we are only interested in primary node committing the changes. Unfortunately, using this WriteConcern means that you will have no guarantees for any of the ReadConcerns stated above,\nAny {number} greater than 1 states that we want to get the data committed by the primary and {number - 1} of secondary nodes. This WriteConcern depends on the number of nodes in the system, since it can denote more than the majority of the nodes and in this case would provide us with read guarantees,\nmajority, which means that majority of nodes acknowledge the changes in data. This WriteConcern provides us with read guarantees, and also gives us benefits of eventual consistency.\nEnough theory, I want to see it in action!\nFor this example I’m using MongoDB community edition 6.0 for macOS.\nI’ve created a MongoDB server consisting of one config database and one shard replica set with three members. You can find the steps in the official MongoDB documentation.\nTo start, we need some data that’s already present in the database, so that we can see the changes that are being made. First, let’s connect to one of our nodes in a replica set:\nmongosh --host localhost --port 27027\nThen, let’s insert the following data:\n\ndb.blog.insertMany([\n    {\n    \"title\": \"GC, hands off my data!\",\n    \"author\": \"Allegro Blogperson\",\n    \"date\": ISODate(\"2022-06-30\"),\n    \"url\": \"https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html\"\n    },\n    {\n    \"title\": \"How to facilitate EventStorming workshops\",\n    \"author\": \"Blog Stormer\",\n    \"date\": ISODate(\"2022-07-19\"),\n    \"url\": \"https://blog.allegro.tech/2022/07/event-storming-workshops.html\"\n    },\n    {\n    \"title\": \"MBox: server-driven UI for mobile apps\",\n    \"author\": \"Mobile Guru\",\n    \"date\": ISODate(\"2022-08-03\"),\n    \"url\": \"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html\"\n    }\n])\n\n\nLet’s also connect to a different node in our replica set:\nmongosh --host localhost --port 27028\nWe can verify here that the data is present:\n\ndb.blog.find()\n\n\nThis should return all our data that we inserted earlier.\n\n[\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d3\"),\n    title: 'MBox: server-driven UI for mobile apps',\n    author: 'Mobile Guru',\n    date: ISODate(\"2022-08-03\"),\n    url: 'https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html'\n    },\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d2\"),\n    title: 'How to facilitate EventStorming workshops',\n    author: 'Blog Stormer',\n    date: ISODate(\"2022-07-19\"),\n    url: 'https://blog.allegro.tech/2022/07/event-storming-workshops.html'\n    },\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d1\"),\n    title: 'GC, hands off my data!',\n    author: 'Allegro Blogperson',\n    date: ISODate(\"2022-06-30\"),\n    url: 'https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html'\n    }\n]\n\n\nTime to add some data in a transaction. To do that, we first need to establish a transaction using the following command (in the first shell):\nvar session = db.getMongo().startSession()\nAfter we start the session, it’s time to open the transaction:\nsession.startTransaction({\"readConcern\": {\"level\": \"snapshot\"}, \"writeConcern\": {\"w\": \"majority\"}})\nThen, to make sure that we are using our collection in the context of a session, we need to run this:\nvar blog = session.getDatabase('test').getCollection('blog');\nWe can now insert new data into our collection while transaction is active:\n\nblog.insertOne({\n        \"title\": \"Transactions in MongoDB\",\n        \"author\": \"Piotr Kisielewicz\",\n        \"date\": ISODate(\"2022-11-30\"),\n        \"url\": \"https://blog.allegro.tech/2022/11/transactions-in-mongodb.html\"\n    })\n\n\n\n{acknowledged: true, insertedId: ObjectId(\"6319e60accc51dfa32ca495a\")}\n\n\nBefore committing our transaction, let’s try to read the data from another replica (in the second shell which we opened previously):\n\ndb.blog.find()\n[\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d3\"),\n    title: 'MBox: server-driven UI for mobile apps',\n    author: 'Mobile Guru',\n    date: ISODate(\"2022-08-03\"),\n    url: 'https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html'\n    },\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d2\"),\n    title: 'How to facilitate EventStorming workshops',\n    author: 'Blog Stormer',\n    date: ISODate(\"2022-07-19\"),\n    url: 'https://blog.allegro.tech/2022/07/event-storming-workshops.html'\n    },\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d1\"),\n    title: 'GC, hands off my data!',\n    author: 'Allegro Blogperson',\n    date: ISODate(\"2022-06-30\"),\n    url: 'https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html'\n    }\n]\n\n\nAs you can see, the new record is nowhere to be found. Let’s now commit the transaction (in the first shell):\n\nsession.commitTransaction();\n{\n    readOnly: false,\n    ok: 1,\n    lastCommittedOpTime: Timestamp({ t: 1662641697, i: 1 }),\n    '$clusterTime': {\n        clusterTime: Timestamp({ t: 1662641697, i: 1 }),\n        signature: {\n            hash: Binary(Buffer.from(\"0000000000000000000000000000000000000000\", \"hex\"), 0),\n            keyId: 0\n        }\n    },\n    operationTime: Timestamp({ t: 1662641697, i: 1 })\n}\n\n\nAnd now, let’s run the same query (in the second shell):\n\ndb.blog.find()\n[\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d3\"),\n    title: 'MBox: server-driven UI for mobile apps',\n    author: 'Mobile Guru',\n    date: ISODate(\"2022-08-03\"),\n    url: 'https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html'\n    },\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d2\"),\n    title: 'How to facilitate EventStorming workshops',\n    author: 'Blog Stormer',\n    date: ISODate(\"2022-07-19\"),\n    url: 'https://blog.allegro.tech/2022/07/event-storming-workshops.html'\n    },\n    {\n    _id: ObjectId(\"6311eebd6effda71326b35d1\"),\n    title: 'GC, hands off my data!',\n    author: 'Allegro Blogperson',\n    date: ISODate(\"2022-06-30\"),\n    url: 'https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html'\n    },\n    {\n    _id: ObjectId(\"6319e60accc51dfa32ca495a\"),\n    title: 'Transactions in MongoDB',\n    author: 'Piotr Kisielewicz',\n    date: ISODate(\"2022-09-30\"),\n    url: 'https://blog.allegro.tech/2022/09/transactions-in-mongodb.html'\n    }\n]\n\n\nSuccess! The data is now present after the commit, and we could see it being absent before committing the change.\nIt was all in CLI, is there support for transactions in code?\nYeah, the drivers are already there! For example, if you are using Spring, the only thing you need to do to get MongoDB transaction support is to annotate your methods with @Transactional.\nIf you’re using specific driver libraries, e.g. mongo-java-driver, then here’s a code snippet for you:\n\nval client = new MongoClient(uri)\nval db = client.getDatabase(\"blog\")\nval blogCollection = db.getCollection(\"blogPost\", BlogPost::class.java)\nval session = client.startSession()\ntry {\n    session.startTransaction(TransactionOptions.builder().writeConcern(WriteConcern.MAJORITY).build())\n    blogCollection.insertOne(session, BlogPost())\n    session.commitTransaction()\n} catch (e: MongoCommandException) {\n    session.abortTransaction()\n} finally {\n    session.close()\n}\n\n\nDo we use MongoDB transactions at Allegro?\nNot yet. We are evaluating transactions internally and are checking whether they would fit our use cases.\nFurther reading\nACID transactions in MongoDB\nMongoDB transactions\nRead concern\nWrite concern\nspring-data-mongodb\nmongo-java-driver","guid":"https://blog.allegro.tech/2022/12/transactions-in-mongodb.html","categories":["tech","techradar","NoSQL"],"isoDate":"2022-12-20T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Vanilla JS is not dead! Microfrontends without web performance issues.","link":"https://blog.allegro.tech/2022/11/vanilla-js-is-not-dead.html","pubDate":"Thu, 10 Nov 2022 00:00:00 +0100","authors":{"author":[{"name":["Krzysztof Mikuta"],"photo":["https://blog.allegro.tech/img/authors/krzysztof.mikuta.jpg"],"url":["https://blog.allegro.tech/authors/krzysztof.mikuta"]}]},"content":"<p>Building a complex web platform can be a real challenge, especially when parts of it are delivered by independent teams.\nPicking out the correct architecture is crucial, but maintaining it can be even more challenging.\nFrontend microservices, aka microfrontends, is an architecture that gives a lot of flexibility, but can cause\nperformance issues in the future, if not managed well. This article presents an approach to the microfrontends\narchitecture to keep the frontend technology stack efficient based on the complexity of user interface.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>It’s 2022. In the frontend world, we have at least four major frameworks and libraries that have been around for a while\nand provide great resources to build fast and responsive user interfaces. The idea of delivering frontend components in\nvanilla JS seems to be pointless. Why should I even think about getting rid of the great features provided by well known,\nprecisely documented and strongly supported mature libraries? Well, as always, it pretty much depends on the\narchitecture. You have a single big frontend application running in React? Great! You have a couple applications with\na bunch of shared components inside an Angular monorepo? Good for you! But what if you have a big platform with huge\ntraffic, where frontend features are being delivered as independent fragments by independent teams across\nthe whole company? Well, let’s talk about the last option and go through some reasonable use cases for vanilla JS/TS as\nAllegro platform is built upon frontend microservices.</p>\n\n<h2 id=\"dealing-with-the-frontend-microservices-architecture\">Dealing with the frontend microservices architecture</h2>\n<p>The idea of splitting up the frontend of a big e-commerce platform into smaller pieces has been described in\nthe article <a href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">Managing Frontend in the Microservices Architecture</a>.\nIt’s been 6 years since the article appeared and even more since the architecture was implemented at Allegro.\nNowadays, we manage over 1000 microservices and 600+ Opbox components that power the Allegro platform.\nWe can say that it works pretty well for us. As software engineers, we don’t need to worry about things like routing,\nSSR or monitoring, because they’re already served by Opbox. Also, we have overcome the problems the architecture causes\nand implemented efficient solutions. One problem has been described in the article\n<a href=\"/2021/07/css-architecture-and-performance-of-micro-frontends.html\">CSS Architecture and Performance in Micro Frontends</a>.</p>\n\n<p>To clear things up a little bit, imagine building a page made of tens of components, delivered by independent teams.\nEvery component, even the simplest one was implemented using one of the popular libraries. Seems harmless, but it can\ntruly hurt web performance. Rendering plain HTML on the server is much faster than evaluation of library mechanisms to\nproduce static markup. Moreover, client bundles need to be fetched in a browser, but they are pretty heavy as they\ninclude not only the custom code, but the libraries’ code as well… It’s going to take even more time when the internet\nconnection is weak (try setting up throttling in the dev tools). Well, undeniably working with distributed components\nrequires a lot of discipline. Also, monitoring and measuring is pretty important to figure out if the components\nthe team takes care of perform well. If you want to learn more, take a look at the article\n<a href=\"/2021/06/measuring-web-performance.html\">Measuring Web Performance</a>.</p>\n\n<p>How much discipline do you need to keep the system fast and efficient? Enough to have a reasonable approach to\npick out the correct technology to solve the problem. You know you’re asking for trouble, when you decide to use\na complex rendering library for rendering static labels that don’t behave in a reactive way. What could you do instead?\nJust map data to plain HTML! This is the case for vanilla JS. In the next paragraph, I’ll present and discuss\nthe approach we use on our team.</p>\n\n<h2 id=\"pick-the-right-technology\">Pick the right technology</h2>\n<p>For organizational purposes, we decided to define three types of complexity of UI components and assigned\nthree technology stacks that are suitable to solve different kinds of problems. Let’s dive into the details.</p>\n\n<h3 id=\"simple-ui-component\">Simple UI Component</h3>\n<p>This one doesn’t do anything spectacular. In most cases, it’s entirely rendered on the server and has no\nclient side scripting, or it may have some simple event handling. You can easily navigate through the platform using\njust an HTML anchor, can’t you? Also, CSS is so powerful nowadays that javascript is not always necessary to implement\ndynamic behaviors in browsers. The approach for such a component is simple: take the response of the backend service,\nwrite some HTML and CSS representing this data and send it to the client.</p>\n\n<h3 id=\"ui-component-that-is-reactive\">UI Component that is reactive</h3>\n<p>In this case, the component is rendered server-side, but the client side scripts run in a browser\nto provide reactivity. Partial changes of the state require updating the existing parts of the DOM. The challenge here\nis to implement a fine-grained reactivity mechanism, organize the code in a functional manner and separate\nside effects. The first thing can be easily handled using reactive streams like <a href=\"https://github.com/staltz/xstream\">xstream</a>,\nwhich is lighter than the well known rxjs, but still powerful. To keep code organized in a functional manner we borrowed\nthe <a href=\"https://cycle.js.org/model-view-intent.html\">Model-View-Intent pattern from cycle.js</a> and adjusted it to our case,\nwhere the HTML is provided by the server and “hydrated” on the client side. The idea is simple: we mount event handlers\nin Intent, map it to state in Model and react to changes in View. At the end of the system there are side effects\nthat run as a result of reactive subscriptions inside View. It’s still vanilla JS/TS on the server side and\nvanilla JS/TS with a touch of reactivity on the client side.</p>\n\n<h3 id=\"complex-ui-component\">Complex UI component</h3>\n<p>This one can be rendered on the server side, but then always hydrates on the client side. It can also be entirely\nrendered on the client side, forming a single-page application. It’s strongly reactive, changes its state constantly\nand re-renders. Also, state changes affect many parts of the DOM. You surely know it is a great use case for libraries\nlike React, where state changes trigger a reconciliation algorithm, which figures out what has changed and operates on\nan effective layer called Virtual DOM. I don’t think this approach requires any more explanation, as it’s the most\npopular approach in the frontend world nowadays. We just write one piece of code, run renderToString() on the server\nand hydrate() on the client, that’s it.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n<p>The presented approach may sound artificially complicated, but it does do its job. Using a sledgehammer to crack a nut\ncauses web performance issues that can go even further in a distributed environment. Spending a little bit more time on\nplanning features to pick an effective technology definitely pays off! Here are some conclusions based on our experience\nwe would like to share:</p>\n<ul>\n  <li>Don’t reinvent the wheel! Look for small, stable, well-supported packages in the npm registry.\nUse <a href=\"https://bundlephobia.com/\">bundlephobia.com</a> to analyze them and look for alternatives if needed.</li>\n  <li>If code complexity grows, use a library/framework. Don’t write your own! I know it’s tempting and trendy,\nbut you will end up maintaining this code instead of focusing on business features.</li>\n  <li>Monitor bundle sizes to ensure your code transpiles efficiently. You will figure out which expressions add more code\nto the bundle. You can set up extra tests for checking bundle sizes during your build pipeline to ensure you’re not\nrunning out of limits.</li>\n  <li>Separate side effects like DOM manipulations from business logic. It will make the code more predictable\nand easily testable. There are a bunch of patterns and state management libraries that can help you.</li>\n  <li>Respond fast by rendering on the server side, and hydrate wisely on the client side. If the component you create\nis not reactive, make it a server component that does not need to hydrate on the client. It’s a great way to optimize\nwebsite interactivity.</li>\n</ul>\n","contentSnippet":"Building a complex web platform can be a real challenge, especially when parts of it are delivered by independent teams.\nPicking out the correct architecture is crucial, but maintaining it can be even more challenging.\nFrontend microservices, aka microfrontends, is an architecture that gives a lot of flexibility, but can cause\nperformance issues in the future, if not managed well. This article presents an approach to the microfrontends\narchitecture to keep the frontend technology stack efficient based on the complexity of user interface.\nIntroduction\nIt’s 2022. In the frontend world, we have at least four major frameworks and libraries that have been around for a while\nand provide great resources to build fast and responsive user interfaces. The idea of delivering frontend components in\nvanilla JS seems to be pointless. Why should I even think about getting rid of the great features provided by well known,\nprecisely documented and strongly supported mature libraries? Well, as always, it pretty much depends on the\narchitecture. You have a single big frontend application running in React? Great! You have a couple applications with\na bunch of shared components inside an Angular monorepo? Good for you! But what if you have a big platform with huge\ntraffic, where frontend features are being delivered as independent fragments by independent teams across\nthe whole company? Well, let’s talk about the last option and go through some reasonable use cases for vanilla JS/TS as\nAllegro platform is built upon frontend microservices.\nDealing with the frontend microservices architecture\nThe idea of splitting up the frontend of a big e-commerce platform into smaller pieces has been described in\nthe article Managing Frontend in the Microservices Architecture.\nIt’s been 6 years since the article appeared and even more since the architecture was implemented at Allegro.\nNowadays, we manage over 1000 microservices and 600+ Opbox components that power the Allegro platform.\nWe can say that it works pretty well for us. As software engineers, we don’t need to worry about things like routing,\nSSR or monitoring, because they’re already served by Opbox. Also, we have overcome the problems the architecture causes\nand implemented efficient solutions. One problem has been described in the article\nCSS Architecture and Performance in Micro Frontends.\nTo clear things up a little bit, imagine building a page made of tens of components, delivered by independent teams.\nEvery component, even the simplest one was implemented using one of the popular libraries. Seems harmless, but it can\ntruly hurt web performance. Rendering plain HTML on the server is much faster than evaluation of library mechanisms to\nproduce static markup. Moreover, client bundles need to be fetched in a browser, but they are pretty heavy as they\ninclude not only the custom code, but the libraries’ code as well… It’s going to take even more time when the internet\nconnection is weak (try setting up throttling in the dev tools). Well, undeniably working with distributed components\nrequires a lot of discipline. Also, monitoring and measuring is pretty important to figure out if the components\nthe team takes care of perform well. If you want to learn more, take a look at the article\nMeasuring Web Performance.\nHow much discipline do you need to keep the system fast and efficient? Enough to have a reasonable approach to\npick out the correct technology to solve the problem. You know you’re asking for trouble, when you decide to use\na complex rendering library for rendering static labels that don’t behave in a reactive way. What could you do instead?\nJust map data to plain HTML! This is the case for vanilla JS. In the next paragraph, I’ll present and discuss\nthe approach we use on our team.\nPick the right technology\nFor organizational purposes, we decided to define three types of complexity of UI components and assigned\nthree technology stacks that are suitable to solve different kinds of problems. Let’s dive into the details.\nSimple UI Component\nThis one doesn’t do anything spectacular. In most cases, it’s entirely rendered on the server and has no\nclient side scripting, or it may have some simple event handling. You can easily navigate through the platform using\njust an HTML anchor, can’t you? Also, CSS is so powerful nowadays that javascript is not always necessary to implement\ndynamic behaviors in browsers. The approach for such a component is simple: take the response of the backend service,\nwrite some HTML and CSS representing this data and send it to the client.\nUI Component that is reactive\nIn this case, the component is rendered server-side, but the client side scripts run in a browser\nto provide reactivity. Partial changes of the state require updating the existing parts of the DOM. The challenge here\nis to implement a fine-grained reactivity mechanism, organize the code in a functional manner and separate\nside effects. The first thing can be easily handled using reactive streams like xstream,\nwhich is lighter than the well known rxjs, but still powerful. To keep code organized in a functional manner we borrowed\nthe Model-View-Intent pattern from cycle.js and adjusted it to our case,\nwhere the HTML is provided by the server and “hydrated” on the client side. The idea is simple: we mount event handlers\nin Intent, map it to state in Model and react to changes in View. At the end of the system there are side effects\nthat run as a result of reactive subscriptions inside View. It’s still vanilla JS/TS on the server side and\nvanilla JS/TS with a touch of reactivity on the client side.\nComplex UI component\nThis one can be rendered on the server side, but then always hydrates on the client side. It can also be entirely\nrendered on the client side, forming a single-page application. It’s strongly reactive, changes its state constantly\nand re-renders. Also, state changes affect many parts of the DOM. You surely know it is a great use case for libraries\nlike React, where state changes trigger a reconciliation algorithm, which figures out what has changed and operates on\nan effective layer called Virtual DOM. I don’t think this approach requires any more explanation, as it’s the most\npopular approach in the frontend world nowadays. We just write one piece of code, run renderToString() on the server\nand hydrate() on the client, that’s it.\nConclusions\nThe presented approach may sound artificially complicated, but it does do its job. Using a sledgehammer to crack a nut\ncauses web performance issues that can go even further in a distributed environment. Spending a little bit more time on\nplanning features to pick an effective technology definitely pays off! Here are some conclusions based on our experience\nwe would like to share:\nDon’t reinvent the wheel! Look for small, stable, well-supported packages in the npm registry.\nUse bundlephobia.com to analyze them and look for alternatives if needed.\nIf code complexity grows, use a library/framework. Don’t write your own! I know it’s tempting and trendy,\nbut you will end up maintaining this code instead of focusing on business features.\nMonitor bundle sizes to ensure your code transpiles efficiently. You will figure out which expressions add more code\nto the bundle. You can set up extra tests for checking bundle sizes during your build pipeline to ensure you’re not\nrunning out of limits.\nSeparate side effects like DOM manipulations from business logic. It will make the code more predictable\nand easily testable. There are a bunch of patterns and state management libraries that can help you.\nRespond fast by rendering on the server side, and hydrate wisely on the client side. If the component you create\nis not reactive, make it a server component that does not need to hydrate on the client. It’s a great way to optimize\nwebsite interactivity.","guid":"https://blog.allegro.tech/2022/11/vanilla-js-is-not-dead.html","categories":["tech","frontend","microservices","webperf","javascript"],"isoDate":"2022-11-09T23:00:00.000Z","thumbnail":"images/post-headers/javascript.png"},{"title":"Probabilistic Data Structures and Algorithms in NoSQL databases","link":"https://blog.allegro.tech/2022/10/probabilistic-algorithms.html","pubDate":"Tue, 04 Oct 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>One of the <a href=\"https://en.wikipedia.org/wiki/ACID\">four fundamental</a> features of transactional databases is durability. It says that once a\ntransaction is committed, the stored data remains available even if the database crashes. If we upload some information\ninto the database, we must be able to read it later, no matter what happens.</p>\n\n<p>It is so elementary that we frequently don’t even think about it: if we save a record with the ’42’\nvalue in a database, we will get ’42’ every time we read that\nrecord, until the next modification. The durability concept can be generalized somewhat, by considering not only transactional\ndatabases but those that do not provide transactions. After all, in each of them, after a\ncorrect write we can be sure that the stored information is in the database and we have access\nto it.</p>\n\n<p>But it turns out that there are databases that provide us with solutions making that the concept of durability —\neven in this generalized form — no longer so obvious. What would you say if we stored\n1 000 records in a database, and the database claimed that there were only 998 of them? Or, if we\ncreated a database storing sets of values and in some cases the database would claim that an\nelement was in that set, while in fact it was not? Seeing such a behavior many would probably start\nlooking for an error. However, behavior like this is not necessarily an error, as long as we use a database\nthat implements probabilistic algorithms and data structures. Solutions based on these methods allow some\ninaccuracy in the results, but in return they are able to provide us with great savings in the resources\nused. More interesting is that there is a good chance that you are already using such a DB.</p>\n\n<p>In this post we will learn about two probability-based techniques, perform some experiments and\nconsider when it is worth using a database that lies to us a bit.</p>\n\n<h2 id=\"fast-cardinality-aggregation\">Fast cardinality aggregation</h2>\n\n<p>Some time ago I had the opportunity to work on a service based on Elasticsearch. This service collects\nhuge amounts of data, which is later analyzed by our customer care specialists. One of the key elements to be analyzed\nis a simple aggregate — the number of unique occurrences of certain values. In mathematics, this\nquantity is called the power of the set or the cardinal number.</p>\n\n<p>The easiest way to understand this is to use an example: imagine that I take out all the banknotes\nfrom my wallet and it turns out that I have 10 of them, with the following nominal values:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>If we arranged them by value, we would end up collecting these 10\nbanknotes in four piles with values: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 100]</code>, so the cardinal number of the set containing my 10\nbanknotes equals: 4.</p>\n\n<p>Elasticsearch has a special function: <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html\">cardinality</a>, which is used to determine the power of the set and we use\nthis function specifically to count unique occurrences that I mentioned earlier.</p>\n\n<p>It may seem that counting unique occurrences of values is a trivial task.\nLet’s go back to our example with the banknotes. You can think of many ways to check how many\nunique values there are in this list, probably one of the simplest is to use the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class. One of its main features is\nthat it de-duplicates the elements added to it, thus it stores only one occurrence of each.</p>\n\n<p>After adding 10 values of banknotes: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]</code> to an instance of the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>\nclass, it will ultimately only store the values <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 100]</code> (not necessarily in that order, but it\ndoesn’t matter it this case). So all we need to do is check the size of this set and we have the result we were\nlooking for: 4.</p>\n\n<p>This solution is simple and looks tempting, yet it has a certain drawback: the more unique elements the set stores,\nthe more memory our program needs. In an extreme case, when each added element is different from\nthe others, the memory complexity of this approach will be linear. This is bad news when we\nwant to operate on a large volume of data, because we will immediately use all available memory.\nIf, additionally, requests for the cardinal number come from\nclients with high intensity, and the input set contains billions of elements, it is easy to imagine that the\napproach described above has no chance of success.</p>\n\n<p>How to address this issue? In such a situation we can switch to one of ingenious probabilistic algorithms. Their\nmain feature is that they give approximate rather than exact results. The huge advantage, on the\nother hand, is that they are much less resource-intensive.</p>\n\n<h2 id=\"near-optimal-cardinality-estimator\">Near-optimal cardinality estimator</h2>\n\n<p>One such algorithm — HyperLogLog (HLL) — has been implemented in the aforementioned\nElasticsearch to build the cardinality function. It is used to count the unique values of a given field of\nan indexed document, and it does so with a certain approximation, using very little memory.\nInterestingly, you can control the accuracy of this approximation with a special parameter. This is\nbecause in addition to the field to be counted, the cardinality function also accepts a\n<code class=\"language-plaintext highlighter-rouge\">precision_threshold</code> argument, due to which we can specify how much inaccuracy we agree to, in\nexchange for less or more memory usage.</p>\n\n<p>Obviously, in some cases even a small error is unacceptable. We must then abandon the probabilistic\napproach and look for another solution. However, for a sizable class of problems, certain\napproximation is completely sufficient. Imagine a video clip uploaded to a popular streaming service.\nIf the author of the clip has a bit of luck, the counter of unique views of his/her work starts spinning\nvery quickly. In case of very high popularity, when displaying the current number of visits, full\naccuracy will not matter so much; we can reconcile with displaying a value that differs from the\nactual one by a few percent. It is completely sufficient that the accurate data — e.g. for monetization\npurposes — is available the next day, when we calculate it accurately using, for example, Apache Spark.</p>\n\n<p>Implementing such a counter of unique visitors into a site operating on huge data sets, we could\ntherefore consider using the HLL algorithm.</p>\n\n<p>Readers interested in a detailed description of the HLL algorithm are referred to a great article on\n<a href=\"http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation\">Damn Cool Algorithms post</a>.\nHowever, its most important features are worth noting here:</p>\n<ul>\n  <li>the results, although approximate, are deterministic,</li>\n  <li>the maximum possible error is known,</li>\n  <li>amount of memory used is fixed.</li>\n</ul>\n\n<p>The last two features are closely related and can be controlled: we can decrease the error level by increasing\nthe available memory limit and vice versa.\nThere are many ready-made implementations of the HLL algorithm available, so it’s worth reaching\nfor one of them and doing some experiments. I will use <a href=\"https://datasketches.apache.org/docs/HLL/HLL.html\">datasketches</a>\nand compare the memory consumption with the classic approach using the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. Moreover, I will add a third variant based\non a <code class=\"language-plaintext highlighter-rouge\">distinct</code> method from the Kotlin language, which — like the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> constructor — de-duplicates\nelements from the list.</p>\n\n<p>Below there is a code snippet of a simple program that determines the cardinal number of a set of numbers using <code class=\"language-plaintext highlighter-rouge\">HashSet</code>\nclass from Java language. In order to be able to run some trials, I’ve introduced a couple of basic parameters. The\ninput list consists of <code class=\"language-plaintext highlighter-rouge\">n</code> numbers, while using the <code class=\"language-plaintext highlighter-rouge\">f</code> parameter and the <code class=\"language-plaintext highlighter-rouge\">modulo</code> function I decide what\npart of the input list is unique. For example, for n=1 000 000 and f=0.1, the result will be a cardinal\nnumber equal to 100 000.</p>\n\n<p>Please note the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> constructor parameter. By default, when the constructor is empty - this class is\n<a href=\"https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/HashSet.html#%3Cinit%3E()\">initialized with the value 16</a>,\nwhich means that before adding the 17th element, memory reallocation must occur for next portion of elements, which takes time.\nTo eliminate this extra time I allocate in advance as much memory as needed.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">mod</span> <span class=\"p\">=</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">*</span> <span class=\"n\">f</span><span class=\"p\">).</span><span class=\"nf\">toLong</span><span class=\"p\">()</span>\n<span class=\"kd\">val</span> <span class=\"py\">set</span> <span class=\"p\">=</span> <span class=\"nc\">HashSet</span><span class=\"p\">&lt;</span><span class=\"nc\">Long</span><span class=\"p\">&gt;(</span><span class=\"n\">mod</span><span class=\"p\">.</span><span class=\"nf\">toInt</span><span class=\"p\">())</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">elapsed</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"mi\">0</span> <span class=\"n\">until</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">set</span><span class=\"p\">.</span><span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"p\">%</span> <span class=\"n\">mod</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"n\">cardinality</span> <span class=\"p\">=</span> <span class=\"k\">set</span><span class=\"p\">.</span><span class=\"n\">size</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Two other programs do exactly the same thing: determine the cardinal number of a set of numbers, but one uses Kotlin\n<code class=\"language-plaintext highlighter-rouge\">distinct</code> method and the second one uses HLL algorithm. You can find full code of all three applications\non this <a href=\"https://github.com/mknasiecki/prob-alg-post\">repository</a>.</p>\n\n<p>All three programs, in addition to the result, also measure total execution time. Moreover, using\n<a href=\"https://openjdk.java.net/tools/svc/jconsole/\">jConsole</a> I am also able to measure the amount of memory used. I decided\nto measure the total memory used by the\nprograms, because measuring the size of the data structures is not a trivial task.</p>\n\n<p>We start by checking the variant n=1 000 000/f=0.25 as a result of which we should get a power of set\nequal 250 000. Let’s take a look at the results:</p>\n\n<p><em>n=1 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>250 000</td>\n      <td>250 000</td>\n      <td>249 979.9</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>71</td>\n      <td>106</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>42</td>\n      <td>73</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>In case of such a small set the deviation of the result of the HLL variant from the true value is far less than\n1%, while in this case you can already see the benefits of this method; the amount of memory used is\nhalf compared to the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> version and as much as 3 times less when compared to the\nversion using the Kotlin language function.</p>\n\n<p>It is worth pausing here for a moment to consider what is the reason for such a big difference in consumed memory.\nThe first two programs are based on collections of objects, thus storing in memory entire instances along with their references.\nThe HLL method, on the other hand, uses memory-efficient bit arrays that store data based on object hashes. This makes\nit insensitive to the original size of the processed data. It means that the benefits of using HLL increase with the\nmemory needed to store the objects you want to count. The results presented above would be even more spectacular if we\nused, for example, email addresses or IP addresses instead of numbers.</p>\n\n<p>During the next attempt we increase the value of the <code class=\"language-plaintext highlighter-rouge\">n</code> parameter tenfold:</p>\n\n<p><em>n=10 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>2 500 000</td>\n      <td>2 500 000</td>\n      <td>2 484 301.4</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>483</td>\n      <td>863</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>233</td>\n      <td>574</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The error value has increased slightly, while the difference in memory usage and the performance\ntime is even greater than before. Therefore, it is worthwhile to increase the size of the set again:</p>\n\n<p><em>n=100 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>25 000 000</td>\n      <td>25 000 000</td>\n      <td>25 301 157.2</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.2</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>3857</td>\n      <td>7718</td>\n      <td>1538</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>1800</td>\n      <td>5300</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Deviation from the correct result exceeded 1%; the times also went up, although they are still many\ntimes shorter compared to other variants. It’s worth noting that the amount of memory used has practically not changed.</p>\n\n<p>Now let’s see what happens when we change the second parameter, which determines the number of\nunique elements in the input set:</p>\n\n<p><em>n=10 000 000/f=0.5</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>5 000 000</td>\n      <td>5 000 000</td>\n      <td>5 067 045.2</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.34</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>467</td>\n      <td>914</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>420</td>\n      <td>753</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>n=10 000 000/f=0.75</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>7 500 000</td>\n      <td>7 500 000</td>\n      <td>7 619 136.7</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.59</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>589</td>\n      <td>1187</td>\n      <td>191</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>616</td>\n      <td>843</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Again, the results clearly show the advantages of the HLL algorithm. With a relatively low error we\nsignificantly reduced the amount of memory used and the time required for calculations.\nAs you can see and as expected, the classical approach gives accurate results but it consumes a lot of\nmemory, while the solution using HLL brings results characterized by approx. 1% error, but in return\nwe use much less memory. A certain surprise for me is the poor result of the Kotlin <code class=\"language-plaintext highlighter-rouge\">distinct</code> function; I\nexpected results more similar to the variant based on the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. Presumably the key difference is that it returns an instance\nof the <code class=\"language-plaintext highlighter-rouge\">List</code> class rather than <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. This requires further investigation, which is beyond the scope of my considerations.</p>\n\n<p>The HLL algorithm is implemented in several solutions, including the aforementioned Elasticsearch,\nas well as in e.g. <a href=\"https://redis.com/redis-best-practices/counting/hyperloglog/\">Redis</a> and <a href=\"https://prestodb.io/docs/current/functions/hyperloglog.html\">Presto</a>. The above experiments clearly show that the approximate method, in case\nwe need to process huge amounts of data, is a good idea provided that we allow a result with a small\nerror.</p>\n\n<h2 id=\"memory-efficient-presence-test\">Memory-efficient presence test</h2>\n\n<p>It turns out that the HLL is not the only probabilistic algorithm available in popular databases —\nanother example of this approach is the Bloom Filter. This is an implementation of a memory-saving structure that is\nused in the so-called presence test. Let’s go back to our example with my cash: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]</code>.\nImagine that we want to test whether there is a 100 value banknote in my wallet. In this case the answer is positive, but the test\nfor the 200 value banknote should be false, since there is no such a banknote in the wallet.</p>\n\n<p>Of course, we are able again to implement a solution to this problem by simply using the properties\nof the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class and the <code class=\"language-plaintext highlighter-rouge\">contains</code> method. However, similarly as in case of determining the\ncardinality — the memory requirement increases with the size of the dataset.\nAgain, the solution for this problem may be an approximate method.</p>\n\n<p>Similarly as in case of the HLL algorithm the Bloom Filter allows for some inaccuracy, and in this case\nthis means false positive results. This is because it can happen that the Bloom Filter finds that an element\nbelongs to a given set, while in fact it is not there. However, the opposite situation is not possible\nso if the Bloom Filter states that an element is not part of the set, it is certainly true. Referring this to\nour example with the content of my wallet, the Bloom Filter could therefore assure me that there was a\n200 value banknote in it, while standing at the checkout in a store it would turn out that,\nunfortunately, it is not there. What a pity…</p>\n\n<p>Before we move on to examine how this algorithm works, let’s consider where it could be useful. A\ntypical example is a recommendation system. Imagine we are designing a system intended to suggest\narticles for users to read, a feature common on social media sites. Such a system needs to store a\nlist of articles read by each user so that it does not suggest them again. It is easy to imagine that\nstoring these articles with each user in the classic way would quickly exhaust memory resources. If we\ndon’t use any data removal mechanism, the database will grow indefinitely. The discussed Bloom\nFilter fits perfectly here as it will allow us to save a lot of memory, although, one must consider\nconsequences of its limitations related to possible false results. It may happen that we will get false\ninformation that a certain article has already been read by someone, while in fact this is not true.\nConsequently, we will not offer that user to read the material. On the other hand, the opposite\nsituation is not possible: we will never display to a user a recommendation of an article that he/she\nhas already read.</p>\n\n<p>At this point it is worth checking how much we gain by accepting the inconvenience described\nabove. I have prepared two implementations of a program that adds to a set of values and then\nchecks if they are there.\nThe first program uses the classic approach — the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class, while the second uses the Bloom\nFilter available in the popular <a href=\"https://guava.dev/releases/20.0/api/docs/com/google/common/hash/BloomFilter.html\">guava</a> library.\nAgain, using jConsole we register for both programs the amount of memory used, and additionally — for the version with\nthe Bloom Filter we also check the\nnumber of false positives. This value can be easily controlled, as the maximum allowed false positive\nrate can be set in the API; for needs of the following tests we will set it to 1%.</p>\n\n<p>Moreover, we will measure the total time of adding values to the set and the total time of querying\nwhether there are values in the set.</p>\n\n<p>Same as before we will perform a number of tests using the following parameters: <code class=\"language-plaintext highlighter-rouge\">n</code> — the size of the set of\nnumbers, and <code class=\"language-plaintext highlighter-rouge\">f</code> — what part of it should be added to the set. The configuration n=1 000 000 and f=0.1 means\nthat the first 100 000 numbers out of 1 000 000 will be added to the set. So, in the first part, the program will\nadd 100 000 numbers to the set and then — in the second stage — it will perform a presence test\nby checking whether the numbers above 100 000 belong to the set. There is no point in checking the\nnumbers added to the set beforehand, because we know that Bloom Filters do not give false\nnegative results. On the other hand, if any number above 100 000 is found according to the Bloom Filter in\nthe set, we will consider it a false positive.</p>\n\n<p>Following code snippet presents fragment of the Bloom Filter variant:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">insertions</span> <span class=\"p\">=</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">*</span> <span class=\"n\">f</span><span class=\"p\">).</span><span class=\"nf\">toInt</span><span class=\"p\">()</span>\n<span class=\"kd\">val</span> <span class=\"py\">filter</span> <span class=\"p\">=</span> <span class=\"nc\">BloomFilter</span><span class=\"p\">.</span><span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"nc\">Funnels</span><span class=\"p\">.</span><span class=\"nf\">integerFunnel</span><span class=\"p\">(),</span> <span class=\"n\">insertions</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"kd\">var</span> <span class=\"py\">falsePositives</span> <span class=\"p\">=</span> <span class=\"mi\">0</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">insertTime</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"mi\">0</span> <span class=\"n\">until</span> <span class=\"n\">insertions</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"n\">filter</span><span class=\"p\">.</span><span class=\"nf\">put</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">queryTime</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"n\">insertions</span> <span class=\"n\">until</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">.</span><span class=\"nf\">mightContain</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">))</span> <span class=\"p\">{</span>\n            <span class=\"n\">falsePositives</span><span class=\"p\">++;</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">fpRatio</span> <span class=\"p\">=</span> <span class=\"n\">falsePositives</span><span class=\"p\">/</span><span class=\"n\">n</span><span class=\"p\">.</span><span class=\"nf\">toDouble</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>Again — you can find full code of both programs on aforementioned <a href=\"https://github.com/mknasiecki/prob-alg-post\">repository</a>.</p>\n\n<p>Let’s start with the following configuration: n=10 000 000/f=0.1:</p>\n\n<p><em>n=10 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>81</td>\n      <td>293</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>82</td>\n      <td>846</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>94</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>As you can see, the Bloom Filter returned less than 1% false results, but — at the same time — it used three times\nless memory than HashSet variant. Unfortunately, the times of Bloom Filter’s version are significantly higher.\nLet’s check what happens when we increase the size of the input set:</p>\n\n<p><em>n=100 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>593</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>988</td>\n      <td>944</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>876</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>n=500 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>1975</td>\n      <td>1372</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>4115</td>\n      <td>4923</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>4400</td>\n      <td>81</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The number of false positives is still below the preset 1%, the amount of memory used is still lower\nthan the classic implementation, and interestingly also the times of the probabilistic variant are\nlower, at least for inserting. Thus, it can be seen that along with the increase in the size of the data the benefit of this\nmethod increases.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>The above results clearly show that by accepting a small share of false answers, we can gain significant savings in memory usage.\nSimilarly to the HLL algorithm, the structure based on the Bloom Filters is available in many popular\ndatabases like <a href=\"https://redis.com/redis-best-practices/bloom-filter-pattern/\">Redis</a>,\n<a href=\"https://hbase.apache.org/2.2/devapidocs/org/apache/hadoop/hbase/util/BloomFilter.html\">HBase</a>\nor <a href=\"https://cassandra.apache.org/doc/latest/cassandra/operating/bloom_filters.html\">Cassandra</a>.</p>\n\n<p>The simple experiments we conducted showed that probabilistic algorithms can save a lot\nof memory, which is especially important if our database stores huge amounts of data. In such cases it\nis sometimes worth letting your database lie to you a little.</p>\n","contentSnippet":"One of the four fundamental features of transactional databases is durability. It says that once a\ntransaction is committed, the stored data remains available even if the database crashes. If we upload some information\ninto the database, we must be able to read it later, no matter what happens.\nIt is so elementary that we frequently don’t even think about it: if we save a record with the ’42’\nvalue in a database, we will get ’42’ every time we read that\nrecord, until the next modification. The durability concept can be generalized somewhat, by considering not only transactional\ndatabases but those that do not provide transactions. After all, in each of them, after a\ncorrect write we can be sure that the stored information is in the database and we have access\nto it.\nBut it turns out that there are databases that provide us with solutions making that the concept of durability —\neven in this generalized form — no longer so obvious. What would you say if we stored\n1 000 records in a database, and the database claimed that there were only 998 of them? Or, if we\ncreated a database storing sets of values and in some cases the database would claim that an\nelement was in that set, while in fact it was not? Seeing such a behavior many would probably start\nlooking for an error. However, behavior like this is not necessarily an error, as long as we use a database\nthat implements probabilistic algorithms and data structures. Solutions based on these methods allow some\ninaccuracy in the results, but in return they are able to provide us with great savings in the resources\nused. More interesting is that there is a good chance that you are already using such a DB.\nIn this post we will learn about two probability-based techniques, perform some experiments and\nconsider when it is worth using a database that lies to us a bit.\nFast cardinality aggregation\nSome time ago I had the opportunity to work on a service based on Elasticsearch. This service collects\nhuge amounts of data, which is later analyzed by our customer care specialists. One of the key elements to be analyzed\nis a simple aggregate — the number of unique occurrences of certain values. In mathematics, this\nquantity is called the power of the set or the cardinal number.\nThe easiest way to understand this is to use an example: imagine that I take out all the banknotes\nfrom my wallet and it turns out that I have 10 of them, with the following nominal values:\n\n[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]\n\n\nIf we arranged them by value, we would end up collecting these 10\nbanknotes in four piles with values: [10, 20, 50, 100], so the cardinal number of the set containing my 10\nbanknotes equals: 4.\nElasticsearch has a special function: cardinality, which is used to determine the power of the set and we use\nthis function specifically to count unique occurrences that I mentioned earlier.\nIt may seem that counting unique occurrences of values is a trivial task.\nLet’s go back to our example with the banknotes. You can think of many ways to check how many\nunique values there are in this list, probably one of the simplest is to use the HashSet class. One of its main features is\nthat it de-duplicates the elements added to it, thus it stores only one occurrence of each.\nAfter adding 10 values of banknotes: [10, 20, 50, 20, 50, 100, 50, 20, 10, 10] to an instance of the HashSet\nclass, it will ultimately only store the values [10, 20, 50, 100] (not necessarily in that order, but it\ndoesn’t matter it this case). So all we need to do is check the size of this set and we have the result we were\nlooking for: 4.\nThis solution is simple and looks tempting, yet it has a certain drawback: the more unique elements the set stores,\nthe more memory our program needs. In an extreme case, when each added element is different from\nthe others, the memory complexity of this approach will be linear. This is bad news when we\nwant to operate on a large volume of data, because we will immediately use all available memory.\nIf, additionally, requests for the cardinal number come from\nclients with high intensity, and the input set contains billions of elements, it is easy to imagine that the\napproach described above has no chance of success.\nHow to address this issue? In such a situation we can switch to one of ingenious probabilistic algorithms. Their\nmain feature is that they give approximate rather than exact results. The huge advantage, on the\nother hand, is that they are much less resource-intensive.\nNear-optimal cardinality estimator\nOne such algorithm — HyperLogLog (HLL) — has been implemented in the aforementioned\nElasticsearch to build the cardinality function. It is used to count the unique values of a given field of\nan indexed document, and it does so with a certain approximation, using very little memory.\nInterestingly, you can control the accuracy of this approximation with a special parameter. This is\nbecause in addition to the field to be counted, the cardinality function also accepts a\nprecision_threshold argument, due to which we can specify how much inaccuracy we agree to, in\nexchange for less or more memory usage.\nObviously, in some cases even a small error is unacceptable. We must then abandon the probabilistic\napproach and look for another solution. However, for a sizable class of problems, certain\napproximation is completely sufficient. Imagine a video clip uploaded to a popular streaming service.\nIf the author of the clip has a bit of luck, the counter of unique views of his/her work starts spinning\nvery quickly. In case of very high popularity, when displaying the current number of visits, full\naccuracy will not matter so much; we can reconcile with displaying a value that differs from the\nactual one by a few percent. It is completely sufficient that the accurate data — e.g. for monetization\npurposes — is available the next day, when we calculate it accurately using, for example, Apache Spark.\nImplementing such a counter of unique visitors into a site operating on huge data sets, we could\ntherefore consider using the HLL algorithm.\nReaders interested in a detailed description of the HLL algorithm are referred to a great article on\nDamn Cool Algorithms post.\nHowever, its most important features are worth noting here:\nthe results, although approximate, are deterministic,\nthe maximum possible error is known,\namount of memory used is fixed.\nThe last two features are closely related and can be controlled: we can decrease the error level by increasing\nthe available memory limit and vice versa.\nThere are many ready-made implementations of the HLL algorithm available, so it’s worth reaching\nfor one of them and doing some experiments. I will use datasketches\nand compare the memory consumption with the classic approach using the HashSet. Moreover, I will add a third variant based\non a distinct method from the Kotlin language, which — like the HashSet constructor — de-duplicates\nelements from the list.\nBelow there is a code snippet of a simple program that determines the cardinal number of a set of numbers using HashSet\nclass from Java language. In order to be able to run some trials, I’ve introduced a couple of basic parameters. The\ninput list consists of n numbers, while using the f parameter and the modulo function I decide what\npart of the input list is unique. For example, for n=1 000 000 and f=0.1, the result will be a cardinal\nnumber equal to 100 000.\nPlease note the HashSet constructor parameter. By default, when the constructor is empty - this class is\ninitialized with the value 16,\nwhich means that before adding the 17th element, memory reallocation must occur for next portion of elements, which takes time.\nTo eliminate this extra time I allocate in advance as much memory as needed.\n\nval mod = (n * f).toLong()\nval set = HashSet<Long>(mod.toInt())\n\nval elapsed = measureTimeMillis {\n    for (i in 0 until n) {\n        set.add(i % mod)\n    }\n\n    cardinality = set.size\n}\n\n\nTwo other programs do exactly the same thing: determine the cardinal number of a set of numbers, but one uses Kotlin\ndistinct method and the second one uses HLL algorithm. You can find full code of all three applications\non this repository.\nAll three programs, in addition to the result, also measure total execution time. Moreover, using\njConsole I am also able to measure the amount of memory used. I decided\nto measure the total memory used by the\nprograms, because measuring the size of the data structures is not a trivial task.\nWe start by checking the variant n=1 000 000/f=0.25 as a result of which we should get a power of set\nequal 250 000. Let’s take a look at the results:\nn=1 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      250 000\n      250 000\n      249 979.9\n    \nerror [%]\n      0\n      0\n      0.01\n    \ntime [ms]\n      71\n      106\n      53\n    \nmemory [MB]\n      42\n      73\n      21\n    \nIn case of such a small set the deviation of the result of the HLL variant from the true value is far less than\n1%, while in this case you can already see the benefits of this method; the amount of memory used is\nhalf compared to the HashSet version and as much as 3 times less when compared to the\nversion using the Kotlin language function.\nIt is worth pausing here for a moment to consider what is the reason for such a big difference in consumed memory.\nThe first two programs are based on collections of objects, thus storing in memory entire instances along with their references.\nThe HLL method, on the other hand, uses memory-efficient bit arrays that store data based on object hashes. This makes\nit insensitive to the original size of the processed data. It means that the benefits of using HLL increase with the\nmemory needed to store the objects you want to count. The results presented above would be even more spectacular if we\nused, for example, email addresses or IP addresses instead of numbers.\nDuring the next attempt we increase the value of the n parameter tenfold:\nn=10 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      2 500 000\n      2 500 000\n      2 484 301.4\n    \nerror [%]\n      0\n      0\n      0.63\n    \ntime [ms]\n      483\n      863\n      189\n    \nmemory [MB]\n      233\n      574\n      21\n    \nThe error value has increased slightly, while the difference in memory usage and the performance\ntime is even greater than before. Therefore, it is worthwhile to increase the size of the set again:\nn=100 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      25 000 000\n      25 000 000\n      25 301 157.2\n    \nerror [%]\n      0\n      0\n      1.2\n    \ntime [ms]\n      3857\n      7718\n      1538\n    \nmemory [MB]\n      1800\n      5300\n      21\n    \nDeviation from the correct result exceeded 1%; the times also went up, although they are still many\ntimes shorter compared to other variants. It’s worth noting that the amount of memory used has practically not changed.\nNow let’s see what happens when we change the second parameter, which determines the number of\nunique elements in the input set:\nn=10 000 000/f=0.5\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      5 000 000\n      5 000 000\n      5 067 045.2\n    \nerror [%]\n      0\n      0\n      1.34\n    \ntime [ms]\n      467\n      914\n      183\n    \nmemory [MB]\n      420\n      753\n      21\n    \nn=10 000 000/f=0.75\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      7 500 000\n      7 500 000\n      7 619 136.7\n    \nerror [%]\n      0\n      0\n      1.59\n    \ntime [ms]\n      589\n      1187\n      191\n    \nmemory [MB]\n      616\n      843\n      26\n    \nAgain, the results clearly show the advantages of the HLL algorithm. With a relatively low error we\nsignificantly reduced the amount of memory used and the time required for calculations.\nAs you can see and as expected, the classical approach gives accurate results but it consumes a lot of\nmemory, while the solution using HLL brings results characterized by approx. 1% error, but in return\nwe use much less memory. A certain surprise for me is the poor result of the Kotlin distinct function; I\nexpected results more similar to the variant based on the HashSet. Presumably the key difference is that it returns an instance\nof the List class rather than HashSet. This requires further investigation, which is beyond the scope of my considerations.\nThe HLL algorithm is implemented in several solutions, including the aforementioned Elasticsearch,\nas well as in e.g. Redis and Presto. The above experiments clearly show that the approximate method, in case\nwe need to process huge amounts of data, is a good idea provided that we allow a result with a small\nerror.\nMemory-efficient presence test\nIt turns out that the HLL is not the only probabilistic algorithm available in popular databases —\nanother example of this approach is the Bloom Filter. This is an implementation of a memory-saving structure that is\nused in the so-called presence test. Let’s go back to our example with my cash: [10, 20, 50, 20, 50, 100, 50, 20, 10, 10].\nImagine that we want to test whether there is a 100 value banknote in my wallet. In this case the answer is positive, but the test\nfor the 200 value banknote should be false, since there is no such a banknote in the wallet.\nOf course, we are able again to implement a solution to this problem by simply using the properties\nof the HashSet class and the contains method. However, similarly as in case of determining the\ncardinality — the memory requirement increases with the size of the dataset.\nAgain, the solution for this problem may be an approximate method.\nSimilarly as in case of the HLL algorithm the Bloom Filter allows for some inaccuracy, and in this case\nthis means false positive results. This is because it can happen that the Bloom Filter finds that an element\nbelongs to a given set, while in fact it is not there. However, the opposite situation is not possible\nso if the Bloom Filter states that an element is not part of the set, it is certainly true. Referring this to\nour example with the content of my wallet, the Bloom Filter could therefore assure me that there was a\n200 value banknote in it, while standing at the checkout in a store it would turn out that,\nunfortunately, it is not there. What a pity…\nBefore we move on to examine how this algorithm works, let’s consider where it could be useful. A\ntypical example is a recommendation system. Imagine we are designing a system intended to suggest\narticles for users to read, a feature common on social media sites. Such a system needs to store a\nlist of articles read by each user so that it does not suggest them again. It is easy to imagine that\nstoring these articles with each user in the classic way would quickly exhaust memory resources. If we\ndon’t use any data removal mechanism, the database will grow indefinitely. The discussed Bloom\nFilter fits perfectly here as it will allow us to save a lot of memory, although, one must consider\nconsequences of its limitations related to possible false results. It may happen that we will get false\ninformation that a certain article has already been read by someone, while in fact this is not true.\nConsequently, we will not offer that user to read the material. On the other hand, the opposite\nsituation is not possible: we will never display to a user a recommendation of an article that he/she\nhas already read.\nAt this point it is worth checking how much we gain by accepting the inconvenience described\nabove. I have prepared two implementations of a program that adds to a set of values and then\nchecks if they are there.\nThe first program uses the classic approach — the HashSet class, while the second uses the Bloom\nFilter available in the popular guava library.\nAgain, using jConsole we register for both programs the amount of memory used, and additionally — for the version with\nthe Bloom Filter we also check the\nnumber of false positives. This value can be easily controlled, as the maximum allowed false positive\nrate can be set in the API; for needs of the following tests we will set it to 1%.\nMoreover, we will measure the total time of adding values to the set and the total time of querying\nwhether there are values in the set.\nSame as before we will perform a number of tests using the following parameters: n — the size of the set of\nnumbers, and f — what part of it should be added to the set. The configuration n=1 000 000 and f=0.1 means\nthat the first 100 000 numbers out of 1 000 000 will be added to the set. So, in the first part, the program will\nadd 100 000 numbers to the set and then — in the second stage — it will perform a presence test\nby checking whether the numbers above 100 000 belong to the set. There is no point in checking the\nnumbers added to the set beforehand, because we know that Bloom Filters do not give false\nnegative results. On the other hand, if any number above 100 000 is found according to the Bloom Filter in\nthe set, we will consider it a false positive.\nFollowing code snippet presents fragment of the Bloom Filter variant:\n\nval insertions = (n * f).toInt()\nval filter = BloomFilter.create(Funnels.integerFunnel(), insertions, 0.01)\nvar falsePositives = 0\n\nval insertTime = measureTimeMillis {\n    for (i in 0 until insertions) {\n        filter.put(i)\n    }\n}\n\nval queryTime = measureTimeMillis {\n    for (i in insertions until n) {\n        if (filter.mightContain(i)) {\n            falsePositives++;\n        }\n    }\n}\n\nval fpRatio = falsePositives/n.toDouble()\n\n\nAgain — you can find full code of both programs on aforementioned repository.\nLet’s start with the following configuration: n=10 000 000/f=0.1:\nn=10 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      81\n      293\n    \nquery time [ms]\n      82\n      846\n    \nmemory [MB]\n      94\n      30\n    \nAs you can see, the Bloom Filter returned less than 1% false results, but — at the same time — it used three times\nless memory than HashSet variant. Unfortunately, the times of Bloom Filter’s version are significantly higher.\nLet’s check what happens when we increase the size of the input set:\nn=100 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      593\n      318\n    \nquery time [ms]\n      988\n      944\n    \nmemory [MB]\n      876\n      29\n    \nn=500 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      1975\n      1372\n    \nquery time [ms]\n      4115\n      4923\n    \nmemory [MB]\n      4400\n      81\n    \nThe number of false positives is still below the preset 1%, the amount of memory used is still lower\nthan the classic implementation, and interestingly also the times of the probabilistic variant are\nlower, at least for inserting. Thus, it can be seen that along with the increase in the size of the data the benefit of this\nmethod increases.\nSummary\nThe above results clearly show that by accepting a small share of false answers, we can gain significant savings in memory usage.\nSimilarly to the HLL algorithm, the structure based on the Bloom Filters is available in many popular\ndatabases like Redis,\nHBase\nor Cassandra.\nThe simple experiments we conducted showed that probabilistic algorithms can save a lot\nof memory, which is especially important if our database stores huge amounts of data. In such cases it\nis sometimes worth letting your database lie to you a little.","guid":"https://blog.allegro.tech/2022/10/probabilistic-algorithms.html","categories":["tech","performance","NoSQL"],"isoDate":"2022-10-03T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Example of modularization in Allegro Pay Android application","link":"https://blog.allegro.tech/2022/09/example-of-modularization-in-allegro-pay-android-application.html","pubDate":"Mon, 26 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Kwiatek"],"photo":["https://blog.allegro.tech/img/authors/michal.kwiatek.jpg"],"url":["https://blog.allegro.tech/authors/michal.kwiatek"]}]},"content":"<p>Currently, in the Android world, the topic of modularization is very popular. Many bloggers describe their experiences\nwith it and analyze what <a href=\"https://developer.android.com/topic/modularization/patterns\">Google recommends</a>. Our team\nstarted the modularization process before it was hot. I will describe our reasons, decisions, problems and give you some\nadvice. We will see if modularization makes sense and what it brings to the table. I will also post some statistics\nshowing what it looked like before and after the modularization process.</p>\n\n<h2 id=\"some-theory\">Some theory</h2>\n\n<h3 id=\"module\">Module</h3>\n\n<blockquote>\n  <p>A <a href=\"https://developer.android.com/studio/projects#:~:text=inside%20your%20project.-,Modules,test%2C%20and%20debug%20each%20module\">module</a>\nis a collection of source files and build settings that allow you to divide your project into discrete units of\nfunctionality. Your project can have one or many modules, and one module may use another module as a dependency. You can\nindependently build, test, and debug each module.</p>\n</blockquote>\n\n<h3 id=\"background\">Background</h3>\n\n<p>Allegro Pay is a payment method on Allegro that allows you to postpone the payment by 30 days or divide it into smaller\nparts. People who use Allegro Pay know how many functionalities it has, those who don’t use it yet will know after\nreading this article. It started from 3 modules. At the time of writing this article the Allegro application for the\nAndroid platform consists of over 120 modules, 9 of which are maintained by Allegro Pay Team. In this quarter, we\nfocused on extracting several domains (features) from the main Allegro Pay module into separate, smaller and specialized\nmodules.</p>\n\n<h2 id=\"what-made-us-start-the-modularization-process\">What made us start the modularization process?</h2>\n\n<p>The main reason for the modularization process was the build time of one of these 3 modules — containing the entire\nAllegro Pay domain. Our internal monitoring tools showed that build times started to average 100 seconds, and at their\nworst point grew to just over 120 seconds. The module contains over 40k LoC (lines of code). In addition, we faced\nproblems when introducing changes, such as conflicts or the possibility of accidental modification of another\nfunctionality.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/before_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h3 id=\"cheat\">Cheat</h3>\n\n<p>I mention the build time for a reason. In our case, in a multi-module project, we use some Gradle instructions. Our\n<code class=\"language-plaintext highlighter-rouge\">gradle.properties</code> file looks something like this:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// some instructions</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">parallel</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">configureondemand</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">caching</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"c1\">// more instructions</span>\n</code></pre></div></div>\n\n<p>The first instruction\nenables <a href=\"https://docs.gradle.org/current/userguide/performance.html#parallel_execution\">parallelization</a>\nso that Gradle can perform more than one task at a time as long as the tasks are in different modules. The second one\nallows you to\n<a href=\"https://docs.gradle.org/current/userguide/multi_project_configuration_and_execution.html#sec:configuration_on_demand\">configure modules</a>\nthat are relevant only to the task you want, rather than configuring them all, which is the default behavior.\nImportantly, this instruction should be used for multi-module projects. And the last one is\n<a href=\"https://docs.gradle.org/current/userguide/build_cache.html\">caching</a>. It is „a cache mechanism that aims to save time\nby reusing outputs produced by other builds. The build cache works by storing (locally or remotely) build outputs and\nallowing builds to fetch these outputs from the cache when it is determined that inputs have not changed, avoiding the\nexpensive work of regenerating them.” By default, the build cache is disabled.</p>\n\n<h2 id=\"refinement-decisions-and-plans\">Refinement, decisions and plans</h2>\n\n<p>At one of the weekly meetings, we discussed how to solve the problem of the growing module and the increasing number of\ndependencies and functionalities. We decided that the best way would be to extract several domains (features) into\nseparate modules. Every new module should contain the implemented part of the domain that it represents according to the\nname and a small contract module that can be attached to other modules in order to provide them with the implemented\nfunctionality. So, we have planned the following modules:</p>\n\n<ol>\n  <li>ais (a banking service that isn’t relevant in the context of this article) with contract module,</li>\n  <li>common,</li>\n  <li>consolidation with contract module,</li>\n  <li>onboarding with contract module,</li>\n  <li>overpayment with contract module,</li>\n  <li>repayment with contract module.</li>\n</ol>\n\n<h2 id=\"contract\">Contract</h2>\n\n<p>The contract is a special module containing all the necessary interfaces, classes and methods that allow you to use the\nfunctionality in other places in an easy way. It is defined inside the module containing the functionality\nimplementation. It should be emphasized here that the implementation module can only be based on a contract. This\nsolution means that every developer working on the project knows where to find the necessary information and interfaces\nto run any feature.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">interface</span> <span class=\"nc\">AllegroPaySomeProcessHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">createSomeIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nc\">Context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">:</span> <span class=\"nc\">OtherData</span><span class=\"p\">):</span> <span class=\"nc\">Intent</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">observeSomeResult</span><span class=\"p\">():</span> <span class=\"nc\">Observable</span><span class=\"p\">&lt;</span><span class=\"nc\">SomeResultEvent</span><span class=\"p\">&gt;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">internal</span> <span class=\"kd\">class</span> <span class=\"nc\">AllegroPaySomeProcessHandlerImpl</span> <span class=\"p\">:</span> <span class=\"nc\">AllegroPaySomeProcessHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">createSomeIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nc\">Context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">:</span> <span class=\"nc\">OtherData</span><span class=\"p\">):</span> <span class=\"nc\">Intent</span> <span class=\"p\">=</span>\n        <span class=\"nc\">SomeActivity</span><span class=\"p\">.</span><span class=\"nf\">getIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">observeSomeResult</span><span class=\"p\">():</span> <span class=\"nc\">Observable</span><span class=\"p\">&lt;</span><span class=\"nc\">SomeResultEvent</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nc\">DataBus</span><span class=\"p\">.</span><span class=\"nf\">listen</span><span class=\"p\">(</span><span class=\"nc\">SomeResultEvent</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>The above example shows what one of the assumptions of object-oriented programming — encapsulation — looks like in\npractice. The <em>AllegroPaySomeProcessHandler</em> interface provides two methods, one of them creates\nthe <a href=\"https://developer.android.com/reference/android/content/Intent\">Intent</a> necessary to run the process, and the other\nobserves its result. The exact implementation is hidden in an internal class, not accessible from the contract module.\nEvery change of interface implementation is transparent to contract clients. Example of how to declare a dependency on a\ncontract:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nf\">dependencies</span> <span class=\"p\">{</span>\n    <span class=\"n\">implementation</span> <span class=\"nf\">project</span> <span class=\"p\">(</span><span class=\"err\">'</span><span class=\"p\">:</span><span class=\"n\">allegropay-some</span><span class=\"p\">:</span><span class=\"n\">contract</span><span class=\"err\">'</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"tool\">Tool</h2>\n\n<p>The Allegro application consists of many modules and it is important to provide programmers with the right tools to work\neffectively. In the organization, the delivery of this type of tools is handled by the core team. A tool that allows us\nto check whether our module meets the requirement set for it is\nthe <a href=\"https://github.com/jraska/modules-graph-assert\">Module Graph Assert</a>. It is a Gradle plugin which „helps keep your\nmodule graph healthy and lean.” This tool defines the types of modules that are allowed in the application, the\ndependencies between them and the height of the dependency tree. The following types are defined in the Allegro\napplication: <em>App</em>, <em>Feature</em>, <em>Contract</em>, <em>Library</em>, <em>Util</em> and <em>NeedsMigration</em>. The last type tells us that the\nmodule still requires work from its owners and appropriate adaptation to one of the other types. We can also define\nallowed and restricted dependencies between modules, e.g. a contract may depend only on another contract or a module\nmarked as a feature depends only on the contract or library. Allegro app configuration:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">moduleGraphAssert</span> <span class=\"o\">{</span>\n    <span class=\"n\">maxHeight</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n    <span class=\"n\">allowed</span> <span class=\"o\">=</span> <span class=\"o\">[</span>\n        <span class=\"s1\">'App -&gt; Feature'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Library'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Util'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Library'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Util'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Contract -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'NeedsMigration -&gt; .*'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'.* -&gt; NeedsMigration'</span><span class=\"o\">,</span>\n    <span class=\"o\">]</span>\n    <span class=\"n\">restricted</span> <span class=\"o\">=</span> <span class=\"o\">[</span>\n        <span class=\"s1\">'Contract -X&gt; NeedsMigration'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Library -X&gt; .*'</span>\n    <span class=\"o\">]</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"initial-modules\">Initial modules</h2>\n\n<p>The first separated feature module was overpayment. We immediately prepared a common module containing functionalities\nused in more than one Allegro Pay module. The contract that is shown earlier contains one method returning an Intent\nneeded to run the overpayment process. The feature module includes user-visible screens, use cases and network\ncommunication. Several thousand lines of code were added to this module and the time needed to build the main Allegro\nPay module was shortened. At that time, the build time of the main module was around 87.5 seconds, common and\noverpayment modules around 10.5 seconds.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/first_modules_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"following-modules\">Following modules</h2>\n\n<p>In the next stages, we separated the ais, consolidation and repayment modules. The current values of the build times of\nindividual modules are around 33.7 seconds for the Allegro Pay main module, 13.4 seconds for the ais, 12 seconds for the\nconsolidation, 10.6 seconds for the repayment. The extraction process was analogous to that of the first module.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/few_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"onboarding-module\">Onboarding module</h2>\n\n<p>This module was the most challenging and possibly the most time consuming. This was due to the combination of the\nprocess being available from multiple screens in different modules and ensuring unchanged functionality. During this\nmodularization process, we discovered the possibility of optimizing and reducing the amount of code. This module\ncontains approximately 10k LoC and the build time is less than 20 seconds. It is a really huge module.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/onboarding_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"other-two-modules\">Other two modules</h2>\n\n<p>If you remember, I mentioned three modules at the beginning of this text. So far, I have described the division of the\nlargest module. Let me now describe others in more detail. The first is the special analytical module. Includes an\nexternal library and a small contract. It was created at the same time as the main Allegro Pay module. The current value\nof the build time is 3 seconds and the module has more than 150 lines of code.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/sms_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<p>The second is the SMS verification module. It contains a functionality that allows users to authorize operations by\nproviding SMS code. Currently, it is used in the processes of buying, consolidation, onboarding and overpayment. We only\nwrote a contract here, which provides a universal and easy interface. The build time is approximately 9 seconds and the\nmodule contains almost 2k lines of code.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/sa_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"fin\">Fin</h2>\n\n<p>Probably for some of you, the division method used may be associated with the Latin term <em>divide et impera</em>. This\nparadigm of algorithm design could also be used in the modularization process by dividing one large module into several\nsmaller ones, each specialized in one task. The use of the concept of this paradigm, encapsulation by creating a\ncontract and Gradle configuration allowed to significantly reduce the build time and speed up the development of the\napplication. This solution introduces consistency in the module and decreases the possibility of introducing a\nregression by encapsulating each individual domain. Also the problem with the redundant conflicts has been minimalized.\nAfter the implementation of the modules described above, the main module containing the Allegro Pay responsibilities has\nshrunk significantly, and now contains around 18.4k LoC (which means it was reduced by half). In addition,\nmodularization will allow us to add new features and extend the existing ones in an easier and safer way. It was an\ninteresting challenge from a technical point of view.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/after_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n","contentSnippet":"Currently, in the Android world, the topic of modularization is very popular. Many bloggers describe their experiences\nwith it and analyze what Google recommends. Our team\nstarted the modularization process before it was hot. I will describe our reasons, decisions, problems and give you some\nadvice. We will see if modularization makes sense and what it brings to the table. I will also post some statistics\nshowing what it looked like before and after the modularization process.\nSome theory\nModule\nA module\nis a collection of source files and build settings that allow you to divide your project into discrete units of\nfunctionality. Your project can have one or many modules, and one module may use another module as a dependency. You can\nindependently build, test, and debug each module.\nBackground\nAllegro Pay is a payment method on Allegro that allows you to postpone the payment by 30 days or divide it into smaller\nparts. People who use Allegro Pay know how many functionalities it has, those who don’t use it yet will know after\nreading this article. It started from 3 modules. At the time of writing this article the Allegro application for the\nAndroid platform consists of over 120 modules, 9 of which are maintained by Allegro Pay Team. In this quarter, we\nfocused on extracting several domains (features) from the main Allegro Pay module into separate, smaller and specialized\nmodules.\nWhat made us start the modularization process?\nThe main reason for the modularization process was the build time of one of these 3 modules — containing the entire\nAllegro Pay domain. Our internal monitoring tools showed that build times started to average 100 seconds, and at their\nworst point grew to just over 120 seconds. The module contains over 40k LoC (lines of code). In addition, we faced\nproblems when introducing changes, such as conflicts or the possibility of accidental modification of another\nfunctionality.\n\nCheat\nI mention the build time for a reason. In our case, in a multi-module project, we use some Gradle instructions. Our\ngradle.properties file looks something like this:\n\n// some instructions\norg.gradle.parallel = true\norg.gradle.configureondemand = true\norg.gradle.caching = true\n// more instructions\n\n\nThe first instruction\nenables parallelization\nso that Gradle can perform more than one task at a time as long as the tasks are in different modules. The second one\nallows you to\nconfigure modules\nthat are relevant only to the task you want, rather than configuring them all, which is the default behavior.\nImportantly, this instruction should be used for multi-module projects. And the last one is\ncaching. It is „a cache mechanism that aims to save time\nby reusing outputs produced by other builds. The build cache works by storing (locally or remotely) build outputs and\nallowing builds to fetch these outputs from the cache when it is determined that inputs have not changed, avoiding the\nexpensive work of regenerating them.” By default, the build cache is disabled.\nRefinement, decisions and plans\nAt one of the weekly meetings, we discussed how to solve the problem of the growing module and the increasing number of\ndependencies and functionalities. We decided that the best way would be to extract several domains (features) into\nseparate modules. Every new module should contain the implemented part of the domain that it represents according to the\nname and a small contract module that can be attached to other modules in order to provide them with the implemented\nfunctionality. So, we have planned the following modules:\nais (a banking service that isn’t relevant in the context of this article) with contract module,\ncommon,\nconsolidation with contract module,\nonboarding with contract module,\noverpayment with contract module,\nrepayment with contract module.\nContract\nThe contract is a special module containing all the necessary interfaces, classes and methods that allow you to use the\nfunctionality in other places in an easy way. It is defined inside the module containing the functionality\nimplementation. It should be emphasized here that the implementation module can only be based on a contract. This\nsolution means that every developer working on the project knows where to find the necessary information and interfaces\nto run any feature.\n\ninterface AllegroPaySomeProcessHandler {\n\n    fun createSomeIntent(context: Context, someId: String, otherData: OtherData): Intent\n\n    fun observeSomeResult(): Observable<SomeResultEvent>\n}\n\n\n\ninternal class AllegroPaySomeProcessHandlerImpl : AllegroPaySomeProcessHandler {\n\n    override fun createSomeIntent(context: Context, someId: String, otherData: OtherData): Intent =\n        SomeActivity.getIntent(context, someId, otherData)\n\n    override fun observeSomeResult(): Observable<SomeResultEvent> =\n        DataBus.listen(SomeResultEvent::class.java)\n}\n\n\nThe above example shows what one of the assumptions of object-oriented programming — encapsulation — looks like in\npractice. The AllegroPaySomeProcessHandler interface provides two methods, one of them creates\nthe Intent necessary to run the process, and the other\nobserves its result. The exact implementation is hidden in an internal class, not accessible from the contract module.\nEvery change of interface implementation is transparent to contract clients. Example of how to declare a dependency on a\ncontract:\n\ndependencies {\n    implementation project (':allegropay-some:contract')\n}\n\n\nTool\nThe Allegro application consists of many modules and it is important to provide programmers with the right tools to work\neffectively. In the organization, the delivery of this type of tools is handled by the core team. A tool that allows us\nto check whether our module meets the requirement set for it is\nthe Module Graph Assert. It is a Gradle plugin which „helps keep your\nmodule graph healthy and lean.” This tool defines the types of modules that are allowed in the application, the\ndependencies between them and the height of the dependency tree. The following types are defined in the Allegro\napplication: App, Feature, Contract, Library, Util and NeedsMigration. The last type tells us that the\nmodule still requires work from its owners and appropriate adaptation to one of the other types. We can also define\nallowed and restricted dependencies between modules, e.g. a contract may depend only on another contract or a module\nmarked as a feature depends only on the contract or library. Allegro app configuration:\n\nmoduleGraphAssert {\n    maxHeight = 5\n    allowed = [\n        'App -> Feature',\n        'App -> Library',\n        'App -> Util',\n        'App -> Contract',\n        'Feature -> Library',\n        'Feature -> Util',\n        'Feature -> Contract',\n        'Contract -> Contract',\n        'NeedsMigration -> .*',\n        '.* -> NeedsMigration',\n    ]\n    restricted = [\n        'Contract -X> NeedsMigration',\n        'Library -X> .*'\n    ]\n}\n\n\nInitial modules\nThe first separated feature module was overpayment. We immediately prepared a common module containing functionalities\nused in more than one Allegro Pay module. The contract that is shown earlier contains one method returning an Intent\nneeded to run the overpayment process. The feature module includes user-visible screens, use cases and network\ncommunication. Several thousand lines of code were added to this module and the time needed to build the main Allegro\nPay module was shortened. At that time, the build time of the main module was around 87.5 seconds, common and\noverpayment modules around 10.5 seconds.\n\nFollowing modules\nIn the next stages, we separated the ais, consolidation and repayment modules. The current values of the build times of\nindividual modules are around 33.7 seconds for the Allegro Pay main module, 13.4 seconds for the ais, 12 seconds for the\nconsolidation, 10.6 seconds for the repayment. The extraction process was analogous to that of the first module.\n\nOnboarding module\nThis module was the most challenging and possibly the most time consuming. This was due to the combination of the\nprocess being available from multiple screens in different modules and ensuring unchanged functionality. During this\nmodularization process, we discovered the possibility of optimizing and reducing the amount of code. This module\ncontains approximately 10k LoC and the build time is less than 20 seconds. It is a really huge module.\n\nOther two modules\nIf you remember, I mentioned three modules at the beginning of this text. So far, I have described the division of the\nlargest module. Let me now describe others in more detail. The first is the special analytical module. Includes an\nexternal library and a small contract. It was created at the same time as the main Allegro Pay module. The current value\nof the build time is 3 seconds and the module has more than 150 lines of code.\n\nThe second is the SMS verification module. It contains a functionality that allows users to authorize operations by\nproviding SMS code. Currently, it is used in the processes of buying, consolidation, onboarding and overpayment. We only\nwrote a contract here, which provides a universal and easy interface. The build time is approximately 9 seconds and the\nmodule contains almost 2k lines of code.\n\nFin\nProbably for some of you, the division method used may be associated with the Latin term divide et impera. This\nparadigm of algorithm design could also be used in the modularization process by dividing one large module into several\nsmaller ones, each specialized in one task. The use of the concept of this paradigm, encapsulation by creating a\ncontract and Gradle configuration allowed to significantly reduce the build time and speed up the development of the\napplication. This solution introduces consistency in the module and decreases the possibility of introducing a\nregression by encapsulating each individual domain. Also the problem with the redundant conflicts has been minimalized.\nAfter the implementation of the modules described above, the main module containing the Allegro Pay responsibilities has\nshrunk significantly, and now contains around 18.4k LoC (which means it was reduced by half). In addition,\nmodularization will allow us to add new features and extend the existing ones in an easier and safer way. It was an\ninteresting challenge from a technical point of view.","guid":"https://blog.allegro.tech/2022/09/example-of-modularization-in-allegro-pay-android-application.html","categories":["tech","kotlin","mobile","android","modularization","gradle","allegro-pay"],"isoDate":"2022-09-25T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999880597874","name":"Front-End Software Engineer - Delivery Experience","uuid":"b43c3e88-1ee5-4ea1-9145-8fdc256ae1e6","jobAdId":"05b6643a-43dd-46a2-b998-bfbaa0a31c68","refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-01-24T15:22:51.859Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Katowice, Lublin, Gdańsk, Łódź","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"743f6067-ce19-4a83-9a0d-10d49cd63004","valueLabel":"Delivery Experience"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999880597874","creator":{"name":"Martyna Maziarska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999880589538","name":"Software Engineer (Scala) - Data&AI","uuid":"05e4ec2c-7ebb-4f32-b7c8-99ab8f2de44d","jobAdId":"4bdac056-8e6f-43d1-9437-9b396cefea0d","refNumber":"REF3897K","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-01-24T14:49:57.433Z","location":{"city":"Poznań, Warszawa, Kraków","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"3df4cd37-be2f-409a-8871-60e1c2319007","valueLabel":"Nie"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"50675cc6-acd3-46a5-901c-54e68167e826","valueLabel":"Nie"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"c4ab8d4b-916c-49d3-9ffb-bad301fb62f6","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"25416dd5-3ba7-4e71-ab12-7b3af24269dc","valueLabel":"Nie"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999880589538","creator":{"name":"Martyna Stafa"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999880589545","name":"Software Engineer (Scala) - Data&AI","uuid":"12cbd8f4-7d63-411f-adbe-172ca66db674","jobAdId":"3e89e7a1-e30b-470f-9cc7-28bf8d6f8fcc","refNumber":"REF3897K","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-01-24T14:49:25.495Z","location":{"city":"Poznań, Warszawa, Kraków","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"3df4cd37-be2f-409a-8871-60e1c2319007","valueLabel":"Nie"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"50675cc6-acd3-46a5-901c-54e68167e826","valueLabel":"Nie"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"a6765624-e047-4a26-9481-9621086d8b96","valueLabel":"Nie"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"c4ab8d4b-916c-49d3-9ffb-bad301fb62f6","valueLabel":"Nie"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"31873284-1e97-427d-8918-6ce504344351","valueLabel":"Nie"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"25416dd5-3ba7-4e71-ab12-7b3af24269dc","valueLabel":"Nie"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999880589545","creator":{"name":"Martyna Stafa"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999879703298","name":"Front-End Software Engineer - Delivery Experience","uuid":"d0a50962-ce05-4b2e-9e25-8f6300775e39","jobAdId":"89c1b056-3e0e-4544-b8ed-001c7833fe0f","refNumber":"REF3941R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-01-20T10:23:52.994Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Katowice, Lublin, Gdańsk, Łódź","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"743f6067-ce19-4a83-9a0d-10d49cd63004","valueLabel":"Delivery Experience"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999879703298","creator":{"name":"Paulina Siwek"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999879329833","name":"Senior Software Engineer (Java/Kotlin) - Delivery Experience","uuid":"ac0b310c-6564-444a-9277-8c1a5c5f6eb6","jobAdId":"bd251e77-8c7b-4496-90d8-f824c3193f46","refNumber":"REF3176R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2023-01-19T09:15:20.654Z","location":{"city":"Warszawa, Poznań, Kraków, Wrocław, Gdańsk, Katowice, Lublin, Łódź, Toruń","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"engineering","label":"Engineering"},"typeOfEmployment":{"id":"permanent","label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"743f6067-ce19-4a83-9a0d-10d49cd63004","valueLabel":"Delivery Experience"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999879329833","creator":{"name":"Monika Walaszek"},"language":{"code":"en","label":"English","labelNative":"English (US)"}}],"events":[{"created":1673254351000,"duration":7200000,"id":"290822249","name":"Allegro Tech Live #33 - iOS od podszewki","date_in_series_pattern":false,"status":"past","time":1674147600000,"local_date":"2023-01-19","local_time":"18:00","updated":1674157831000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":39,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/290822249/","description":"**➡ Transmisja i nagranie ze spotkania:** **[https://www.youtube.com/watch?v=c-AIoOnby3M](https://www.youtube.com/watch?v=c-AIoOnby3M)** **Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w…","visibility":"public","member_pay_fee":false},{"created":1669910743000,"duration":7200000,"id":"290082009","name":"Allegro Tech Live #32 - Storage (nie)standardowy","date_in_series_pattern":false,"status":"past","time":1671123600000,"local_date":"2022-12-15","local_time":"18:00","updated":1671134039000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":53,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/290082009/","description":"**➡ Transmisja spotkania:** **[https://www.youtube.com/watch?v=fQLyw_IpaPQ ](https://www.youtube.com/watch?v=fQLyw_IpaPQ)** **Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach,…","visibility":"public","member_pay_fee":false},{"created":1667908912000,"duration":5400000,"id":"289621472","name":"Allegro Tech Live #31 - Frontend: reporting i optymalizacje","date_in_series_pattern":false,"status":"past","time":1669914000000,"local_date":"2022-12-01","local_time":"18:00","updated":1669925130000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":43,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/289621472/","description":"**➡ Rejestracja:** https://app.evenea.pl/event/allegro-tech-talk-31 **Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym…","visibility":"public","member_pay_fee":false},{"created":1664275530000,"duration":5400000,"id":"288748190","name":"Allegro Tech Live #30 - Dług technologiczny - jak go spłacić i nie zbankrutować","date_in_series_pattern":false,"status":"past","time":1665676800000,"local_date":"2022-10-13","local_time":"18:00","updated":1665684962000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/288748190/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-30](https://app.evenea.pl/event/allegro-tech-talk-30) UWAGA: Z różnych powodów (mniej lub bardziej zależnych od nas) czwartkowe spotkanie Allegro Tech Talk przenosimy w pełni do świata online. Chociaż…","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"S03E10 - Zosia Śmierzchalska, Jakub Dodot - O tym jak przygotowujemy rozwiązania dla klientów w oparciu o badania","link":"https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/","pubDate":"Thu, 12 Jan 2023 00:00:00 GMT","content":"W jaki sposób przygotowujemy rozwiązania dla klientów Allegro w oparciu o badania? Jak wygląda ścieżka projektu od eksploracji do wdrożenia i późniejszego monitorowania? Jaką korzyść dają badania usability? Dlaczego warto, aby badanie było prowadzone przez dwoje badaczy? O współpracy między badaczami i projektantami UX rozmawialiśmy z Zofią Śmierzchalską - Design Managerką i Jakubem Dodotem - Senior UX Research Managerem w Allegro.","contentSnippet":"W jaki sposób przygotowujemy rozwiązania dla klientów Allegro w oparciu o badania? Jak wygląda ścieżka projektu od eksploracji do wdrożenia i późniejszego monitorowania? Jaką korzyść dają badania usability? Dlaczego warto, aby badanie było prowadzone przez dwoje badaczy? O współpracy między badaczami i projektantami UX rozmawialiśmy z Zofią Śmierzchalską - Design Managerką i Jakubem Dodotem - Senior UX Research Managerem w Allegro.","guid":"https://podcast.allegro.tech/o-rozwiazaniach-opartych-na-badaniach/","isoDate":"2023-01-12T00:00:00.000Z"},{"title":"S03E09 - Adrianna Dworniak, Łukasz Gomółka - O Allegro Family","link":"https://podcast.allegro.tech/o-allegro-family/","pubDate":"Thu, 15 Dec 2022 00:00:00 GMT","content":"Czy współpraca dwóch Product Managerów przy jednym produkcie jest możliwa i jak zadbać o jej efektywność? Czym jest “churn” i na czym polega w branży e-commerce? Co wspólnego z błędnymi rekomendacjami zakupowymi mają maty do ćwiczeń? Jak rozwiązywać wielopłaszczyznowe problemy, dzięki kompleksowemu zrozumieniu perspektywy klienta i bez wykorzystywania restrykcji? Jak powstawało Allegro Family oraz dlaczego czasem warto zacząć weryfikację pomysłu na produkt od rozmowy z… teściową? Na te i inne pytania odpowiadają  Adrianna Dworniak - Senior Product Manager i Łukasz Gomółka - Product Team Manager w Allegro.","contentSnippet":"Czy współpraca dwóch Product Managerów przy jednym produkcie jest możliwa i jak zadbać o jej efektywność? Czym jest “churn” i na czym polega w branży e-commerce? Co wspólnego z błędnymi rekomendacjami zakupowymi mają maty do ćwiczeń? Jak rozwiązywać wielopłaszczyznowe problemy, dzięki kompleksowemu zrozumieniu perspektywy klienta i bez wykorzystywania restrykcji? Jak powstawało Allegro Family oraz dlaczego czasem warto zacząć weryfikację pomysłu na produkt od rozmowy z… teściową? Na te i inne pytania odpowiadają  Adrianna Dworniak - Senior Product Manager i Łukasz Gomółka - Product Team Manager w Allegro.","guid":"https://podcast.allegro.tech/o-allegro-family/","isoDate":"2022-12-15T00:00:00.000Z"},{"title":"S03E08 - Michał Wiśniewski - O pracy w Allegro Ads","link":"https://podcast.allegro.tech/o-pracy-w-allegro-ads/","pubDate":"Thu, 01 Dec 2022 00:00:00 GMT","content":"Za co odpowiada, jak działa i co oferuje klientom Allegro Ads? Co charakteryzuje DSP (Demand Side Platform), jeden z produktów w ekosystemie Allegro Ads? Czego dotyczy wyśrubowanie wymagań technicznych w tym produkcie i z jakimi wyzwaniami mierzymy się biznesowo? Skąd biorą się pomysły na rozwój Allegro Ads? I wreszcie - jak wyglądała droga Michała od programu e-xperience do roli Junior Software Engineera oraz czym zaskoczyła go najbardziej? Posłuchajcie kolejnego odcinka Allegro Tech Podcast.","contentSnippet":"Za co odpowiada, jak działa i co oferuje klientom Allegro Ads? Co charakteryzuje DSP (Demand Side Platform), jeden z produktów w ekosystemie Allegro Ads? Czego dotyczy wyśrubowanie wymagań technicznych w tym produkcie i z jakimi wyzwaniami mierzymy się biznesowo? Skąd biorą się pomysły na rozwój Allegro Ads? I wreszcie - jak wyglądała droga Michała od programu e-xperience do roli Junior Software Engineera oraz czym zaskoczyła go najbardziej? Posłuchajcie kolejnego odcinka Allegro Tech Podcast.","guid":"https://podcast.allegro.tech/o-pracy-w-allegro-ads/","isoDate":"2022-12-01T00:00:00.000Z"},{"title":"S03E07 - Patrycja Haraburda, Artur Chabera - O pracy Mobile Software Engineerów w Allegro","link":"https://podcast.allegro.tech/o-pracy-mobile-engineerow-w-allegro/","pubDate":"Thu, 17 Nov 2022 00:00:00 GMT","content":"Jak wygląda codzienność Mobile Software Engineerów w Allegro i skala, z którą pracują? Jak przebiega proces developmentu w przypadku aplikacji mobilnych? Jak radzimy sobie z łączeniem interesów różnych zespołów rozwijających tę samą aplikację i w czym pomaga nam tu modularyzacja? Co nam dają testy usability i dlaczego warto pamiętać o dostępności? Jak integruje się i rozwija społeczność mobilna w Allegro? Wszystko wyjaśnią Wam Patrycja Haraburda i Artur Chabera - Mobile Software Engineerowie w Allegro.","contentSnippet":"Jak wygląda codzienność Mobile Software Engineerów w Allegro i skala, z którą pracują? Jak przebiega proces developmentu w przypadku aplikacji mobilnych? Jak radzimy sobie z łączeniem interesów różnych zespołów rozwijających tę samą aplikację i w czym pomaga nam tu modularyzacja? Co nam dają testy usability i dlaczego warto pamiętać o dostępności? Jak integruje się i rozwija społeczność mobilna w Allegro? Wszystko wyjaśnią Wam Patrycja Haraburda i Artur Chabera - Mobile Software Engineerowie w Allegro.","guid":"https://podcast.allegro.tech/o-pracy-mobile-engineerow-w-allegro/","isoDate":"2022-11-17T00:00:00.000Z"}]},"__N_SSG":true}