{"pageProps":{"posts":[{"title":"Evaluating performance of time series collections","link":"https://blog.allegro.tech/2021/12/performance-evaluation-of-timeseries.html","pubDate":"Mon, 20 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>A few years ago, I was working on a new version of <a href=\"https://allegro.tech\">Allegro</a> purchase ratings.\nIt was a time of a pretty large revolution in the rating system when we moved this product away from our monolith, also\nintroducing quite significant changes to the concept of purchase rating itself. Replacing the system of positive, neutral\nor negative rating, we introduced an approach based on “thumbs up” and “thumbs down” as well as the option to rate several\nelements of the purchase separately: the time and cost of delivery, or products’ conformity with its description. The\nproduct-related revolution was accompanied by a major change in technology. Apart from transitioning towards the\nmicroservices architecture, we also decided to migrate data from a large relational database to MongoDB. There were many\nreasons for this decision: from the non-relational nature of our model, through the need for easier scaling, to the wish\nfor cost reduction. Upon completion of the works, we were for the most part content with the decision that we made. The\nnew solution was more user-friendly, easier to maintain and worked smoothly. The sole exception was aggregation queries,\nspecifically: determining the average of seller ratings in a specified period of time. While at the level of the 99th\npercentile times were very low, some queries were much slower. We spent a lot of time optimising both queries and the\ncode, and had to use some programming tricks to achieve satisfactory results. While we were able to solve our problems in\nthe end, the final conclusion was that the aggregation of data in large MongoDB collections is quite challenging.</p>\n\n<h2 id=\"to-the-rescue-time-series\">To the rescue: Time series</h2>\n<p>A new version of MongoDB, 5.0, has been recently launched. The list of changes included one that I found particularly\ninteresting: the time series collections. It is a method of effective storing and processing of time-ordered value series.\nA classic example for this case is measuring the temperature of air. These measurements are taken\nperiodically (for instance every hour), and their sequence forms time series. We then often review such data in an\nappropriate order, as well as calculate the maximum and minimum values, or the arithmetic mean. Therefore, in the said\ncase of use, a database must be highly efficient when saving the data, store records in a compact manner due to the\nlarge number thereof, and must quickly calculate aggregates. Although in the case of temperature readings database write\noperations are made on a regular basis, it turns out that in the case of time series it is not mandatory, and the only\nthing that truly matters is the presence of time. While reading about this topic, I instantly remembered my countless\nlate nights struggling with slow ratings aggregations. Therefore, I decided to explore this topic and see how the\nsolution works in practice.</p>\n\n<p>Before the release of MongoDB 5.0, the only way to efficiently process time series was to store pre-calculated\naggregates, or use a <a href=\"https://www.mongodb.com/blog/post/building-with-patterns-the-bucket-pattern\">bucket pattern</a>,\nwhich was obviously associated with additional work and complexity of the\ncode. Now, things have been made much easier as this additional complexity is covered by a convenient abstraction. In\nMongoDB, time series are not actual collections, but materialised views that cover physical collections. This\nabstraction is intended to simplify complicated operations based on “buckets” of documents. You can read more about the\nconcept of storing data in the new kind of collection on the MongoDB <a href=\"https://www.mongodb.com/developer/how-to/new-time-series-collections/\">official blog</a>.\nEveryone who is interested in using this solution should take a look at it. In my article, on the other hand, I would\nlike to verify whether the processing of time series is really as fast as promised by the authors.</p>\n\n<h2 id=\"data-preparation\">Data preparation</h2>\n<p>For the purposes of our considerations I will use a simple document describing the rating in the form of:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"rating\" : 2.0\n}\n</code></pre></div></div>\n\n<p>Attentive readers will immediately notice the absence of the field containing the ID of the seller whom the rating\nconcerns. It was done intentionally, otherwise I would have to\ncreate an additional index covering this field. However, I did not want to introduce any additional elements in my\nexperiment that could have any impact on the results. Let’s assume for this experiment that we are rating a\nrestaurant, not Allegro sellers, therefore all ratings in the collection concern the restaurant only.</p>\n\n<p>Now we can create two collections storing an identical set of data. One will be a standard collection, and the other will\nbe a time series. The time series collection has to be created manually by indicating the field specifying the time label:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">createCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ts</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"na\">timeseries</span><span class=\"p\">:</span> <span class=\"p\">{</span> <span class=\"na\">timeField</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span> <span class=\"p\">}</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If you did not install Mongo 5.0 from scratch, but updated its previous version, you should make sure that\nit is set to an adequate level of compatibility. Otherwise, the above command will not create a time series collection.\nYou can check it with this command:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">adminCommand</span><span class=\"p\">(</span> <span class=\"p\">{</span> <span class=\"na\">getParameter</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"na\">featureCompatibilityVersion</span><span class=\"p\">:</span> <span class=\"mi\">1</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If the returned value is less than 5.0, you need to issue:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">adminCommand</span><span class=\"p\">(</span> <span class=\"p\">{</span> <span class=\"na\">setFeatureCompatibilityVersion</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">5.0</span><span class=\"dl\">\"</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Upon creating a collection, it is also worth checking that a time series has actually been created:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">runCommand</span><span class=\"p\">(</span> <span class=\"p\">{</span> <span class=\"na\">listCollections</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We look for <code class=\"language-plaintext highlighter-rouge\">\"type\" : \"timeseries\"</code> entry:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n    \"name\" : \"coll-ts\",\n    \"type\" : \"timeseries\",\n    \"options\" : {\n        \"timeseries\" : {\n            \"timeField\" : \"timestamp\",\n            \"granularity\" : \"seconds\",\n            \"bucketMaxSpanSeconds\" : 3600\n        }\n    },\n    \"info\" : {\n        \"readOnly\" : false\n    }\n}\n</code></pre></div></div>\n\n<p>We will also create the second collection manually (although it is not necessary, because it would be created with the\nfirst INSERT command). We will want to check the speed of data search based on time field, so we will create a unique index:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">createCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ord</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">createIndex</span><span class=\"p\">({</span><span class=\"na\">timestamp</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"na\">unique</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>Let’s use the following scripts to fill both collections with 10 million documents:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// Save as fill-ts.js</span>\n<span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ts</span><span class=\"dl\">\"</span><span class=\"p\">).</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"kd\">var</span> <span class=\"nx\">startTime</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"nx\">startTime</span><span class=\"p\">.</span><span class=\"nx\">getTime</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"nx\">i</span> <span class=\"o\">*</span> <span class=\"mi\">60000</span><span class=\"p\">),</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">rating</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">floor</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">random</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n    <span class=\"p\">})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// Save as fill-ord.js</span>\n<span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ord</span><span class=\"dl\">\"</span><span class=\"p\">).</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"kd\">var</span> <span class=\"nx\">startTime</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"nx\">startTime</span><span class=\"p\">.</span><span class=\"nx\">getTime</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"nx\">i</span> <span class=\"o\">*</span> <span class=\"mi\">60000</span><span class=\"p\">),</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">rating</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">floor</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">random</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n    <span class=\"p\">})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>It is worth to measure the execution time of scripts to compare write times to both collections. For this purpose I use\n<code class=\"language-plaintext highlighter-rouge\">time</code> command and <code class=\"language-plaintext highlighter-rouge\">mongo</code> command-line client, <code class=\"language-plaintext highlighter-rouge\">test</code> is the name of my database.\nTo avoid network latency I perform the measurements on my laptop with a local instance of MongoDB version 5.0.3 running.</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill-ts.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill-ord.js\n</code></pre></div></div>\n\n<p>As expected, the filling of the time series collection was faster: 3:30,11 vs 4:39,48. Such a difference can be essential\nif our system performs many write operations in a short period of time. At the very beginning of our experiments\ncollection of a new type comes to the forefront.</p>\n\n<p>Now when our collections already contain data, we can take a look at the size of files. On the\n<a href=\"https://docs.mongodb.com/manual/core/timeseries-collections/\">manual page</a> we can read that:</p>\n\n<blockquote>\n  <p>Compared to normal collections, storing time series data in time series collections improves query efficiency and\nreduces the disk usage</p>\n</blockquote>\n\n<p>Let’s find out how true that is.</p>\n\n<h2 id=\"a-closer-look-at-the-data\">A closer look at the data</h2>\n\n<p>In the first place, it is worth making sure that documents in both collections look the same:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({}).</span><span class=\"nx\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({}).</span><span class=\"nx\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Both documents should be similar to this one:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"_id\" : ObjectId(\"6184126fc42d1ab73d5208e4\"),\n    \"rating\" : 2.0\n}\n</code></pre></div></div>\n\n<p>The documents have the same schema. In addition, the <code class=\"language-plaintext highlighter-rouge\">_id</code> key was automatically generated in both cases, although our\nfilling script did not contain them.</p>\n\n<p>Let’s move on to indexes now and use the commands: <code class=\"language-plaintext highlighter-rouge\">db.getCollection('coll-ord').getIndexes()</code> and <code class=\"language-plaintext highlighter-rouge\">db.getCollection('coll-ts').getIndexes()</code>\nto get the indexes of both collections.</p>\n\n<p>The normal collection has two indexes, one that was created automatically for the <code class=\"language-plaintext highlighter-rouge\">_id</code> key and the one that we created manually:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>// coll-ord:\n[\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"_id\" : 1\n        },\n        \"name\" : \"_id_\"\n    },\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"timestamp\" : 1.0\n        },\n        \"name\" : \"timestamp_1\",\n        \"unique\" : true\n    }\n]\n</code></pre></div></div>\n\n<p>What is interesting is that the time series collection has no index at all:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>// coll-ts:\n[]\n</code></pre></div></div>\n\n<p>The lack of the index for the <code class=\"language-plaintext highlighter-rouge\">_id</code> key of\ncourse means that, by default, the time series collection will have to perform the <code class=\"language-plaintext highlighter-rouge\">COLLSCAN</code> operation if we want to\nsearch documents based on <code class=\"language-plaintext highlighter-rouge\">_id</code>. The index is probably missing simply to save disc space and storage time,\nstemming from the assumption that the time series collections are mainly used to search based on time. The lack of\nthe index for the timestamp field is much more surprising. Does it mean that time-based searches in time series will\nalso cause <code class=\"language-plaintext highlighter-rouge\">COLLCSAN</code> and work slowly? The answer to this question can be found in the documentation:</p>\n<blockquote>\n  <p>The internal index for a time series collection is not displayed</p>\n</blockquote>\n\n<p>So, there actually is an index, but it is different from those created manually, and even from indexes created\nautomatically for the <code class=\"language-plaintext highlighter-rouge\">_id</code> key.\nAs I wrote in another <a href=\"/2021/10/comparing-mongodb-composite-indexes.html\">post</a>, indexes are not all the\nsame, so it’s worth taking a closer look at this one. Let’s check the query execution plans:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">executionStats</span><span class=\"dl\">'</span><span class=\"p\">)</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">executionStats</span><span class=\"dl\">'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>//coll-ord\n\"winningPlan\" : {\n    \"stage\" : \"FETCH\",\n    \"inputStage\" : {\n        \"stage\" : \"IXSCAN\",\n        \"keyPattern\" : {\n            \"timestamp\" : 1.0\n        }\n    },\n    \"indexName\" : \"timestamp_1\",\n}\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>//coll-ts\n\"winningPlan\" : {\n    \"stage\" : \"COLLSCAN\",\n    \"filter\" : {\n        \"$and\" : [\n            {\n                \"_id\" : {\n                    \"$lte\" : ObjectId(\"6171ff00ffffffffffffffff\")\n                }\n            },\n            {\n                \"_id\" : {\n                    \"$gte\" : ObjectId(\"6171f0f00000000000000000\")\n                }\n            },\n            {\n                \"control.max.timestamp\" : {\n                    \"$_internalExprGte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            },\n            {\n                \"control.min.timestamp\" : {\n                    \"$_internalExprLte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            }\n        ]\n    },\n}\n</code></pre></div></div>\n\n<p>It turns out that while in the case of the regular collection the plan shows the use of the index, in the time series\ncollection we see the <code class=\"language-plaintext highlighter-rouge\">COLLSCAN</code> operation. It doesn’t mean that this operation is slow, though. The execution times of\nboth operations were similar. We will move on to a more detailed time comparison in a moment; for now we should only\nnote that the hidden index in the time series collection follows specific rules, it is not only invisible, but it also\ncannot be seen in the execution plan, although it clearly affects the speed of the search.</p>\n\n<p>And what happens if we add sorting?</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({}).</span><span class=\"nx\">sort</span><span class=\"p\">({</span><span class=\"na\">timestamp</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n \"ok\" : 0,\n \"errmsg\" : \"PlanExecutor error during aggregation :: caused by :: Sort exceeded memory limit of 104857600 bytes,\n but did not opt in to external sorting.\",\n \"code\" : 292,\n \"codeName\" : \"QueryExceededMemoryLimitNoDiskUseAllowed\"\n}\n</code></pre></div></div>\n\n<p>Surprise! The internal index for the time series collection does not have a sorting feature. This means that if we add\nthe sort clause to our query, the operation will take very long, or even fail because of exceeding the memory\nlimit. It is surprising because I did not find any information on this in the documentation. Therefore, if we plan to\nsort our data based on the field with time, we will need to index this field manually. It means, of\ncourse, that the benefits stemming from a lower disc usage and faster saving times will unfortunately diminish.</p>\n\n<p>Since we are talking about the use of disc space, let’s check the data size:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">stats</span><span class=\"p\">()</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">stats</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>We will compare several fields:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">size</code>: data size before the compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">storageSize</code>: size of data after the compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">totalIndexSize</code>: size of indexes,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">totalSize</code>: total size of data and indexes.</li>\n</ul>\n\n<p>Results are gathered in the table below (in bytes, space is thousand separator):</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Field</th>\n      <th style=\"text-align: right\">Normal collection</th>\n      <th style=\"text-align: right\">Time series collection</th>\n      <th style=\"text-align: center\">Diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>size</td>\n      <td style=\"text-align: right\">570 000 000</td>\n      <td style=\"text-align: right\">747 855 228</td>\n      <td style=\"text-align: center\">+ 31%</td>\n    </tr>\n    <tr>\n      <td>storageSize</td>\n      <td style=\"text-align: right\">175 407 104</td>\n      <td style=\"text-align: right\">119 410 688</td>\n      <td style=\"text-align: center\">-31%</td>\n    </tr>\n    <tr>\n      <td>totalIndexSize</td>\n      <td style=\"text-align: right\">232 701 952</td>\n      <td style=\"text-align: right\">0</td>\n      <td style=\"text-align: center\">-100%</td>\n    </tr>\n    <tr>\n      <td>totalSize</td>\n      <td style=\"text-align: right\">408 109 056</td>\n      <td style=\"text-align: right\">119 410 688</td>\n      <td style=\"text-align: center\">-70%</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>As you can see, raw data of the time series collection may take up more space, but after their compression the new-type\ncollection turns out to be the winner in the size-on-disc category. After adding the size of indexes created in the\nregular collection, the difference will be even greater. Therefore, we must admit that the way time series data are\npacked on the disc is impressive.</p>\n\n<p>Now it’s time to compare query execution times for both collections.</p>\n\n<h2 id=\"speed-is-all-that-matters\">Speed is all that matters</h2>\n\n<p>Using the script below (saved as <code class=\"language-plaintext highlighter-rouge\">gen-find.sh</code> file), I generated two files containing commands getting documents from\nboth collections based on the\ntime label:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for</span> <span class=\"o\">((</span>i <span class=\"o\">=</span> 1<span class=\"p\">;</span> i &lt;<span class=\"o\">=</span> <span class=\"nv\">$1</span><span class=\"p\">;</span> i++ <span class=\"o\">))</span><span class=\"p\">;</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n  <span class=\"nv\">t</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">date</span> <span class=\"nt\">-jv</span> +<span class=\"k\">${</span><span class=\"nv\">x</span><span class=\"k\">}</span>M <span class=\"nt\">-f</span> <span class=\"s2\">\"%Y-%m-%d %H:%M:%S\"</span> <span class=\"s2\">\"2021-10-22 0:00:00\"</span> +%Y-%m-%dT%H:%M:%S<span class=\"si\">)</span>\n  <span class=\"nb\">echo</span> <span class=\"s2\">\"db.getCollection('</span><span class=\"nv\">$2</span><span class=\"s2\">').find({'timestamp' : new ISODate('</span><span class=\"nv\">$t</span><span class=\"s2\">')})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find-<span class=\"nv\">$2</span>.js\n</code></pre></div></div>\n\n<p>The script takes as parameters: the number of queries, and the name of the collection that we want to search. I\ngenerated a million queries (it may take some time depending on your hardware, so you can start with a lower amount of\nqueries):</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./gen-find.sh 1000000 coll-ts\n./gen-find.sh 1000000 coll-ord\n</code></pre></div></div>\n\n<p>Then I checked the time of the execution of both query sequences using the command:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find-coll-ts.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find-coll-ord.js\n</code></pre></div></div>\n\n<p>The standard collection was a bit slower: 16,854 for <code class=\"language-plaintext highlighter-rouge\">coll-ord</code> vs 16,038 for <code class=\"language-plaintext highlighter-rouge\">coll-ts</code>. Although the difference is small,\nanother point goes to time series: simple search is slightly faster than in the case of the regular collection.</p>\n\n<p>But we’re yet to discuss the most interesting part. Time series is primarily used for quick aggregate counting. Let’s\nsee what the comparison looks like when calculating the arithmetic mean in a given time interval.</p>\n\n<p>The script below (saved as <code class=\"language-plaintext highlighter-rouge\">gen-aggregate.sh</code>) creates a list of queries calculating the arithmetic mean of ratings for\na randomly selected six-hour interval:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for</span> <span class=\"o\">((</span>i <span class=\"o\">=</span> 1<span class=\"p\">;</span> i &lt;<span class=\"o\">=</span> <span class=\"nv\">$1</span><span class=\"p\">;</span> i++ <span class=\"o\">))</span><span class=\"p\">;</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x1</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n  <span class=\"nv\">x2</span><span class=\"o\">=</span><span class=\"k\">$((</span> x1 <span class=\"o\">+</span> <span class=\"m\">360</span><span class=\"k\">))</span>\n  <span class=\"nv\">t1</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">date</span> <span class=\"nt\">-jv</span> +<span class=\"k\">${</span><span class=\"nv\">x1</span><span class=\"k\">}</span>M <span class=\"nt\">-f</span> <span class=\"s2\">\"%Y-%m-%d %H:%M:%S\"</span> <span class=\"s2\">\"2021-10-22 0:00:00\"</span> +%Y-%m-%dT%H:%M:%S<span class=\"si\">)</span>\n  <span class=\"nv\">t2</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">date</span> <span class=\"nt\">-jv</span> +<span class=\"k\">${</span><span class=\"nv\">x2</span><span class=\"k\">}</span>M <span class=\"nt\">-f</span> <span class=\"s2\">\"%Y-%m-%d %H:%M:%S\"</span> <span class=\"s2\">\"2021-10-22 0:00:00\"</span> +%Y-%m-%dT%H:%M:%S<span class=\"si\">)</span>\n  <span class=\"nb\">echo</span> <span class=\"s2\">\"db.getCollection('</span><span class=\"nv\">$2</span><span class=\"s2\">').aggregate([{ </span><span class=\"se\">\\$</span><span class=\"s2\">match: { \"</span>timestamp<span class=\"s2\">\" : \"</span> <span class=\"se\">\\</span>\n    <span class=\"s2\">\"{</span><span class=\"se\">\\$</span><span class=\"s2\">gte:new ISODate('</span><span class=\"nv\">$t1</span><span class=\"s2\">'),</span><span class=\"se\">\\$</span><span class=\"s2\">lt:new ISODate('</span><span class=\"nv\">$t2</span><span class=\"s2\">')} } },\"</span> <span class=\"se\">\\</span>\n    <span class=\"s2\">\"{ </span><span class=\"se\">\\$</span><span class=\"s2\">group: { _id: null, avg: { </span><span class=\"se\">\\$</span><span class=\"s2\">avg: </span><span class=\"se\">\\\"\\$</span><span class=\"s2\">rating</span><span class=\"se\">\\\"</span><span class=\"s2\"> } } }])\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> aggregate-<span class=\"nv\">$2</span>-<span class=\"nv\">$1</span>.js\n</code></pre></div></div>\n\n<p>I prepared three script sets with: 10K, 50K and 100K queries for both collections:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./gen-aggregate.sh 10000 coll-ts\n./gen-aggregate.sh 50000 coll-ts\n./gen-aggregate.sh 100000 coll-ts\n\n./gen-aggregate.sh 10000 coll-ord\n./gen-aggregate.sh 50000 coll-ord\n./gen-aggregate.sh 100000 coll-ord\n</code></pre></div></div>\n\n<p>I made the measurements using following commands:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ts-10000.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ord-10000.js\n\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ts-50000.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ord-50000.js\n\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ts-100000.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ord-100000.js\n\n</code></pre></div></div>\n\n<p>The results are shown in the table below:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center\">Number of queries</th>\n      <th style=\"text-align: right\">Normal collection [min:sec]</th>\n      <th style=\"text-align: right\">Time series [min:sec]</th>\n      <th style=\"text-align: center\">Diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center\">10000</td>\n      <td style=\"text-align: right\">0:21,947</td>\n      <td style=\"text-align: right\">0:16,835</td>\n      <td style=\"text-align: center\">-23%</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">50000</td>\n      <td style=\"text-align: right\">1:37,02</td>\n      <td style=\"text-align: right\">1:11,18</td>\n      <td style=\"text-align: center\">-26%</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">100000</td>\n      <td style=\"text-align: right\">4:33,29</td>\n      <td style=\"text-align: right\">2:21,37</td>\n      <td style=\"text-align: center\">-48%</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Although there are far fewer queries this time than in the previous experiment, the differences in times are much\ngreater. It clearly proves that time series collections are indeed spreading their wings when we want to use aggregation\nqueries and have been developed mainly for this purpose. The reason is probably the sequential way of writing data,\nwhich shows a noticeable improvement when running ranged queries. The results clearly show also how much of an impact\nquery performance can have on an element that often doesn’t get the adequate attention: proper modelling the data.</p>\n\n<h2 id=\"limitations\">Limitations</h2>\n\n<p>Of course, time series collections also have their limitations and should not be used as a golden hammer.\nWe have already mentioned the first one — the lack of the primary key index. It stems from the assumption\nthat searches will be primarily based on time, so there is no point in creating an index that will be useful for very\nfew people. Of course, we can create this index ourselves.</p>\n\n<p>It is also not possible to sort by time field, which is another inconvenience. If we want to have\nsorting queries, we have to create an additional index.</p>\n\n<p>Although these two missing indexes may seem to be an easy thing to fix, we must remember that it involves additional use\nof disc space as well as longer indexing time during the saving of the document, which means that the benefits\nstemming from the use of time series will be somewhat reduced.</p>\n\n<p>The third, and perhaps most import limitation is the immutability of the document. Once saved, documents cannot be\nupdated or deleted. The only way to delete data is to use <code class=\"language-plaintext highlighter-rouge\">drop()</code> command or to define retention for the collection\nusing the <code class=\"language-plaintext highlighter-rouge\">expireAfterSeconds</code> parameter, which, like TTL indexes, will automatically delete documents certain time\nafter creation.</p>\n\n<p>The lack of possibility to manipulate the saved documents will probably be the main reason why programmers will be\nhesitant to use time series. We should mention, however, that the authors of MongoDB will probably add the possibility\nto edit and delete documents in the future:</p>\n\n<blockquote>\n  <p>While we know some of these limitations may be impactful to your current use case, we promise we’re working on this\nright now and would love for you to provide your feedback!</p>\n</blockquote>\n\n<h2 id=\"summary\">Summary</h2>\n<p>Adding the ability to store time series in MongoDB is a step in the right direction. First tests show that in certain\ncases the new type of collections really does work better than the regular ones. They use less disc space, are faster at\nsaving and searching by the time. But high performance always comes at a cost, in this case: the cost of reduced flexibility.\nTherefore, the final decision to use time series should be preceded by an analysis of\nadvantages and disadvantages of both in particular cases. We should also hope that the authors of the database are\nworking on improving it and will soon eliminate most limitations.</p>\n","contentSnippet":"A few years ago, I was working on a new version of Allegro purchase ratings.\nIt was a time of a pretty large revolution in the rating system when we moved this product away from our monolith, also\nintroducing quite significant changes to the concept of purchase rating itself. Replacing the system of positive, neutral\nor negative rating, we introduced an approach based on “thumbs up” and “thumbs down” as well as the option to rate several\nelements of the purchase separately: the time and cost of delivery, or products’ conformity with its description. The\nproduct-related revolution was accompanied by a major change in technology. Apart from transitioning towards the\nmicroservices architecture, we also decided to migrate data from a large relational database to MongoDB. There were many\nreasons for this decision: from the non-relational nature of our model, through the need for easier scaling, to the wish\nfor cost reduction. Upon completion of the works, we were for the most part content with the decision that we made. The\nnew solution was more user-friendly, easier to maintain and worked smoothly. The sole exception was aggregation queries,\nspecifically: determining the average of seller ratings in a specified period of time. While at the level of the 99th\npercentile times were very low, some queries were much slower. We spent a lot of time optimising both queries and the\ncode, and had to use some programming tricks to achieve satisfactory results. While we were able to solve our problems in\nthe end, the final conclusion was that the aggregation of data in large MongoDB collections is quite challenging.\nTo the rescue: Time series\nA new version of MongoDB, 5.0, has been recently launched. The list of changes included one that I found particularly\ninteresting: the time series collections. It is a method of effective storing and processing of time-ordered value series.\nA classic example for this case is measuring the temperature of air. These measurements are taken\nperiodically (for instance every hour), and their sequence forms time series. We then often review such data in an\nappropriate order, as well as calculate the maximum and minimum values, or the arithmetic mean. Therefore, in the said\ncase of use, a database must be highly efficient when saving the data, store records in a compact manner due to the\nlarge number thereof, and must quickly calculate aggregates. Although in the case of temperature readings database write\noperations are made on a regular basis, it turns out that in the case of time series it is not mandatory, and the only\nthing that truly matters is the presence of time. While reading about this topic, I instantly remembered my countless\nlate nights struggling with slow ratings aggregations. Therefore, I decided to explore this topic and see how the\nsolution works in practice.\nBefore the release of MongoDB 5.0, the only way to efficiently process time series was to store pre-calculated\naggregates, or use a bucket pattern,\nwhich was obviously associated with additional work and complexity of the\ncode. Now, things have been made much easier as this additional complexity is covered by a convenient abstraction. In\nMongoDB, time series are not actual collections, but materialised views that cover physical collections. This\nabstraction is intended to simplify complicated operations based on “buckets” of documents. You can read more about the\nconcept of storing data in the new kind of collection on the MongoDB official blog.\nEveryone who is interested in using this solution should take a look at it. In my article, on the other hand, I would\nlike to verify whether the processing of time series is really as fast as promised by the authors.\nData preparation\nFor the purposes of our considerations I will use a simple document describing the rating in the form of:\n\n{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"rating\" : 2.0\n}\n\n\nAttentive readers will immediately notice the absence of the field containing the ID of the seller whom the rating\nconcerns. It was done intentionally, otherwise I would have to\ncreate an additional index covering this field. However, I did not want to introduce any additional elements in my\nexperiment that could have any impact on the results. Let’s assume for this experiment that we are rating a\nrestaurant, not Allegro sellers, therefore all ratings in the collection concern the restaurant only.\nNow we can create two collections storing an identical set of data. One will be a standard collection, and the other will\nbe a time series. The time series collection has to be created manually by indicating the field specifying the time label:\n\ndb.createCollection(\"coll-ts\", { timeseries: { timeField: \"timestamp\" } } )\n\n\nIf you did not install Mongo 5.0 from scratch, but updated its previous version, you should make sure that\nit is set to an adequate level of compatibility. Otherwise, the above command will not create a time series collection.\nYou can check it with this command:\n\ndb.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )\n\n\nIf the returned value is less than 5.0, you need to issue:\n\ndb.adminCommand( { setFeatureCompatibilityVersion: \"5.0\" } )\n\n\nUpon creating a collection, it is also worth checking that a time series has actually been created:\n\ndb.runCommand( { listCollections: 1.0 } )\n\n\nWe look for \"type\" : \"timeseries\" entry:\n\n{\n    \"name\" : \"coll-ts\",\n    \"type\" : \"timeseries\",\n    \"options\" : {\n        \"timeseries\" : {\n            \"timeField\" : \"timestamp\",\n            \"granularity\" : \"seconds\",\n            \"bucketMaxSpanSeconds\" : 3600\n        }\n    },\n    \"info\" : {\n        \"readOnly\" : false\n    }\n}\n\n\nWe will also create the second collection manually (although it is not necessary, because it would be created with the\nfirst INSERT command). We will want to check the speed of data search based on time field, so we will create a unique index:\n\ndb.createCollection(\"coll-ord\")\ndb.getCollection('coll-ord').createIndex({timestamp: 1}, {unique: true})\n\n\nLet’s use the following scripts to fill both collections with 10 million documents:\n\n// Save as fill-ts.js\nvar bulk = db.getCollection(\"coll-ts\").initializeUnorderedBulkOp();\nvar startTime = new Date(\"2021-10-22T00:00:00.000Z\")\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({\n       \"timestamp\": new Date(startTime.getTime() + i * 60000),\n       \"rating\": Math.floor(1 + Math.random() * 4)\n    })\n}\nbulk.execute();\n\n\n\n// Save as fill-ord.js\nvar bulk = db.getCollection(\"coll-ord\").initializeUnorderedBulkOp();\nvar startTime = new Date(\"2021-10-22T00:00:00.000Z\")\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({\n       \"timestamp\": new Date(startTime.getTime() + i * 60000),\n       \"rating\": Math.floor(1 + Math.random() * 4)\n    })\n}\nbulk.execute();\n\n\nIt is worth to measure the execution time of scripts to compare write times to both collections. For this purpose I use\ntime command and mongo command-line client, test is the name of my database.\nTo avoid network latency I perform the measurements on my laptop with a local instance of MongoDB version 5.0.3 running.\n\ntime mongo test fill-ts.js\n\n\n\ntime mongo test fill-ord.js\n\n\nAs expected, the filling of the time series collection was faster: 3:30,11 vs 4:39,48. Such a difference can be essential\nif our system performs many write operations in a short period of time. At the very beginning of our experiments\ncollection of a new type comes to the forefront.\nNow when our collections already contain data, we can take a look at the size of files. On the\nmanual page we can read that:\nCompared to normal collections, storing time series data in time series collections improves query efficiency and\nreduces the disk usage\nLet’s find out how true that is.\nA closer look at the data\nIn the first place, it is worth making sure that documents in both collections look the same:\n\ndb.getCollection('coll-ts').find({}).limit(1)\ndb.getCollection('coll-ord').find({}).limit(1)\n\n\nBoth documents should be similar to this one:\n\n{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"_id\" : ObjectId(\"6184126fc42d1ab73d5208e4\"),\n    \"rating\" : 2.0\n}\n\n\nThe documents have the same schema. In addition, the _id key was automatically generated in both cases, although our\nfilling script did not contain them.\nLet’s move on to indexes now and use the commands: db.getCollection('coll-ord').getIndexes() and db.getCollection('coll-ts').getIndexes()\nto get the indexes of both collections.\nThe normal collection has two indexes, one that was created automatically for the _id key and the one that we created manually:\n\n// coll-ord:\n[\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"_id\" : 1\n        },\n        \"name\" : \"_id_\"\n    },\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"timestamp\" : 1.0\n        },\n        \"name\" : \"timestamp_1\",\n        \"unique\" : true\n    }\n]\n\n\nWhat is interesting is that the time series collection has no index at all:\n\n// coll-ts:\n[]\n\n\nThe lack of the index for the _id key of\ncourse means that, by default, the time series collection will have to perform the COLLSCAN operation if we want to\nsearch documents based on _id. The index is probably missing simply to save disc space and storage time,\nstemming from the assumption that the time series collections are mainly used to search based on time. The lack of\nthe index for the timestamp field is much more surprising. Does it mean that time-based searches in time series will\nalso cause COLLCSAN and work slowly? The answer to this question can be found in the documentation:\nThe internal index for a time series collection is not displayed\nSo, there actually is an index, but it is different from those created manually, and even from indexes created\nautomatically for the _id key.\nAs I wrote in another post, indexes are not all the\nsame, so it’s worth taking a closer look at this one. Let’s check the query execution plans:\n\ndb.getCollection('coll-ord').find({\"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\")}).explain('executionStats')\ndb.getCollection('coll-ts').find({\"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\")}).explain('executionStats')\n\n\n\n//coll-ord\n\"winningPlan\" : {\n    \"stage\" : \"FETCH\",\n    \"inputStage\" : {\n        \"stage\" : \"IXSCAN\",\n        \"keyPattern\" : {\n            \"timestamp\" : 1.0\n        }\n    },\n    \"indexName\" : \"timestamp_1\",\n}\n\n\n\n//coll-ts\n\"winningPlan\" : {\n    \"stage\" : \"COLLSCAN\",\n    \"filter\" : {\n        \"$and\" : [\n            {\n                \"_id\" : {\n                    \"$lte\" : ObjectId(\"6171ff00ffffffffffffffff\")\n                }\n            },\n            {\n                \"_id\" : {\n                    \"$gte\" : ObjectId(\"6171f0f00000000000000000\")\n                }\n            },\n            {\n                \"control.max.timestamp\" : {\n                    \"$_internalExprGte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            },\n            {\n                \"control.min.timestamp\" : {\n                    \"$_internalExprLte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            }\n        ]\n    },\n}\n\n\nIt turns out that while in the case of the regular collection the plan shows the use of the index, in the time series\ncollection we see the COLLSCAN operation. It doesn’t mean that this operation is slow, though. The execution times of\nboth operations were similar. We will move on to a more detailed time comparison in a moment; for now we should only\nnote that the hidden index in the time series collection follows specific rules, it is not only invisible, but it also\ncannot be seen in the execution plan, although it clearly affects the speed of the search.\nAnd what happens if we add sorting?\n\ndb.getCollection('coll-ts').find({}).sort({timestamp: 1})\n\n\n\n{\n \"ok\" : 0,\n \"errmsg\" : \"PlanExecutor error during aggregation :: caused by :: Sort exceeded memory limit of 104857600 bytes,\n but did not opt in to external sorting.\",\n \"code\" : 292,\n \"codeName\" : \"QueryExceededMemoryLimitNoDiskUseAllowed\"\n}\n\n\nSurprise! The internal index for the time series collection does not have a sorting feature. This means that if we add\nthe sort clause to our query, the operation will take very long, or even fail because of exceeding the memory\nlimit. It is surprising because I did not find any information on this in the documentation. Therefore, if we plan to\nsort our data based on the field with time, we will need to index this field manually. It means, of\ncourse, that the benefits stemming from a lower disc usage and faster saving times will unfortunately diminish.\nSince we are talking about the use of disc space, let’s check the data size:\n\ndb.getCollection('coll-ts').stats()\ndb.getCollection('coll-ord').stats()\n\n\nWe will compare several fields:\nsize: data size before the compression,\nstorageSize: size of data after the compression,\ntotalIndexSize: size of indexes,\ntotalSize: total size of data and indexes.\nResults are gathered in the table below (in bytes, space is thousand separator):\nField\n      Normal collection\n      Time series collection\n      Diff\n    \nsize\n      570 000 000\n      747 855 228\n      + 31%\n    \nstorageSize\n      175 407 104\n      119 410 688\n      -31%\n    \ntotalIndexSize\n      232 701 952\n      0\n      -100%\n    \ntotalSize\n      408 109 056\n      119 410 688\n      -70%\n    \nAs you can see, raw data of the time series collection may take up more space, but after their compression the new-type\ncollection turns out to be the winner in the size-on-disc category. After adding the size of indexes created in the\nregular collection, the difference will be even greater. Therefore, we must admit that the way time series data are\npacked on the disc is impressive.\nNow it’s time to compare query execution times for both collections.\nSpeed is all that matters\nUsing the script below (saved as gen-find.sh file), I generated two files containing commands getting documents from\nboth collections based on the\ntime label:\n\n#!/bin/bash\nRANDOM=42\nfor ((i = 1; i <= $1; i++ ));\ndo\n  x=$(( $RANDOM % 10000000))\n  t=$(date -jv +${x}M -f \"%Y-%m-%d %H:%M:%S\" \"2021-10-22 0:00:00\" +%Y-%m-%dT%H:%M:%S)\n  echo \"db.getCollection('$2').find({'timestamp' : new ISODate('$t')})\"\ndone >> find-$2.js\n\n\nThe script takes as parameters: the number of queries, and the name of the collection that we want to search. I\ngenerated a million queries (it may take some time depending on your hardware, so you can start with a lower amount of\nqueries):\n\n./gen-find.sh 1000000 coll-ts\n./gen-find.sh 1000000 coll-ord\n\n\nThen I checked the time of the execution of both query sequences using the command:\n\ntime mongo test find-coll-ts.js\ntime mongo test find-coll-ord.js\n\n\nThe standard collection was a bit slower: 16,854 for coll-ord vs 16,038 for coll-ts. Although the difference is small,\nanother point goes to time series: simple search is slightly faster than in the case of the regular collection.\nBut we’re yet to discuss the most interesting part. Time series is primarily used for quick aggregate counting. Let’s\nsee what the comparison looks like when calculating the arithmetic mean in a given time interval.\nThe script below (saved as gen-aggregate.sh) creates a list of queries calculating the arithmetic mean of ratings for\na randomly selected six-hour interval:\n\n#!/bin/bash\nRANDOM=42\nfor ((i = 1; i <= $1; i++ ));\ndo\n  x1=$(( $RANDOM % 10000000))\n  x2=$(( x1 + 360))\n  t1=$(date -jv +${x1}M -f \"%Y-%m-%d %H:%M:%S\" \"2021-10-22 0:00:00\" +%Y-%m-%dT%H:%M:%S)\n  t2=$(date -jv +${x2}M -f \"%Y-%m-%d %H:%M:%S\" \"2021-10-22 0:00:00\" +%Y-%m-%dT%H:%M:%S)\n  echo \"db.getCollection('$2').aggregate([{ \\$match: { \"timestamp\" : \" \\\n    \"{\\$gte:new ISODate('$t1'),\\$lt:new ISODate('$t2')} } },\" \\\n    \"{ \\$group: { _id: null, avg: { \\$avg: \\\"\\$rating\\\" } } }])\"\ndone >> aggregate-$2-$1.js\n\n\nI prepared three script sets with: 10K, 50K and 100K queries for both collections:\n\n./gen-aggregate.sh 10000 coll-ts\n./gen-aggregate.sh 50000 coll-ts\n./gen-aggregate.sh 100000 coll-ts\n\n./gen-aggregate.sh 10000 coll-ord\n./gen-aggregate.sh 50000 coll-ord\n./gen-aggregate.sh 100000 coll-ord\n\n\nI made the measurements using following commands:\n\ntime mongo test aggregate-coll-ts-10000.js\ntime mongo test aggregate-coll-ord-10000.js\n\ntime mongo test aggregate-coll-ts-50000.js\ntime mongo test aggregate-coll-ord-50000.js\n\ntime mongo test aggregate-coll-ts-100000.js\ntime mongo test aggregate-coll-ord-100000.js\n\n\n\nThe results are shown in the table below:\nNumber of queries\n      Normal collection [min:sec]\n      Time series [min:sec]\n      Diff\n    \n10000\n      0:21,947\n      0:16,835\n      -23%\n    \n50000\n      1:37,02\n      1:11,18\n      -26%\n    \n100000\n      4:33,29\n      2:21,37\n      -48%\n    \nAlthough there are far fewer queries this time than in the previous experiment, the differences in times are much\ngreater. It clearly proves that time series collections are indeed spreading their wings when we want to use aggregation\nqueries and have been developed mainly for this purpose. The reason is probably the sequential way of writing data,\nwhich shows a noticeable improvement when running ranged queries. The results clearly show also how much of an impact\nquery performance can have on an element that often doesn’t get the adequate attention: proper modelling the data.\nLimitations\nOf course, time series collections also have their limitations and should not be used as a golden hammer.\nWe have already mentioned the first one — the lack of the primary key index. It stems from the assumption\nthat searches will be primarily based on time, so there is no point in creating an index that will be useful for very\nfew people. Of course, we can create this index ourselves.\nIt is also not possible to sort by time field, which is another inconvenience. If we want to have\nsorting queries, we have to create an additional index.\nAlthough these two missing indexes may seem to be an easy thing to fix, we must remember that it involves additional use\nof disc space as well as longer indexing time during the saving of the document, which means that the benefits\nstemming from the use of time series will be somewhat reduced.\nThe third, and perhaps most import limitation is the immutability of the document. Once saved, documents cannot be\nupdated or deleted. The only way to delete data is to use drop() command or to define retention for the collection\nusing the expireAfterSeconds parameter, which, like TTL indexes, will automatically delete documents certain time\nafter creation.\nThe lack of possibility to manipulate the saved documents will probably be the main reason why programmers will be\nhesitant to use time series. We should mention, however, that the authors of MongoDB will probably add the possibility\nto edit and delete documents in the future:\nWhile we know some of these limitations may be impactful to your current use case, we promise we’re working on this\nright now and would love for you to provide your feedback!\nSummary\nAdding the ability to store time series in MongoDB is a step in the right direction. First tests show that in certain\ncases the new type of collections really does work better than the regular ones. They use less disc space, are faster at\nsaving and searching by the time. But high performance always comes at a cost, in this case: the cost of reduced flexibility.\nTherefore, the final decision to use time series should be preceded by an analysis of\nadvantages and disadvantages of both in particular cases. We should also hope that the authors of the database are\nworking on improving it and will soon eliminate most limitations.","guid":"https://blog.allegro.tech/2021/12/performance-evaluation-of-timeseries.html","categories":["tech","mongodb","performance","time series"],"isoDate":"2021-12-19T23:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"Clean Architecture Story","link":"https://blog.allegro.tech/2021/12/clean-architecture-story.html","pubDate":"Mon, 13 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Michał Kowalcze"],"photo":["https://blog.allegro.tech/img/authors/michal.kowalcze.jpg"],"url":["https://blog.allegro.tech/authors/michal.kowalcze"]}]},"content":"<p><a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">The Clean Architecture</a> concept has been\naround for some time and keeps surfacing in one place or another, yet it is not widely adopted. In this post I would\nlike to introduce this topic in a less conventional way: starting with customer’s needs and going through various\nstages to present a solution that is clean enough to satisfy concepts from the aforementioned blog (or\n<a href=\"https://www.goodreads.com/book/show/18043011-clean-architecture\">the book</a> with the same name).</p>\n\n<h2 id=\"the-perspective\">The perspective</h2>\n<p>Why do we need software architecture? What is it anyway? An extensive definition can be found in a place a bit unexpected\nfor an agile world — an enterprise-architecture definition from <a href=\"https://en.wikipedia.org/wiki/The_Open_Group_Architecture_Framework\">TOGAF</a>:</p>\n\n<ul>\n  <li>The fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and\nin the principles of its design and evolution. (Source: ISO/IEC/IEEE 42010:2011)</li>\n  <li>The structure of components, their inter-relationships, and the principles and guidelines governing their design and\nevolution over time.</li>\n</ul>\n\n<p>And what do we need such a governing structure or shape for? Basically it allows us to make cost/time-efficient choices\nwhen it comes to development. And deployment. And operation. And maintenance.</p>\n\n<p>It also allows us to keep as many options open as possible, so our future choices are not limited by an overcommitment\nfrom the past.</p>\n\n<p>So — we have our perspective defined. Let’s dive into a real-world problem!</p>\n\n<h2 id=\"the-challenge\">The challenge</h2>\n<p>You are a young, promising programmer sitting in a dorm and one afternoon a stranger appears. “I run a small company\nthat delivers packages from furniture shops to customers. I need a database that will allow reservation of slots. Is it\nsomething you are able to deliver?” “Of course!” — what else could a young, promising programmer answer?</p>\n\n<h2 id=\"the-false-start\">The false start</h2>\n<p>The customer needs a database, so what can we start with? The database schema, of course! We can identify entities with\nease: a transport slot, a schedule, a user (we need some authentication, right?), a … something? Okay, perhaps it is\nnot the easiest way. So why don’t we start with something else?</p>\n\n<p>Let’s choose the technology to use! Let’s go with React frontend, Java+Spring backend, some SQL as persistence. To\npresent a clickable version to our customer we need some warm-up work to set up an environment, create a deployable\nservice version or GUI mockups, configure persistence and so on. In general: to pay attention to technical details —\ncode necessary to set up something working, of which non-devs are usually not aware. It simply has to be done before we\nstart talking about nitty-gritty for business logic.</p>\n\n<h2 id=\"the-use-case-driven-approach\">The use-case-driven approach</h2>\n<p>What if instead of starting with what we already know — how to visualize relationships, how to build a web-system — we\nstarted with what we didn’t know? Simply — by asking questions such as: How is the system going to be used? By whom?</p>\n\n<h2 id=\"use-cases\">Use cases</h2>\n<p>In other words — what are the use cases for the system? Let’s define the challenge once more using high-level actors\nand interactions: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_cases.png\" alt=\"Use cases\" /> and pick the first\nrequired interaction: shop makes a reservation. What is required to make a reservation? Hmm, I think that it would be\ngood to get the current schedule in the first place. Why am I using “get” instead of “display”? “Display” already\nsuggests a way of delivering output, when we hear “display” a computer screen comes to our minds, with a web\napplication. Single page web app, of course. “Get” is more neutral, it does not constrain our vision by a specific\npresentation method. Frankly — is there anything wrong with delivering the current schedule over the phone, for\nexample?</p>\n\n<h3 id=\"getting-the-schedule\">Getting the schedule</h3>\n<p>So, we can start thinking about our schedule model — let it be a single instance representing a day with slots inside.\nGreat, we have our entities! How to get one? Well, we need to check if there is already a stored schedule and if so\n— retrieve it from the repository. If the schedule is not available we have to create one. Based on…? Exactly — we do\nnot know yet, all we can say is that it will probably be something flexible. Something to discuss with our customer\n— but this does not prevent us from going forward with our first use case. Logic is indeed simple:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">:</span> <span class=\"nc\">LocalDate</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">{</span>\n  <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">daySchedulerRepository</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n  <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">daySchedule</span> <span class=\"p\">!=</span> <span class=\"k\">null</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">daySchedule</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">val</span> <span class=\"py\">newSchedule</span> <span class=\"p\">=</span> <span class=\"n\">dayScheduleCreator</span><span class=\"p\">.</span><span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">daySchedulerRepository</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">newSchedule</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/6dfeee53554a4ccf37e81aa50a2bd24af7e02cce\">GitHub</a>)</p>\n\n<p>And even with this simple logic we identified a hidden assumption regarding the schedule definition: that there is a\nrecipe for creating a daily schedule. What is more we can test retrieval of a schedule — with definition of schedule\ncreator if required — without any irrelevant details, like database, UI, framework and so on. Test only business rules,\nwithout unnecessary details.</p>\n\n<h2 id=\"reserving-the-slot\">Reserving the slot</h2>\n<p>To finish the reservation we have to add at least one more use case — one for reservation of a free slot. Provided that\nwe re-use existing logic, the interaction is still simple:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">reserve</span><span class=\"p\">(</span><span class=\"n\">slotId</span><span class=\"p\">:</span> <span class=\"nc\">SlotId</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">{</span>\n  <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">getScheduleUseCase</span><span class=\"p\">.</span><span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span> <span class=\"p\">=</span> <span class=\"n\">slotId</span><span class=\"p\">.</span><span class=\"n\">day</span><span class=\"p\">)</span>\n\n  <span class=\"kd\">val</span> <span class=\"py\">modifiedSchedule</span> <span class=\"p\">=</span> <span class=\"n\">daySchedule</span><span class=\"p\">.</span><span class=\"nf\">reserveSlot</span><span class=\"p\">(</span><span class=\"n\">slotId</span><span class=\"p\">.</span><span class=\"n\">index</span><span class=\"p\">)</span>\n\n  <span class=\"k\">return</span> <span class=\"n\">dayScheduleRepository</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">modifiedSchedule</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/7b7961b28107c3c89d40ce69a8383bf9f32337b0\">GitHub</a>)</p>\n\n<p>And, as we can see — the slot reservation business rule (and constraint) is implemented at the domain model itself — so\nwe are safe, that any other interaction, any other use case, is not going to break these rules. This approach also\nsimplifies testing, as business rules can be verified in separation from the use case interaction logic.</p>\n\n<h2 id=\"where-is-the-clean-architecture\">Where is the “Clean Architecture”?</h2>\n<p>Let‘s stop with business logic for a moment. We created quite thoughtful, extensible code for sure, but why are we\ntalking about “Clean” architecture? We already used Domain-Driven Design and Hexagonal architecture concepts. Is there\nanything more? Imagine that another person is going to help us with implementation. She is not aware of the source code\nyet and simply would like to take a look at the codebase. And she sees: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_case_classes.png\" alt=\"Use case classes\" />\nIt looks like something to her, doesn‘t it? A kind of reservation system! It is not yet another domain service with\nsome methods that have no clear connection with possible uses — the list of classes itself describes what the system\ncan do.</p>\n\n<h2 id=\"the-first-assumption\">The first assumption</h2>\n<p>We have a mocked implementation as the schedule creator. It is OK to test logic at the unit test level, but not enough\nto run a prototype.</p>\n\n<p>After a short call with our customer we know more about the daily schedule — there are six slots, two hours each,\nstarting at 8:oo a.m. We also know that this recipe for the daily schedule is very, very simple and it is going to be\nchanged soon (e.g. to accommodate for holidays, etc.). All these issues will be solved later, now we are at the\nprototype stage and our desired outcome is to have a working demo for our stranger.</p>\n\n<p>Where to put this simple implementation of the schedule creator? So far, the domain used an interface for that. Are we\ngoing to put an implementation of this interface to the infrastructure package and treat it as something outside the\ndomain? Certainly not! It is not complicated and this is part of the domain itself, we simply replace the mocked\nimplementation of the schedule creator with class specification.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">package</span> <span class=\"nn\">eu.kowalcze.michal.arch.clean.example.domain.model</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">DayScheduleCreator</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">:</span> <span class=\"nc\">LocalDate</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">=</span> <span class=\"nc\">DaySchedule</span><span class=\"p\">(</span>\n        <span class=\"n\">scheduleDay</span><span class=\"p\">,</span>\n        <span class=\"nf\">createStandardSlots</span><span class=\"p\">()</span>\n    <span class=\"p\">)</span>\n<span class=\"c1\">//...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/2792fc31e98d76a610561636f03073dee73fbb47\">GitHub</a>)</p>\n\n<h2 id=\"the-prototype\">The prototype</h2>\n<p>I will not be original here — for the first prototype version the REST API sounds like something reasonable. Do we care\nabout other infrastructure at the moment? Persistence? No! In the previous commits a map-based persistence layer is\nused for unit tests and this solution is good enough to start with. As long as the system is not restarted, of course.</p>\n\n<p>What is important at this stage? We are introducing an <strong>API</strong> — this is a separate layer, so it is crucial to ensure\nthat domain classes are not exposed to the outside world — and that we do not introduce a dependency on the API into\nthe domain.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">package</span> <span class=\"nn\">eu.kowalcze.michal.arch.clean.example.api</span>\n\n<span class=\"nd\">@Controller</span>\n<span class=\"kd\">class</span> <span class=\"nc\">GetScheduleEndpoint</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">getScheduleUseCase</span><span class=\"p\">:</span> <span class=\"nc\">GetScheduleUseCase</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@GetMapping</span><span class=\"p\">(</span><span class=\"s\">\"/schedules/{localDate}\"</span><span class=\"p\">)</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">getSchedules</span><span class=\"p\">(</span><span class=\"nd\">@PathVariable</span> <span class=\"n\">localDate</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">DayScheduleDto</span> <span class=\"p\">{</span>\n        <span class=\"kd\">val</span> <span class=\"py\">scheduleDay</span> <span class=\"p\">=</span> <span class=\"nc\">LocalDate</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">localDate</span><span class=\"p\">)</span>\n        <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">getScheduleUseCase</span><span class=\"p\">.</span><span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">daySchedule</span><span class=\"p\">.</span><span class=\"nf\">toApi</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/b1d1c3fe3901d9328bdfaf560331d35131f8224b\">GitHub</a>)</p>\n\n<h2 id=\"the-abstractions\">The abstractions</h2>\n<h3 id=\"use-case\">Use Case</h3>\n<p>Checking the implementation of endpoints (see comments in the code) we can see that conceptually each endpoint executes\nlogic according to the same structure: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_case_flow.png\" alt=\"Use case flow\" />\nWell, why don’t we make some abstraction for this? Sounds like a crazy idea? Let‘s check! Based on our code and the\ndiagram above we can identify the <code class=\"language-plaintext highlighter-rouge\">UseCase</code> abstraction — something that takes some input (domain input, to be precise)\nand converts it to a (domain) output.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">interface</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">INPUT</span><span class=\"p\">):</span> <span class=\"nc\">OUTPUT</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/006811b49ae4531b96b300c964d3a66d725183bf\">GitHub</a>)</p>\n\n<h3 id=\"use-case-executor\">Use Case Executor</h3>\n<p>Great! We have use cases and I just realized that I would like to have an email in my inbox each time an exception is\nthrown — and I do not want to depend on a spring-specific mechanism to do this. A common <code class=\"language-plaintext highlighter-rouge\">UseCaseExecutor</code> will be a\ngreat help to address this non-functional requirement.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">notificationGateway</span><span class=\"p\">:</span> <span class=\"nc\">NotificationGateway</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;,</span> <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">INPUT</span><span class=\"p\">):</span> <span class=\"nc\">OUTPUT</span> <span class=\"p\">{</span>\n        <span class=\"k\">try</span> <span class=\"p\">{</span>\n            <span class=\"k\">return</span> <span class=\"n\">useCase</span><span class=\"p\">.</span><span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">Exception</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"n\">notificationGateway</span><span class=\"p\">.</span><span class=\"nf\">notify</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">)</span>\n            <span class=\"k\">throw</span> <span class=\"n\">e</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/54d3187aed94427bb60af9781d0eec573c8c8db0\">GitHub</a>)</p>\n\n<h3 id=\"framework-independent-response\">Framework-independent response</h3>\n<p>In order to handle the next requirements in our plan we have to change the logic a bit — add the possibility of returning\nspring-specific response entities from the executor itself. To make our code reusable in a non-spring world (ktor,\nanyone?) we separated the plain executor from spring specific decorator, so that it is possible to use this code easily\nin other frameworks.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">data class</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;(</span>\n    <span class=\"kd\">val</span> <span class=\"py\">responseCode</span><span class=\"p\">:</span> <span class=\"nc\">Int</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">output</span><span class=\"p\">:</span> <span class=\"nc\">API_OUTPUT</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">SpringUseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">useCaseExecutor</span><span class=\"p\">:</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">,</span> <span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n        <span class=\"n\">toApiConversion</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">)</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span>\n    <span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"n\">useCaseExecutor</span><span class=\"p\">.</span><span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">,</span> <span class=\"n\">input</span><span class=\"p\">,</span> <span class=\"n\">toApiConversion</span><span class=\"p\">).</span><span class=\"nf\">toSpringResponse</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;.</span><span class=\"nf\">toSpringResponse</span><span class=\"p\">():</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"nc\">ResponseEntity</span><span class=\"p\">.</span><span class=\"nf\">status</span><span class=\"p\">(</span><span class=\"n\">responseCode</span><span class=\"p\">).</span><span class=\"nf\">body</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/d44f7f993fab2e749e3048561b3ac4d3cff6fd88\">GitHub</a>)</p>\n\n<h3 id=\"handle-domain-exceptions\">Handle domain exceptions</h3>\n<p>Ooops. Our prototype is running and we observe exceptions resulting in HTTP 500 errors. It would be nice to convert\nthese to dedicated response codes in a reasonable way yet without using much of spring infrastructure, for simplified\nmaintenance (and possible future changes). This can be easily achieved by adding another parameter to use case\nexecution, like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">notificationGateway</span><span class=\"p\">:</span> <span class=\"nc\">NotificationGateway</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n        <span class=\"n\">toApiConversion</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">)</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">handledExceptions</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"nc\">ExceptionHandler</span><span class=\"p\">.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">Any</span><span class=\"p\">)?</span> <span class=\"p\">=</span> <span class=\"k\">null</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n\n        <span class=\"k\">try</span> <span class=\"p\">{</span>\n            <span class=\"kd\">val</span> <span class=\"py\">domainOutput</span> <span class=\"p\">=</span> <span class=\"n\">useCase</span><span class=\"p\">.</span><span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"nf\">toApiConversion</span><span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">Exception</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"c1\">// conceptual logic</span>\n            <span class=\"kd\">val</span> <span class=\"py\">exceptionHandler</span> <span class=\"p\">=</span> <span class=\"nc\">ExceptionHandler</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span>\n            <span class=\"n\">handledExceptions</span><span class=\"o\">?.</span><span class=\"nf\">let</span> <span class=\"p\">{</span> <span class=\"n\">exceptionHandler</span><span class=\"p\">.</span><span class=\"nf\">handledExceptions</span><span class=\"p\">()</span> <span class=\"p\">}</span>\n            <span class=\"k\">return</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">(</span><span class=\"n\">responseCodeIfExceptionIsHandled</span><span class=\"p\">,</span> <span class=\"n\">exceptionHandler</span><span class=\"p\">.</span><span class=\"n\">message</span> <span class=\"o\">?:</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/ac6763f19e2f3f61adc1f8b02bab6cb1e1a65c11\">GitHub</a>)</p>\n\n<h3 id=\"handle-dto-conversion-exceptions\">Handle DTO conversion exceptions</h3>\n<p>By simply replacing input with:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">inputProvider</span><span class=\"p\">:</span> <span class=\"nc\">Any</span><span class=\"p\">.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/a9ef4bb835977a4bd4a62eb754d8563340bd3d4e\">GitHub</a>)</p>\n\n<p>we are able to handle exceptions raised during creation of input domain objects in a uniform way, without any\nadditional try/catches at the endpoint level.</p>\n\n<h2 id=\"the-outcome\">The outcome</h2>\n\n<p>What is the result of our journey across some functional requirements and a bit more non-functional requirements? By\nlooking at the definition of an endpoint we have full documentation of its behaviour, including exceptions. Our code is\neasily portable to some different API (e.g. EJB), we have fully-auditable modifications, and we can exchange layers\nquite freely. Also analysis of whole service is simplified, as possible use cases are explicitely stated.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@PutMapping</span><span class=\"p\">(</span><span class=\"s\">\"/schedules/{localDate}/{index}\"</span><span class=\"p\">,</span> <span class=\"n\">produces</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s\">\"application/json\"</span><span class=\"p\">],</span> <span class=\"n\">consumes</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s\">\"application/json\"</span><span class=\"p\">])</span>\n<span class=\"k\">fun</span> <span class=\"nf\">getSchedules</span><span class=\"p\">(</span><span class=\"nd\">@PathVariable</span> <span class=\"n\">localDate</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"nd\">@PathVariable</span> <span class=\"n\">index</span><span class=\"p\">:</span> <span class=\"nc\">Int</span><span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"n\">useCaseExecutor</span><span class=\"p\">.</span><span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span> <span class=\"p\">=</span> <span class=\"n\">reserveSlotUseCase</span><span class=\"p\">,</span>\n        <span class=\"n\">inputProvider</span> <span class=\"p\">=</span> <span class=\"p\">{</span> <span class=\"nc\">SlotId</span><span class=\"p\">(</span><span class=\"nc\">LocalDate</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">localDate</span><span class=\"p\">),</span> <span class=\"n\">index</span><span class=\"p\">)</span> <span class=\"p\">},</span>\n        <span class=\"n\">toApiConversion</span> <span class=\"p\">=</span> <span class=\"p\">{</span>\n            <span class=\"kd\">val</span> <span class=\"py\">dayScheduleDto</span> <span class=\"p\">=</span> <span class=\"n\">it</span><span class=\"p\">.</span><span class=\"nf\">toApi</span><span class=\"p\">()</span>\n            <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">(</span><span class=\"nc\">HttpServletResponse</span><span class=\"p\">.</span><span class=\"nc\">SC_ACCEPTED</span><span class=\"p\">,</span> <span class=\"n\">dayScheduleDto</span><span class=\"p\">)</span>\n        <span class=\"p\">},</span>\n        <span class=\"n\">handledExceptions</span> <span class=\"p\">=</span> <span class=\"p\">{</span>\n            <span class=\"nf\">exception</span><span class=\"p\">(</span><span class=\"nc\">InvalidSlotIndexException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">,</span> <span class=\"nc\">UNPROCESSABLE_ENTITY</span><span class=\"p\">,</span> <span class=\"s\">\"INVALID-SLOT-ID\"</span><span class=\"p\">)</span>\n            <span class=\"nf\">exception</span><span class=\"p\">(</span><span class=\"nc\">SlotAlreadyReservedException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">,</span> <span class=\"nc\">CONFLICT</span><span class=\"p\">,</span> <span class=\"s\">\"SLOT-ALREADY-RESERVED\"</span><span class=\"p\">)</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>(repository: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example\">GitHub</a>)</p>\n\n<p>A simple evaluation of our solution with measures mentioned at the beginning:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left\">Aspect</th>\n      <th style=\"text-align: left\">Evaluation</th>\n      <th style=\"text-align: center\">Has advantage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left\">Development</td>\n      <td style=\"text-align: left\"><code class=\"language-plaintext highlighter-rouge\">UseCase</code> abstraction forces unification of approach across different teams in a more significant way than standard service approach.</td>\n      <td style=\"text-align: center\">✓</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Deployment</td>\n      <td style=\"text-align: left\">We did not consider deployment in our example. It certainly is not going to be different/harder than in case of hexagonal architecture.</td>\n      <td style=\"text-align: center\"> </td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Operation</td>\n      <td style=\"text-align: left\">Use case-based approach reveals operation of the system, which reduces learning curve for both development and maintenance.</td>\n      <td style=\"text-align: center\">✓</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Maintenance</td>\n      <td style=\"text-align: left\">Entry threshold might be lower compared to hexagonal approach, as service is separated horizontally (into layers) and vertically (into use cases with common domain model).</td>\n      <td style=\"text-align: center\">✓</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Keeping options open</td>\n      <td style=\"text-align: left\">Similar to hexagonal architecture approach.</td>\n      <td style=\"text-align: center\"> </td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"tldr\">TL;DR</h3>\n<p>It is like hexagonal architecture with one additional dimension, composed of use cases, giving better insight into\noperations of a system and streamlining development and maintenance. Solution that was created during this narrative\nallows for creation of a self-documenting API endpoint.</p>\n\n<h2 id=\"high-level-overview\">High-level overview</h2>\n<p>With all this read we can switch our view to the high-level perspective:</p>\n\n<p><img src=\"/img/articles/2021-12-13-clean-architecture-story/clean_architecture_diagram.png\" alt=\"The Clean Architecture Diagram\" /></p>\n\n<p>and describe abstractions. Starting from the inside we have:</p>\n<ul>\n  <li><strong>Domain Model</strong>, <strong>Services</strong> and <strong>Gateways</strong>, which are responsible for defining\nbusiness rules for the domain.</li>\n  <li><strong>UseCase</strong>, which orchestrates execution of business rules.</li>\n  <li><strong>UseCaseExecutor</strong> providing common behavior for all use cases.</li>\n  <li><strong>API</strong> connecting service with the outside world.</li>\n  <li><strong>Implementation of gateways</strong>, which connects with other services or persistence providers.</li>\n  <li><strong>Configuration</strong>, responsible for gluing all elements together.</li>\n</ul>\n\n<p>I hope that you enjoy this simple story and find the concept of\n<a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">the Clean Architecture</a> useful.\nThank you for reading!</p>\n","contentSnippet":"The Clean Architecture concept has been\naround for some time and keeps surfacing in one place or another, yet it is not widely adopted. In this post I would\nlike to introduce this topic in a less conventional way: starting with customer’s needs and going through various\nstages to present a solution that is clean enough to satisfy concepts from the aforementioned blog (or\nthe book with the same name).\nThe perspective\nWhy do we need software architecture? What is it anyway? An extensive definition can be found in a place a bit unexpected\nfor an agile world — an enterprise-architecture definition from TOGAF:\nThe fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and\nin the principles of its design and evolution. (Source: ISO/IEC/IEEE 42010:2011)\nThe structure of components, their inter-relationships, and the principles and guidelines governing their design and\nevolution over time.\nAnd what do we need such a governing structure or shape for? Basically it allows us to make cost/time-efficient choices\nwhen it comes to development. And deployment. And operation. And maintenance.\nIt also allows us to keep as many options open as possible, so our future choices are not limited by an overcommitment\nfrom the past.\nSo — we have our perspective defined. Let’s dive into a real-world problem!\nThe challenge\nYou are a young, promising programmer sitting in a dorm and one afternoon a stranger appears. “I run a small company\nthat delivers packages from furniture shops to customers. I need a database that will allow reservation of slots. Is it\nsomething you are able to deliver?” “Of course!” — what else could a young, promising programmer answer?\nThe false start\nThe customer needs a database, so what can we start with? The database schema, of course! We can identify entities with\nease: a transport slot, a schedule, a user (we need some authentication, right?), a … something? Okay, perhaps it is\nnot the easiest way. So why don’t we start with something else?\nLet’s choose the technology to use! Let’s go with React frontend, Java+Spring backend, some SQL as persistence. To\npresent a clickable version to our customer we need some warm-up work to set up an environment, create a deployable\nservice version or GUI mockups, configure persistence and so on. In general: to pay attention to technical details —\ncode necessary to set up something working, of which non-devs are usually not aware. It simply has to be done before we\nstart talking about nitty-gritty for business logic.\nThe use-case-driven approach\nWhat if instead of starting with what we already know — how to visualize relationships, how to build a web-system — we\nstarted with what we didn’t know? Simply — by asking questions such as: How is the system going to be used? By whom?\nUse cases\nIn other words — what are the use cases for the system? Let’s define the challenge once more using high-level actors\nand interactions:  and pick the first\nrequired interaction: shop makes a reservation. What is required to make a reservation? Hmm, I think that it would be\ngood to get the current schedule in the first place. Why am I using “get” instead of “display”? “Display” already\nsuggests a way of delivering output, when we hear “display” a computer screen comes to our minds, with a web\napplication. Single page web app, of course. “Get” is more neutral, it does not constrain our vision by a specific\npresentation method. Frankly — is there anything wrong with delivering the current schedule over the phone, for\nexample?\nGetting the schedule\nSo, we can start thinking about our schedule model — let it be a single instance representing a day with slots inside.\nGreat, we have our entities! How to get one? Well, we need to check if there is already a stored schedule and if so\n— retrieve it from the repository. If the schedule is not available we have to create one. Based on…? Exactly — we do\nnot know yet, all we can say is that it will probably be something flexible. Something to discuss with our customer\n— but this does not prevent us from going forward with our first use case. Logic is indeed simple:\n\nfun getSchedule(scheduleDay: LocalDate): DaySchedule {\n  val daySchedule = daySchedulerRepository.get(scheduleDay)\n  if (daySchedule != null) {\n    return daySchedule\n  }\n\n  val newSchedule = dayScheduleCreator.create(scheduleDay)\n  return daySchedulerRepository.save(newSchedule)\n}\n\n\n(full commit: GitHub)\nAnd even with this simple logic we identified a hidden assumption regarding the schedule definition: that there is a\nrecipe for creating a daily schedule. What is more we can test retrieval of a schedule — with definition of schedule\ncreator if required — without any irrelevant details, like database, UI, framework and so on. Test only business rules,\nwithout unnecessary details.\nReserving the slot\nTo finish the reservation we have to add at least one more use case — one for reservation of a free slot. Provided that\nwe re-use existing logic, the interaction is still simple:\n\nfun reserve(slotId: SlotId): DaySchedule {\n  val daySchedule = getScheduleUseCase.getSchedule(scheduleDay = slotId.day)\n\n  val modifiedSchedule = daySchedule.reserveSlot(slotId.index)\n\n  return dayScheduleRepository.save(modifiedSchedule)\n}\n\n\n(full commit: GitHub)\nAnd, as we can see — the slot reservation business rule (and constraint) is implemented at the domain model itself — so\nwe are safe, that any other interaction, any other use case, is not going to break these rules. This approach also\nsimplifies testing, as business rules can be verified in separation from the use case interaction logic.\nWhere is the “Clean Architecture”?\nLet‘s stop with business logic for a moment. We created quite thoughtful, extensible code for sure, but why are we\ntalking about “Clean” architecture? We already used Domain-Driven Design and Hexagonal architecture concepts. Is there\nanything more? Imagine that another person is going to help us with implementation. She is not aware of the source code\nyet and simply would like to take a look at the codebase. And she sees: \nIt looks like something to her, doesn‘t it? A kind of reservation system! It is not yet another domain service with\nsome methods that have no clear connection with possible uses — the list of classes itself describes what the system\ncan do.\nThe first assumption\nWe have a mocked implementation as the schedule creator. It is OK to test logic at the unit test level, but not enough\nto run a prototype.\nAfter a short call with our customer we know more about the daily schedule — there are six slots, two hours each,\nstarting at 8:oo a.m. We also know that this recipe for the daily schedule is very, very simple and it is going to be\nchanged soon (e.g. to accommodate for holidays, etc.). All these issues will be solved later, now we are at the\nprototype stage and our desired outcome is to have a working demo for our stranger.\nWhere to put this simple implementation of the schedule creator? So far, the domain used an interface for that. Are we\ngoing to put an implementation of this interface to the infrastructure package and treat it as something outside the\ndomain? Certainly not! It is not complicated and this is part of the domain itself, we simply replace the mocked\nimplementation of the schedule creator with class specification.\n\npackage eu.kowalcze.michal.arch.clean.example.domain.model\n\nclass DayScheduleCreator {\n    fun create(scheduleDay: LocalDate): DaySchedule = DaySchedule(\n        scheduleDay,\n        createStandardSlots()\n    )\n//...\n}\n\n\n(full commit: GitHub)\nThe prototype\nI will not be original here — for the first prototype version the REST API sounds like something reasonable. Do we care\nabout other infrastructure at the moment? Persistence? No! In the previous commits a map-based persistence layer is\nused for unit tests and this solution is good enough to start with. As long as the system is not restarted, of course.\nWhat is important at this stage? We are introducing an API — this is a separate layer, so it is crucial to ensure\nthat domain classes are not exposed to the outside world — and that we do not introduce a dependency on the API into\nthe domain.\n\npackage eu.kowalcze.michal.arch.clean.example.api\n\n@Controller\nclass GetScheduleEndpoint(private val getScheduleUseCase: GetScheduleUseCase) {\n\n    @GetMapping(\"/schedules/{localDate}\")\n    fun getSchedules(@PathVariable localDate: String): DayScheduleDto {\n        val scheduleDay = LocalDate.parse(localDate)\n        val daySchedule = getScheduleUseCase.getSchedule(scheduleDay)\n        return daySchedule.toApi()\n    }\n\n}\n\n\n(full commit: GitHub)\nThe abstractions\nUse Case\nChecking the implementation of endpoints (see comments in the code) we can see that conceptually each endpoint executes\nlogic according to the same structure: \nWell, why don’t we make some abstraction for this? Sounds like a crazy idea? Let‘s check! Based on our code and the\ndiagram above we can identify the UseCase abstraction — something that takes some input (domain input, to be precise)\nand converts it to a (domain) output.\n\ninterface UseCase<INPUT, OUTPUT> {\n    fun apply(input: INPUT): OUTPUT\n}\n\n\n(full commit: GitHub)\nUse Case Executor\nGreat! We have use cases and I just realized that I would like to have an email in my inbox each time an exception is\nthrown — and I do not want to depend on a spring-specific mechanism to do this. A common UseCaseExecutor will be a\ngreat help to address this non-functional requirement.\n\nclass UseCaseExecutor(private val notificationGateway: NotificationGateway) {\n    fun <INPUT, OUTPUT> execute(useCase: UseCase<INPUT, OUTPUT>, input: INPUT): OUTPUT {\n        try {\n            return useCase.apply(input)\n        } catch (e: Exception) {\n            notificationGateway.notify(useCase, e)\n            throw e\n        }\n    }\n}\n\n\n(full commit: GitHub)\nFramework-independent response\nIn order to handle the next requirements in our plan we have to change the logic a bit — add the possibility of returning\nspring-specific response entities from the executor itself. To make our code reusable in a non-spring world (ktor,\nanyone?) we separated the plain executor from spring specific decorator, so that it is possible to use this code easily\nin other frameworks.\n\ndata class UseCaseApiResult<API_OUTPUT>(\n    val responseCode: Int,\n    val output: API_OUTPUT,\n)\n\nclass SpringUseCaseExecutor(private val useCaseExecutor: UseCaseExecutor) {\n    fun <DOMAIN_INPUT, DOMAIN_OUTPUT, API_OUTPUT> execute(\n        useCase: UseCase<DOMAIN_INPUT, DOMAIN_OUTPUT>,\n        input: DOMAIN_INPUT,\n        toApiConversion: (domainOutput: DOMAIN_OUTPUT) -> UseCaseApiResult<API_OUTPUT>\n    ): ResponseEntity<API_OUTPUT> {\n        return useCaseExecutor.execute(useCase, input, toApiConversion).toSpringResponse()\n    }\n}\n\nprivate fun <API_OUTPUT> UseCaseApiResult<API_OUTPUT>.toSpringResponse(): ResponseEntity<API_OUTPUT> =\n    ResponseEntity.status(responseCode).body(output)\n\n\n(full commit: GitHub)\nHandle domain exceptions\nOoops. Our prototype is running and we observe exceptions resulting in HTTP 500 errors. It would be nice to convert\nthese to dedicated response codes in a reasonable way yet without using much of spring infrastructure, for simplified\nmaintenance (and possible future changes). This can be easily achieved by adding another parameter to use case\nexecution, like this:\n\nclass UseCaseExecutor(private val notificationGateway: NotificationGateway) {\n    fun <DOMAIN_INPUT, DOMAIN_OUTPUT> execute(\n        useCase: UseCase<DOMAIN_INPUT, DOMAIN_OUTPUT>,\n        input: DOMAIN_INPUT,\n        toApiConversion: (domainOutput: DOMAIN_OUTPUT) -> UseCaseApiResult<*>,\n        handledExceptions: (ExceptionHandler.() -> Any)? = null,\n    ): UseCaseApiResult<*> {\n\n        try {\n            val domainOutput = useCase.apply(input)\n            return toApiConversion(domainOutput)\n        } catch (e: Exception) {\n            // conceptual logic\n            val exceptionHandler = ExceptionHandler(e)\n            handledExceptions?.let { exceptionHandler.handledExceptions() }\n            return UseCaseApiResult(responseCodeIfExceptionIsHandled, exceptionHandler.message ?: e.message)\n        }\n    }\n}\n\n\n(full commit: GitHub)\nHandle DTO conversion exceptions\nBy simply replacing input with:\n\ninputProvider: Any.() -> DOMAIN_INPUT,\n\n\n(full commit: GitHub)\nwe are able to handle exceptions raised during creation of input domain objects in a uniform way, without any\nadditional try/catches at the endpoint level.\nThe outcome\nWhat is the result of our journey across some functional requirements and a bit more non-functional requirements? By\nlooking at the definition of an endpoint we have full documentation of its behaviour, including exceptions. Our code is\neasily portable to some different API (e.g. EJB), we have fully-auditable modifications, and we can exchange layers\nquite freely. Also analysis of whole service is simplified, as possible use cases are explicitely stated.\n\n@PutMapping(\"/schedules/{localDate}/{index}\", produces = [\"application/json\"], consumes = [\"application/json\"])\nfun getSchedules(@PathVariable localDate: String, @PathVariable index: Int): ResponseEntity<*> =\n    useCaseExecutor.execute(\n        useCase = reserveSlotUseCase,\n        inputProvider = { SlotId(LocalDate.parse(localDate), index) },\n        toApiConversion = {\n            val dayScheduleDto = it.toApi()\n            UseCaseApiResult(HttpServletResponse.SC_ACCEPTED, dayScheduleDto)\n        },\n        handledExceptions = {\n            exception(InvalidSlotIndexException::class, UNPROCESSABLE_ENTITY, \"INVALID-SLOT-ID\")\n            exception(SlotAlreadyReservedException::class, CONFLICT, \"SLOT-ALREADY-RESERVED\")\n        },\n    )\n\n\n(repository: GitHub)\nA simple evaluation of our solution with measures mentioned at the beginning:\nAspect\n      Evaluation\n      Has advantage\n    \nDevelopment\n      UseCase abstraction forces unification of approach across different teams in a more significant way than standard service approach.\n      ✓\n    \nDeployment\n      We did not consider deployment in our example. It certainly is not going to be different/harder than in case of hexagonal architecture.\n       \n    \nOperation\n      Use case-based approach reveals operation of the system, which reduces learning curve for both development and maintenance.\n      ✓\n    \nMaintenance\n      Entry threshold might be lower compared to hexagonal approach, as service is separated horizontally (into layers) and vertically (into use cases with common domain model).\n      ✓\n    \nKeeping options open\n      Similar to hexagonal architecture approach.\n       \n    \nTL;DR\nIt is like hexagonal architecture with one additional dimension, composed of use cases, giving better insight into\noperations of a system and streamlining development and maintenance. Solution that was created during this narrative\nallows for creation of a self-documenting API endpoint.\nHigh-level overview\nWith all this read we can switch our view to the high-level perspective:\n\nand describe abstractions. Starting from the inside we have:\nDomain Model, Services and Gateways, which are responsible for defining\nbusiness rules for the domain.\nUseCase, which orchestrates execution of business rules.\nUseCaseExecutor providing common behavior for all use cases.\nAPI connecting service with the outside world.\nImplementation of gateways, which connects with other services or persistence providers.\nConfiguration, responsible for gluing all elements together.\nI hope that you enjoy this simple story and find the concept of\nthe Clean Architecture useful.\nThank you for reading!","guid":"https://blog.allegro.tech/2021/12/clean-architecture-story.html","categories":["tech","architecture","clean-architecture","ddd","kotlin"],"isoDate":"2021-12-12T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Which skills to choose in order to be more valuable","link":"https://blog.allegro.tech/2021/12/choose-your-skills.html","pubDate":"Thu, 09 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Piotr Prusiński"],"photo":["https://blog.allegro.tech/img/authors/piotr.prusinski.jpg"],"url":["https://blog.allegro.tech/authors/piotr.prusinski"]}]},"content":"<p>At some point in your career, you realize that it’s time to try to advance through the hierarchy. You think you are\ndoing a good job. You are constantly developing and learning something new. But at the same time, someone you know, with\nmuch less experience and knowledge than you, has long been higher up the hierarchy than you. Then you ask yourself:\n<em>what is wrong with me</em>? In my case, the answer turned out to be properly gathering the expectations concerning my skills\nand work.</p>\n\n<p>Below, you can read my insight about it. You will find out how you can steer your development, so that you become much\nmore valuable to your team, leader or company and how to combine all this with your talents and interests.</p>\n\n<h2 id=\"how-to-start-the-development\">How to start the development</h2>\n\n<p>As a member of a project team, you want to deliver a fully valuable solution. The key to success is to discover what’s\nbehind the word <em>valuable</em> to your client. Is it enough to gather requirements? Unfortunately not. To find out exactly\nwhat stands behind the word <em>valuable</em> you need to use a prototyping method, such as an iterative\n<a href=\"https://www.wikipedia.org/wiki/Minimum_viable_product\">MVP</a> (Minimum Viable Product) approach. For example, suppose you\nhave to paint your daughter’s room. You have several choices. You can choose the colour by yourself and paint the room\nwithout asking her for her opinion. The chance she will like the colour is small. So you will make a non-compliant\nproduct. You can bring her a sampler and ask her to choose a colour. The final colour when applied to the wall, may turn\nout to be different and then, the product will also not be as expected. You can also ask her about the colour and buy a\nfew samplers, paint a piece of wall with them and ask if any meet the requirements, if yes then paint the whole room\nwith the chosen colour, if not, buy another 5 colours and so on over and over until you get it right. This is the MVP\nprocess. First you gather requirements, then you prototype, which means you make a real product. You present it and\nreceive feedback (you learn) and decide what to do next. You repeat this process until you meet the client’s\nexpectations. This approach gives you the assurance that you have done exactly what was needed.</p>\n\n<p>Imagine that your skills and the work you do are products. We already know that to make a valuable product you need to\ngather requirements and prototype with the client. Your customer who cares most about your work and skills are the team,\nthe leader and the company you work for. However, what you do must also suit you. If you want your work and your\nknowledge to be of value to them, you need to gather their requirements first. Then quickly show the result and make\nsure it’s what they wanted. You are probably wondering what this prototyping is for in the case of your work? You have\ndone some work, it has been accepted so what more is there to talk about. There could be lots of reasons. You could have\nspent too much time on it, it’s even possible that what you did is wrong. You would have to put in a huge amount of time\nand work to improve it, which is why it was accepted as it is. Without prototyping, your work might appear to be of good\nquality to you, but the team might not be happy with it.</p>\n\n<p>Let’s start with requirements gathering. Unfortunately it is impossible to gather universal requirements for a given\nposition since these requirements are different at different companies and even teams. The same person who has certain\nskills and performs certain tasks may be a senior in one company, where in another, he or she may be a junior. In one of\nthe companies I worked for, a good programmer was required to know the products and the business, then the tenure at the\ncompany mattered as well and being a support for the business. Performing a large number of tasks was not so important.\nIn another company, there were people who knew their product very well and were an excellent support to the business.\nHowever, they did not perform their tasks on time and did not take on issues that required them to undertake a lot of\nresponsibility. They still had junior or mid-level status.</p>\n\n<p><img src=\"/img/articles/2021-12-09-choose-your-skills/expectations.png\" alt=\"Expectations\" />\nThis begs the question: how do I gather my skills requirements?</p>\n\n<h2 id=\"assess-yourself-with-some-evaluation-sheets\">Assess yourself with some evaluation sheets</h2>\n\n<p>At Allegro, in the case of programmers and testers, we have a list of expectations prepared within the leadership\ncommunity. With such a list, it is much easier to collect requirements and talk about your own development towards\npromotion (in the team or in the company).</p>\n\n<p>In my case, after working at Allegro for a while, I decided that it might be a good idea to finally get my development\non track. My leader asked me to assess myself using a requirements sheet. This allowed me to identify the current stage\nI was at. The same sheet about me was completed by my leader. Then we calibrated ourselves. This sheet is divided into 3\nmain parts:</p>\n\n<ul>\n  <li>Technical knowledge</li>\n  <li>Attitude</li>\n  <li>Product knowledge</li>\n</ul>\n\n<p>Each part has a list of skills, each with an extensive description. For example, one of the descriptions of technical\nknowledge looked like this:</p>\n\n<ul>\n  <li>You know your technological stack very well (systems, applications, languages, tools). Having a technical problem to\nsolve, you know what your technological abilities are, and you know how to use them to achieve your goal.</li>\n</ul>\n\n<p>I rated each skill on a scale of 1 to 5. Ideally, I would like each skill to be close to a 5. If I rated myself\nsomewhere at a 4 and my leader rated me a 3, during calibration, I found out that the requirements for that skill were\ndifferent from what I thought. He said, there were things I hadn’t used yet. To have a 4 at this point I need to know\nthem e.g. Cassandra database. With that I learned the first relevant requirement that I didn’t know about. After the\nwhole process of gathering requirements this way, I already knew what I needed to work on. Now all I had to do was\nselect a few of them and start working. But how to choose these requirements, which I will implement with passion and\nactually do them well?</p>\n\n<h2 id=\"how-to-choose-skills-to-improve\">How to choose skills to improve?</h2>\n\n<p>There are many strategies on how to choose skills to develop. All in all, you need to use the right method for the right\ntype of skill to get a good result. The first step is to get a mentor. Usually it will be your leader, but it can also\nbe an experienced team member, e.g. a senior.</p>\n\n<p>Leaders at Allegro gain their development knowledge from many available training sessions. You should find a similar\nperson in your environment. Such a person will help you on your way and will simulate the customer of the product that\nyou will be making. Show him or her a list of things you should develop. Ask them which are the most important for your\nteam. It is possible that the list of things will become very short. Now just pick two or three things and prepare\nprototypes. You can choose them intuitively or match them with your talents. Just take a Gallup\ntest (<a href=\"https://www.gallup.com/cliftonstrengths/en/home.aspx\">CliftonStrengths</a>) to discover them. This test will help\nyou choose a strategy for developing a particular skill. This test will work well for soft skills. For hard skills\na <a href=\"https://en.wikipedia.org/wiki/Lean_startup\">lean startup strategy</a> may be better. Below are two examples of what this\ncan look like in practice.</p>\n\n<h2 id=\"how-gallup-test-can-help-you-find-activities-that-improve-you\">How Gallup test can help you find activities that improve you</h2>\n\n<p>The <a href=\"https://www.gallup.com/cliftonstrengths/en/home.aspx\">Gallup test</a>\nhelps you find activities that you naturally enjoy doing. This natural response to certain types of information,\nsituations and people in the context of the Gallup test is called a talent. The test result is divided into four main\ndomains:</p>\n\n<ul>\n  <li>Executing</li>\n  <li>Influencing</li>\n  <li>Relationship Building</li>\n  <li>Strategic Thinking</li>\n</ul>\n\n<p>Some of these will have a direct impact on your work, others will not. For example a description of your character in\nthe context of building relationships with people will be interesting information and will certainly raise your\nawareness, but will not help us in our challenge. Other talents that come out of the challenge may fit into the skill\nyou are developing or show how you should develop it.</p>\n\n<p>Of course with these talents, the knowledge itself that you have a talent does not immediately translate into skills,\nfor example finding out that you have a talent for standing on your head, it does not mean that you will immediately\nstand on your head. You have to develop this talent, and you probably are already doing similar things because you\nsimply have a talent for it, and you enjoy it. So the Gallup test shows the things you should focus on. They are so\ngeneric that you can easily match them to your problems.</p>\n\n<p><img src=\"/img/articles/2021-12-09-choose-your-skills/gallup.png\" alt=\"Gallup\" /></p>\n\n<p>Here’s what it was like for me. I did the Gallup test, and it came out that I am a learner. I like to learn, and I\nshould share knowledge - which should make me happy. Comparing this talent to the list of skills I should develop, I\nfound out that I don’t share knowledge with others. I decided that a good form of knowledge transfer is to make\npresentations. I completed training in making presentations. I also estimated the time for preparation and set a\ndeadline. While preparing, I realized that this knowledge sharing strategy was not for me. I finished this presentation\nand decided to change my knowledge sharing strategy. Such a change in strategy is called a pivot. I discussed the\nproblem with my mentor and decided to make a pivot towards writing articles. Being in this situation you have two\nchoices: either you do a pivot, which means you change your strategy, or you persevere, which means you continue to\ndevelop using your current strategy, after getting proper feedback from your mentor and team of course. Let us now look\nat the second type of skill.</p>\n\n<h2 id=\"how-startup-method-can-help-you-develop-your-skills\">How startup method can help you develop your skills</h2>\n\n<p>Let us divide hard skills into two groups. In the first group there are skills you acquire while learning by yourself,\ne.g. a new programming language. For this you need internal motivation and discipline to achieve the expected high\nlevel. The second group is the one where you acquire skills automatically e.g. you use some framework in your project.\nFor the first group you can use the startup method.\n<img src=\"/img/articles/2021-12-09-choose-your-skills/startup.png\" alt=\"Startup\" />\nAn example scenario of what the process of developing hard skills might look like:</p>\n\n<ol>\n  <li>Choose one skill with your mentor (the order does not matter). Additionally, collect the requirements.</li>\n  <li>Estimate how long it will take you to develop that skill. Put it on your calendar.</li>\n  <li>Together with your mentor, set a first stage, i.e. the first prototype of your work and work on it for e.g. 2-3\nweeks.</li>\n  <li>After this time, show what you have done to your mentor or the whole team.</li>\n  <li>Draw conclusions. Collect feedback. Do you like doing it? Did the result meet their expectations?</li>\n  <li>If not, make a pivot to another strategy or take another skill. If so, improve what you collected during the feedback\nand keep developing.</li>\n</ol>\n\n<p>In the worst case scenario you will go through all the skills. You will then have a fairly broad knowledge base. I have\nnever heard the opinion that someone learned something and now regrets it. In a positive scenario you will probably find\nsomething you do well and enjoy it.</p>\n\n<p>We will now look at the second group of skills, which you develop automatically while working on a project. Here again,\nyou should first consult with your mentor on the best strategy to follow. There are two strategies in such cases: to\ngain deep knowledge in one of these skills or to learn a little about all of them. The mentor should know what the team\nexpects from a specialist in the given position. This automatically affects the implementation of the tasks in the\nproject:</p>\n\n<ol>\n  <li>You perform tasks only in the domain you know and after some time you become a specialist in this area, but you do\nnot take tasks from another domain that is new to you. Your value is the speed of task completion.</li>\n  <li>You always take tasks from a new domain, you are brave and not afraid of challenges. You will then have a broad\ngeneral knowledge of all products but will complete tasks much more slowly.</li>\n</ol>\n\n<p>This is just one example. It is important that the mentor is clear about his requirements and that he gives you feedback\nafter you have done a few of these tasks to make sure that this is what you wanted.</p>\n\n<h2 id=\"pivot-or-persevere\">Pivot or persevere</h2>\n\n<p>Finally, let me explain the startup terms for pivot. The term pivot means to change from one strategy to another. For\nexample, I did one pivot during my development process. I was developing the skill of knowledge sharing. I had several\nstrategies to choose from:</p>\n\n<ul>\n  <li>Making presentations</li>\n  <li>Writing articles</li>\n  <li>Training preparation</li>\n</ul>\n\n<p>I made a presentation and after collecting feedback I had to make a <code class=\"language-plaintext highlighter-rouge\">pivot</code> and changed my strategy to another one. I\nchose to share knowledge by writing articles. This article is the result of that. If this strategy works for me then I\nwill do a <code class=\"language-plaintext highlighter-rouge\">persevere</code> and continue using the chosen strategy.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>If you feel that you should already be in a higher position it is likely that the expectations for your job are quite\ndifferent than you think. To change this you should first find a mentor and then, with their help, gather the\nrequirements. A good place to start is with a list of requirements or a list of skills to begin discussing with your\nmentor. From this list you should select a few skills and then choose appropriate strategies for their development. To\nmake it easier for you to choose a strategy, do a Gallup test. Additionally, you can use the startup MVP approach.\nUltimately, the key to success is to gather requirements from your leader, team or entire company and then show the\nresults as quickly as possible. If the chosen skill or strategy doesn’t suit you or the feedback collected is strongly\nnegative, don’t worry, make a pivot and choose something else. After a few iterations you will definitely achieve your\ngoal.</p>\n","contentSnippet":"At some point in your career, you realize that it’s time to try to advance through the hierarchy. You think you are\ndoing a good job. You are constantly developing and learning something new. But at the same time, someone you know, with\nmuch less experience and knowledge than you, has long been higher up the hierarchy than you. Then you ask yourself:\nwhat is wrong with me? In my case, the answer turned out to be properly gathering the expectations concerning my skills\nand work.\nBelow, you can read my insight about it. You will find out how you can steer your development, so that you become much\nmore valuable to your team, leader or company and how to combine all this with your talents and interests.\nHow to start the development\nAs a member of a project team, you want to deliver a fully valuable solution. The key to success is to discover what’s\nbehind the word valuable to your client. Is it enough to gather requirements? Unfortunately not. To find out exactly\nwhat stands behind the word valuable you need to use a prototyping method, such as an iterative\nMVP (Minimum Viable Product) approach. For example, suppose you\nhave to paint your daughter’s room. You have several choices. You can choose the colour by yourself and paint the room\nwithout asking her for her opinion. The chance she will like the colour is small. So you will make a non-compliant\nproduct. You can bring her a sampler and ask her to choose a colour. The final colour when applied to the wall, may turn\nout to be different and then, the product will also not be as expected. You can also ask her about the colour and buy a\nfew samplers, paint a piece of wall with them and ask if any meet the requirements, if yes then paint the whole room\nwith the chosen colour, if not, buy another 5 colours and so on over and over until you get it right. This is the MVP\nprocess. First you gather requirements, then you prototype, which means you make a real product. You present it and\nreceive feedback (you learn) and decide what to do next. You repeat this process until you meet the client’s\nexpectations. This approach gives you the assurance that you have done exactly what was needed.\nImagine that your skills and the work you do are products. We already know that to make a valuable product you need to\ngather requirements and prototype with the client. Your customer who cares most about your work and skills are the team,\nthe leader and the company you work for. However, what you do must also suit you. If you want your work and your\nknowledge to be of value to them, you need to gather their requirements first. Then quickly show the result and make\nsure it’s what they wanted. You are probably wondering what this prototyping is for in the case of your work? You have\ndone some work, it has been accepted so what more is there to talk about. There could be lots of reasons. You could have\nspent too much time on it, it’s even possible that what you did is wrong. You would have to put in a huge amount of time\nand work to improve it, which is why it was accepted as it is. Without prototyping, your work might appear to be of good\nquality to you, but the team might not be happy with it.\nLet’s start with requirements gathering. Unfortunately it is impossible to gather universal requirements for a given\nposition since these requirements are different at different companies and even teams. The same person who has certain\nskills and performs certain tasks may be a senior in one company, where in another, he or she may be a junior. In one of\nthe companies I worked for, a good programmer was required to know the products and the business, then the tenure at the\ncompany mattered as well and being a support for the business. Performing a large number of tasks was not so important.\nIn another company, there were people who knew their product very well and were an excellent support to the business.\nHowever, they did not perform their tasks on time and did not take on issues that required them to undertake a lot of\nresponsibility. They still had junior or mid-level status.\n\nThis begs the question: how do I gather my skills requirements?\nAssess yourself with some evaluation sheets\nAt Allegro, in the case of programmers and testers, we have a list of expectations prepared within the leadership\ncommunity. With such a list, it is much easier to collect requirements and talk about your own development towards\npromotion (in the team or in the company).\nIn my case, after working at Allegro for a while, I decided that it might be a good idea to finally get my development\non track. My leader asked me to assess myself using a requirements sheet. This allowed me to identify the current stage\nI was at. The same sheet about me was completed by my leader. Then we calibrated ourselves. This sheet is divided into 3\nmain parts:\nTechnical knowledge\nAttitude\nProduct knowledge\nEach part has a list of skills, each with an extensive description. For example, one of the descriptions of technical\nknowledge looked like this:\nYou know your technological stack very well (systems, applications, languages, tools). Having a technical problem to\nsolve, you know what your technological abilities are, and you know how to use them to achieve your goal.\nI rated each skill on a scale of 1 to 5. Ideally, I would like each skill to be close to a 5. If I rated myself\nsomewhere at a 4 and my leader rated me a 3, during calibration, I found out that the requirements for that skill were\ndifferent from what I thought. He said, there were things I hadn’t used yet. To have a 4 at this point I need to know\nthem e.g. Cassandra database. With that I learned the first relevant requirement that I didn’t know about. After the\nwhole process of gathering requirements this way, I already knew what I needed to work on. Now all I had to do was\nselect a few of them and start working. But how to choose these requirements, which I will implement with passion and\nactually do them well?\nHow to choose skills to improve?\nThere are many strategies on how to choose skills to develop. All in all, you need to use the right method for the right\ntype of skill to get a good result. The first step is to get a mentor. Usually it will be your leader, but it can also\nbe an experienced team member, e.g. a senior.\nLeaders at Allegro gain their development knowledge from many available training sessions. You should find a similar\nperson in your environment. Such a person will help you on your way and will simulate the customer of the product that\nyou will be making. Show him or her a list of things you should develop. Ask them which are the most important for your\nteam. It is possible that the list of things will become very short. Now just pick two or three things and prepare\nprototypes. You can choose them intuitively or match them with your talents. Just take a Gallup\ntest (CliftonStrengths) to discover them. This test will help\nyou choose a strategy for developing a particular skill. This test will work well for soft skills. For hard skills\na lean startup strategy may be better. Below are two examples of what this\ncan look like in practice.\nHow Gallup test can help you find activities that improve you\nThe Gallup test\nhelps you find activities that you naturally enjoy doing. This natural response to certain types of information,\nsituations and people in the context of the Gallup test is called a talent. The test result is divided into four main\ndomains:\nExecuting\nInfluencing\nRelationship Building\nStrategic Thinking\nSome of these will have a direct impact on your work, others will not. For example a description of your character in\nthe context of building relationships with people will be interesting information and will certainly raise your\nawareness, but will not help us in our challenge. Other talents that come out of the challenge may fit into the skill\nyou are developing or show how you should develop it.\nOf course with these talents, the knowledge itself that you have a talent does not immediately translate into skills,\nfor example finding out that you have a talent for standing on your head, it does not mean that you will immediately\nstand on your head. You have to develop this talent, and you probably are already doing similar things because you\nsimply have a talent for it, and you enjoy it. So the Gallup test shows the things you should focus on. They are so\ngeneric that you can easily match them to your problems.\n\nHere’s what it was like for me. I did the Gallup test, and it came out that I am a learner. I like to learn, and I\nshould share knowledge - which should make me happy. Comparing this talent to the list of skills I should develop, I\nfound out that I don’t share knowledge with others. I decided that a good form of knowledge transfer is to make\npresentations. I completed training in making presentations. I also estimated the time for preparation and set a\ndeadline. While preparing, I realized that this knowledge sharing strategy was not for me. I finished this presentation\nand decided to change my knowledge sharing strategy. Such a change in strategy is called a pivot. I discussed the\nproblem with my mentor and decided to make a pivot towards writing articles. Being in this situation you have two\nchoices: either you do a pivot, which means you change your strategy, or you persevere, which means you continue to\ndevelop using your current strategy, after getting proper feedback from your mentor and team of course. Let us now look\nat the second type of skill.\nHow startup method can help you develop your skills\nLet us divide hard skills into two groups. In the first group there are skills you acquire while learning by yourself,\ne.g. a new programming language. For this you need internal motivation and discipline to achieve the expected high\nlevel. The second group is the one where you acquire skills automatically e.g. you use some framework in your project.\nFor the first group you can use the startup method.\n\nAn example scenario of what the process of developing hard skills might look like:\nChoose one skill with your mentor (the order does not matter). Additionally, collect the requirements.\nEstimate how long it will take you to develop that skill. Put it on your calendar.\nTogether with your mentor, set a first stage, i.e. the first prototype of your work and work on it for e.g. 2-3\nweeks.\nAfter this time, show what you have done to your mentor or the whole team.\nDraw conclusions. Collect feedback. Do you like doing it? Did the result meet their expectations?\nIf not, make a pivot to another strategy or take another skill. If so, improve what you collected during the feedback\nand keep developing.\nIn the worst case scenario you will go through all the skills. You will then have a fairly broad knowledge base. I have\nnever heard the opinion that someone learned something and now regrets it. In a positive scenario you will probably find\nsomething you do well and enjoy it.\nWe will now look at the second group of skills, which you develop automatically while working on a project. Here again,\nyou should first consult with your mentor on the best strategy to follow. There are two strategies in such cases: to\ngain deep knowledge in one of these skills or to learn a little about all of them. The mentor should know what the team\nexpects from a specialist in the given position. This automatically affects the implementation of the tasks in the\nproject:\nYou perform tasks only in the domain you know and after some time you become a specialist in this area, but you do\nnot take tasks from another domain that is new to you. Your value is the speed of task completion.\nYou always take tasks from a new domain, you are brave and not afraid of challenges. You will then have a broad\ngeneral knowledge of all products but will complete tasks much more slowly.\nThis is just one example. It is important that the mentor is clear about his requirements and that he gives you feedback\nafter you have done a few of these tasks to make sure that this is what you wanted.\nPivot or persevere\nFinally, let me explain the startup terms for pivot. The term pivot means to change from one strategy to another. For\nexample, I did one pivot during my development process. I was developing the skill of knowledge sharing. I had several\nstrategies to choose from:\nMaking presentations\nWriting articles\nTraining preparation\nI made a presentation and after collecting feedback I had to make a pivot and changed my strategy to another one. I\nchose to share knowledge by writing articles. This article is the result of that. If this strategy works for me then I\nwill do a persevere and continue using the chosen strategy.\nSummary\nIf you feel that you should already be in a higher position it is likely that the expectations for your job are quite\ndifferent than you think. To change this you should first find a mentor and then, with their help, gather the\nrequirements. A good place to start is with a list of requirements or a list of skills to begin discussing with your\nmentor. From this list you should select a few skills and then choose appropriate strategies for their development. To\nmake it easier for you to choose a strategy, do a Gallup test. Additionally, you can use the startup MVP approach.\nUltimately, the key to success is to gather requirements from your leader, team or entire company and then show the\nresults as quickly as possible. If the chosen skill or strategy doesn’t suit you or the feedback collected is strongly\nnegative, don’t worry, make a pivot and choose something else. After a few iterations you will definitely achieve your\ngoal.","guid":"https://blog.allegro.tech/2021/12/choose-your-skills.html","categories":["tech","education","skills","startup","lean startup","mvp"],"isoDate":"2021-12-08T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Moving towards Micronaut","link":"https://blog.allegro.tech/2021/11/micronaut.html","pubDate":"Mon, 22 Nov 2021 00:00:00 +0100","authors":{"author":[{"name":["Konrad Kamiński"],"photo":["https://blog.allegro.tech/img/authors/konrad.kaminski.jpg"],"url":["https://blog.allegro.tech/authors/konrad.kaminski"]}]},"content":"<p><a href=\"https://micronaut.io\">Micronaut</a> is one of the new application frameworks that have recently sprung up. It promises\nlow memory usage and faster application startup. At <a href=\"https://allegro.tech/\">Allegro</a> we decided to give it a try. In this article we’ll learn what\ncame out of it and if it’s worth considering when creating microservices-based systems.</p>\n\n<h2 id=\"paradise-city\">Paradise city</h2>\n<p>At Allegro we run a few hundred microservices, most of which use Spring Framework. We also have services created in other technologies.\nAnd to make things more complicated we run them on a few different types of clouds - our own <a href=\"http://mesos.apache.org/\">Mesos</a>-based as well as private and public <a href=\"https://kubernetes.io/\">k8s</a>-based ones.\nTherefore in order for all of it to work consistently and smoothly we created a number of supporting libraries and at the same time we defined a kind of\ncontract for all services. This way, if there is ever a need or will to create a service with a new shiny technology, it should be feasible with as little\nwork as possible. You can read more about this approach in <a href=\"https://blog.allegro.tech/2020/07/common-code-approach.html\">a great article by Piotr Betkier</a>.</p>\n\n<h2 id=\"you-gotta-fight-for-your-right-to-party\">(You Gotta) Fight for Your Right (To Party!)</h2>\n<p>In order to be up to date with current technologies, at Allegro we run hackathons where we try out the “trendy” solutions. Over a year ago\nwe decided to taste Micronaut. The framework represents one of the new approaches to some of the inherent problems of existing solutions:\nit steers clear of using Java Reflection and does as much as it can at compile or rather build time. Major things achieved this way are:</p>\n<ul>\n  <li>lower memory usage - Java Reflection in most current JDK implementations is a memory hog; Micronaut has its own implementation of Java Reflection-like API\nwhich doesn’t suffer from that problem,</li>\n  <li>faster startup - Java Reflection is also not a speed daemon; doing things ahead of time means less has to be done at runtime,</li>\n  <li>ability to create native apps - <a href=\"https://www.graalvm.org\">GraalVM</a>, another new kid on the block, allows creating native binaries out of a JVM-based application; however, there\nare some caveats and one of them is… Java Reflection (basically if your application uses it, it has to provide some metadata for the native compiler). Since\nMicronaut has its own implementation, the problem is simply non-existent.</li>\n</ul>\n\n<p>We wanted to see how difficult it is to create a new microservice with Micronaut that would run on our on-premise cloud and do something meaningful. So during\nour hackathon we defined the following goals for our simple app:</p>\n<ul>\n  <li>it should be possible to deploy the app on our on-premise cloud,</li>\n  <li>the app should provide and use basic functionalities, such as:\n    <ul>\n      <li>telemetry,</li>\n      <li>configuration management,</li>\n      <li>REST endpoints,</li>\n      <li>ability to call other microservices,</li>\n      <li>ability to send messages to <a href=\"https://github.com/allegro/hermes\">Hermes</a>,</li>\n      <li>database access (we voted for MongoDB),</li>\n      <li>(optionally) ability to be compiled into a native binary with GraalVM.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>After a very satisfying hackathon we carried the day. Our microservice had all the above-mentioned functionalities - some of them obviously\nin a makeshift form, but that didn’t matter. We achieved all the goals.</p>\n\n<h2 id=\"highway-to-hell\">Highway to Hell</h2>\n<p>The result of the hackathon pushed us forward to make something even bolder. We wanted to have a real Micronaut-based application in our production environment.\nTo make things harder - we wanted to convert an existing Spring-based system to a Micronaut-based one. Though we reached our destination, the road\nwas quite bumpy. Let’s see what awaits those who take that path.</p>\n\n<h3 id=\"paranoid\">Paranoid</h3>\n<p>To ease a migration from Spring a special <a href=\"https://micronaut-projects.github.io/micronaut-spring/latest/guide/\">micronaut-spring</a> project has been created.\nIt supports a limited selection of Spring annotations and functionality so that in theory one can just replace Spring dependencies with Micronaut ones.\nSpecifically among the most interesting features are:</p>\n<ul>\n  <li>standard inversion of control annotations: <code class=\"language-plaintext highlighter-rouge\">@Component</code>, <code class=\"language-plaintext highlighter-rouge\">@Service</code>, <code class=\"language-plaintext highlighter-rouge\">@Repository</code>,<code class=\"language-plaintext highlighter-rouge\">@Bean</code>, <code class=\"language-plaintext highlighter-rouge\">@Autowired</code>, <code class=\"language-plaintext highlighter-rouge\">@Configuration</code>, <code class=\"language-plaintext highlighter-rouge\">@Primary</code> and many others\nare converted into their Micronaut counterparts,</li>\n  <li>standard Spring interfaces: <code class=\"language-plaintext highlighter-rouge\">@Environment</code>, <code class=\"language-plaintext highlighter-rouge\">@ApplicationEventPublisher</code>, <code class=\"language-plaintext highlighter-rouge\">@ApplicationContext</code>, <code class=\"language-plaintext highlighter-rouge\">@BeanFactory</code> and their <code class=\"language-plaintext highlighter-rouge\">@*Aware</code> versions are also\nadapted to their Micronaut counterparts,</li>\n  <li>MVC controller annotations: <code class=\"language-plaintext highlighter-rouge\">@RestController</code>, <code class=\"language-plaintext highlighter-rouge\">@GetMapping</code>, <code class=\"language-plaintext highlighter-rouge\">@PostMapping</code> and many others are converted into their Micronaut counterparts.</li>\n</ul>\n\n<p>This makes the whole exercise simpler, but unfortunately it also comes at a price. Not all features are supported (e.g. Spring’s <code class=\"language-plaintext highlighter-rouge\">@PathVariable</code> is not)\nand for those which are, they sometimes have subtle differences. For this reason oftentimes you simply have to revert to the regular manual code\nconversion. The problem is that this kind of approach will lead you to a mixed solution - you’ll have both Micronaut and Spring annotations in your code.\nAnd then a question arises: which annotations should I use for the newly created code? Do we stick with the old Spring annotations if there is even\none instance of it in the current codebase? Or maybe treat this old code as a “necessary evil” and always put Micronaut annotations for the added functionality?</p>\n\n<p>We came to the conclusion that we did not want to use <em>micronaut-spring</em> at all. This\nof course led to more work, but in the end we think it was worth it. The converted application does not have any Spring dependencies, no “technical debt”.</p>\n\n<h3 id=\"sad-but-true\">Sad but True</h3>\n\n<p>One of the things not covered at all by <em>micronaut-spring</em> is exception handling in MVC.\nIn Spring our handlers looked something like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.springframework.http.HttpStatus.BAD_REQUEST</span>\n\n<span class=\"nd\">@ControllerAdvice</span>\n<span class=\"nd\">@Order</span><span class=\"p\">(</span><span class=\"nc\">Ordered</span><span class=\"p\">.</span><span class=\"nc\">HIGHEST_PRECEDENCE</span><span class=\"p\">)</span>\n<span class=\"kd\">class</span> <span class=\"nc\">DefaultExceptionHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@ExceptionHandler</span><span class=\"p\">(</span><span class=\"nc\">SomeException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">)</span>\n    <span class=\"nd\">@ResponseBody</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">handleSomeException</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">SomeException</span><span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">(</span><span class=\"nc\">ApiError</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">),</span> <span class=\"nc\">BAD_REQUEST</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>In Micronaut exception handling can be done locally (i.e. functions handling exception will only be used for the exceptions thrown by the controller the\nfunctions are defined in) or globally. Since our Spring handlers acted globally, the equivalent Micronaut code is as follows:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">io.micronaut.http.HttpStatus.BAD_REQUEST</span>\n<span class=\"k\">import</span> <span class=\"nn\">io.micronaut.http.annotation.Error</span> <span class=\"k\">as</span> <span class=\"nc\">HttpError</span>\n\n<span class=\"nd\">@Controller</span>\n<span class=\"kd\">class</span> <span class=\"nc\">DefaultExceptionHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@Error</span><span class=\"p\">(</span><span class=\"n\">global</span> <span class=\"p\">=</span> <span class=\"k\">true</span><span class=\"p\">)</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">handleSomeException</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">SomeException</span><span class=\"p\">):</span> <span class=\"nc\">HttpResponse</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nc\">HttpResponseFactory</span><span class=\"p\">.</span><span class=\"nc\">INSTANCE</span><span class=\"p\">.</span><span class=\"nf\">status</span><span class=\"p\">(</span><span class=\"nc\">BAD_REQUEST</span><span class=\"p\">,</span> <span class=\"nc\">ApiError</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">))</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"dirty-deeds-done-dirt-cheap\">Dirty Deeds Done Dirt Cheap</h3>\n\n<p>At Allegro we use many different types of databases. The application of this exercise used MongoDB. As it turned out we couldn’t have chosen worse. Don’t get me\nwrong - Micronaut supports most of the databases out there, but not all are treated equally well.</p>\n\n<p>Since our system used <a href=\"https://spring.io/projects/spring-data\">Spring Data</a>, we tried to find something similar from the Micronaut world. <a href=\"https://micronaut-projects.github.io/micronaut-data/latest/guide/\">Micronaut Data</a>\nis - as its authors say - “inspired by <em>GORM</em> and <em>Spring Data</em>”. Unfortunately the inspiration doesn’t go too far. And in case of MongoDB it actually <a href=\"https://github.com/micronaut-projects/micronaut-data/issues/220\">doesn’t even\ntake a step</a>. Instead, we used <a href=\"https://micronaut-projects.github.io/micronaut-mongodb/latest/guide/\">Micronaut MongoDB</a> library. This simple project will provide\nyour services only with either a <a href=\"https://mongodb.github.io/mongo-java-driver/4.3/apidocs/mongodb-driver-legacy/com/mongodb/MongoClient.html\">blocking MongoClient</a> or a\n<a href=\"https://mongodb.github.io/mongo-java-driver/4.3/apidocs/mongodb-driver-reactivestreams/com/mongodb/reactivestreams/client/MongoClient.html\">reactive MongoClient</a>\nalong with some healthchecks. Not enough even for a modest application.</p>\n\n<p>Fortunately some good people created <a href=\"https://litote.org/kmongo/\">kmongo</a> - a little library that helped us a lot in converting the database access part of our app.\nAt the end of the day, however, we had to create some support code to ease the migration.</p>\n\n<p>The original application database access code was in the form of reactive repositories:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.springframework.data.annotation.Id</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.springframework.data.mongodb.core.mapping.Document</span>\n\n<span class=\"nd\">@Document</span><span class=\"p\">(</span><span class=\"n\">collection</span> <span class=\"p\">=</span> <span class=\"s\">\"users\"</span><span class=\"p\">)</span>\n<span class=\"kd\">data class</span> <span class=\"nc\">User</span><span class=\"p\">(</span>\n    <span class=\"nd\">@Id</span> <span class=\"kd\">val</span> <span class=\"py\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">name</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">UserRepository</span><span class=\"p\">:</span> <span class=\"nc\">ReactiveMongoRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">,</span> <span class=\"nc\">String</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">findFirstByTypeOrderByNameDesc</span><span class=\"p\">(</span><span class=\"n\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>We wanted to preserve the interface and as much code as possible. Here is what we had to do to get this effect.</p>\n\n<p>First we decided that our components would use <code class=\"language-plaintext highlighter-rouge\">MongoDatabase</code> rather than <code class=\"language-plaintext highlighter-rouge\">MongoClient</code> offered by <em>Micronaut MongoDB</em>.\nWe had only one database so that was an obvious choice.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Factory</span>\n<span class=\"kd\">class</span> <span class=\"nc\">MongoConfig</span> <span class=\"p\">{</span>\n    <span class=\"nd\">@Singleton</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">mongoDatabase</span><span class=\"p\">(</span><span class=\"n\">mongoClient</span><span class=\"p\">:</span> <span class=\"nc\">MongoClient</span><span class=\"p\">,</span> <span class=\"n\">configuration</span><span class=\"p\">:</span> <span class=\"nc\">DefaultMongoConfiguration</span><span class=\"p\">):</span> <span class=\"nc\">MongoDatabase</span> <span class=\"p\">=</span>\n        <span class=\"n\">mongoClient</span><span class=\"p\">.</span><span class=\"nf\">getDatabase</span><span class=\"p\">(</span><span class=\"n\">configuration</span><span class=\"p\">.</span><span class=\"n\">connectionString</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">().</span><span class=\"n\">database</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Then there was the question of configuring <em>kmongo</em>. It wasn’t as straightforward as we’d thought it would be. Let’s take a look at the\nfinal code.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Factory</span>\n<span class=\"kd\">class</span> <span class=\"nc\">KMongoFactory</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@Singleton</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">kCodecRegistry</span><span class=\"p\">():</span> <span class=\"nc\">CodecRegistry</span> <span class=\"p\">{</span>\n        <span class=\"nc\">ObjectMappingConfiguration</span><span class=\"p\">.</span><span class=\"nf\">addCustomCodec</span><span class=\"p\">(</span><span class=\"nc\">JodaDateSerializationCodec</span><span class=\"p\">)</span> <span class=\"c1\">// 1 - custom Joda DateTime coded</span>\n        <span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"nf\">registerBsonModule</span><span class=\"p\">(</span><span class=\"nc\">JodaModule</span><span class=\"p\">())</span>                  <span class=\"c1\">// 2 - register default Joda module</span>\n        <span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"nf\">registerBsonModule</span><span class=\"p\">(</span><span class=\"nc\">JodaDateSerializationModule</span><span class=\"p\">)</span>   <span class=\"c1\">// 3 - register custom Joda module</span>\n        <span class=\"nf\">with</span><span class=\"p\">(</span><span class=\"nc\">KMongoConfiguration</span><span class=\"p\">.</span><span class=\"n\">bsonMapper</span><span class=\"p\">.</span><span class=\"n\">factory</span> <span class=\"k\">as</span> <span class=\"nc\">BsonFactory</span><span class=\"p\">)</span> <span class=\"p\">{</span>         <span class=\"c1\">// 4 - change BigDecimal handling</span>\n            <span class=\"nf\">disable</span><span class=\"p\">(</span><span class=\"nc\">BsonGenerator</span><span class=\"p\">.</span><span class=\"nc\">Feature</span><span class=\"p\">.</span><span class=\"nc\">WRITE_BIGDECIMALS_AS_DECIMAL128</span><span class=\"p\">)</span>\n            <span class=\"nf\">enable</span><span class=\"p\">(</span><span class=\"nc\">BsonGenerator</span><span class=\"p\">.</span><span class=\"nc\">Feature</span><span class=\"p\">.</span><span class=\"nc\">WRITE_BIGDECIMALS_AS_STRINGS</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n        <span class=\"k\">return</span> <span class=\"nc\">ClassMappingType</span><span class=\"p\">.</span><span class=\"nf\">codecRegistry</span><span class=\"p\">(</span><span class=\"nc\">MongoClientSettings</span><span class=\"p\">.</span><span class=\"nf\">getDefaultCodecRegistry</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p><code class=\"language-plaintext highlighter-rouge\">MongoDB</code> driver expects a <code class=\"language-plaintext highlighter-rouge\">CodecRegistry</code> which defines how to encode a Java object into Mongo <code class=\"language-plaintext highlighter-rouge\">BSON</code>, so that it can be persisted in a database. By default\n<em>kmongo</em> supports a simple, <a href=\"https://github.com/FasterXML/jackson\">Jackson</a> based converter. However, there were a few issues in\nour application which forced us to create some customizations:</p>\n\n<ul>\n  <li><a href=\"https://www.joda.org/joda-time/\">Joda</a> date types in entity classes - our app has a long history and it still uses <em>Joda</em> date types.\nUnfortunately they do not work with <em>kmongo</em>, so we had to teach it how to handle them. It required a few steps.\n    <ul>\n      <li>\n        <p>(1) <em>kmongo</em> had to know how to serialize a <em>Joda</em> date type to a <code class=\"language-plaintext highlighter-rouge\">MongoDB</code> date type:</p>\n\n        <div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">object</span> <span class=\"nc\">JodaDateSerializationCodec</span> <span class=\"p\">:</span> <span class=\"nc\">Codec</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">encode</span><span class=\"p\">(</span><span class=\"n\">writer</span><span class=\"p\">:</span> <span class=\"nc\">BsonWriter</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">DateTime</span><span class=\"p\">?,</span> <span class=\"n\">encoderContext</span><span class=\"p\">:</span> <span class=\"nc\">EncoderContext</span><span class=\"p\">?)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">value</span> <span class=\"p\">==</span> <span class=\"k\">null</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"n\">writer</span><span class=\"p\">.</span><span class=\"nf\">writeNull</span><span class=\"p\">()</span>\n        <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n            <span class=\"n\">writer</span><span class=\"p\">.</span><span class=\"nf\">writeDateTime</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">.</span><span class=\"n\">millis</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">getEncoderClass</span><span class=\"p\">():</span> <span class=\"nc\">Class</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">:</span> <span class=\"nc\">BsonReader</span><span class=\"p\">,</span> <span class=\"n\">decoderContext</span><span class=\"p\">:</span> <span class=\"nc\">DecoderContext</span><span class=\"p\">?):</span> <span class=\"nc\">DateTime</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nc\">DateTime</span><span class=\"p\">(</span><span class=\"n\">reader</span><span class=\"p\">.</span><span class=\"nf\">readDateTime</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>(2) <em>Jackson</em> used by <em>kmongo</em> also had to know how to handle <em>Joda</em> date types,</li>\n      <li>\n        <p>(3) to make things harder sometimes we stored a datetime as a long value, therefore we had to add support for that as well:</p>\n\n        <div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">object</span> <span class=\"nc\">JodaDateSerializationModule</span> <span class=\"p\">:</span> <span class=\"nc\">SimpleModule</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"nf\">init</span> <span class=\"p\">{</span>\n        <span class=\"nf\">addSerializer</span><span class=\"p\">(</span><span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"nc\">JodaDateSerializer</span><span class=\"p\">())</span>\n        <span class=\"nf\">addDeserializer</span><span class=\"p\">(</span><span class=\"nc\">DateTime</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"nc\">JodaDateDeserializer</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">JodaDateSerializer</span> <span class=\"p\">:</span> <span class=\"nc\">JsonSerializer</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;()</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">DateTime</span><span class=\"p\">,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?)</span> <span class=\"p\">{</span>\n        <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeObject</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">.</span><span class=\"nf\">toDate</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">JodaDateDeserializer</span> <span class=\"p\">:</span> <span class=\"nc\">JsonDeserializer</span><span class=\"p\">&lt;</span><span class=\"nc\">DateTime</span><span class=\"p\">&gt;()</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">deserialize</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">:</span> <span class=\"nc\">JsonParser</span><span class=\"p\">,</span> <span class=\"n\">ctxt</span><span class=\"p\">:</span> <span class=\"nc\">DeserializationContext</span><span class=\"p\">?):</span> <span class=\"nc\">DateTime</span> <span class=\"p\">=</span>\n        <span class=\"k\">when</span> <span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"n\">currentToken</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"nc\">JsonToken</span><span class=\"p\">.</span><span class=\"nc\">VALUE_NUMBER_INT</span> <span class=\"p\">-&gt;</span> <span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"nf\">readValueAs</span><span class=\"p\">(</span><span class=\"nc\">Long</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">).</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"o\">::</span><span class=\"nc\">DateTime</span><span class=\"p\">)</span>\n            <span class=\"k\">else</span> <span class=\"p\">-&gt;</span> <span class=\"n\">parser</span><span class=\"p\">.</span><span class=\"nf\">readValueAs</span><span class=\"p\">(</span><span class=\"nc\">Date</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">).</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"o\">::</span><span class=\"nc\">DateTime</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div>        </div>\n      </li>\n      <li>(4) finally we stored <code class=\"language-plaintext highlighter-rouge\">BigDecimal</code> values as plain <code class=\"language-plaintext highlighter-rouge\">String</code>, which is not a default behaviour of <em>kmongo</em>, so we had to change it.</li>\n    </ul>\n  </li>\n</ul>\n\n<p>As you can see some of the problems we had to face came from using either old technologies or not using them properly. It turned out there were more issues.</p>\n\n<p>For our entity IDs we usually used an artificial <code class=\"language-plaintext highlighter-rouge\">String</code> value. <code class=\"language-plaintext highlighter-rouge\">MongoDB</code> has special support for it in a form of <a href=\"https://docs.mongodb.com/manual/reference/method/ObjectId/\"><code class=\"language-plaintext highlighter-rouge\">ObjectId</code></a>\ntype, which we gladly used in our application. But, here a new issue came up - in order to make our integration tests easier to read and write we used <code class=\"language-plaintext highlighter-rouge\">String</code>-type IDs\nnot conformant to <code class=\"language-plaintext highlighter-rouge\">ObjectId</code> restrictions (so for example our user IDs were <code class=\"language-plaintext highlighter-rouge\">user-1</code>, <code class=\"language-plaintext highlighter-rouge\">user-2</code>, etc.).\n<em>Spring Data</em> handles this transparently, but here we had to introduce one more customization. Our entity classes now\nhad to contain a special annotation indicating what serializer to use for our ID fields:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">import</span> <span class=\"nn\">org.bson.codecs.pojo.annotations.BsonId</span>\n\n<span class=\"kd\">data class</span> <span class=\"nc\">User</span><span class=\"p\">(</span>\n    <span class=\"nd\">@BsonId</span> <span class=\"nd\">@JsonSerialize</span><span class=\"p\">(</span><span class=\"n\">using</span> <span class=\"p\">=</span> <span class=\"nc\">CustomIdJsonSerializer</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">)</span> <span class=\"kd\">val</span> <span class=\"py\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">name</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">CustomIdJsonSerializer</span> <span class=\"p\">:</span> <span class=\"nc\">StdScalarSerializer</span><span class=\"p\">&lt;</span><span class=\"nc\">String</span><span class=\"p\">&gt;(</span><span class=\"nc\">String</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">,</span> <span class=\"k\">false</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">?,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?)</span> <span class=\"p\">=</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">value</span> <span class=\"p\">!=</span> <span class=\"k\">null</span> <span class=\"p\">&amp;&amp;</span> <span class=\"nc\">ObjectId</span><span class=\"p\">.</span><span class=\"nf\">isValid</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">))</span> <span class=\"p\">{</span> <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeObjectId</span><span class=\"p\">(</span><span class=\"nc\">ObjectId</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">))</span> <span class=\"p\">}</span>\n        <span class=\"k\">else</span> <span class=\"p\">{</span> <span class=\"n\">gen</span><span class=\"p\">.</span><span class=\"nf\">writeString</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">)</span> <span class=\"p\">}</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">serializeWithType</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">?,</span> <span class=\"n\">gen</span><span class=\"p\">:</span> <span class=\"nc\">JsonGenerator</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">:</span> <span class=\"nc\">SerializerProvider</span><span class=\"p\">?,</span> <span class=\"n\">typeSer</span><span class=\"p\">:</span> <span class=\"nc\">TypeSerializer</span><span class=\"p\">?)</span> <span class=\"p\">=</span>\n        <span class=\"nf\">serialize</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">,</span> <span class=\"n\">gen</span><span class=\"p\">,</span> <span class=\"n\">serializers</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">isEmpty</span><span class=\"p\">(</span><span class=\"n\">value</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Boolean</span> <span class=\"p\">=</span> <span class=\"n\">value</span><span class=\"p\">.</span><span class=\"nf\">isEmpty</span><span class=\"p\">()</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">acceptJsonFormatVisitor</span><span class=\"p\">(</span><span class=\"n\">visitor</span><span class=\"p\">:</span> <span class=\"nc\">JsonFormatVisitorWrapper</span><span class=\"p\">?,</span> <span class=\"n\">typeHint</span><span class=\"p\">:</span> <span class=\"nc\">JavaType</span><span class=\"p\">?)</span> <span class=\"p\">=</span> <span class=\"nf\">visitStringFormat</span><span class=\"p\">(</span><span class=\"n\">visitor</span><span class=\"p\">,</span> <span class=\"n\">typeHint</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>With the basics set up we could now focus on how to make the <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes work with as little effort as possible. We decided to create a base <code class=\"language-plaintext highlighter-rouge\">BaseRepository</code>\nclass letting us write concrete <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes easier:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">abstract</span> <span class=\"kd\">class</span> <span class=\"nc\">BaseRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;(</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">mongoDatabase</span><span class=\"p\">:</span> <span class=\"nc\">MongoDatabase</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">collectionName</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span>\n    <span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">clazz</span><span class=\"p\">:</span> <span class=\"nc\">Class</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span>\n<span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">open</span> <span class=\"k\">fun</span> <span class=\"nf\">findById</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">findOne</span><span class=\"p\">(</span><span class=\"nf\">eq</span><span class=\"p\">(</span><span class=\"s\">\"_id\"</span><span class=\"p\">,</span> <span class=\"n\">id</span><span class=\"p\">.</span><span class=\"nf\">maybeObjectId</span><span class=\"p\">()))</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">findOne</span><span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">:</span> <span class=\"nc\">Bson</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span> <span class=\"nf\">withCollection</span> <span class=\"p\">{</span>\n        <span class=\"nf\">find</span><span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">).</span><span class=\"nf\">toMono</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">R</span><span class=\"p\">&gt;</span> <span class=\"nf\">withCollection</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">:</span> <span class=\"nc\">MongoCollection</span><span class=\"p\">&lt;</span><span class=\"nc\">T</span><span class=\"p\">&gt;.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">R</span><span class=\"p\">):</span> <span class=\"nc\">R</span> <span class=\"p\">=</span>\n                <span class=\"n\">mongoDatabase</span>\n                    <span class=\"p\">.</span><span class=\"nf\">getCollection</span><span class=\"p\">(</span><span class=\"n\">collectionName</span><span class=\"p\">,</span> <span class=\"n\">clazz</span><span class=\"p\">)</span>\n                    <span class=\"p\">.</span><span class=\"nf\">let</span><span class=\"p\">(</span><span class=\"n\">fn</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Finally we wrote the <code class=\"language-plaintext highlighter-rouge\">*Repository</code> classes. A rewritten version of the <code class=\"language-plaintext highlighter-rouge\">UserRepository</code> mentioned at the beginning of this section looked like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@Context</span>\n<span class=\"kd\">class</span> <span class=\"nc\">UserRepository</span><span class=\"p\">(</span>\n    <span class=\"n\">mongoDatabase</span><span class=\"p\">:</span> <span class=\"nc\">MongoDatabase</span>\n<span class=\"p\">):</span> <span class=\"nc\">BaseRepository</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;(</span><span class=\"n\">mongoDatabase</span><span class=\"p\">,</span> <span class=\"s\">\"users\"</span><span class=\"p\">,</span> <span class=\"nc\">User</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">findFirstByTypeOrderByNameDesc</span><span class=\"p\">(</span><span class=\"n\">type</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">Mono</span><span class=\"p\">&lt;</span><span class=\"nc\">User</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nf\">withCollection</span> <span class=\"p\">{</span>\n            <span class=\"nf\">find</span><span class=\"p\">(</span><span class=\"nf\">and</span><span class=\"p\">(</span><span class=\"nf\">eq</span><span class=\"p\">(</span><span class=\"s\">\"type\"</span><span class=\"p\">,</span> <span class=\"n\">type</span><span class=\"p\">)))</span>\n                <span class=\"p\">.</span><span class=\"nf\">sort</span><span class=\"p\">(</span><span class=\"nf\">descending</span><span class=\"p\">(</span><span class=\"s\">\"name\"</span><span class=\"p\">))</span>\n                <span class=\"p\">.</span><span class=\"nf\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"p\">.</span><span class=\"nf\">toMono</span><span class=\"p\">()</span>\n        <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h3 id=\"fear-of-the-dark\">Fear of the Dark</h3>\n\n<p><a href=\"https://spockframework.org/spock/docs/2.0/index.html\">Spock</a> is our framework of choice for writing tests. We still tend to use it even in <code class=\"language-plaintext highlighter-rouge\">Kotlin</code> applications,\nalthough sometimes the resulting code is not as clear as it’d be if not for <code class=\"language-plaintext highlighter-rouge\">Groovy</code> (`coroutines!). So how does\n<em>Micronaut</em> work with <em>Spock</em>? Actually, quite well.</p>\n\n<p>For testing there is a <a href=\"https://github.com/micronaut-projects/micronaut-test\">micronaut-test</a> project, which provides testing extensions for <em>Spock</em>\nand many other testing libraries. <a href=\"https://docs.spring.io/spring-framework/docs/current/reference/html/testing.html\">The general approach to writing test cases with Spring</a> which we were familiar with,\nis very similar in <em>micronaut-test</em>. Let’s have a look at a simple test case:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@MicronautTest</span> <span class=\"c1\">// 1</span>\n<span class=\"kd\">class</span> <span class=\"nc\">SimpleIntSpec</span> <span class=\"kd\">extends</span> <span class=\"n\">Specification</span> <span class=\"o\">{</span>\n    <span class=\"nd\">@Inject</span> <span class=\"c1\">// 2</span>\n    <span class=\"n\">UserService</span> <span class=\"n\">userService</span>\n\n    <span class=\"kt\">def</span> <span class=\"s2\">\"should persist a user\"</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n        <span class=\"nl\">given:</span>\n        <span class=\"n\">userService</span><span class=\"o\">.</span><span class=\"na\">createUser</span><span class=\"o\">(</span><span class=\"s2\">\"user-1\"</span><span class=\"o\">,</span> <span class=\"s2\">\"John\"</span><span class=\"o\">,</span> <span class=\"s2\">\"Doe\"</span><span class=\"o\">)</span>\n\n        <span class=\"nl\">when:</span>\n        <span class=\"kt\">def</span> <span class=\"n\">user</span> <span class=\"o\">=</span> <span class=\"n\">userService</span><span class=\"o\">.</span><span class=\"na\">getUser</span><span class=\"o\">(</span><span class=\"s2\">\"user-1\"</span><span class=\"o\">)</span>\n\n        <span class=\"nl\">then:</span>\n        <span class=\"n\">user</span><span class=\"o\">.</span><span class=\"na\">firstName</span> <span class=\"o\">==</span> <span class=\"s2\">\"John\"</span>\n        <span class=\"n\">user</span><span class=\"o\">.</span><span class=\"na\">lastName</span> <span class=\"o\">==</span> <span class=\"s2\">\"Doe\"</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<p>There are two interesting things in this test case:</p>\n<ul>\n  <li>(1) <code class=\"language-plaintext highlighter-rouge\">@MicronautTest</code> is an annotation you have to put in your test classes to start <em>Micronaut</em> application,</li>\n  <li>(2) <code class=\"language-plaintext highlighter-rouge\">@Inject</code> is <a href=\"https://micronaut.io/\">Micronaut</a>’s version of <code class=\"language-plaintext highlighter-rouge\">@Autowired</code> (or… <code class=\"language-plaintext highlighter-rouge\">@Inject</code>, which is also supported by <code class=\"language-plaintext highlighter-rouge\">Spring</code>). Be aware that since\n<em>Micronaut</em> <code class=\"language-plaintext highlighter-rouge\">3.0.0</code> you should use <code class=\"language-plaintext highlighter-rouge\">@jakarta.inject.Inject</code> annotation instead of the former <code class=\"language-plaintext highlighter-rouge\">@javax.inject.Inject</code>.</li>\n</ul>\n\n<p>If your tests make API calls to your application via REST endpoints, and you run your web container on a random port (which is common), then the way to retrieve it\nis through the use of the injected <code class=\"language-plaintext highlighter-rouge\">EmbeddedServer</code>:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@MicronautTest</span>\n<span class=\"kd\">class</span> <span class=\"nc\">ApiIntSpec</span> <span class=\"kd\">extends</span> <span class=\"n\">Specification</span> <span class=\"o\">{</span>\n    <span class=\"nd\">@Inject</span>\n    <span class=\"n\">EmbeddedServer</span> <span class=\"n\">server</span>\n\n    <span class=\"kt\">def</span> <span class=\"s2\">\"should create a user using API call\"</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n        <span class=\"nl\">given:</span>\n        <span class=\"kt\">def</span> <span class=\"n\">url</span> <span class=\"o\">=</span> <span class=\"s2\">\"http://localhost:{$server.port}/users\"</span>\n\n        <span class=\"nl\">when:</span>\n        <span class=\"c1\">// here goes your test...</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"money\">Money</h2>\n\n<p>As a side effect, an additional benefit you get when you use <em>Micronaut</em> is a faster development cycle. As stated at the\nbeginning of this post, one of the main features of this framework is faster startup. Therefore, when writing test cases and then running tests,\ntheir execution time is lower than their Spring equivalent. This may not be significant if your tests are few, but sooner or later\ntheir number will grow and then the speed will become more visible and important. For large codebases time savings can be really impressive.</p>\n\n<h2 id=\"should-i-stay-or-should-i-go\">Should I stay or should I go</h2>\n\n<p>The experience we gained during migration to <em>Micronaut</em> gave us more courage and assurance. So when the time came to decide what technology\nto use for a quite large greenfield project, we didn’t hesitate (well, we actually did, but not for long).\nSix months later with the system running in the production environment we’re happy we started that long journey. And if you’re considering\n<em>Micronaut</em> for one of your projects, I can wholeheartedly recommend: go for it.</p>\n","contentSnippet":"Micronaut is one of the new application frameworks that have recently sprung up. It promises\nlow memory usage and faster application startup. At Allegro we decided to give it a try. In this article we’ll learn what\ncame out of it and if it’s worth considering when creating microservices-based systems.\nParadise city\nAt Allegro we run a few hundred microservices, most of which use Spring Framework. We also have services created in other technologies.\nAnd to make things more complicated we run them on a few different types of clouds - our own Mesos-based as well as private and public k8s-based ones.\nTherefore in order for all of it to work consistently and smoothly we created a number of supporting libraries and at the same time we defined a kind of\ncontract for all services. This way, if there is ever a need or will to create a service with a new shiny technology, it should be feasible with as little\nwork as possible. You can read more about this approach in a great article by Piotr Betkier.\n(You Gotta) Fight for Your Right (To Party!)\nIn order to be up to date with current technologies, at Allegro we run hackathons where we try out the “trendy” solutions. Over a year ago\nwe decided to taste Micronaut. The framework represents one of the new approaches to some of the inherent problems of existing solutions:\nit steers clear of using Java Reflection and does as much as it can at compile or rather build time. Major things achieved this way are:\nlower memory usage - Java Reflection in most current JDK implementations is a memory hog; Micronaut has its own implementation of Java Reflection-like API\nwhich doesn’t suffer from that problem,\nfaster startup - Java Reflection is also not a speed daemon; doing things ahead of time means less has to be done at runtime,\nability to create native apps - GraalVM, another new kid on the block, allows creating native binaries out of a JVM-based application; however, there\nare some caveats and one of them is… Java Reflection (basically if your application uses it, it has to provide some metadata for the native compiler). Since\nMicronaut has its own implementation, the problem is simply non-existent.\nWe wanted to see how difficult it is to create a new microservice with Micronaut that would run on our on-premise cloud and do something meaningful. So during\nour hackathon we defined the following goals for our simple app:\nit should be possible to deploy the app on our on-premise cloud,\nthe app should provide and use basic functionalities, such as:\n    \ntelemetry,\nconfiguration management,\nREST endpoints,\nability to call other microservices,\nability to send messages to Hermes,\ndatabase access (we voted for MongoDB),\n(optionally) ability to be compiled into a native binary with GraalVM.\nAfter a very satisfying hackathon we carried the day. Our microservice had all the above-mentioned functionalities - some of them obviously\nin a makeshift form, but that didn’t matter. We achieved all the goals.\nHighway to Hell\nThe result of the hackathon pushed us forward to make something even bolder. We wanted to have a real Micronaut-based application in our production environment.\nTo make things harder - we wanted to convert an existing Spring-based system to a Micronaut-based one. Though we reached our destination, the road\nwas quite bumpy. Let’s see what awaits those who take that path.\nParanoid\nTo ease a migration from Spring a special micronaut-spring project has been created.\nIt supports a limited selection of Spring annotations and functionality so that in theory one can just replace Spring dependencies with Micronaut ones.\nSpecifically among the most interesting features are:\nstandard inversion of control annotations: @Component, @Service, @Repository,@Bean, @Autowired, @Configuration, @Primary and many others\nare converted into their Micronaut counterparts,\nstandard Spring interfaces: @Environment, @ApplicationEventPublisher, @ApplicationContext, @BeanFactory and their @*Aware versions are also\nadapted to their Micronaut counterparts,\nMVC controller annotations: @RestController, @GetMapping, @PostMapping and many others are converted into their Micronaut counterparts.\nThis makes the whole exercise simpler, but unfortunately it also comes at a price. Not all features are supported (e.g. Spring’s @PathVariable is not)\nand for those which are, they sometimes have subtle differences. For this reason oftentimes you simply have to revert to the regular manual code\nconversion. The problem is that this kind of approach will lead you to a mixed solution - you’ll have both Micronaut and Spring annotations in your code.\nAnd then a question arises: which annotations should I use for the newly created code? Do we stick with the old Spring annotations if there is even\none instance of it in the current codebase? Or maybe treat this old code as a “necessary evil” and always put Micronaut annotations for the added functionality?\nWe came to the conclusion that we did not want to use micronaut-spring at all. This\nof course led to more work, but in the end we think it was worth it. The converted application does not have any Spring dependencies, no “technical debt”.\nSad but True\nOne of the things not covered at all by micronaut-spring is exception handling in MVC.\nIn Spring our handlers looked something like this:\n\nimport org.springframework.http.HttpStatus.BAD_REQUEST\n\n@ControllerAdvice\n@Order(Ordered.HIGHEST_PRECEDENCE)\nclass DefaultExceptionHandler {\n\n    @ExceptionHandler(SomeException::class)\n    @ResponseBody\n    fun handleSomeException(e: SomeException): ResponseEntity<*> = ResponseEntity(ApiError(e.message), BAD_REQUEST)\n}\n\n\nIn Micronaut exception handling can be done locally (i.e. functions handling exception will only be used for the exceptions thrown by the controller the\nfunctions are defined in) or globally. Since our Spring handlers acted globally, the equivalent Micronaut code is as follows:\n\nimport io.micronaut.http.HttpStatus.BAD_REQUEST\nimport io.micronaut.http.annotation.Error as HttpError\n\n@Controller\nclass DefaultExceptionHandler {\n\n    @Error(global = true)\n    fun handleSomeException(e: SomeException): HttpResponse<*> =\n        HttpResponseFactory.INSTANCE.status(BAD_REQUEST, ApiError(e.message))\n}\n\n\nDirty Deeds Done Dirt Cheap\nAt Allegro we use many different types of databases. The application of this exercise used MongoDB. As it turned out we couldn’t have chosen worse. Don’t get me\nwrong - Micronaut supports most of the databases out there, but not all are treated equally well.\nSince our system used Spring Data, we tried to find something similar from the Micronaut world. Micronaut Data\nis - as its authors say - “inspired by GORM and Spring Data”. Unfortunately the inspiration doesn’t go too far. And in case of MongoDB it actually doesn’t even\ntake a step. Instead, we used Micronaut MongoDB library. This simple project will provide\nyour services only with either a blocking MongoClient or a\nreactive MongoClient\nalong with some healthchecks. Not enough even for a modest application.\nFortunately some good people created kmongo - a little library that helped us a lot in converting the database access part of our app.\nAt the end of the day, however, we had to create some support code to ease the migration.\nThe original application database access code was in the form of reactive repositories:\n\nimport org.springframework.data.annotation.Id\nimport org.springframework.data.mongodb.core.mapping.Document\n\n@Document(collection = \"users\")\ndata class User(\n    @Id val id: String,\n    val name: String,\n    val type: String\n)\n\nclass UserRepository: ReactiveMongoRepository<User, String> {\n    fun findFirstByTypeOrderByNameDesc(type: String): Mono<User>\n}\n\n\nWe wanted to preserve the interface and as much code as possible. Here is what we had to do to get this effect.\nFirst we decided that our components would use MongoDatabase rather than MongoClient offered by Micronaut MongoDB.\nWe had only one database so that was an obvious choice.\n\n@Factory\nclass MongoConfig {\n    @Singleton\n    fun mongoDatabase(mongoClient: MongoClient, configuration: DefaultMongoConfiguration): MongoDatabase =\n        mongoClient.getDatabase(configuration.connectionString.get().database)\n}\n\n\nThen there was the question of configuring kmongo. It wasn’t as straightforward as we’d thought it would be. Let’s take a look at the\nfinal code.\n\n@Factory\nclass KMongoFactory {\n\n    @Singleton\n    fun kCodecRegistry(): CodecRegistry {\n        ObjectMappingConfiguration.addCustomCodec(JodaDateSerializationCodec) // 1 - custom Joda DateTime coded\n        KMongoConfiguration.registerBsonModule(JodaModule())                  // 2 - register default Joda module\n        KMongoConfiguration.registerBsonModule(JodaDateSerializationModule)   // 3 - register custom Joda module\n        with(KMongoConfiguration.bsonMapper.factory as BsonFactory) {         // 4 - change BigDecimal handling\n            disable(BsonGenerator.Feature.WRITE_BIGDECIMALS_AS_DECIMAL128)\n            enable(BsonGenerator.Feature.WRITE_BIGDECIMALS_AS_STRINGS)\n        }\n        return ClassMappingType.codecRegistry(MongoClientSettings.getDefaultCodecRegistry())\n    }\n}\n\n\nMongoDB driver expects a CodecRegistry which defines how to encode a Java object into Mongo BSON, so that it can be persisted in a database. By default\nkmongo supports a simple, Jackson based converter. However, there were a few issues in\nour application which forced us to create some customizations:\nJoda date types in entity classes - our app has a long history and it still uses Joda date types.\nUnfortunately they do not work with kmongo, so we had to teach it how to handle them. It required a few steps.\n    \n(1) kmongo had to know how to serialize a Joda date type to a MongoDB date type:\n\nobject JodaDateSerializationCodec : Codec<DateTime> {\n    override fun encode(writer: BsonWriter, value: DateTime?, encoderContext: EncoderContext?) {\n        if (value == null) {\n            writer.writeNull()\n        } else {\n            writer.writeDateTime(value.millis)\n        }\n    }\n\n    override fun getEncoderClass(): Class<DateTime> {\n        return DateTime::class.java\n    }\n\n    override fun decode(reader: BsonReader, decoderContext: DecoderContext?): DateTime {\n        return DateTime(reader.readDateTime())\n    }\n}\n\n        \n(2) Jackson used by kmongo also had to know how to handle Joda date types,\n(3) to make things harder sometimes we stored a datetime as a long value, therefore we had to add support for that as well:\n\nobject JodaDateSerializationModule : SimpleModule() {\n    init {\n        addSerializer(DateTime::class.java, JodaDateSerializer())\n        addDeserializer(DateTime::class.java, JodaDateDeserializer())\n    }\n}\n\nclass JodaDateSerializer : JsonSerializer<DateTime>() {\n    override fun serialize(value: DateTime, gen: JsonGenerator, serializers: SerializerProvider?) {\n        gen.writeObject(value.toDate())\n    }\n}\n\nclass JodaDateDeserializer : JsonDeserializer<DateTime>() {\n    override fun deserialize(parser: JsonParser, ctxt: DeserializationContext?): DateTime =\n        when (parser.currentToken) {\n            JsonToken.VALUE_NUMBER_INT -> parser.readValueAs(Long::class.java).let(::DateTime)\n            else -> parser.readValueAs(Date::class.java).let(::DateTime)\n        }\n}\n\n        \n(4) finally we stored BigDecimal values as plain String, which is not a default behaviour of kmongo, so we had to change it.\nAs you can see some of the problems we had to face came from using either old technologies or not using them properly. It turned out there were more issues.\nFor our entity IDs we usually used an artificial String value. MongoDB has special support for it in a form of ObjectId\ntype, which we gladly used in our application. But, here a new issue came up - in order to make our integration tests easier to read and write we used String-type IDs\nnot conformant to ObjectId restrictions (so for example our user IDs were user-1, user-2, etc.).\nSpring Data handles this transparently, but here we had to introduce one more customization. Our entity classes now\nhad to contain a special annotation indicating what serializer to use for our ID fields:\n\nimport org.bson.codecs.pojo.annotations.BsonId\n\ndata class User(\n    @BsonId @JsonSerialize(using = CustomIdJsonSerializer::class) val id: String,\n    val name: String,\n    val type: String\n)\n\nclass CustomIdJsonSerializer : StdScalarSerializer<String>(String::class.java, false) {\n    override fun serialize(value: String?, gen: JsonGenerator, serializers: SerializerProvider?) =\n        if (value != null && ObjectId.isValid(value)) { gen.writeObjectId(ObjectId(value)) }\n        else { gen.writeString(value) }\n\n    override fun serializeWithType(value: String?, gen: JsonGenerator, serializers: SerializerProvider?, typeSer: TypeSerializer?) =\n        serialize(value, gen, serializers)\n\n    override fun isEmpty(value: String): Boolean = value.isEmpty()\n\n    override fun acceptJsonFormatVisitor(visitor: JsonFormatVisitorWrapper?, typeHint: JavaType?) = visitStringFormat(visitor, typeHint)\n}\n\n\nWith the basics set up we could now focus on how to make the *Repository classes work with as little effort as possible. We decided to create a base BaseRepository\nclass letting us write concrete *Repository classes easier:\n\nabstract class BaseRepository<T>(\n    private val mongoDatabase: MongoDatabase,\n    private val collectionName: String,\n    private val clazz: Class<T>\n) {\n    open fun findById(id: String): Mono<T> = findOne(eq(\"_id\", id.maybeObjectId()))\n\n    fun findOne(filter: Bson): Mono<T> = withCollection {\n        find(filter).toMono()\n    }\n\n    fun <R> withCollection(fn: MongoCollection<T>.() -> R): R =\n                mongoDatabase\n                    .getCollection(collectionName, clazz)\n                    .let(fn)\n}\n\n\nFinally we wrote the *Repository classes. A rewritten version of the UserRepository mentioned at the beginning of this section looked like this:\n\n@Context\nclass UserRepository(\n    mongoDatabase: MongoDatabase\n): BaseRepository<User>(mongoDatabase, \"users\", User::class.java) {\n\n    fun findFirstByTypeOrderByNameDesc(type: String): Mono<User> =\n        withCollection {\n            find(and(eq(\"type\", type)))\n                .sort(descending(\"name\"))\n                .limit(1)\n                .toMono()\n        }\n}\n\n\nFear of the Dark\nSpock is our framework of choice for writing tests. We still tend to use it even in Kotlin applications,\nalthough sometimes the resulting code is not as clear as it’d be if not for Groovy (`coroutines!). So how does\nMicronaut work with Spock? Actually, quite well.\nFor testing there is a micronaut-test project, which provides testing extensions for Spock\nand many other testing libraries. The general approach to writing test cases with Spring which we were familiar with,\nis very similar in micronaut-test. Let’s have a look at a simple test case:\n\n@MicronautTest // 1\nclass SimpleIntSpec extends Specification {\n    @Inject // 2\n    UserService userService\n\n    def \"should persist a user\"() {\n        given:\n        userService.createUser(\"user-1\", \"John\", \"Doe\")\n\n        when:\n        def user = userService.getUser(\"user-1\")\n\n        then:\n        user.firstName == \"John\"\n        user.lastName == \"Doe\"\n    }\n}\n\n\nThere are two interesting things in this test case:\n(1) @MicronautTest is an annotation you have to put in your test classes to start Micronaut application,\n(2) @Inject is Micronaut’s version of @Autowired (or… @Inject, which is also supported by Spring). Be aware that since\nMicronaut 3.0.0 you should use @jakarta.inject.Inject annotation instead of the former @javax.inject.Inject.\nIf your tests make API calls to your application via REST endpoints, and you run your web container on a random port (which is common), then the way to retrieve it\nis through the use of the injected EmbeddedServer:\n\n@MicronautTest\nclass ApiIntSpec extends Specification {\n    @Inject\n    EmbeddedServer server\n\n    def \"should create a user using API call\"() {\n        given:\n        def url = \"http://localhost:{$server.port}/users\"\n\n        when:\n        // here goes your test...\n    }\n}\n\n\nMoney\nAs a side effect, an additional benefit you get when you use Micronaut is a faster development cycle. As stated at the\nbeginning of this post, one of the main features of this framework is faster startup. Therefore, when writing test cases and then running tests,\ntheir execution time is lower than their Spring equivalent. This may not be significant if your tests are few, but sooner or later\ntheir number will grow and then the speed will become more visible and important. For large codebases time savings can be really impressive.\nShould I stay or should I go\nThe experience we gained during migration to Micronaut gave us more courage and assurance. So when the time came to decide what technology\nto use for a quite large greenfield project, we didn’t hesitate (well, we actually did, but not for long).\nSix months later with the system running in the production environment we’re happy we started that long journey. And if you’re considering\nMicronaut for one of your projects, I can wholeheartedly recommend: go for it.","guid":"https://blog.allegro.tech/2021/11/micronaut.html","categories":["tech","backend","performance","micronaut","kotlin","graalvm"],"isoDate":"2021-11-21T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999785422127","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"229d607a-333b-431b-9abe-78137730f5fd","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:56:17.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785422127","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999785421861","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"a6b2b59e-28e3-4bfa-89ab-b13ab97f06c8","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:54:52.000Z","location":{"city":"Warszawa, Poznań, Kraków, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785421861","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999779485268","name":"Chief Architect","uuid":"5ea8adaa-e9ae-4cb2-a1dc-b27600247ffb","refNumber":"REF2835R","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T13:53:22.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Gdańsk, Łódź, Katowice, Lublin","region":"Masovian Voivodeship","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572787","label":"IT - Technical Platform"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572787","valueLabel":"IT - Technical Platform"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"chief architect, architekt, architect, platform, architektura"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779485268","creator":{"name":"Angelika Szymkiewicz"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779448775","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"c8e577cc-c93a-43e7-8e73-e430989798d7","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:36.000Z","location":{"city":"Warszawa, Kraków, Poznań, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448775","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779448676","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"7cb35dfc-f53c-4b51-81ac-61b683060f4c","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:00.000Z","location":{"city":"Warszawa, Poznań, Kraków, Toruń, Wrocław, Gdańsk, Katowice, Łódź, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Więcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"Błonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448676","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1638356820000,"duration":7200000,"id":"282421464","name":"Allegro Tech Labs #9 Online: System design workshop","date_in_series_pattern":false,"status":"past","time":1639501200000,"local_date":"2021-12-14","local_time":"18:00","updated":1639514166000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":62,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/282421464/","description":"❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: https://app.evenea.pl/event/allegro-tech-labs-9/ ❗ To już druga edycja naszych warsztatów System Design Workshop! Czy chcesz poznać tajniki tworzenia systemów?…","how_to_find_us":"https://app.evenea.pl/event/allegro-tech-labs-9/","visibility":"public","member_pay_fee":false},{"created":1635344914000,"duration":7200000,"id":"281692274","name":"Allegro Tech Live #23 - Przygody backendowców w C#","date_in_series_pattern":false,"status":"past","time":1636045200000,"local_date":"2021-11-04","local_time":"18:00","updated":1636056125000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":29,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281692274/","description":"------ Rejestracja: https://app.evenea.pl/event/allegro-tech-live-23/------- Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Kiedyś spotykaliśmy się w naszych biurach, a teraz…","visibility":"public","member_pay_fee":false},{"created":1634290537000,"duration":5400000,"id":"281441586","name":"Allegro Tech Live #22 - Jak wygląda codzienność lidera w Allegro?","date_in_series_pattern":false,"status":"past","time":1634832000000,"local_date":"2021-10-21","local_time":"18:00","updated":1634841007000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":67,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281441586/","description":"!!!! Rejestracja: https://app.evenea.pl/event/allegro-tech-live-22/ !!!! Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale…","visibility":"public","member_pay_fee":false},{"created":1633336352000,"duration":7200000,"id":"281199452","name":"Allegro Tech Live #21 - Jak zautomatyzować bezpieczeństwo IT?","date_in_series_pattern":false,"status":"past","time":1633622400000,"local_date":"2021-10-07","local_time":"18:00","updated":1633634017000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281199452/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Michoński"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Michoński"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Michoński","summary":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wygląda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zespół odpowiedzialny za narzędzia i infrastrukturę dla przetwarzania danych? Kiedy możemy mówić o dużych danych i ile petabajtów przetwarza Allegro? Skąd pochodzą dane Allegro i dlaczego jest ich tak dużo oraz z jakiego powodu dopiero teraz przenosimy się do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz – Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Gałek"]},"title":"Od inżyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Gałek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Gałek","summary":"Czym jest Opbox i jakie wyzwania przed nim stoją? Jak w Allegro angażujemy się w rozwój kultury Open Source? Ile mamy projektów na GitHubie i jak świętujemy Hacktoberfest? W jaki sposób można rozwinąć się od inżyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Gałek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}