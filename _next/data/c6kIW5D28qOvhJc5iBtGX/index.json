{"pageProps":{"posts":[{"title":"How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study","link":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","pubDate":"Tue, 13 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Kamil Starczak"],"photo":["https://blog.allegro.tech/img/authors/kamil.starczak.jpg"],"url":["https://blog.allegro.tech/authors/kamil.starczak"]}]},"content":"<p>Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with <a href=\"https://azure.microsoft.com/services/cosmos-db/\">Azure\nCosmos DB</a> and batch processing.</p>\n\n<h2 id=\"our-story\">Our story</h2>\n\n<p>At Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.</p>\n\n<p>In this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:</p>\n\n<ul>\n  <li>The overall time of such a batch operation cannot exceed 5 minutes per 1 million records.</li>\n  <li>The processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.</li>\n  <li>The solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.</li>\n  <li>The solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.</li>\n</ul>\n\n<p>The outcomes of this case study are published as an open source repository (see <a href=\"#our-library\">Our library</a>).</p>\n\n<h2 id=\"cosmos-db--the-basics\">Cosmos DB — the basics</h2>\n\n<p>Before going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the <a href=\"#database-utilization\">Database utilization</a> chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (<a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/#features\">source</a>). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.</p>\n\n<p>How does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>foreach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n</code></pre></div></div>\n\n<p>For each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.</p>\n\n<p>What’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.</p>\n\n<h2 id=\"cosmos-db--scaling-and-provisioning\">Cosmos DB — scaling and provisioning</h2>\n\n<p>Cosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img01.png\" alt=\"Cosmos DB Request Units overview\" /></p>\n\n<p>Source: <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\">https://docs.microsoft.com/en-us/azure/cosmos-db/request-units</a></p>\n\n<p>But what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.</p>\n\n<p>At this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.</p>\n\n<h3 id=\"manual\">Manual</h3>\n\n<p>In the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.</p>\n\n<p>What happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img02.png\" alt=\"Manual mode visualized\" /></p>\n\n<h3 id=\"autoscale\">Autoscale</h3>\n\n<p>The second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img03.png\" alt=\"Autoscale mode visualized\" /></p>\n\n<p>What’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.</p>\n\n<p>Nevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.</p>\n\n<h3 id=\"serverless\">Serverless</h3>\n\n<p>The last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img04.png\" alt=\"Serverless mode visualized\" /></p>\n\n<p>Unfortunately, if we read the docs, we can find some additional information:</p>\n\n<ul>\n  <li>The maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.</li>\n  <li>The guarantees for the operation latencies are worse — 30ms instead of 10ms.</li>\n  <li>Serverless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.</li>\n  <li>Moreover, the maximum throughput during a single second is 5000 RUs.</li>\n</ul>\n\n<p>As we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.</p>\n\n<p>To sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.</p>\n\n<h2 id=\"database-utilization\">Database utilization</h2>\n\n<p>Let’s go back to the sample code I was running.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Foreach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n</code></pre></div></div>\n\n<p>It processed 50k records in about 10 minutes. How loaded was the database?</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img05.png\" alt=\"Normalized RU Consumption metric\" /></p>\n\n<p>Normalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.</p>\n\n<p>If we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.</p>\n\n<p>This time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.</p>\n\n<h3 id=\"ru-limiter\">RU limiter</h3>\n\n<p>Is there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img06.png\" alt=\"RU limiter visualized\" /></p>\n\n<p>The mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img07.png\" alt=\"Total Request Units metric\" /></p>\n\n<h3 id=\"partition-key-ranges\">Partition key ranges</h3>\n\n<p>It almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img08.png\" alt=\"Normalized RU Consumption metric\" /></p>\n\n<p>Something is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled <code class=\"language-plaintext highlighter-rouge\">PartitionKeyRangeId</code>. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img09.png\" alt=\"Normalized RU Consumption metric split by Partition Key ranges\" /></p>\n\n<p>If we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.</p>\n\n<h3 id=\"a-bit-of-reverse-engineering\">A bit of reverse engineering</h3>\n\n<p>Is there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.</p>\n\n<h2 id=\"the-autoscaler-auto-scaler\">The “Autoscaler auto scaler”</h2>\n\n<p>The problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.</p>\n\n<p>The one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.</p>\n\n<p>But what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img10.png\" alt=\"Autoscaler visualized during batch processing\" /></p>\n\n<p>With this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.</p>\n\n<p>This way, we have fulfilled all the requirements that were set at the beginning:</p>\n\n<ul>\n  <li>Batch processing time — 5 minutes per 1 million records.</li>\n  <li>No risk of starving other processes, thanks to the RU limiter.</li>\n  <li>Cost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.</li>\n  <li>Scalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.</li>\n</ul>\n\n<h2 id=\"our-library\">Our library</h2>\n\n<p>The outcomes of this work have been published as an opensource .NET library on our GitHub page:\n<a href=\"https://github.com/allegro/cosmosdb-utils/tree/main/src/Allegro.CosmosDb.BatchUtilities\">Allegro.CosmosDb.BatchUtilities</a>.\nFeel free to use it or even contribute new features.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n\n<p>And here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:</p>\n\n<ul>\n  <li>Cosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.</li>\n  <li>Cosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.</li>\n  <li>You must pay close attention to the costs, as it is very easy to get high bills by misusing the service.</li>\n  <li>It’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.</li>\n</ul>\n","contentSnippet":"Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with Azure\nCosmos DB and batch processing.\nOur story\nAt Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.\nIn this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:\nThe overall time of such a batch operation cannot exceed 5 minutes per 1 million records.\nThe processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.\nThe solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.\nThe solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.\nThe outcomes of this case study are published as an open source repository (see Our library).\nCosmos DB — the basics\nBefore going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the Database utilization chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (source). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.\nHow does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:\n\nforeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nFor each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.\nWhat’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.\nCosmos DB — scaling and provisioning\nCosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.\n\nSource: https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\nBut what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.\nAt this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.\nManual\nIn the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.\nWhat happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.\n\nAutoscale\nThe second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.\n\nWhat’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.\nNevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.\nServerless\nThe last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.\n\nUnfortunately, if we read the docs, we can find some additional information:\nThe maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.\nThe guarantees for the operation latencies are worse — 30ms instead of 10ms.\nServerless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.\nMoreover, the maximum throughput during a single second is 5000 RUs.\nAs we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.\nTo sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.\nDatabase utilization\nLet’s go back to the sample code I was running.\n\nForeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nIt processed 50k records in about 10 minutes. How loaded was the database?\n\nNormalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.\nIf we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.\nThis time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.\nRU limiter\nIs there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.\n\nThe mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.\n\nPartition key ranges\nIt almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.\n\nSomething is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled PartitionKeyRangeId. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.\n\nIf we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.\nA bit of reverse engineering\nIs there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.\nThe “Autoscaler auto scaler”\nThe problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.\nThe one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.\nBut what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).\n\nWith this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.\nThis way, we have fulfilled all the requirements that were set at the beginning:\nBatch processing time — 5 minutes per 1 million records.\nNo risk of starving other processes, thanks to the RU limiter.\nCost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.\nScalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.\nOur library\nThe outcomes of this work have been published as an opensource .NET library on our GitHub page:\nAllegro.CosmosDb.BatchUtilities.\nFeel free to use it or even contribute new features.\nConclusions\nAnd here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:\nCosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.\nCosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.\nYou must pay close attention to the costs, as it is very easy to get high bills by misusing the service.\nIt’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.","guid":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","categories":["tech","cloud","azure","cosmosdb"],"isoDate":"2022-09-12T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"MBox: server-driven UI for mobile apps","link":"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html","pubDate":"Wed, 03 Aug 2022 00:00:00 +0200","authors":{"author":[{"name":["Paulina Sadowska"],"photo":["https://blog.allegro.tech/img/authors/paulina.sadowska.jpg"],"url":["https://blog.allegro.tech/authors/paulina.sadowska"]}]},"content":"<p>In this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the\nfirst version of the in-house server-driven rendering tool called MBox and used it to render the\nhomepage in the Allegro app on <a href=\"https://play.google.com/store/apps/details?id=pl.allegro\">Android</a>\nand <a href=\"https://apps.apple.com/pl/app/allegro/id305659772\">iOS</a>. We have come a long way since then, and now we use this\ntool to render more and more screens in the Allegro app.</p>\n\n<p>After over three years of working on MBox, we want to share how it works and the key advantages and challenges of using this approach.</p>\n\n<h2 id=\"why-server-driven-ui\">Why server-driven UI?</h2>\n\n<p>The idea behind MBox was to make mobile development faster without compromising the app quality. Implementing a\nfeature twice for Android and iOS takes a lot of time and requires two people with unique skill sets (knowledge of\nAndroid and iOS frameworks). There is also the risk that both apps will not behave consistently because each person may\ninterpret the requirements slightly differently.</p>\n\n<p>Using a server-driven UI solves that problem because each business feature is implemented only once on the backend.\nThat gives us consistency out of the box and shortens the time needed to implement the feature.\nAlso, developers don’t need to know mobile frameworks to develop for mobile anymore.</p>\n\n<p>Another advantage of server-driven UI is that it allows releasing features independently of the release train. We\ncan deploy changes multiple times a day and when something goes wrong — roll back to the previous version immediately.\nIt gives teams a lot more flexibility and allows them to experiment and iterate much faster. What’s more, deployed\nchanges are visible to all clients, no matter which app version they use.</p>\n\n<h2 id=\"how-does-mbox-work\">How does MBox work?</h2>\n\n<h3 id=\"defining-the-screen-layout\">Defining the screen layout</h3>\n\n<p>While designing MBox, we wanted to create a tool that would give developers total flexibility to implement any layout\nthey need — as long as it’s consistent with our design system, Metrum.</p>\n\n<p>That’s why MBox screens are built using primitive components, which our rendering libraries map to native views.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/1_MBox_SG.png\" alt=\"MessageWidget structure\" /></p>\n\n<p>Developers can arrange MBox components freely using different types of containers that MBox supports (<code class=\"language-plaintext highlighter-rouge\">flex-container</code>,\n<code class=\"language-plaintext highlighter-rouge\">stack-container</code>, <code class=\"language-plaintext highlighter-rouge\">absolute-container</code>, <code class=\"language-plaintext highlighter-rouge\">list-container</code>, etc.). Those components can be styled and configured to match\ndifferent business scenarios.</p>\n\n<p>MBox renders components on mobile apps consistently, but it also respects slight differences unique to Android and\niOS platforms.\nFor example, dialog action in MBox supports the same functionalities on both platforms, but the dialog itself looks\ndifferent on Android and iOS:</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/6_alert.png\" alt=\"MBox dialog action on Android and iOS\" /></p>\n\n<p>That gives MBox screens a native look and feel and perfectly blends in with parts of the app developed\nnatively, without MBox. We had to add a label that shows which parts of the app are rendered by MBox, because\neven mobile developers couldn’t tell where native screens ended and MBox started.</p>\n\n<h3 id=\"what-about-more-complex-views\">What about more complex views?</h3>\n\n<p>Creating more complex, reusable views is also possible. For example, our design system specifies something called the\nmessage: an element with a vertical line, an icon, and some texts and buttons. However, because this element is complex\nand its requirements may change over time, it’s defined on the backend service as a widget — the element that developers\ncan reuse across different screens.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/2_MessageWidget.png\" alt=\"MessageWidget structure\" /></p>\n\n<p>If the requirements for the message widget change, we can easily modify it on the backend side without the need to\nrelease the app. That’s because it’s not defined directly in the MBox libraries included in the mobile apps, but\nspecified on the backend using MBox components.</p>\n\n<h3 id=\"unified-tracking\">Unified tracking</h3>\n\n<p>Besides defining layouts, MBox also allows us to specify tracking events on the backend. For tracking events,\nconsistency is crucial. If events are not triggered under the same scenarios and with the same data\non both platforms, it’s hard to compare the data and make business decisions.</p>\n\n<p>MBox solves that problem. All events tracked on MBox screens are defined on the backend, meaning unified tracking\nbetween Android and iOS and across different app versions.</p>\n\n<h3 id=\"testing\">Testing</h3>\n\n<p>Since the MBox rendering engine is a core of more and more screens in the app, it had to be thoroughly covered by unit\ntests and integration tests. We also have screenshot tests that ensure that MBox components render correctly. That\nallows us to find out early about possible regressions.</p>\n\n<p>Teams that develop screens using MBox also have various tools that allow them to test their features. They can write\nunit tests in the MBox backend service and check if correct MBox components are created for the given data.\nThey can also add a URL of their page to Visual Regression — the tool that creates a screenshot of\nthe page whenever someone commits anything to the MBox backend and if any change in the page is detected, the author is\nautomatically notified in their pull request.</p>\n\n<p>Feature teams can also write UI tests for the native apps to test how their page integrates with the rest of the app and\nif all interactions work as expected. However, those tests have to be written on both platforms by the mobile developers\nand should take into account that the content of the page under tests can be changed on the backend.</p>\n\n<h2 id=\"the-journey-to-make-mbox-interactive\">The journey to make MBox interactive</h2>\n\n<p>When we started working on MBox, we were focused mainly on pages that contain a lot of frequently changing content but\nnot many interactions with users. In the first version of MBox, it was possible to define only basic actions like\nopening a new screen or adding an offer to the cart. That changed gradually when new teams started using MBox.</p>\n\n<p>To make MBox more interactive, we used the same atomic approach we adopted when designing MBox layout components. We\ngradually added generic actions that were not custom-made to serve specific business features but were reusable across\ndifferent use cases.</p>\n\n<h3 id=\"for-example\">For example:</h3>\n\n<p>One of the first challenges that we faced was allowing the implementation of an “add to watchlist” star in MBox. We\ncould’ve just added the ”watchlist star” component that checks if a user is logged in (redirects to the login page if it’s\nnot), adds an offer to the watchlist, and changes the star icon from empty to full. In the short term, it should have\nbeen easier. But it’s not a way that would allow MBox to scale.</p>\n\n<p>Instead, we designed a couple of atomic mechanisms that allow building this feature on the backend and could be reused\nin the future in different use cases.</p>\n\n<p>We added a logic component called <code class=\"language-plaintext highlighter-rouge\">multivariant</code> that allows changing one component into another thanks to the\n<code class=\"language-plaintext highlighter-rouge\">changeVariant</code> action. That enabled us to switch the star icon from empty to full. Next, we added the <code class=\"language-plaintext highlighter-rouge\">sendRequest</code>\naction\nthat sends requests with given URL, headers, and other data to our services. That allows adding and removing an offer to\nand from the watchlist. Lastly, we added the <code class=\"language-plaintext highlighter-rouge\">loginIfNeeded</code> action that allows checking if a user is logged in and\nredirecting to the login screen if needed. That allows ensuring the user is logged in before making the request.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/3_add_to_watched.png\" alt=\"Add to watchlist: scheme\" /></p>\n\n<p>Of course, doing it this way took much more time than just implementing the ”add to watchlist” component in MBox libraries\nnatively. But this is the way that scales and gives us flexibility.</p>\n\n<p>Over time mechanisms that we designed earlier were reused on other screens. And more and more often, when the new team\nwanted to use MBox on their screen, most of the building blocks they needed were already there. It definitely\nwouldn’t be the case if not for our atomic approach.</p>\n\n<h2 id=\"the-challenges\">The challenges</h2>\n\n<p>We also encountered many challenges while working on MBox.</p>\n\n<h3 id=\"consistency-of-the-engines\">Consistency of the engines</h3>\n\n<p>We create two separate rendering engines for mobile platforms, so we must be extra cautious to ensure everything works consistently.\nEven a tiny inconsistency in the behavior of the engines may be hugely problematic for the developers that use MBox.\nIt may force them to, for example, define different layouts for each mobile platform.</p>\n\n<p>To make sure the engines are consistent, each feature in MBox is implemented synchronously by a pair of developers\n(Android and iOS) who consult with each other regularly. During the work, they make sure that they interpret the\nrequirements and cover edge cases in the same way.\nThe new features are ready to merge only after thorough tests on both platforms that check both correctness and\nconsistency.</p>\n\n<h3 id=\"versioning\">Versioning</h3>\n\n<p>On MBox, we also have to pay close attention to versioning. We use semantic versioning in the engines. Each new feature\nhas to be marked with the same minor and major version on both platforms. We also prepare changelogs containing\ninformation about what\nfunctionalities are available in which version.</p>\n\n<p>On the backend, we allow checking the version of the MBox engine that the user has and serve different content depending\non it.\nFor example, when the screen contains the <code class=\"language-plaintext highlighter-rouge\">switch</code> component, supported since version <code class=\"language-plaintext highlighter-rouge\">1.21</code>,\nwe can define that for users who have the app with the older versions of MBox, <code class=\"language-plaintext highlighter-rouge\">checkbox</code> will be displayed instead.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/5_fallback.png\" alt=\"Fallback mechanism\" /></p>\n\n<h3 id=\"testing-changes-introduced-to-the-engines\">Testing changes introduced to the engines</h3>\n\n<p>And last but not least: testing. Because MBox is used to render various screens in Allegro mobile apps, we must be\ncautious whenever we introduce engine changes to avoid negatively impacting existing MBox screens.\nThe screenshot and UI tests cover every MBox component and action. We’re also encouraging feature teams to add their\nscreens to the Visual Regression and cover their screens with UI tests in the mobile repositories. All those things\nallow us to minimize the risk of introducing a regression.</p>\n\n<h2 id=\"how-does-mbox-connect-to-other-parts-of-the-allegro-ecosystem\">How does MBox connect to other parts of the Allegro ecosystem?</h2>\n\n<p>Consistency across mobile platforms is not everything. Another important aspect of our work is making sure mobile and\nweb platforms are as consistent as possible, respecting native differences that make each platform unique.</p>\n\n<p>MBox integrates with our content management system, also used for the web (<a href=\"https://blog.allegro.tech/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">Opbox</a> Page Manager). The screen’s content\nconfigured in the Opbox admin panel is sent through the Opbox services to the MBox backend service. The MBox service\nmaps the\ndata into MBox components that make up the MBox screen. Then the screen definition in JSON format is sent to apps and is\nrendered using native views.</p>\n\n<p>The same data from Opbox is also used to render the web equivalent of the same screen. Opbox defines its own mappings\nfor the web: Opbox Components, which describe how to map the data into HTML elements that make up the Allegro web pages.</p>\n\n<p><img src=\"/img/articles/2022-08-03-mbox-server-driven-ui-for-mobile-apps/4_architecture.png\" alt=\"Add to watchlist: scheme\" /></p>\n\n<p>Integration with Opbox gives us a lot of advantages. Very often, to change the content in the app and web, you don’t\nneed to change the code at all — all you need to do is change the content in the Opbox admin panel.</p>\n\n<p>Another huge advantage is that we have unified tracking between all platforms and can use the same tools for A/B testing\nthat are used for the web. Previously, code for A/B tests had to be written for each mobile platform separately in\nnative\ncode and then cleaned up after the finished experiment. Now, some experiments work out of the box since Opbox sends\ndifferent data to MBox depending on the experiment variant the user falls into. Sometimes a little bit of code in the\nMBox backend is required to conduct an experiment, but it’s not comparable to the amount of work A/B tests take when\nthey’re performed in the native code without MBox.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n\n<p>MBox is a tool that changed how we work on mobile apps in Allegro. It allowed us to shorten the development time without\ncompromising the quality and stability of the app and without losing the native look and feel of the Allegro apps.</p>\n\n<p>We have come a long way during those three years since we started working on MBox. At first, our ambition was to create\na tool that would be used on content screens with very few interactions. Over time, we pushed the boundaries of what\nMBox\nis capable of and entered screens with more and more interactions with the user.</p>\n\n<p>Currently MBox is used in over 25 screens in Allegro mobile apps and the number is still growing. In the first half of\n2022 alone, 27 teams made changes to the app using MBox and created about 300 pull requests. We deployed changes over\n100 times which means ~4.15 releases a week.</p>\n\n<p>We’re confident that it’s not the end of the possibilities ahead of us. We still see how we can make MBox even more\npowerful. We’d love to shorten development time even more by providing tools that allow defining MBox screens in\nTypeScript. That’ll enable developers to reuse some parts of the code between mobile and web and take advantage of\nbetter tools such as hot reload. Another thing we’re currently focused on is adding the binding mechanism to MBox and\nthe client-side logic to allow defining the business logic on the backend. Implementing those mechanisms will allow\nintroducing even more interactivity into MBox screens.</p>\n\n<p>But that is the topic for the next articles. Stay tuned!</p>\n","contentSnippet":"In this article, we want to share our approach to using server-driven UI in native mobile apps. In 2019 we created the\nfirst version of the in-house server-driven rendering tool called MBox and used it to render the\nhomepage in the Allegro app on Android\nand iOS. We have come a long way since then, and now we use this\ntool to render more and more screens in the Allegro app.\nAfter over three years of working on MBox, we want to share how it works and the key advantages and challenges of using this approach.\nWhy server-driven UI?\nThe idea behind MBox was to make mobile development faster without compromising the app quality. Implementing a\nfeature twice for Android and iOS takes a lot of time and requires two people with unique skill sets (knowledge of\nAndroid and iOS frameworks). There is also the risk that both apps will not behave consistently because each person may\ninterpret the requirements slightly differently.\nUsing a server-driven UI solves that problem because each business feature is implemented only once on the backend.\nThat gives us consistency out of the box and shortens the time needed to implement the feature.\nAlso, developers don’t need to know mobile frameworks to develop for mobile anymore.\nAnother advantage of server-driven UI is that it allows releasing features independently of the release train. We\ncan deploy changes multiple times a day and when something goes wrong — roll back to the previous version immediately.\nIt gives teams a lot more flexibility and allows them to experiment and iterate much faster. What’s more, deployed\nchanges are visible to all clients, no matter which app version they use.\nHow does MBox work?\nDefining the screen layout\nWhile designing MBox, we wanted to create a tool that would give developers total flexibility to implement any layout\nthey need — as long as it’s consistent with our design system, Metrum.\nThat’s why MBox screens are built using primitive components, which our rendering libraries map to native views.\n\nDevelopers can arrange MBox components freely using different types of containers that MBox supports (flex-container,\nstack-container, absolute-container, list-container, etc.). Those components can be styled and configured to match\ndifferent business scenarios.\nMBox renders components on mobile apps consistently, but it also respects slight differences unique to Android and\niOS platforms.\nFor example, dialog action in MBox supports the same functionalities on both platforms, but the dialog itself looks\ndifferent on Android and iOS:\n\nThat gives MBox screens a native look and feel and perfectly blends in with parts of the app developed\nnatively, without MBox. We had to add a label that shows which parts of the app are rendered by MBox, because\neven mobile developers couldn’t tell where native screens ended and MBox started.\nWhat about more complex views?\nCreating more complex, reusable views is also possible. For example, our design system specifies something called the\nmessage: an element with a vertical line, an icon, and some texts and buttons. However, because this element is complex\nand its requirements may change over time, it’s defined on the backend service as a widget — the element that developers\ncan reuse across different screens.\n\nIf the requirements for the message widget change, we can easily modify it on the backend side without the need to\nrelease the app. That’s because it’s not defined directly in the MBox libraries included in the mobile apps, but\nspecified on the backend using MBox components.\nUnified tracking\nBesides defining layouts, MBox also allows us to specify tracking events on the backend. For tracking events,\nconsistency is crucial. If events are not triggered under the same scenarios and with the same data\non both platforms, it’s hard to compare the data and make business decisions.\nMBox solves that problem. All events tracked on MBox screens are defined on the backend, meaning unified tracking\nbetween Android and iOS and across different app versions.\nTesting\nSince the MBox rendering engine is a core of more and more screens in the app, it had to be thoroughly covered by unit\ntests and integration tests. We also have screenshot tests that ensure that MBox components render correctly. That\nallows us to find out early about possible regressions.\nTeams that develop screens using MBox also have various tools that allow them to test their features. They can write\nunit tests in the MBox backend service and check if correct MBox components are created for the given data.\nThey can also add a URL of their page to Visual Regression — the tool that creates a screenshot of\nthe page whenever someone commits anything to the MBox backend and if any change in the page is detected, the author is\nautomatically notified in their pull request.\nFeature teams can also write UI tests for the native apps to test how their page integrates with the rest of the app and\nif all interactions work as expected. However, those tests have to be written on both platforms by the mobile developers\nand should take into account that the content of the page under tests can be changed on the backend.\nThe journey to make MBox interactive\nWhen we started working on MBox, we were focused mainly on pages that contain a lot of frequently changing content but\nnot many interactions with users. In the first version of MBox, it was possible to define only basic actions like\nopening a new screen or adding an offer to the cart. That changed gradually when new teams started using MBox.\nTo make MBox more interactive, we used the same atomic approach we adopted when designing MBox layout components. We\ngradually added generic actions that were not custom-made to serve specific business features but were reusable across\ndifferent use cases.\nFor example:\nOne of the first challenges that we faced was allowing the implementation of an “add to watchlist” star in MBox. We\ncould’ve just added the ”watchlist star” component that checks if a user is logged in (redirects to the login page if it’s\nnot), adds an offer to the watchlist, and changes the star icon from empty to full. In the short term, it should have\nbeen easier. But it’s not a way that would allow MBox to scale.\nInstead, we designed a couple of atomic mechanisms that allow building this feature on the backend and could be reused\nin the future in different use cases.\nWe added a logic component called multivariant that allows changing one component into another thanks to the\nchangeVariant action. That enabled us to switch the star icon from empty to full. Next, we added the sendRequest\naction\nthat sends requests with given URL, headers, and other data to our services. That allows adding and removing an offer to\nand from the watchlist. Lastly, we added the loginIfNeeded action that allows checking if a user is logged in and\nredirecting to the login screen if needed. That allows ensuring the user is logged in before making the request.\n\nOf course, doing it this way took much more time than just implementing the ”add to watchlist” component in MBox libraries\nnatively. But this is the way that scales and gives us flexibility.\nOver time mechanisms that we designed earlier were reused on other screens. And more and more often, when the new team\nwanted to use MBox on their screen, most of the building blocks they needed were already there. It definitely\nwouldn’t be the case if not for our atomic approach.\nThe challenges\nWe also encountered many challenges while working on MBox.\nConsistency of the engines\nWe create two separate rendering engines for mobile platforms, so we must be extra cautious to ensure everything works consistently.\nEven a tiny inconsistency in the behavior of the engines may be hugely problematic for the developers that use MBox.\nIt may force them to, for example, define different layouts for each mobile platform.\nTo make sure the engines are consistent, each feature in MBox is implemented synchronously by a pair of developers\n(Android and iOS) who consult with each other regularly. During the work, they make sure that they interpret the\nrequirements and cover edge cases in the same way.\nThe new features are ready to merge only after thorough tests on both platforms that check both correctness and\nconsistency.\nVersioning\nOn MBox, we also have to pay close attention to versioning. We use semantic versioning in the engines. Each new feature\nhas to be marked with the same minor and major version on both platforms. We also prepare changelogs containing\ninformation about what\nfunctionalities are available in which version.\nOn the backend, we allow checking the version of the MBox engine that the user has and serve different content depending\non it.\nFor example, when the screen contains the switch component, supported since version 1.21,\nwe can define that for users who have the app with the older versions of MBox, checkbox will be displayed instead.\n\nTesting changes introduced to the engines\nAnd last but not least: testing. Because MBox is used to render various screens in Allegro mobile apps, we must be\ncautious whenever we introduce engine changes to avoid negatively impacting existing MBox screens.\nThe screenshot and UI tests cover every MBox component and action. We’re also encouraging feature teams to add their\nscreens to the Visual Regression and cover their screens with UI tests in the mobile repositories. All those things\nallow us to minimize the risk of introducing a regression.\nHow does MBox connect to other parts of the Allegro ecosystem?\nConsistency across mobile platforms is not everything. Another important aspect of our work is making sure mobile and\nweb platforms are as consistent as possible, respecting native differences that make each platform unique.\nMBox integrates with our content management system, also used for the web (Opbox Page Manager). The screen’s content\nconfigured in the Opbox admin panel is sent through the Opbox services to the MBox backend service. The MBox service\nmaps the\ndata into MBox components that make up the MBox screen. Then the screen definition in JSON format is sent to apps and is\nrendered using native views.\nThe same data from Opbox is also used to render the web equivalent of the same screen. Opbox defines its own mappings\nfor the web: Opbox Components, which describe how to map the data into HTML elements that make up the Allegro web pages.\n\nIntegration with Opbox gives us a lot of advantages. Very often, to change the content in the app and web, you don’t\nneed to change the code at all — all you need to do is change the content in the Opbox admin panel.\nAnother huge advantage is that we have unified tracking between all platforms and can use the same tools for A/B testing\nthat are used for the web. Previously, code for A/B tests had to be written for each mobile platform separately in\nnative\ncode and then cleaned up after the finished experiment. Now, some experiments work out of the box since Opbox sends\ndifferent data to MBox depending on the experiment variant the user falls into. Sometimes a little bit of code in the\nMBox backend is required to conduct an experiment, but it’s not comparable to the amount of work A/B tests take when\nthey’re performed in the native code without MBox.\nConclusions\nMBox is a tool that changed how we work on mobile apps in Allegro. It allowed us to shorten the development time without\ncompromising the quality and stability of the app and without losing the native look and feel of the Allegro apps.\nWe have come a long way during those three years since we started working on MBox. At first, our ambition was to create\na tool that would be used on content screens with very few interactions. Over time, we pushed the boundaries of what\nMBox\nis capable of and entered screens with more and more interactions with the user.\nCurrently MBox is used in over 25 screens in Allegro mobile apps and the number is still growing. In the first half of\n2022 alone, 27 teams made changes to the app using MBox and created about 300 pull requests. We deployed changes over\n100 times which means ~4.15 releases a week.\nWe’re confident that it’s not the end of the possibilities ahead of us. We still see how we can make MBox even more\npowerful. We’d love to shorten development time even more by providing tools that allow defining MBox screens in\nTypeScript. That’ll enable developers to reuse some parts of the code between mobile and web and take advantage of\nbetter tools such as hot reload. Another thing we’re currently focused on is adding the binding mechanism to MBox and\nthe client-side logic to allow defining the business logic on the backend. Implementing those mechanisms will allow\nintroducing even more interactivity into MBox screens.\nBut that is the topic for the next articles. Stay tuned!","guid":"https://blog.allegro.tech/2022/08/mbox-server-driven-ui-for-mobile-apps.html","categories":["tech","Server-driven UI","mobile","mbox"],"isoDate":"2022-08-02T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How to facilitate EventStorming workshops","link":"https://blog.allegro.tech/2022/07/event-storming-workshops.html","pubDate":"Tue, 19 Jul 2022 00:00:00 +0200","authors":{"author":[{"name":["Krzysztof Przychodzki"],"photo":["https://blog.allegro.tech/img/authors/krzysztof.przychodzki.jpg"],"url":["https://blog.allegro.tech/authors/krzysztof.przychodzki"]}]},"content":"<p>With this article, I would like to introduce you to EventStorming and explain to you how to get started. I am not discovering\nanything new, just gathering available knowledge in one place. What I will show you is a few tips on how to conduct\nand facilitate EventStorming workshops.</p>\n\n<h2 id=\"guide-to-big-picture-eventstorming\">Guide to Big Picture EventStorming</h2>\n\n<h3 id=\"introducing-eventstorming\">Introducing EventStorming</h3>\n\n<p>In 2013 Alberto Brandolini posted an <a href=\"https://ziobrando.blogspot.com/2013/11/introducing-event-storming.html\">article</a>\nabout a new workshop format for quick exploration of complex business domains. It was warmly welcomed by the DDD community.\nIn 2015 <a href=\"https://www.thoughtworks.com/radar/techniques/event-storming\">Technology Radar</a> described EventStorming as <em>worthy of attention</em>\nand three years later as <em>a recommended method</em> for business domain modelling in information systems.</p>\n\n<p>During the years a lot has changed, the technique has developed and matured but the main idea remained the same:</p>\n<blockquote>\n  <p><em>EventStorming is a flexible workshop format that allows a massive collaborative exploration of complex domains (…)\nwhere software and business practitioners are building together a behavioural model of the whole business line.</em></p>\n</blockquote>\n\n<p>The above definition is from Alberto Brandolini’s <em><a href=\"https://leanpub.com/introducing_eventstorming\">Introducing EventStorming</a></em>,\nto which I will be referring in this article.</p>\n\n<h2 id=\"before-launching\">Before launching</h2>\n\n<h3 id=\"provide-unlimited-modelling-space\">Provide unlimited modelling space</h3>\n\n<p>Why is it important? Because you want participants to explore and experiment during the workshop. You don’t want to\nimpose limits on them or to allow a situation where someone doesn’t add an event because there is no space left.</p>\n\n<p>For stationary session you need:</p>\n\n<ul>\n  <li>wall where you attach plotter paper (it is easier to stick post-its on plotter paper),</li>\n  <li>stickies in different colours, shapes and quantity (will discuss it later),</li>\n  <li>markers</li>\n</ul>\n\n<p>When it has to be online, you can use a virtual boards such as:</p>\n\n<ul>\n  <li><a href=\"https://miro.com/\">miro</a></li>\n  <li><a href=\"https://www.mural.co/\">mural</a></li>\n</ul>\n\n<h3 id=\"approach\">Approach</h3>\n\n<p>There are two approaches to facilitate and that depends on general participants’ understanding of the business process.</p>\n\n<p>If your team does not know the domain it is good to conduct a workshop in an exploratory way, because there are a lot\nof unknowns. You can start with adding a central event, or if the domain is large - several events. Then look at\nwhat is happening before and after those events regarding time flow.</p>\n\n<p>However, if your participants are familiar with the system (domain) and the goal is to discover only a part of it, see\nhow something works or immerse into a specific <em>use-case</em>. You may want to impose certain boundaries - e.g. by\ninitial and final events.</p>\n\n<p>Depending on what you want to achieve and how deep you want to explore your business, we can distinguish three possible formats:</p>\n\n<ul>\n  <li>Big Picture EventStorming - when you want to look at your business from above (<em>a helicopter view</em>),</li>\n  <li>Process Level EventStorming - going deeper with details but you still focus on whole view,</li>\n  <li>Design Level EventStorming - you break down your current process into smaller areas and then model them step by step using DDD, CQRS and/or Event Sourcing.</li>\n</ul>\n\n<p>In his book Alberto Brandolini is mentioning also other formats, however, I would like to narrow the scope for the most\nimportant ones. In this article I focus on the <em>Big Picture</em> approach as it is the first and crucial step to start exploring our business.</p>\n\n<h2 id=\"building-blocks\">Building blocks</h2>\n\n<p>I focus here on the main building blocks without going into details. A comprehensive description can be found in the\nbook mentioned earlier.</p>\n\n<h3 id=\"invite-the-right-people---business-ux-it\">Invite the right people - business, UX, IT</h3>\n<p><em>but how do you describe the right people?</em></p>\n\n<ul>\n  <li>those who have questions\n    <ul>\n      <li>developers, architects, designers etc.</li>\n    </ul>\n  </li>\n  <li>and those who know the answers\n    <ul>\n      <li>you will need people that care about the problem</li>\n      <li>people who know the business. Try to gather people who know and understand it. Don’t confuse them with users —\npeople who are using our business/system (I mean these two words interchangeably and will use <em>business</em> across the\narticle) — these two are totally opposite.</li>\n    </ul>\n  </li>\n</ul>\n\n<h3 id=\"orange-sticky-note\">Orange sticky note</h3>\n\n<p>On which we will write down our events in the following form:</p>\n\n<ul>\n  <li><em>Verb in past tense</em> to indicate that it already happened</li>\n  <li>\n    <p><em>Relevant for domain experts</em> - describing specific and pertinent events or changes in our business - these\nare changes that at the end of the day we want to save in the database.</p>\n\n    <p><img src=\"/img/articles/2022-07-19-eventstorming/image1.png\" alt=\"domain event\" /></p>\n  </li>\n</ul>\n\n<blockquote>\n  <p><strong>Tip</strong>: It is a good practice to define the concept of an event together (with participants) at the beginning of the\nworkshop. Then we can verify our definition with events that are appearing on the wall.</p>\n</blockquote>\n\n<p>For example:</p>\n\n<p><img src=\"/img/articles/2022-07-19-eventstorming/image5.png\" alt=\"example of events\" /></p>\n\n<ul>\n  <li>we have verbs in past tense,</li>\n  <li>they are all relevant changes in our <em>blog business</em>,</li>\n</ul>\n\n<h2 id=\"phases-of-big-picture-eventstorming-workshop\">Phases of Big Picture EventStorming workshop</h2>\n\n<h3 id=\"introduction\">Introduction</h3>\n\n<p>It is good to start the workshop with a short introduction of all participants - but it has to be rather quick before\neverybody gets bored. Generally you can omit this step and ask only new participants to introduce themselves.</p>\n\n<blockquote>\n  <p><em>We are going to explore the business process as a whole by placing all the relevant events along a timeline. We will\nhighlight ideas, risks and opportunities along the way.</em></p>\n</blockquote>\n\n<p>What is necessary - we need to set a goal. What will we model? Say “What is our goal? What we will model?” and try to\nanalyse.</p>\n<blockquote>\n  <p><strong>Tip</strong>: Remember - EventStorming is not a goal by itself - it is only a tool / framework.</p>\n</blockquote>\n\n<h3 id=\"because-the-big-picture-is-all-about-events\">Because the Big Picture is all about events</h3>\n\n<p>Provide participants with an idea of a domain event, why it is important and that it has to be a relevant change in our\nsystem. Imagine you do not have a computer and by the end of the day every event in our system needs to be written\ndown in a notebook, by hand. Is the event ‘offer was shown’ a relevant change you want or is it worth noting?</p>\n\n<blockquote>\n  <p><strong>Tip</strong>: A good ice-breaker is also demonstrating how to peel the sticky note so it would not curl\nup… <a href=\"https://www.youtube.com/watch?v=rPHLxOLuyLY\">for example here</a>.</p>\n</blockquote>\n\n<h2 id=\"phase-1-chaotic-exploration-brain-dump\">Phase 1 Chaotic exploration (brain dump)</h2>\n\n<h3 id=\"what-is-happening\">What is happening</h3>\n\n<p>All participants are using orange sticky notes, writing down events and putting them on the board. When events start\nappearing on the board, a discussion will naturally start about what kind of events they are, when they are happening or\nhow or what is triggering them.</p>\n\n<h3 id=\"your-role-as-a-facilitator\">Your role as a facilitator</h3>\n\n<p>Explain that we treat our whole board as a timeline and time is passing from left to right - it helps to see what is\nhappening before and after. Sometimes it is worth showing the importance of time in an example:</p>\n\n<ul>\n  <li>a locker was opened,</li>\n  <li>a package was taken out,</li>\n  <li>the door was closed.</li>\n</ul>\n\n<p>In a different order it does not make sense.</p>\n\n<p>Your role as a facilitator is to listen and observe - how fast new stickies are appearing, where discussion is taking\nplace (try to capture events people are arguing about). Encourage the team to try to identify as many events as possible.\nIf somebody is wrong, it’s okay and others will correct.</p>\n\n<p>When someone is mentioning some mysterious term, capture its definition. As a facilitator you can and you should ask\nobvious questions as it takes the burden off the other participants.</p>\n\n<p>This is also a phase where divergent thinking takes place as a part of <em>chaotic exploration</em>. So on the board we have a lot\nof events (ideas). Some of them are better and some are worse but we do not judge them at this point — later we will see\nwhere they lead us. Once again you should encourage the participants to generate new ideas and set aside critical thinking and judgement.</p>\n\n<blockquote>\n  <p><strong>Tips</strong>:\nAs an icebreaker you can place the first event or events - to show how easy it is, and help draw participants into\nworkshops.</p>\n\n  <p>Try to eliminate actors from the events - because we don’t want to impose mental boundaries as we may not notice that\nthere is some other case. For example instead of <code class=\"language-plaintext highlighter-rouge\">Buyer added item to cart</code> use <code class=\"language-plaintext highlighter-rouge\">Item added to cart</code>.</p>\n</blockquote>\n\n<h3 id=\"how-to-manage-people\">How to manage people</h3>\n\n<p>Sometimes it is a good idea to divide them into smaller groups and make them work together on the same issue\nor, quite the opposite, to focus on different areas of the system.</p>\n\n<p>Depending on whether we are exploring or modelling the process, especially during online sessions, I think it is good to\nhave boundaries — like a start event and an end event — among which everybody can create their vision. Then\nthe most difficult part is to merge it. Another approach is to give a free hand to your participants and see how the process is going to develop.</p>\n\n<h3 id=\"how-long-should-it-take\">How long should it take?</h3>\n\n<p>When the speed of new events showing up dramatically slows down, it is a good time to proceed to the next phase.\nUsually chaotic exploration takes about 5 to 15 minutes, but I have noticed that after about 8 minutes people are getting\nbored and busy with other things. So especially during online meetings, when you do not control the environment (like\ncomputers, phones, chat, mails…) it is easy to lose attention. And if you add to it a <em>zoom fatigue</em> syndrome, you can\nspoil the whole session when key participants leave.</p>\n\n<h2 id=\"phase-2-timeline\">Phase 2 Timeline</h2>\n\n<p>After the divergent step, now it is the time for the emergent phase where we want to explore and experiment - this is what\nwe will be doing during the next phases.</p>\n\n<h3 id=\"what-is-happening-1\">What is happening</h3>\n\n<p>Now our goal is to make sure we are actually following the timeline - we would like the flow of events to be consistent\nfrom the beginning to the end.</p>\n\n<h3 id=\"your-role-as-a-facilitator-1\">Your role as a facilitator:</h3>\n\n<p>A lot of events are going to change their place, also participants will find them irrelevant or duplicated and that is\nokay. Remove the duplicates, but be careful — ask if those duplicated events mean the same thing for everybody! Do\nnot hesitate to add, remove or change some sticky notes on the board.</p>\n\n<p>At this step some issue points may appear, so it is good to mark them as <strong>hotspots</strong>. Use red sticky notes and\nwrite the issue down but this is not a good time to deliberate about it now. Try to postpone this discussion until we have\nstructured the whole process.</p>\n\n<p><img src=\"/img/articles/2022-07-19-eventstorming/image2.png\" alt=\"hot-spot\" /></p>\n\n<blockquote>\n  <p><strong>Tip</strong>: During the online session when everybody is working solo, it is hard to merge all events and, including\nattention problems, you may be left alone. So my solution is to introduce the next phase right now.</p>\n\n  <p>Depending on the team - you can pick some random person who is going to start creating a timeline based\non available events. To sustain attention, replace this person with another one. In case of inconsistencies with the timeline,\nwe complete it with the missing events.</p>\n\n  <p>However, you can do all of it — if among participants there are some shy people or your participants’ supervisor is in\nthe room, when you tell the story you can make intentional errors or ask silly questions. All of this eventually will\nhelp to explore the domain.</p>\n\n  <p>Because as a facilitator you do not have to know everything — especially the domain or business your participants are\nexploring — you help them effectively and safely discover processes, find new solutions or define problems.</p>\n</blockquote>\n\n<h2 id=\"phase-3-explicit-walk-through-and-reverse-narrative\">Phase 3 Explicit walk-through and reverse narrative</h2>\n\n<h3 id=\"what-is-happening-2\">What is happening:</h3>\n\n<p>Next step is to do a walk-through by creating some sort of a story that can be told based on the events placed on the board.\nDuring this step a lot of discussions (arguments) are going to take place. Maybe some events are missing, so do not hesitate to add,\nremove or change some sticky notes on the board. We should focus on the happy path in the first place.</p>\n\n<h3 id=\"1-explicit-walk-through\">1. Explicit walk-through</h3>\n\n<h3 id=\"your-role-as-a-facilitator-2\">Your role as a facilitator:</h3>\n\n<p>Pick some random person who is going to start telling a story based on available events according to timeflow (from left\nto right). Sometimes the team gets blocked. In this situation you can add or move an event and place it in an obviously\nwrong place. Your error will be fixed quickly and help the team to move on.</p>\n\n<h3 id=\"how-to-help-the-participants-discover-more\">How to help the participants discover more?</h3>\n\n<p>The answer is simple — by asking questions. There are some useful questions that you can ask when discussing\nalmost every event, e.g.:</p>\n\n<ul>\n  <li>Why did this event happen?</li>\n  <li>What are the consequences of this event?</li>\n  <li>What has to / needs to happen next?</li>\n</ul>\n\n<p>Going deeper (of course that depends on how deep you want to go)</p>\n\n<ul>\n  <li>What, how, when, why is it changing?</li>\n  <li>When it can’t change?</li>\n  <li>How does this affect the business?</li>\n</ul>\n\n<blockquote>\n  <p><strong>Tip</strong>: Also in this phase it can be convenient to introduce actors (phase 4 - people and systems) — if it\nhelps to tell a story or better understand the process do not hesitate (remember I told you that EventStorming is a tool?)</p>\n</blockquote>\n\n<h3 id=\"2-reverse-narrative\">2. Reverse narrative</h3>\n\n<h3 id=\"your-role-as-a-facilitator-3\">Your role as a facilitator:</h3>\n\n<p>Sometimes it is good to propose a reverse narrative / reverse chronology. Pick an event from the end of the flow and\nlook for the event that made it possible - it must be consistent - no magic gaps between events. Again if we miss some\nevents - add them.</p>\n\n<p>Some questions you can ask:</p>\n\n<ul>\n  <li>Before\n    <ul>\n      <li><em>What has happened before X</em></li>\n      <li><em>What else has to happen for X to happen</em></li>\n    </ul>\n  </li>\n  <li>Between - we take two corresponding events\n    <ul>\n      <li><em>Is there anything else happening between X and Y</em></li>\n    </ul>\n  </li>\n  <li>Alternative - ask about alternative events\n    <ul>\n      <li><em>What if X did not happen</em></li>\n      <li><em>What if 10% of X happened or 150% of X happened</em></li>\n    </ul>\n  </li>\n</ul>\n\n<h2 id=\"phase-4-people-and-systems\">Phase 4 People and systems</h2>\n\n<h3 id=\"what-is-happening-3\">What is happening</h3>\n\n<p>When we finish enforcing the timeline and we have a consistent flow of our business we can add people and external\nsystems. We need them for clarity and better understanding of events and forces governing our business.</p>\n\n<p>For marking people we use a yellow sticky note with a symbolic drawing of a person or clock if we want to show that time\nmatters. External systems may be represented by large pink stickies with their names on it. By an external system I mean\na piece of the whole process which is beyond our control e.g. an application, a department, other companies.</p>\n\n<p><img src=\"/img/articles/2022-07-19-eventstorming/image3.png\" alt=\"actor\" /></p>\n\n<p><img src=\"/img/articles/2022-07-19-eventstorming/image4.png\" alt=\"system\" /></p>\n\n<h3 id=\"who-is-an-actor\">Who is an actor?</h3>\n\n<p>In his book Alberto Brandolini explains that</p>\n<blockquote>\n  <p><em>The goal is not to match the right yellow sticky note to every event in the flow. Adding significant people adds more\nclarity, but the goal is to trigger some insightful conversation: wherever the behaviour depends on a different type\nof user, wherever special actions need to be taken, and so on.</em></p>\n</blockquote>\n\n<p>The lack of precision is helping in discussion and exploration. It can be a specific person for example:</p>\n\n<ul>\n  <li><em>in our business model only Mrs. Smith can issue an invoice</em>.</li>\n  <li>or <em>after some time reservation is cancelled</em> so even <em>time</em> can be an actor.</li>\n</ul>\n\n<p>Another example:</p>\n\n<ul>\n  <li><em>order cancellation</em> can have two actors: client and CEX worker.</li>\n</ul>\n\n<h2 id=\"phase-5-opportunities-and-risks\">Phase 5 Opportunities and risks</h2>\n\n<p>In this phase we can literally take three steps back and look at the whole business flow as it is.\n<strong>Hot spots</strong> are the most conspicuous things - and it is easy to say where the biggest impediment is. This\nis a great occasion for additional discussion and a subject for further exploration.</p>\n\n<blockquote>\n  <p><strong>Tip</strong>: Remember that each <em>hot spot</em> should be addressed and resolved before the next session takes place.</p>\n</blockquote>\n\n<p>Another way to find where problems might lay is voting for a specific event or marking events that indicate where in our\nflow we are generating / losing money or value. For example by green stripes we indicate events where we are earning money,\nby red stripes where we are losing money or value.</p>\n\n<h2 id=\"it-is-like-pizzas\">It is like Pizzas</h2>\n\n<p>When all hotspots are addressed, you have found the biggest impediment, or you know on what part you have to focus on\nduring next session. The only thing left to do is to close the workshop, thank all stakeholders and participants,\nschedule the next session and ask for feedback.</p>\n\n<p>After the session you will have a clear business narrative on the board. What is more important, participants will\nshare general understanding of the process. They have gone through the massive learning process, gained experience and\nshared the common knowledge — everybody uses the domain language. Due to the fact that we used simple building\nblocks, the outcome is understandable to everyone PMs, UX designers, developers etc.</p>\n\n<p>The steps described above and their sequence should be regarded as optional during the session.\nThere is no such thing as one recipe. For example, if during the <em>timeline step</em> you feel that introduction of\npeople and systems is going to help, do not hesitate to do so. In other cases you will be interested only in\nfinding impediments or where your system is delivering values and do not feel obligated to use all the steps.\nAs Alberto says:</p>\n<blockquote>\n  <p><em>I like to think about it like Pizzas: there’s a common base, but different toppings.</em></p>\n</blockquote>\n\n<h3 id=\"nobody-is-excluded\">Nobody is excluded</h3>\n\n<p>Big Picture EventStorming is the first and crucial step, its outcome is visible and valuable. Depending on what the team\nneeds, it can be sufficient, but if we want to go deeper and explore more, next there are Process Level and Design Level\nEventStorming. We use the same stickies’ grammar enhanced with more colours to explain the complexity of our system. Due\nto the fact that we use the same grammar, developers and businesses can speak the same language — nobody is\nexcluded, isn’t that great?</p>\n\n<p>Those further steps (Process/Design Level) are getting us closer into the domain-driven-design and implementation. We (\ndevelopers/architects) can start thinking how to change what we have learned into working code, because</p>\n<blockquote>\n  <p><em>(…) it’s developer understanding that gets captured in code and released in production.</em></p>\n</blockquote>\n\n<h2 id=\"call-for-action\">Call for action</h2>\n\n<p>If you are Allegro worker and you are interested in EventStorming, you want to develop, participate in workshops\nor help as a facilitator I strongly encourage you to join the guild.\nIf you are not yet working at Allegro but are interested in how we use EventStorming maybe it is good opportunity to\njoin\nus — <a href=\"https://www.linkedin.com/company/allegro-pl/life/team\">#goodtobehere</a>.</p>\n\n<h2 id=\"more-about-eventstroming\">More about EventStroming</h2>\n\n<p>On the Internet you can find a lot of materials about EventStorming. Below is a list of those I found most valuable.</p>\n\n<h3 id=\"books\">Books</h3>\n\n<ol>\n  <li><a href=\"https://leanpub.com/introducing_eventstorming\">Introducing EventStorming</a> Alberto Brandolini’s book —\n<em>EventStorming Bible</em> — mandatory book!</li>\n  <li><a href=\"https://leanpub.com/eventstorming_handbook\">The EventStorming Handbook</a> by Paul Rayner — a great summary of\n<em>Introducing EventStorming</em> with a lot of valuable tips, tricks and recipes. After that you will be able to explain\nEventStorming even to your own child.</li>\n  <li><a href=\"https://gamestorming.com/\">GameStorming A Playbook for Innovators, Rulebreakers, and Changemakers</a> by Dave Gray,\nSunni Brown, James Macanufo — if you want to use the full potential of your storming sessions.</li>\n  <li><a href=\"https://allegro.pl/oferta/facilitator-s-guide-to-participatory-decision-maki-10017700512\">Facilitator’s Guide to Participatory Decision-making</a>\nby Sam Kaner, Lenny Lind — how to be a better facilitator, not only for EventStorming. You will find precious\ninformation about divergent, emergent and convergent thinking and why it is important.</li>\n</ol>\n\n<h3 id=\"link\">Link</h3>\n\n<ol>\n  <li><a href=\"https://github.com/mariuszgil/awesome-eventstorming\">Awesome EventStorming</a> by Mariusz Gil — I belive this is\nthe biggest source of links about EventStorming topics.</li>\n</ol>\n\n<h2 id=\"thanks\">Thanks</h2>\n\n<p>I would like to thank all of my colleagues from <em>Allegro EventStorming Guild</em> for their help in creating this article.</p>\n","contentSnippet":"With this article, I would like to introduce you to EventStorming and explain to you how to get started. I am not discovering\nanything new, just gathering available knowledge in one place. What I will show you is a few tips on how to conduct\nand facilitate EventStorming workshops.\nGuide to Big Picture EventStorming\nIntroducing EventStorming\nIn 2013 Alberto Brandolini posted an article\nabout a new workshop format for quick exploration of complex business domains. It was warmly welcomed by the DDD community.\nIn 2015 Technology Radar described EventStorming as worthy of attention\nand three years later as a recommended method for business domain modelling in information systems.\nDuring the years a lot has changed, the technique has developed and matured but the main idea remained the same:\nEventStorming is a flexible workshop format that allows a massive collaborative exploration of complex domains (…)\nwhere software and business practitioners are building together a behavioural model of the whole business line.\nThe above definition is from Alberto Brandolini’s Introducing EventStorming,\nto which I will be referring in this article.\nBefore launching\nProvide unlimited modelling space\nWhy is it important? Because you want participants to explore and experiment during the workshop. You don’t want to\nimpose limits on them or to allow a situation where someone doesn’t add an event because there is no space left.\nFor stationary session you need:\nwall where you attach plotter paper (it is easier to stick post-its on plotter paper),\nstickies in different colours, shapes and quantity (will discuss it later),\nmarkers\nWhen it has to be online, you can use a virtual boards such as:\nmiro\nmural\nApproach\nThere are two approaches to facilitate and that depends on general participants’ understanding of the business process.\nIf your team does not know the domain it is good to conduct a workshop in an exploratory way, because there are a lot\nof unknowns. You can start with adding a central event, or if the domain is large - several events. Then look at\nwhat is happening before and after those events regarding time flow.\nHowever, if your participants are familiar with the system (domain) and the goal is to discover only a part of it, see\nhow something works or immerse into a specific use-case. You may want to impose certain boundaries - e.g. by\ninitial and final events.\nDepending on what you want to achieve and how deep you want to explore your business, we can distinguish three possible formats:\nBig Picture EventStorming - when you want to look at your business from above (a helicopter view),\nProcess Level EventStorming - going deeper with details but you still focus on whole view,\nDesign Level EventStorming - you break down your current process into smaller areas and then model them step by step using DDD, CQRS and/or Event Sourcing.\nIn his book Alberto Brandolini is mentioning also other formats, however, I would like to narrow the scope for the most\nimportant ones. In this article I focus on the Big Picture approach as it is the first and crucial step to start exploring our business.\nBuilding blocks\nI focus here on the main building blocks without going into details. A comprehensive description can be found in the\nbook mentioned earlier.\nInvite the right people - business, UX, IT\nbut how do you describe the right people?\nthose who have questions\n    \ndevelopers, architects, designers etc.\nand those who know the answers\n    \nyou will need people that care about the problem\npeople who know the business. Try to gather people who know and understand it. Don’t confuse them with users —\npeople who are using our business/system (I mean these two words interchangeably and will use business across the\narticle) — these two are totally opposite.\nOrange sticky note\nOn which we will write down our events in the following form:\nVerb in past tense to indicate that it already happened\nRelevant for domain experts - describing specific and pertinent events or changes in our business - these\nare changes that at the end of the day we want to save in the database.\n\nTip: It is a good practice to define the concept of an event together (with participants) at the beginning of the\nworkshop. Then we can verify our definition with events that are appearing on the wall.\nFor example:\n\nwe have verbs in past tense,\nthey are all relevant changes in our blog business,\nPhases of Big Picture EventStorming workshop\nIntroduction\nIt is good to start the workshop with a short introduction of all participants - but it has to be rather quick before\neverybody gets bored. Generally you can omit this step and ask only new participants to introduce themselves.\nWe are going to explore the business process as a whole by placing all the relevant events along a timeline. We will\nhighlight ideas, risks and opportunities along the way.\nWhat is necessary - we need to set a goal. What will we model? Say “What is our goal? What we will model?” and try to\nanalyse.\nTip: Remember - EventStorming is not a goal by itself - it is only a tool / framework.\nBecause the Big Picture is all about events\nProvide participants with an idea of a domain event, why it is important and that it has to be a relevant change in our\nsystem. Imagine you do not have a computer and by the end of the day every event in our system needs to be written\ndown in a notebook, by hand. Is the event ‘offer was shown’ a relevant change you want or is it worth noting?\nTip: A good ice-breaker is also demonstrating how to peel the sticky note so it would not curl\nup… for example here.\nPhase 1 Chaotic exploration (brain dump)\nWhat is happening\nAll participants are using orange sticky notes, writing down events and putting them on the board. When events start\nappearing on the board, a discussion will naturally start about what kind of events they are, when they are happening or\nhow or what is triggering them.\nYour role as a facilitator\nExplain that we treat our whole board as a timeline and time is passing from left to right - it helps to see what is\nhappening before and after. Sometimes it is worth showing the importance of time in an example:\na locker was opened,\na package was taken out,\nthe door was closed.\nIn a different order it does not make sense.\nYour role as a facilitator is to listen and observe - how fast new stickies are appearing, where discussion is taking\nplace (try to capture events people are arguing about). Encourage the team to try to identify as many events as possible.\nIf somebody is wrong, it’s okay and others will correct.\nWhen someone is mentioning some mysterious term, capture its definition. As a facilitator you can and you should ask\nobvious questions as it takes the burden off the other participants.\nThis is also a phase where divergent thinking takes place as a part of chaotic exploration. So on the board we have a lot\nof events (ideas). Some of them are better and some are worse but we do not judge them at this point — later we will see\nwhere they lead us. Once again you should encourage the participants to generate new ideas and set aside critical thinking and judgement.\nTips:\nAs an icebreaker you can place the first event or events - to show how easy it is, and help draw participants into\nworkshops.\nTry to eliminate actors from the events - because we don’t want to impose mental boundaries as we may not notice that\nthere is some other case. For example instead of Buyer added item to cart use Item added to cart.\nHow to manage people\nSometimes it is a good idea to divide them into smaller groups and make them work together on the same issue\nor, quite the opposite, to focus on different areas of the system.\nDepending on whether we are exploring or modelling the process, especially during online sessions, I think it is good to\nhave boundaries — like a start event and an end event — among which everybody can create their vision. Then\nthe most difficult part is to merge it. Another approach is to give a free hand to your participants and see how the process is going to develop.\nHow long should it take?\nWhen the speed of new events showing up dramatically slows down, it is a good time to proceed to the next phase.\nUsually chaotic exploration takes about 5 to 15 minutes, but I have noticed that after about 8 minutes people are getting\nbored and busy with other things. So especially during online meetings, when you do not control the environment (like\ncomputers, phones, chat, mails…) it is easy to lose attention. And if you add to it a zoom fatigue syndrome, you can\nspoil the whole session when key participants leave.\nPhase 2 Timeline\nAfter the divergent step, now it is the time for the emergent phase where we want to explore and experiment - this is what\nwe will be doing during the next phases.\nWhat is happening\nNow our goal is to make sure we are actually following the timeline - we would like the flow of events to be consistent\nfrom the beginning to the end.\nYour role as a facilitator:\nA lot of events are going to change their place, also participants will find them irrelevant or duplicated and that is\nokay. Remove the duplicates, but be careful — ask if those duplicated events mean the same thing for everybody! Do\nnot hesitate to add, remove or change some sticky notes on the board.\nAt this step some issue points may appear, so it is good to mark them as hotspots. Use red sticky notes and\nwrite the issue down but this is not a good time to deliberate about it now. Try to postpone this discussion until we have\nstructured the whole process.\n\nTip: During the online session when everybody is working solo, it is hard to merge all events and, including\nattention problems, you may be left alone. So my solution is to introduce the next phase right now.\nDepending on the team - you can pick some random person who is going to start creating a timeline based\non available events. To sustain attention, replace this person with another one. In case of inconsistencies with the timeline,\nwe complete it with the missing events.\nHowever, you can do all of it — if among participants there are some shy people or your participants’ supervisor is in\nthe room, when you tell the story you can make intentional errors or ask silly questions. All of this eventually will\nhelp to explore the domain.\nBecause as a facilitator you do not have to know everything — especially the domain or business your participants are\nexploring — you help them effectively and safely discover processes, find new solutions or define problems.\nPhase 3 Explicit walk-through and reverse narrative\nWhat is happening:\nNext step is to do a walk-through by creating some sort of a story that can be told based on the events placed on the board.\nDuring this step a lot of discussions (arguments) are going to take place. Maybe some events are missing, so do not hesitate to add,\nremove or change some sticky notes on the board. We should focus on the happy path in the first place.\n1. Explicit walk-through\nYour role as a facilitator:\nPick some random person who is going to start telling a story based on available events according to timeflow (from left\nto right). Sometimes the team gets blocked. In this situation you can add or move an event and place it in an obviously\nwrong place. Your error will be fixed quickly and help the team to move on.\nHow to help the participants discover more?\nThe answer is simple — by asking questions. There are some useful questions that you can ask when discussing\nalmost every event, e.g.:\nWhy did this event happen?\nWhat are the consequences of this event?\nWhat has to / needs to happen next?\nGoing deeper (of course that depends on how deep you want to go)\nWhat, how, when, why is it changing?\nWhen it can’t change?\nHow does this affect the business?\nTip: Also in this phase it can be convenient to introduce actors (phase 4 - people and systems) — if it\nhelps to tell a story or better understand the process do not hesitate (remember I told you that EventStorming is a tool?)\n2. Reverse narrative\nYour role as a facilitator:\nSometimes it is good to propose a reverse narrative / reverse chronology. Pick an event from the end of the flow and\nlook for the event that made it possible - it must be consistent - no magic gaps between events. Again if we miss some\nevents - add them.\nSome questions you can ask:\nBefore\n    \nWhat has happened before X\nWhat else has to happen for X to happen\nBetween - we take two corresponding events\n    \nIs there anything else happening between X and Y\nAlternative - ask about alternative events\n    \nWhat if X did not happen\nWhat if 10% of X happened or 150% of X happened\nPhase 4 People and systems\nWhat is happening\nWhen we finish enforcing the timeline and we have a consistent flow of our business we can add people and external\nsystems. We need them for clarity and better understanding of events and forces governing our business.\nFor marking people we use a yellow sticky note with a symbolic drawing of a person or clock if we want to show that time\nmatters. External systems may be represented by large pink stickies with their names on it. By an external system I mean\na piece of the whole process which is beyond our control e.g. an application, a department, other companies.\n\n\nWho is an actor?\nIn his book Alberto Brandolini explains that\nThe goal is not to match the right yellow sticky note to every event in the flow. Adding significant people adds more\nclarity, but the goal is to trigger some insightful conversation: wherever the behaviour depends on a different type\nof user, wherever special actions need to be taken, and so on.\nThe lack of precision is helping in discussion and exploration. It can be a specific person for example:\nin our business model only Mrs. Smith can issue an invoice.\nor after some time reservation is cancelled so even time can be an actor.\nAnother example:\norder cancellation can have two actors: client and CEX worker.\nPhase 5 Opportunities and risks\nIn this phase we can literally take three steps back and look at the whole business flow as it is.\nHot spots are the most conspicuous things - and it is easy to say where the biggest impediment is. This\nis a great occasion for additional discussion and a subject for further exploration.\nTip: Remember that each hot spot should be addressed and resolved before the next session takes place.\nAnother way to find where problems might lay is voting for a specific event or marking events that indicate where in our\nflow we are generating / losing money or value. For example by green stripes we indicate events where we are earning money,\nby red stripes where we are losing money or value.\nIt is like Pizzas\nWhen all hotspots are addressed, you have found the biggest impediment, or you know on what part you have to focus on\nduring next session. The only thing left to do is to close the workshop, thank all stakeholders and participants,\nschedule the next session and ask for feedback.\nAfter the session you will have a clear business narrative on the board. What is more important, participants will\nshare general understanding of the process. They have gone through the massive learning process, gained experience and\nshared the common knowledge — everybody uses the domain language. Due to the fact that we used simple building\nblocks, the outcome is understandable to everyone PMs, UX designers, developers etc.\nThe steps described above and their sequence should be regarded as optional during the session.\nThere is no such thing as one recipe. For example, if during the timeline step you feel that introduction of\npeople and systems is going to help, do not hesitate to do so. In other cases you will be interested only in\nfinding impediments or where your system is delivering values and do not feel obligated to use all the steps.\nAs Alberto says:\nI like to think about it like Pizzas: there’s a common base, but different toppings.\nNobody is excluded\nBig Picture EventStorming is the first and crucial step, its outcome is visible and valuable. Depending on what the team\nneeds, it can be sufficient, but if we want to go deeper and explore more, next there are Process Level and Design Level\nEventStorming. We use the same stickies’ grammar enhanced with more colours to explain the complexity of our system. Due\nto the fact that we use the same grammar, developers and businesses can speak the same language — nobody is\nexcluded, isn’t that great?\nThose further steps (Process/Design Level) are getting us closer into the domain-driven-design and implementation. We (\ndevelopers/architects) can start thinking how to change what we have learned into working code, because\n(…) it’s developer understanding that gets captured in code and released in production.\nCall for action\nIf you are Allegro worker and you are interested in EventStorming, you want to develop, participate in workshops\nor help as a facilitator I strongly encourage you to join the guild.\nIf you are not yet working at Allegro but are interested in how we use EventStorming maybe it is good opportunity to\njoin\nus — #goodtobehere.\nMore about EventStroming\nOn the Internet you can find a lot of materials about EventStorming. Below is a list of those I found most valuable.\nBooks\nIntroducing EventStorming Alberto Brandolini’s book —\nEventStorming Bible — mandatory book!\nThe EventStorming Handbook by Paul Rayner — a great summary of\nIntroducing EventStorming with a lot of valuable tips, tricks and recipes. After that you will be able to explain\nEventStorming even to your own child.\nGameStorming A Playbook for Innovators, Rulebreakers, and Changemakers by Dave Gray,\nSunni Brown, James Macanufo — if you want to use the full potential of your storming sessions.\nFacilitator’s Guide to Participatory Decision-making\nby Sam Kaner, Lenny Lind — how to be a better facilitator, not only for EventStorming. You will find precious\ninformation about divergent, emergent and convergent thinking and why it is important.\nLink\nAwesome EventStorming by Mariusz Gil — I belive this is\nthe biggest source of links about EventStorming topics.\nThanks\nI would like to thank all of my colleagues from Allegro EventStorming Guild for their help in creating this article.","guid":"https://blog.allegro.tech/2022/07/event-storming-workshops.html","categories":["eventstorming","tech","communication"],"isoDate":"2022-07-18T22:00:00.000Z","thumbnail":"images/post-headers/eventstorming.png"},{"title":"GC, hands off my data!","link":"https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html","pubDate":"Thu, 30 Jun 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>Certainly one of the main distinguishing features of the Java world is the Garbage Collector.\nUsing it is safe and convenient, it allows us to forget about many tedious responsibilities, letting us focus on the\npure joy of coding. Yet sometimes it can cause a headache too, especially when we notice that GC uses our resources\ntoo intensively. Each of us has probably experienced a time in our career when we wanted to get\nrid of the Garbage Collector from our application because it was running too long, too often, and perhaps even led to temporary system freezes.</p>\n\n<p>What if we could still benefit from the GC, but in special cases, also be able to store data beyond its control? We\ncould still take advantage of its convenience and, at the same time, be able to easily get rid of long GC pauses.</p>\n\n<p>It turns out that it is possible. In this article, we will look at whether and when it is worth storing data\nbeyond the reach of the Garbage Collector’s greedy hands.</p>\n\n<h2 id=\"comfort-comes-at-a-price\">Comfort comes at a price</h2>\n\n<p>At <a href=\"https://allegro.tech\">Allegro</a> we are very keen on metrics. We measure anything that can tell us something about the condition of\nour services. Apart from the most obvious metrics directly related to the application, such as throughput, the number of\nerrors, CPU and memory usage, we also pay a great deal of attention to metrics related to the garbage collecting — GC working\ntime and number of its cycles. Too much time spent on releasing the memory or too frequent GC launches may signal problems with\nmemory leaks or indicate that it is worth considering optimising memory usage or switching to a different GC strategy.</p>\n\n<p>Following the example of large technology companies, we have been organising company meetups within the so-called guilds\nfor some time now. In one of such guilds, over a hundred engineers meet regularly once a month and discuss various\ntopics related to performance, scaling and service optimisation. At one of these meetings, our colleague\ndiscussed the method of determining the actual size of data stored in a cache. Apparently, this is not a\nsimple matter, as internal mechanisms for optimising memory usage, such as deduplication or compression, must be taken\ninto account. After the presentation, an interesting discussion ensued about how much memory\non the heap is actually used by the cache and how long it takes to clean it up. Someone pointed out that there is a hidden cost of using the cache\nthat takes the form of time needed to free the memory of expired cache items, which not everyone is aware of. What is more, the\nmanner in which the cache works does not quite fit the\n<a href=\"http://insightfullogic.com/2013/Feb/20/garbage-collection-java-1/\">generational hypothesis</a> and may mislead the JVM by preventing it\nfrom properly tuning the GC mechanism. I then began to wonder whether it might not be worth keeping the cache in an area\nexcluded from the GC’s control? I knew this is possible, although I had never seen a practical implementation of this\ntechnique. This topic was bothering me for some time, so I decided to investigate.</p>\n\n<h2 id=\"memory-architecture\">Memory architecture</h2>\n\n<p>Any skilled Java programmer knows the division of memory into young and old generation areas. People interested in\ndetails are probably also familiar with the more precise division into eden, survivor, tenured and perm.\nThere are many excellent articles discussing this topic\n(like <a href=\"https://www.betsol.com/blog/java-memory-management-for-java-virtual-machine-jvm/\">this one</a>), so we won’t go\ninto details. Instead, we will focus on a very specialised area of memory that the GC\nhas no control over, which is the off-heap memory, sometimes also called native memory. This is a special area under the\ndirect control of the operating system, which the JVM uses for its own purposes. It stores information about classes and\nmethods, internal thread data and cached code necessary for operation. As I mentioned earlier, off-heap memory is not\nsubject to the GC. In particular, it is excluded from garbage collection processes, which means that programmers\ncreating the JVM code using this area are wholly responsible for freeing memory allocated for\nvariables. There is also a dedicated area to which we — the programmers — have access as well.\nThere is a possibility to write and read data from this space, remembering of course, that the responsibility\nfor cleaning up after unnecessary variables lies entirely with us.</p>\n\n<p>This area can be accessed using a simple API.\nThe following code allocates 100 bytes of off-heap memory and stores a String and an Integer.\nAt the end the data are loaded from the off-heap memory and then printed out.</p>\n\n<div class=\"language-java highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kt\">int</span> <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"mi\">100</span><span class=\"o\">;</span>\n\n<span class=\"nc\">ByteBuffer</span> <span class=\"n\">buff</span> <span class=\"o\">=</span> <span class=\"nc\">ByteBuffer</span><span class=\"o\">.</span><span class=\"na\">allocateDirect</span><span class=\"o\">(</span><span class=\"n\">size</span><span class=\"o\">);</span>\n<span class=\"n\">buff</span><span class=\"o\">.</span><span class=\"na\">put</span><span class=\"o\">(</span><span class=\"s\">\"Michal\"</span><span class=\"o\">.</span><span class=\"na\">getBytes</span><span class=\"o\">());</span>\n<span class=\"n\">buff</span><span class=\"o\">.</span><span class=\"na\">putInt</span><span class=\"o\">(</span><span class=\"mi\">42</span><span class=\"o\">);</span>\n\n<span class=\"n\">buff</span><span class=\"o\">.</span><span class=\"na\">position</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">);</span> <span class=\"c1\">// set the pointer back to the beginning</span>\n\n<span class=\"kt\">byte</span><span class=\"o\">[]</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"kt\">byte</span><span class=\"o\">[</span><span class=\"mi\">6</span><span class=\"o\">];</span> <span class=\"c1\">// length of my name</span>\n<span class=\"n\">buff</span><span class=\"o\">.</span><span class=\"na\">get</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">);</span>\n\n<span class=\"n\">out</span><span class=\"o\">.</span><span class=\"na\">println</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">String</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">));</span>\n<span class=\"n\">out</span><span class=\"o\">.</span><span class=\"na\">println</span><span class=\"o\">(</span><span class=\"n\">buff</span><span class=\"o\">.</span><span class=\"na\">getInt</span><span class=\"o\">());</span>\n</code></pre></div></div>\n\n<p>Note the <code class=\"language-plaintext highlighter-rouge\">allocateDirect</code> method that allocates off-heap memory unlike a similar method: <code class=\"language-plaintext highlighter-rouge\">allocate</code> that allocates\non-heap memory. The behavior of both methods can be compared with the help of a profiler\n(I will use <a href=\"https://openjdk.java.net/tools/svc/jconsole/\">jConsole</a>). The following programs allocate 1GB of memory,\nrespectively, on-heap and off-heap:</p>\n\n<div class=\"language-java highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nc\">ByteBuffer</span><span class=\"o\">.</span><span class=\"na\">allocate</span><span class=\"o\">(</span><span class=\"mi\">1000000000</span><span class=\"o\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-java highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nc\">ByteBuffer</span><span class=\"o\">.</span><span class=\"na\">allocateDirect</span><span class=\"o\">(</span><span class=\"mi\">1000000000</span><span class=\"o\">)</span>\n</code></pre></div></div>\n\n<p>The chart below shows heap memory profile comparison for both programs (on-heap on the left vs. off-heap on the right):</p>\n\n<p><img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/compare.png\" alt=\"on-heap vs off-heap\" /></p>\n\n<p>Such a possibility to bypass Garbage Collector may seem extremely tempting to\ndevelopers struggling with long working time of the GC. However, this raises the question: what type of usage justifies\nthe extra effort involved in manually freeing the memory and the potential risk of error? What are the advantages of\nusing off-heap memory? Is it faster? How much time will we save by bypassing the GC? Why is this method so uncommon?\nTo put it simply: is it worth doing and if so, when?</p>\n\n<h2 id=\"be-gone-gc\">Be gone GC!</h2>\n\n<p>GC is a wonderful tool. It allows us – although sometimes only for a while – to forget about the problems related\nto painful memory management. We can create variables of any type and any scope almost freely, and not worry about what\nhappens to memory once we stop using them. This task is handled by the GC, which does it brilliantly. In each successive\nversion of the JDK we get a new algorithm, which in some specific cases is even better than the previous one.</p>\n\n<p>However, I’m more than sure that many of us have once encountered the problem of long GC time or too frequent GC\ncalls. Every developer has their own ideas on how to deal with this issue - we look for memory leaks, profile the\napplication in search of hot spots, examine the scope of created variables, use object pools, verify the system\nbehaviour with different GC algorithms, and check the cache configuration.</p>\n\n<p>In my case, it is the cache that is often responsible for long GC time. Sometimes it stores large numbers of objects, usually\ncomplex ones, containing references to other objects. What is more, the way cache objects are accessed is often not\nuniform. Some objects are never queried after being inserted into the cache, others are read throughout their whole\nlifecycle. This causes the cache to disrupt the somewhat ideal world order defined by the generational hypothesis. Then,\nGC algorithms are faced with a very difficult task of determining the optimal way to clean up the memory freed by the\nitems removed from the cache. All this causes the cache cleanup to be expensive. This made me wonder if there was\nany benefit in storing cache data outside the heap?</p>\n\n<h2 id=\"off-heap-space-pros-and-cons\">Off-heap space: Pros and cons</h2>\n\n<p>In a sense, the off-heap space lies outside the control of the JVM (though it belongs to the Java process),\nand for this reason, it is not possible to write\ncomplex structures used in JVM languages into it. This raises the need for an intermediate step of serializing the\ndata into a plain byte array, which can then be stored in the off-heap area. When the data is loaded, the reverse\nprocess must be performed: deserialization into a form that we can use in Java. These additional steps will of\ncourse come at an extra cost, which is why accessing off-heap data will, for obvious reasons, take longer than accessing\non-heap data directly.</p>\n\n<p>Since writing and reading data in the off-heap space takes longer, what is the benefit of this approach then? Well, the data\nstored in the off-heap space are not subject to GC processes, so on the one hand we – the programmers – are responsible\nfor each freeing of memory after a given variable is no longer useful. On the other hand, we relieve the management\nprocesses in the JVM by releasing CPU’s time for the rest of the application, so, theoretically, it should\nresult in some resource savings. The question is, do these differences balance each other out to any degree? Will the savings\nassociated with the GC process balance out our longer data access time? If so, does it depend only on the amount of\ndata, or is there a specific usage scenario? To answer these questions, it is necessary to run a few experiments.</p>\n\n<h2 id=\"experiments\">Experiments</h2>\n\n<p>We can store any data structure in the on-heap area, which means that the advantage of this approach lies in the fact\nthat there is no overhead involved in transforming the data to another form, while its disadvantage consists of the\nadditional cost related to the GC. On the other hand, in the case of off-heap storage, there is no GC extra cost,\nbut there is the cost of serialising the data to a byte array.</p>\n\n<p>Over the last years, significant\nprogress has been made in the field of GC and with the right matching of the algorithm to the application profile, its\ntime can be very short. But is there any case where it is worth reaching into the unmanaged space after all?</p>\n\n<p>I decided to start with an overview of what open-source options are currently available. When it comes to the implementation of the\non-heap cache mechanism, the options are numerous – there is well known:\n<a href=\"https://guava.dev/releases/21.0/api/docs/com/google/common/cache/Cache.html\">guava</a>,\n<a href=\"https://www.ehcache.org/\">ehcache</a>, <a href=\"https://github.com/ben-manes/caffeine\">caffeine</a> and many other solutions. However,\nwhen I began researching cache mechanisms offering the possibility of storing data outside GC control, I found out\nthat there are very few solutions left. Out of the popular ones, only <a href=\"https://www.terracotta.org/\">Terracotta</a> is supported.\nIt seems that this is a very niche solution and we do not have many options to choose\nfrom. In terms of less-known projects, I came across <a href=\"https://github.com/OpenHFT/Chronicle-Map\">Chronicle-Map</a>,\n<a href=\"https://github.com/jankotek/MapDB\">MapDB</a> and <a href=\"https://github.com/snazy/ohc\">OHC</a>. I chose the\nlast one because it was created as part of the Cassandra project, which I had some experience with and was curious\nabout how this component worked:</p>\n\n<blockquote>\n  <p>OHC was developed in 2014/15 for Apache Cassandra 2.2 and 3.0 to be used as the new row-cache backend.</p>\n</blockquote>\n\n<p>To run the experiment, I decided to use a service built to provide the offer description based on its unique number. After\ndownloading the offer description from the repository, it is placed in the cache to speed up future calls. Obviously, the\ncache has a limited capacity, which is chosen in such a way that it forces the deletion of items that have been placed\nin it for the longest time ago.</p>\n\n<p>In our cache, the offer number is the key, while its description in the form of a string of characters is the\nvalue. This allows us to easily simulate almost any size of data in the cache (all we have to do is to make the\noffer description longer), and additionally, it makes the overhead related to the aforementioned serialisation\nrelatively small – serialisation of a text string is obviously faster than a complex DTO object.</p>\n\n<p>In my project, I used the <a href=\"https://github.com/ben-manes/caffeine\">Caffeine cache</a> to store the data in the on-heap area\nand OHC library to store it in the off-heap area.</p>\n\n<p>The test scenario consists of querying for descriptions of different offers. During the test, I will\ncollect data on memory and GC parameters using jConsole. I will run the test scenario using <a href=\"https://jmeter.apache.org/\">jMeter</a>,\nwhich additionally will allow me to measure response times.</p>\n\n<p>From my preliminary research I know that this method is only applicable to memory-intensive systems.\nHowever, for the sake of order, let’s first run an experiment on a small cache size with element set to 5 KB:</p>\n<ul>\n  <li>maximum number of cached elements: 10000</li>\n  <li>cached element size: 5.000 bytes</li>\n  <li>10 threads querying for random offers in a loop of 100000 iterations each</li>\n</ul>\n\n<p>Take a look at the screenshots from jConsole below. The results are in line with expectations: no benefit from the use\nof off-heap space. Both the number of garbage collection cycles (63 vs. 65) and GC run time (0.182s vs 0.235s)\nare nearly identical in both cases:</p>\n\n<p><em>The GC profile of on-heap variant:</em>\n<img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/on-heap-small-gc.png\" alt=\"on-heap GC chart\" /></p>\n\n<p><em>The GC profile of off-heap variant:</em>\n<img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/off-heap-small-gc.png\" alt=\"on-heap GC chart\" /></p>\n\n<p>Not much of an improvement for small to medium cache size. However, this result is not disappointing to me because\nI expected it. GC is designed to handle much more memory than 400 MB, it would therefore be strange if we obtained\nan improvement at such an early stage.</p>\n\n<p>Now let’s see how the comparison looks for a much larger cache element size, let’s increase it up to 100 KB.\nAt the same time, due to the fact that I am running the tests on a laptop with limited resources, I will reduce\nthreads configuration and cache maximum element size.</p>\n\n<p>The configuration of the second test is as follows:</p>\n<ul>\n  <li>maximum number of cached elements: 5000</li>\n  <li>cached element size: 100.000 bytes</li>\n  <li>10 threads querying for random offers in a loop of 1000 iterations each</li>\n</ul>\n\n<p>Let’s take a look at the results.</p>\n\n<p><em>The GC profile of on-heap variant:</em>\n<img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/on-heap-gc.png\" alt=\"on-heap GC chart\" />\nMemory usage increases throughout the test, there are 40 GC collection cycles that together last 0.212s.</p>\n\n<p><em>The GC profile of off-heap variant:</em>\n<img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/off-heap-gc.png\" alt=\"on-heap GC chart\" />\nThis time heap memory usage chart definitely looks different, is shaped like a saw, and reaches half of the previous value.\nPlease note also, that this time there are only 13 GC cycles with total time of 0.108s.</p>\n\n<p>The results of the GC profile comparison are therefore as expected, and what about the response times?</p>\n\n<p><em>jMeter metrics of on-heap variant:</em>\n<img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/on-heap-jmeter.png\" alt=\"on-heap GC chart\" /></p>\n\n<p><em>jMeter metrics of off-heap variant:</em>\n<img src=\"/img/articles/2022-06-30-gc-hands-off-my-data/off-heap-jmeter.png\" alt=\"on-heap GC chart\" /></p>\n\n<p>Request time metrics data is also in line with predictions, off-heap variant proved to be slightly slower than on-heap.</p>\n\n<p>Now let’s see what effect increasing the data size will have on the results. Let’s do tests for the following sizes:\n100.000 B, 200.000 B and 300.000 B, jMeter configuration stays unchanged: 10 threads with 1000 iterations each.\nThis time, for the sake of clarity, the results are summarized in a table:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Cached item size</th>\n      <th>Variant</th>\n      <th>GC cycles count</th>\n      <th>GC time</th>\n      <th>Request time (median)</th>\n      <th>Throughput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100.000 B</td>\n      <td>on-heap</td>\n      <td>40</td>\n      <td>0.212 s</td>\n      <td>171 ms</td>\n      <td>83.2 rps</td>\n    </tr>\n    <tr>\n      <td>100.000 B</td>\n      <td>off-heap</td>\n      <td>13</td>\n      <td>0.108 s</td>\n      <td>179 ms</td>\n      <td>78.1 rps</td>\n    </tr>\n    <tr>\n      <td>200.000 B</td>\n      <td>on-heap</td>\n      <td>84</td>\n      <td>0.453 s</td>\n      <td>396 ms</td>\n      <td>38.2 rps</td>\n    </tr>\n    <tr>\n      <td>200.000 B</td>\n      <td>off-heap</td>\n      <td>19</td>\n      <td>0.182 s</td>\n      <td>355 ms</td>\n      <td>40.2 rps</td>\n    </tr>\n    <tr>\n      <td>300.000 B</td>\n      <td>on-heap</td>\n      <td>114</td>\n      <td>0.6s</td>\n      <td>543 ms</td>\n      <td>27.3 rps</td>\n    </tr>\n    <tr>\n      <td>300.000 B</td>\n      <td>off-heap</td>\n      <td>27</td>\n      <td>0.185s</td>\n      <td>528 ms</td>\n      <td>27.9 rps</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>It turns out that as the size of cache item increases, the benefits of using off-heap space grow – all metrics are improved.</p>\n\n<p>What about cache maximum elements? Let’s use 200.000B item size and check what happens when we increase the maximum cache\nelement size, we will test cache for 5000, 10.000 and 15.000 elements:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Cache max elements</th>\n      <th>Variant</th>\n      <th>GC cycles count</th>\n      <th>GC time</th>\n      <th>Request time (median)</th>\n      <th>Throughput</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5000</td>\n      <td>on-heap</td>\n      <td>84</td>\n      <td>0.453 s</td>\n      <td>396 ms</td>\n      <td>38.2 rps</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>off-heap</td>\n      <td>19</td>\n      <td>0.182 s</td>\n      <td>355 ms</td>\n      <td>40.2 rps</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>on-heap</td>\n      <td>81</td>\n      <td>0.46 s</td>\n      <td>393 ms</td>\n      <td>38.8 rps</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>off-heap</td>\n      <td>19</td>\n      <td>0.173 s</td>\n      <td>345 ms</td>\n      <td>42.6 rps</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>on-heap</td>\n      <td>84</td>\n      <td>0.462 s</td>\n      <td>355 ms</td>\n      <td>41.8 rps</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>off-heap</td>\n      <td>19</td>\n      <td>0.167 s</td>\n      <td>344 ms</td>\n      <td>42.6 rps</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>No surprise here either, increasing cache size has a positive impact on both variants. Of course in case of on-heap cache,\nsome of the benefits are offset by the need for cleaning larger memory area.</p>\n\n<p>With the experiments conducted, we can conclude that the more data we store in memory, the greater the benefit of using\nthe off-heap area may be. At the same time, it should be added that these benefits are not huge, just a few RPS more.\nIn the case of systems that store tremendous amounts of data, this method may bring some improvements in terms of resource utilization.\nHowever, for most of our apps and services, that’s probably not the way to go, a code audit is a better idea.</p>\n\n<p>This is probably a good time to highlight how well implemented the current memory sweeper algorithms are. Well done GC!</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n\n<p>Everyone has probably come across a case when an application froze as a result of GC’s operation. As the above data\nshow, there is a relationship between the amount of data stored in memory and the time the GC requires to clean it up –\nthe more data we store on the heap, the longer it takes to free the memory. That is why the cases where we process large\namounts of data provide us with a potential benefit of using the off-heap area. There are some very specialised uses of\nthis technique, such as Spark, which can store large amounts of data for subsequent processing steps and can do so using\nthe off-heap space (you can read more about Spark memory model <a href=\"https://medium.com/walmartglobaltech/decoding-memory-in-spark-parameters-that-are-often-confused-c11be7488a24\">here</a>).\nAnother example of the use of the off-heap approach is the Apache Cassandra database. The OHC used\nin this post was developed from this particular project.</p>\n\n<p>There is a very narrow group of cases where storing data outside of GC control is justifiable. However, for the\nvast majority of applications, a much better approach is to take advantage of ever-improving GC\nimplementations. If you have experienced problems with the slow performance of the GC while developing your business\nservice, you should definitely audit your code first and experiment with different heap size settings and the GC\nalgorithm. When all other methods fail, you can give the off-heap area a try.</p>\n\n<p>However, if you are working on a server that processes massive amounts of data, it is worth considering off-heap\nstorage earlier, similar to Spark or Cassandra solutions.</p>\n\n","contentSnippet":"Certainly one of the main distinguishing features of the Java world is the Garbage Collector.\nUsing it is safe and convenient, it allows us to forget about many tedious responsibilities, letting us focus on the\npure joy of coding. Yet sometimes it can cause a headache too, especially when we notice that GC uses our resources\ntoo intensively. Each of us has probably experienced a time in our career when we wanted to get\nrid of the Garbage Collector from our application because it was running too long, too often, and perhaps even led to temporary system freezes.\nWhat if we could still benefit from the GC, but in special cases, also be able to store data beyond its control? We\ncould still take advantage of its convenience and, at the same time, be able to easily get rid of long GC pauses.\nIt turns out that it is possible. In this article, we will look at whether and when it is worth storing data\nbeyond the reach of the Garbage Collector’s greedy hands.\nComfort comes at a price\nAt Allegro we are very keen on metrics. We measure anything that can tell us something about the condition of\nour services. Apart from the most obvious metrics directly related to the application, such as throughput, the number of\nerrors, CPU and memory usage, we also pay a great deal of attention to metrics related to the garbage collecting — GC working\ntime and number of its cycles. Too much time spent on releasing the memory or too frequent GC launches may signal problems with\nmemory leaks or indicate that it is worth considering optimising memory usage or switching to a different GC strategy.\nFollowing the example of large technology companies, we have been organising company meetups within the so-called guilds\nfor some time now. In one of such guilds, over a hundred engineers meet regularly once a month and discuss various\ntopics related to performance, scaling and service optimisation. At one of these meetings, our colleague\ndiscussed the method of determining the actual size of data stored in a cache. Apparently, this is not a\nsimple matter, as internal mechanisms for optimising memory usage, such as deduplication or compression, must be taken\ninto account. After the presentation, an interesting discussion ensued about how much memory\non the heap is actually used by the cache and how long it takes to clean it up. Someone pointed out that there is a hidden cost of using the cache\nthat takes the form of time needed to free the memory of expired cache items, which not everyone is aware of. What is more, the\nmanner in which the cache works does not quite fit the\ngenerational hypothesis and may mislead the JVM by preventing it\nfrom properly tuning the GC mechanism. I then began to wonder whether it might not be worth keeping the cache in an area\nexcluded from the GC’s control? I knew this is possible, although I had never seen a practical implementation of this\ntechnique. This topic was bothering me for some time, so I decided to investigate.\nMemory architecture\nAny skilled Java programmer knows the division of memory into young and old generation areas. People interested in\ndetails are probably also familiar with the more precise division into eden, survivor, tenured and perm.\nThere are many excellent articles discussing this topic\n(like this one), so we won’t go\ninto details. Instead, we will focus on a very specialised area of memory that the GC\nhas no control over, which is the off-heap memory, sometimes also called native memory. This is a special area under the\ndirect control of the operating system, which the JVM uses for its own purposes. It stores information about classes and\nmethods, internal thread data and cached code necessary for operation. As I mentioned earlier, off-heap memory is not\nsubject to the GC. In particular, it is excluded from garbage collection processes, which means that programmers\ncreating the JVM code using this area are wholly responsible for freeing memory allocated for\nvariables. There is also a dedicated area to which we — the programmers — have access as well.\nThere is a possibility to write and read data from this space, remembering of course, that the responsibility\nfor cleaning up after unnecessary variables lies entirely with us.\nThis area can be accessed using a simple API.\nThe following code allocates 100 bytes of off-heap memory and stores a String and an Integer.\nAt the end the data are loaded from the off-heap memory and then printed out.\n\nint size = 100;\n\nByteBuffer buff = ByteBuffer.allocateDirect(size);\nbuff.put(\"Michal\".getBytes());\nbuff.putInt(42);\n\nbuff.position(0); // set the pointer back to the beginning\n\nbyte[] name = new byte[6]; // length of my name\nbuff.get(name);\n\nout.println(new String(name));\nout.println(buff.getInt());\n\n\nNote the allocateDirect method that allocates off-heap memory unlike a similar method: allocate that allocates\non-heap memory. The behavior of both methods can be compared with the help of a profiler\n(I will use jConsole). The following programs allocate 1GB of memory,\nrespectively, on-heap and off-heap:\n\nByteBuffer.allocate(1000000000)\n\n\n\nByteBuffer.allocateDirect(1000000000)\n\n\nThe chart below shows heap memory profile comparison for both programs (on-heap on the left vs. off-heap on the right):\n\nSuch a possibility to bypass Garbage Collector may seem extremely tempting to\ndevelopers struggling with long working time of the GC. However, this raises the question: what type of usage justifies\nthe extra effort involved in manually freeing the memory and the potential risk of error? What are the advantages of\nusing off-heap memory? Is it faster? How much time will we save by bypassing the GC? Why is this method so uncommon?\nTo put it simply: is it worth doing and if so, when?\nBe gone GC!\nGC is a wonderful tool. It allows us – although sometimes only for a while – to forget about the problems related\nto painful memory management. We can create variables of any type and any scope almost freely, and not worry about what\nhappens to memory once we stop using them. This task is handled by the GC, which does it brilliantly. In each successive\nversion of the JDK we get a new algorithm, which in some specific cases is even better than the previous one.\nHowever, I’m more than sure that many of us have once encountered the problem of long GC time or too frequent GC\ncalls. Every developer has their own ideas on how to deal with this issue - we look for memory leaks, profile the\napplication in search of hot spots, examine the scope of created variables, use object pools, verify the system\nbehaviour with different GC algorithms, and check the cache configuration.\nIn my case, it is the cache that is often responsible for long GC time. Sometimes it stores large numbers of objects, usually\ncomplex ones, containing references to other objects. What is more, the way cache objects are accessed is often not\nuniform. Some objects are never queried after being inserted into the cache, others are read throughout their whole\nlifecycle. This causes the cache to disrupt the somewhat ideal world order defined by the generational hypothesis. Then,\nGC algorithms are faced with a very difficult task of determining the optimal way to clean up the memory freed by the\nitems removed from the cache. All this causes the cache cleanup to be expensive. This made me wonder if there was\nany benefit in storing cache data outside the heap?\nOff-heap space: Pros and cons\nIn a sense, the off-heap space lies outside the control of the JVM (though it belongs to the Java process),\nand for this reason, it is not possible to write\ncomplex structures used in JVM languages into it. This raises the need for an intermediate step of serializing the\ndata into a plain byte array, which can then be stored in the off-heap area. When the data is loaded, the reverse\nprocess must be performed: deserialization into a form that we can use in Java. These additional steps will of\ncourse come at an extra cost, which is why accessing off-heap data will, for obvious reasons, take longer than accessing\non-heap data directly.\nSince writing and reading data in the off-heap space takes longer, what is the benefit of this approach then? Well, the data\nstored in the off-heap space are not subject to GC processes, so on the one hand we – the programmers – are responsible\nfor each freeing of memory after a given variable is no longer useful. On the other hand, we relieve the management\nprocesses in the JVM by releasing CPU’s time for the rest of the application, so, theoretically, it should\nresult in some resource savings. The question is, do these differences balance each other out to any degree? Will the savings\nassociated with the GC process balance out our longer data access time? If so, does it depend only on the amount of\ndata, or is there a specific usage scenario? To answer these questions, it is necessary to run a few experiments.\nExperiments\nWe can store any data structure in the on-heap area, which means that the advantage of this approach lies in the fact\nthat there is no overhead involved in transforming the data to another form, while its disadvantage consists of the\nadditional cost related to the GC. On the other hand, in the case of off-heap storage, there is no GC extra cost,\nbut there is the cost of serialising the data to a byte array.\nOver the last years, significant\nprogress has been made in the field of GC and with the right matching of the algorithm to the application profile, its\ntime can be very short. But is there any case where it is worth reaching into the unmanaged space after all?\nI decided to start with an overview of what open-source options are currently available. When it comes to the implementation of the\non-heap cache mechanism, the options are numerous – there is well known:\nguava,\nehcache, caffeine and many other solutions. However,\nwhen I began researching cache mechanisms offering the possibility of storing data outside GC control, I found out\nthat there are very few solutions left. Out of the popular ones, only Terracotta is supported.\nIt seems that this is a very niche solution and we do not have many options to choose\nfrom. In terms of less-known projects, I came across Chronicle-Map,\nMapDB and OHC. I chose the\nlast one because it was created as part of the Cassandra project, which I had some experience with and was curious\nabout how this component worked:\nOHC was developed in 2014/15 for Apache Cassandra 2.2 and 3.0 to be used as the new row-cache backend.\nTo run the experiment, I decided to use a service built to provide the offer description based on its unique number. After\ndownloading the offer description from the repository, it is placed in the cache to speed up future calls. Obviously, the\ncache has a limited capacity, which is chosen in such a way that it forces the deletion of items that have been placed\nin it for the longest time ago.\nIn our cache, the offer number is the key, while its description in the form of a string of characters is the\nvalue. This allows us to easily simulate almost any size of data in the cache (all we have to do is to make the\noffer description longer), and additionally, it makes the overhead related to the aforementioned serialisation\nrelatively small – serialisation of a text string is obviously faster than a complex DTO object.\nIn my project, I used the Caffeine cache to store the data in the on-heap area\nand OHC library to store it in the off-heap area.\nThe test scenario consists of querying for descriptions of different offers. During the test, I will\ncollect data on memory and GC parameters using jConsole. I will run the test scenario using jMeter,\nwhich additionally will allow me to measure response times.\nFrom my preliminary research I know that this method is only applicable to memory-intensive systems.\nHowever, for the sake of order, let’s first run an experiment on a small cache size with element set to 5 KB:\nmaximum number of cached elements: 10000\ncached element size: 5.000 bytes\n10 threads querying for random offers in a loop of 100000 iterations each\nTake a look at the screenshots from jConsole below. The results are in line with expectations: no benefit from the use\nof off-heap space. Both the number of garbage collection cycles (63 vs. 65) and GC run time (0.182s vs 0.235s)\nare nearly identical in both cases:\nThe GC profile of on-heap variant:\n\nThe GC profile of off-heap variant:\n\nNot much of an improvement for small to medium cache size. However, this result is not disappointing to me because\nI expected it. GC is designed to handle much more memory than 400 MB, it would therefore be strange if we obtained\nan improvement at such an early stage.\nNow let’s see how the comparison looks for a much larger cache element size, let’s increase it up to 100 KB.\nAt the same time, due to the fact that I am running the tests on a laptop with limited resources, I will reduce\nthreads configuration and cache maximum element size.\nThe configuration of the second test is as follows:\nmaximum number of cached elements: 5000\ncached element size: 100.000 bytes\n10 threads querying for random offers in a loop of 1000 iterations each\nLet’s take a look at the results.\nThe GC profile of on-heap variant:\n\nMemory usage increases throughout the test, there are 40 GC collection cycles that together last 0.212s.\nThe GC profile of off-heap variant:\n\nThis time heap memory usage chart definitely looks different, is shaped like a saw, and reaches half of the previous value.\nPlease note also, that this time there are only 13 GC cycles with total time of 0.108s.\nThe results of the GC profile comparison are therefore as expected, and what about the response times?\njMeter metrics of on-heap variant:\n\njMeter metrics of off-heap variant:\n\nRequest time metrics data is also in line with predictions, off-heap variant proved to be slightly slower than on-heap.\nNow let’s see what effect increasing the data size will have on the results. Let’s do tests for the following sizes:\n100.000 B, 200.000 B and 300.000 B, jMeter configuration stays unchanged: 10 threads with 1000 iterations each.\nThis time, for the sake of clarity, the results are summarized in a table:\nCached item size\n      Variant\n      GC cycles count\n      GC time\n      Request time (median)\n      Throughput\n    \n100.000 B\n      on-heap\n      40\n      0.212 s\n      171 ms\n      83.2 rps\n    \n100.000 B\n      off-heap\n      13\n      0.108 s\n      179 ms\n      78.1 rps\n    \n200.000 B\n      on-heap\n      84\n      0.453 s\n      396 ms\n      38.2 rps\n    \n200.000 B\n      off-heap\n      19\n      0.182 s\n      355 ms\n      40.2 rps\n    \n300.000 B\n      on-heap\n      114\n      0.6s\n      543 ms\n      27.3 rps\n    \n300.000 B\n      off-heap\n      27\n      0.185s\n      528 ms\n      27.9 rps\n    \nIt turns out that as the size of cache item increases, the benefits of using off-heap space grow – all metrics are improved.\nWhat about cache maximum elements? Let’s use 200.000B item size and check what happens when we increase the maximum cache\nelement size, we will test cache for 5000, 10.000 and 15.000 elements:\nCache max elements\n      Variant\n      GC cycles count\n      GC time\n      Request time (median)\n      Throughput\n    \n5000\n      on-heap\n      84\n      0.453 s\n      396 ms\n      38.2 rps\n    \n5000\n      off-heap\n      19\n      0.182 s\n      355 ms\n      40.2 rps\n    \n10000\n      on-heap\n      81\n      0.46 s\n      393 ms\n      38.8 rps\n    \n10000\n      off-heap\n      19\n      0.173 s\n      345 ms\n      42.6 rps\n    \n15000\n      on-heap\n      84\n      0.462 s\n      355 ms\n      41.8 rps\n    \n15000\n      off-heap\n      19\n      0.167 s\n      344 ms\n      42.6 rps\n    \nNo surprise here either, increasing cache size has a positive impact on both variants. Of course in case of on-heap cache,\nsome of the benefits are offset by the need for cleaning larger memory area.\nWith the experiments conducted, we can conclude that the more data we store in memory, the greater the benefit of using\nthe off-heap area may be. At the same time, it should be added that these benefits are not huge, just a few RPS more.\nIn the case of systems that store tremendous amounts of data, this method may bring some improvements in terms of resource utilization.\nHowever, for most of our apps and services, that’s probably not the way to go, a code audit is a better idea.\nThis is probably a good time to highlight how well implemented the current memory sweeper algorithms are. Well done GC!\nConclusions\nEveryone has probably come across a case when an application froze as a result of GC’s operation. As the above data\nshow, there is a relationship between the amount of data stored in memory and the time the GC requires to clean it up –\nthe more data we store on the heap, the longer it takes to free the memory. That is why the cases where we process large\namounts of data provide us with a potential benefit of using the off-heap area. There are some very specialised uses of\nthis technique, such as Spark, which can store large amounts of data for subsequent processing steps and can do so using\nthe off-heap space (you can read more about Spark memory model here).\nAnother example of the use of the off-heap approach is the Apache Cassandra database. The OHC used\nin this post was developed from this particular project.\nThere is a very narrow group of cases where storing data outside of GC control is justifiable. However, for the\nvast majority of applications, a much better approach is to take advantage of ever-improving GC\nimplementations. If you have experienced problems with the slow performance of the GC while developing your business\nservice, you should definitely audit your code first and experiment with different heap size settings and the GC\nalgorithm. When all other methods fail, you can give the off-heap area a try.\nHowever, if you are working on a server that processes massive amounts of data, it is worth considering off-heap\nstorage earlier, similar to Spark or Cassandra solutions.","guid":"https://blog.allegro.tech/2022/06/gc-hands-off-my-data.html","categories":["tech","cache","performance","off-heap","garbage collectors"],"isoDate":"2022-06-29T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[],"events":[{"created":1657193453000,"duration":7200000,"id":"287035383","name":"Allegro Tech Labs #10 Online: Poskromić stan w React","date_in_series_pattern":false,"status":"past","time":1658934000000,"local_date":"2022-07-27","local_time":"17:00","updated":1658944632000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":26,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/287035383/","description":"❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: [https://app.evenea.pl/event/allegro-tech-labs-10/](https://app.evenea.pl/event/allegro-tech-labs-10/?fbclid=IwAR1Zj3sIcfx3WEWiFfS_hgiW6BJQD6stYouSGuSqfxDq9YVeom8fTFcrE1Q) ❗ **Allegro Tech Labs** to w 100% zdalna odsłona naszych stacjonarnych spotkań warsztatowych. Zazwyczaj spotykaliśmy się…","visibility":"public","member_pay_fee":false},{"created":1655131243000,"duration":5400000,"id":"286545395","name":"Allegro Tech Live #29 - Wyzwania Product Managera","date_in_series_pattern":false,"status":"past","time":1656604800000,"local_date":"2022-06-30","local_time":"18:00","updated":1656612323000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":88,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/286545395/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","visibility":"public","member_pay_fee":false},{"created":1650552918000,"duration":100800000,"id":"285416318","name":"UX Research Confetti - II edycja","date_in_series_pattern":false,"status":"past","time":1653562800000,"local_date":"2022-05-26","local_time":"13:00","updated":1653666063000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/285416318/","description":"REJESTRACJA NA WYDARZENIE -&gt; https://app.evenea.pl/event/ux-research-confetti-2/ 🎉 Niech ponownie rozsypie się confetti wiedzy o badaniach UX! 🎉 Szukaliśmy konferencji badawczej UX w Polsce i nie znaleźliśmy……","visibility":"public","member_pay_fee":false},{"created":1651656994000,"duration":7200000,"id":"285691203","name":"Allegro Tech Live #28 - Mobile: Architektura softu i architektura sprzętu","date_in_series_pattern":false,"status":"past","time":1652976000000,"local_date":"2022-05-19","local_time":"18:00","updated":1652985850000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":48,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/285691203/","description":"**Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Kiedyś spotykaliśmy się w naszych biurach, a teraz to my gościmy…","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"S03E02 - Barbara Kaczorek, Jakub Kwietko - O tym jak powstawały zielone automaty paczkowe Allegro One Box","link":"https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/","pubDate":"Thu, 08 Sep 2022 00:00:00 GMT","content":"Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.","contentSnippet":"Jak wyglądała współpraca ponad 350 osób przy tak dużym i złożonym projekcie jak uruchomienie Allegro One Box?  Z jakimi wyzwaniami zmierzyły się osoby, które przy nim pracowały? Jak można mierzyć efekty swojej pracy w projektach takich, jak ten? Dlaczego Product Manager musi czasem siedzieć z laptopem za prototypem urządzenia? Na te i inne pytania odpowiadają Barbara Kaczorek - Product Manager w obszarze Delivery Experience w Allegro i Jakub Kwietko - lider zespołów developerskich OpenNet zaangażowanych w powstawanie Allegro One Box. Dobrze wiedzieć: OpenNet to wiodący dostawca rozwiązań technologicznych dla branży logistycznej w Polsce i za granicą, od 2021 roku jest częścią Grupy Allegro.","guid":"https://podcast.allegro.tech/o-tym-jak-powstawaly-zielone-automaty-paczkowe-allegro-one-box/","isoDate":"2022-09-08T00:00:00.000Z"},{"title":"S03E01 - Ewa Ludwiczak - O Quality Assurance w Allegro","link":"https://podcast.allegro.tech/o-quality-assurance-w-allegro/","pubDate":"Thu, 25 Aug 2022 00:00:00 GMT","content":"Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro. ","contentSnippet":"Na czym polega rola testera w Allegro? Dlaczego testerzy w Allegro są blisko technologii i produktu? Jak może rozwinąć się kariera testera, gdzie szukać aktualnej wiedzy i kim jest “Full Stack Tester”? Czy pierwsze kroki w branży IT muszą być trudne i jak programowania uczą się dzieci? Na te i inne pytania odpowiada Ewa Ludwiczak - liderka i testerka w Allegro.","guid":"https://podcast.allegro.tech/o-quality-assurance-w-allegro/","isoDate":"2022-08-25T00:00:00.000Z"},{"title":"S02E12 - Piotr Betkier - Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro/","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","content":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wygląda rola architekta w Allegro? Ile takich osób pracuje w naszej firmie i dlaczego ta rola jest tak różnorodna? Czym jest Andamio i jak rozwijamy naszą platformę – o tym wszystkim opowie Piotr Betkier – Inżynier, Architekt Platformy Technicznej w Allegro oraz twórca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro/","isoDate":"2021-06-16T00:00:00.000Z"},{"title":"S02E11 - Piotr Michoński - Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro/","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","content":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","contentSnippet":"Jak jest zbudowane środowisko uruchomienia aplikacji Allegro? Jak działają serwerownie firmy i ile ich potrzeba, a które elementy Allegro działają w chmurze publicznej? Jak przebiegała transformacja w Allegro i co zmieniało się przez lata? Jak wzrost biznesu wpływa na wielkość infrastruktury i jak infrastruktura Allegro odczuła przyjście pandemii? O tym, a także o rozwoju liderów technologii w Allegro oraz o historii powstania dżingla do naszych podcastów, opowie Piotr Michoński - menadżer Zespołów tworzących infrastrukturę Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro/","isoDate":"2021-06-01T00:00:00.000Z"}]},"__N_SSG":true}