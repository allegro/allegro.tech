{"pageProps":{"posts":[{"title":"Vanilla JS is not dead! Microfrontends without web performance issues.","link":"https://blog.allegro.tech/2022/11/vanilla-js-is-not-dead.html","pubDate":"Thu, 10 Nov 2022 00:00:00 +0100","authors":{"author":[{"name":["Krzysztof Mikuta"],"photo":["https://blog.allegro.tech/img/authors/krzysztof.mikuta.jpg"],"url":["https://blog.allegro.tech/authors/krzysztof.mikuta"]}]},"content":"<p>Building a complex web platform can be a real challenge, especially when parts of it are delivered by independent teams.\nPicking out the correct architecture is crucial, but maintaining it can be even more challenging.\nFrontend microservices, aka microfrontends, is an architecture that gives a lot of flexibility, but can cause\nperformance issues in the future, if not managed well. This article presents an approach to the microfrontends\narchitecture to keep the frontend technology stack efficient based on the complexity of user interface.</p>\n\n<h2 id=\"introduction\">Introduction</h2>\n<p>It’s 2022. In the frontend world, we have at least four major frameworks and libraries that have been around for a while\nand provide great resources to build fast and responsive user interfaces. The idea of delivering frontend components in\nvanilla JS seems to be pointless. Why should I even think about getting rid of the great features provided by well known,\nprecisely documented and strongly supported mature libraries? Well, as always, it pretty much depends on the\narchitecture. You have a single big frontend application running in React? Great! You have a couple applications with\na bunch of shared components inside an Angular monorepo? Good for you! But what if you have a big platform with huge\ntraffic, where frontend features are being delivered as independent fragments by independent teams across\nthe whole company? Well, let’s talk about the last option and go through some reasonable use cases for vanilla JS/TS as\nAllegro platform is built upon frontend microservices.</p>\n\n<h2 id=\"dealing-with-the-frontend-microservices-architecture\">Dealing with the frontend microservices architecture</h2>\n<p>The idea of splitting up the frontend of a big e-commerce platform into smaller pieces has been described in\nthe article <a href=\"/2016/03/Managing-Frontend-in-the-microservices-architecture.html\">Managing Frontend in the Microservices Architecture</a>.\nIt’s been 6 years since the article appeared and even more since the architecture was implemented at Allegro.\nNowadays, we manage over 1000 microservices and 600+ Opbox components that power the Allegro platform.\nWe can say that it works pretty well for us. As software engineers, we don’t need to worry about things like routing,\nSSR or monitoring, because they’re already served by Opbox. Also, we have overcome the problems the architecture causes\nand implemented efficient solutions. One problem has been described in the article\n<a href=\"/2021/07/css-architecture-and-performance-of-micro-frontends.html\">CSS Architecture and Performance in Micro Frontends</a>.</p>\n\n<p>To clear things up a little bit, imagine building a page made of tens of components, delivered by independent teams.\nEvery component, even the simplest one was implemented using one of the popular libraries. Seems harmless, but it can\ntruly hurt web performance. Rendering plain HTML on the server is much faster than evaluation of library mechanisms to\nproduce static markup. Moreover, client bundles need to be fetched in a browser, but they are pretty heavy as they\ninclude not only the custom code, but the libraries’ code as well… It’s going to take even more time when the internet\nconnection is weak (try setting up throttling in the dev tools). Well, undeniably working with distributed components\nrequires a lot of discipline. Also, monitoring and measuring is pretty important to figure out if the components\nthe team takes care of perform well. If you want to learn more, take a look at the article\n<a href=\"/2021/06/measuring-web-performance.html\">Measuring Web Performance</a>.</p>\n\n<p>How much discipline do you need to keep the system fast and efficient? Enough to have a reasonable approach to\npick out the correct technology to solve the problem. You know you’re asking for trouble, when you decide to use\na complex rendering library for rendering static labels that don’t behave in a reactive way. What could you do instead?\nJust map data to plain HTML! This is the case for vanilla JS. In the next paragraph, I’ll present and discuss\nthe approach we use on our team.</p>\n\n<h2 id=\"pick-the-right-technology\">Pick the right technology</h2>\n<p>For organizational purposes, we decided to define three types of complexity of UI components and assigned\nthree technology stacks that are suitable to solve different kinds of problems. Let’s dive into the details.</p>\n\n<h3 id=\"simple-ui-component\">Simple UI Component</h3>\n<p>This one doesn’t do anything spectacular. In most cases, it’s entirely rendered on the server and has no\nclient side scripting, or it may have some simple event handling. You can easily navigate through the platform using\njust an HTML anchor, can’t you? Also, CSS is so powerful nowadays that javascript is not always necessary to implement\ndynamic behaviors in browsers. The approach for such a component is simple: take the response of the backend service,\nwrite some HTML and CSS representing this data and send it to the client.</p>\n\n<h3 id=\"ui-component-that-is-reactive\">UI Component that is reactive</h3>\n<p>In this case, the component is rendered server-side, but the client side scripts run in a browser\nto provide reactivity. Partial changes of the state require updating the existing parts of the DOM. The challenge here\nis to implement a fine-grained reactivity mechanism, organize the code in a functional manner and separate\nside effects. The first thing can be easily handled using reactive streams like <a href=\"https://github.com/staltz/xstream\">xstream</a>,\nwhich is lighter than the well known rxjs, but still powerful. To keep code organized in a functional manner we borrowed\nthe <a href=\"https://cycle.js.org/model-view-intent.html\">Model-View-Intent pattern from cycle.js</a> and adjusted it to our case,\nwhere the HTML is provided by the server and “hydrated” on the client side. The idea is simple: we mount event handlers\nin Intent, map it to state in Model and react to changes in View. At the end of the system there are side effects\nthat run as a result of reactive subscriptions inside View. It’s still vanilla JS/TS on the server side and\nvanilla JS/TS with a touch of reactivity on the client side.</p>\n\n<h3 id=\"complex-ui-component\">Complex UI component</h3>\n<p>This one can be rendered on the server side, but then always hydrates on the client side. It can also be entirely\nrendered on the client side, forming a single-page application. It’s strongly reactive, changes its state constantly\nand re-renders. Also, state changes affect many parts of the DOM. You surely know it is a great use case for libraries\nlike React, where state changes trigger a reconciliation algorithm, which figures out what has changed and operates on\nan effective layer called Virtual DOM. I don’t think this approach requires any more explanation, as it’s the most\npopular approach in the frontend world nowadays. We just write one piece of code, run renderToString() on the server\nand hydrate() on the client, that’s it.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n<p>The presented approach may sound artificially complicated, but it does do its job. Using a sledgehammer to crack a nut\ncauses web performance issues that can go even further in a distributed environment. Spending a little bit more time on\nplanning features to pick an effective technology definitely pays off! Here are some conclusions based on our experience\nwe would like to share:</p>\n<ul>\n  <li>Don’t reinvent the wheel! Look for small, stable, well-supported packages in the npm registry.\nUse <a href=\"https://bundlephobia.com/\">bundlephobia.com</a> to analyze them and look for alternatives if needed.</li>\n  <li>If code complexity grows, use a library/framework. Don’t write your own! I know it’s tempting and trendy,\nbut you will end up maintaining this code instead of focusing on business features.</li>\n  <li>Monitor bundle sizes to ensure your code transpiles efficiently. You will figure out which expressions add more code\nto the bundle. You can set up extra tests for checking bundle sizes during your build pipeline to ensure you’re not\nrunning out of limits.</li>\n  <li>Separate side effects like DOM manipulations from business logic. It will make the code more predictable\nand easily testable. There are a bunch of patterns and state management libraries that can help you.</li>\n  <li>Respond fast by rendering on the server side, and hydrate wisely on the client side. If the component you create\nis not reactive, make it a server component that does not need to hydrate on the client. It’s a great way to optimize\nwebsite interactivity.</li>\n</ul>\n","contentSnippet":"Building a complex web platform can be a real challenge, especially when parts of it are delivered by independent teams.\nPicking out the correct architecture is crucial, but maintaining it can be even more challenging.\nFrontend microservices, aka microfrontends, is an architecture that gives a lot of flexibility, but can cause\nperformance issues in the future, if not managed well. This article presents an approach to the microfrontends\narchitecture to keep the frontend technology stack efficient based on the complexity of user interface.\nIntroduction\nIt’s 2022. In the frontend world, we have at least four major frameworks and libraries that have been around for a while\nand provide great resources to build fast and responsive user interfaces. The idea of delivering frontend components in\nvanilla JS seems to be pointless. Why should I even think about getting rid of the great features provided by well known,\nprecisely documented and strongly supported mature libraries? Well, as always, it pretty much depends on the\narchitecture. You have a single big frontend application running in React? Great! You have a couple applications with\na bunch of shared components inside an Angular monorepo? Good for you! But what if you have a big platform with huge\ntraffic, where frontend features are being delivered as independent fragments by independent teams across\nthe whole company? Well, let’s talk about the last option and go through some reasonable use cases for vanilla JS/TS as\nAllegro platform is built upon frontend microservices.\nDealing with the frontend microservices architecture\nThe idea of splitting up the frontend of a big e-commerce platform into smaller pieces has been described in\nthe article Managing Frontend in the Microservices Architecture.\nIt’s been 6 years since the article appeared and even more since the architecture was implemented at Allegro.\nNowadays, we manage over 1000 microservices and 600+ Opbox components that power the Allegro platform.\nWe can say that it works pretty well for us. As software engineers, we don’t need to worry about things like routing,\nSSR or monitoring, because they’re already served by Opbox. Also, we have overcome the problems the architecture causes\nand implemented efficient solutions. One problem has been described in the article\nCSS Architecture and Performance in Micro Frontends.\nTo clear things up a little bit, imagine building a page made of tens of components, delivered by independent teams.\nEvery component, even the simplest one was implemented using one of the popular libraries. Seems harmless, but it can\ntruly hurt web performance. Rendering plain HTML on the server is much faster than evaluation of library mechanisms to\nproduce static markup. Moreover, client bundles need to be fetched in a browser, but they are pretty heavy as they\ninclude not only the custom code, but the libraries’ code as well… It’s going to take even more time when the internet\nconnection is weak (try setting up throttling in the dev tools). Well, undeniably working with distributed components\nrequires a lot of discipline. Also, monitoring and measuring is pretty important to figure out if the components\nthe team takes care of perform well. If you want to learn more, take a look at the article\nMeasuring Web Performance.\nHow much discipline do you need to keep the system fast and efficient? Enough to have a reasonable approach to\npick out the correct technology to solve the problem. You know you’re asking for trouble, when you decide to use\na complex rendering library for rendering static labels that don’t behave in a reactive way. What could you do instead?\nJust map data to plain HTML! This is the case for vanilla JS. In the next paragraph, I’ll present and discuss\nthe approach we use on our team.\nPick the right technology\nFor organizational purposes, we decided to define three types of complexity of UI components and assigned\nthree technology stacks that are suitable to solve different kinds of problems. Let’s dive into the details.\nSimple UI Component\nThis one doesn’t do anything spectacular. In most cases, it’s entirely rendered on the server and has no\nclient side scripting, or it may have some simple event handling. You can easily navigate through the platform using\njust an HTML anchor, can’t you? Also, CSS is so powerful nowadays that javascript is not always necessary to implement\ndynamic behaviors in browsers. The approach for such a component is simple: take the response of the backend service,\nwrite some HTML and CSS representing this data and send it to the client.\nUI Component that is reactive\nIn this case, the component is rendered server-side, but the client side scripts run in a browser\nto provide reactivity. Partial changes of the state require updating the existing parts of the DOM. The challenge here\nis to implement a fine-grained reactivity mechanism, organize the code in a functional manner and separate\nside effects. The first thing can be easily handled using reactive streams like xstream,\nwhich is lighter than the well known rxjs, but still powerful. To keep code organized in a functional manner we borrowed\nthe Model-View-Intent pattern from cycle.js and adjusted it to our case,\nwhere the HTML is provided by the server and “hydrated” on the client side. The idea is simple: we mount event handlers\nin Intent, map it to state in Model and react to changes in View. At the end of the system there are side effects\nthat run as a result of reactive subscriptions inside View. It’s still vanilla JS/TS on the server side and\nvanilla JS/TS with a touch of reactivity on the client side.\nComplex UI component\nThis one can be rendered on the server side, but then always hydrates on the client side. It can also be entirely\nrendered on the client side, forming a single-page application. It’s strongly reactive, changes its state constantly\nand re-renders. Also, state changes affect many parts of the DOM. You surely know it is a great use case for libraries\nlike React, where state changes trigger a reconciliation algorithm, which figures out what has changed and operates on\nan effective layer called Virtual DOM. I don’t think this approach requires any more explanation, as it’s the most\npopular approach in the frontend world nowadays. We just write one piece of code, run renderToString() on the server\nand hydrate() on the client, that’s it.\nConclusions\nThe presented approach may sound artificially complicated, but it does do its job. Using a sledgehammer to crack a nut\ncauses web performance issues that can go even further in a distributed environment. Spending a little bit more time on\nplanning features to pick an effective technology definitely pays off! Here are some conclusions based on our experience\nwe would like to share:\nDon’t reinvent the wheel! Look for small, stable, well-supported packages in the npm registry.\nUse bundlephobia.com to analyze them and look for alternatives if needed.\nIf code complexity grows, use a library/framework. Don’t write your own! I know it’s tempting and trendy,\nbut you will end up maintaining this code instead of focusing on business features.\nMonitor bundle sizes to ensure your code transpiles efficiently. You will figure out which expressions add more code\nto the bundle. You can set up extra tests for checking bundle sizes during your build pipeline to ensure you’re not\nrunning out of limits.\nSeparate side effects like DOM manipulations from business logic. It will make the code more predictable\nand easily testable. There are a bunch of patterns and state management libraries that can help you.\nRespond fast by rendering on the server side, and hydrate wisely on the client side. If the component you create\nis not reactive, make it a server component that does not need to hydrate on the client. It’s a great way to optimize\nwebsite interactivity.","guid":"https://blog.allegro.tech/2022/11/vanilla-js-is-not-dead.html","categories":["tech","frontend","microservices","webperf","javascript"],"isoDate":"2022-11-09T23:00:00.000Z","thumbnail":"images/post-headers/javascript.png"},{"title":"Probabilistic Data Structures and Algorithms in NoSQL databases","link":"https://blog.allegro.tech/2022/10/probabilistic-algorithms.html","pubDate":"Tue, 04 Oct 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>One of the <a href=\"https://en.wikipedia.org/wiki/ACID\">four fundamental</a> features of transactional databases is durability. It says that once a\ntransaction is committed, the stored data remains available even if the database crashes. If we upload some information\ninto the database, we must be able to read it later, no matter what happens.</p>\n\n<p>It is so elementary that we frequently don’t even think about it: if we save a record with the ’42’\nvalue in a database, we will get ’42’ every time we read that\nrecord, until the next modification. The durability concept can be generalized somewhat, by considering not only transactional\ndatabases but those that do not provide transactions. After all, in each of them, after a\ncorrect write we can be sure that the stored information is in the database and we have access\nto it.</p>\n\n<p>But it turns out that there are databases that provide us with solutions making that the concept of durability —\neven in this generalized form — no longer so obvious. What would you say if we stored\n1 000 records in a database, and the database claimed that there were only 998 of them? Or, if we\ncreated a database storing sets of values and in some cases the database would claim that an\nelement was in that set, while in fact it was not? Seeing such a behavior many would probably start\nlooking for an error. However, behavior like this is not necessarily an error, as long as we use a database\nthat implements probabilistic algorithms and data structures. Solutions based on these methods allow some\ninaccuracy in the results, but in return they are able to provide us with great savings in the resources\nused. More interesting is that there is a good chance that you are already using such a DB.</p>\n\n<p>In this post we will learn about two probability-based techniques, perform some experiments and\nconsider when it is worth using a database that lies to us a bit.</p>\n\n<h2 id=\"fast-cardinality-aggregation\">Fast cardinality aggregation</h2>\n\n<p>Some time ago I had the opportunity to work on a service based on Elasticsearch. This service collects\nhuge amounts of data, which is later analyzed by our customer care specialists. One of the key elements to be analyzed\nis a simple aggregate — the number of unique occurrences of certain values. In mathematics, this\nquantity is called the power of the set or the cardinal number.</p>\n\n<p>The easiest way to understand this is to use an example: imagine that I take out all the banknotes\nfrom my wallet and it turns out that I have 10 of them, with the following nominal values:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">]</span>\n</code></pre></div></div>\n\n<p>If we arranged them by value, we would end up collecting these 10\nbanknotes in four piles with values: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 100]</code>, so the cardinal number of the set containing my 10\nbanknotes equals: 4.</p>\n\n<p>Elasticsearch has a special function: <a href=\"https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html\">cardinality</a>, which is used to determine the power of the set and we use\nthis function specifically to count unique occurrences that I mentioned earlier.</p>\n\n<p>It may seem that counting unique occurrences of values is a trivial task.\nLet’s go back to our example with the banknotes. You can think of many ways to check how many\nunique values there are in this list, probably one of the simplest is to use the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class. One of its main features is\nthat it de-duplicates the elements added to it, thus it stores only one occurrence of each.</p>\n\n<p>After adding 10 values of banknotes: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]</code> to an instance of the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>\nclass, it will ultimately only store the values <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 100]</code> (not necessarily in that order, but it\ndoesn’t matter it this case). So all we need to do is check the size of this set and we have the result we were\nlooking for: 4.</p>\n\n<p>This solution is simple and looks tempting, yet it has a certain drawback: the more unique elements the set stores,\nthe more memory our program needs. In an extreme case, when each added element is different from\nthe others, the memory complexity of this approach will be linear. This is bad news when we\nwant to operate on a large volume of data, because we will immediately use all available memory.\nIf, additionally, requests for the cardinal number come from\nclients with high intensity, and the input set contains billions of elements, it is easy to imagine that the\napproach described above has no chance of success.</p>\n\n<p>How to address this issue? In such a situation we can switch to one of ingenious probabilistic algorithms. Their\nmain feature is that they give approximate rather than exact results. The huge advantage, on the\nother hand, is that they are much less resource-intensive.</p>\n\n<h2 id=\"near-optimal-cardinality-estimator\">Near-optimal cardinality estimator</h2>\n\n<p>One such algorithm — HyperLogLog (HLL) — has been implemented in the aforementioned\nElasticsearch to build the cardinality function. It is used to count the unique values of a given field of\nan indexed document, and it does so with a certain approximation, using very little memory.\nInterestingly, you can control the accuracy of this approximation with a special parameter. This is\nbecause in addition to the field to be counted, the cardinality function also accepts a\n<code class=\"language-plaintext highlighter-rouge\">precision_threshold</code> argument, due to which we can specify how much inaccuracy we agree to, in\nexchange for less or more memory usage.</p>\n\n<p>Obviously, in some cases even a small error is unacceptable. We must then abandon the probabilistic\napproach and look for another solution. However, for a sizable class of problems, certain\napproximation is completely sufficient. Imagine a video clip uploaded to a popular streaming service.\nIf the author of the clip has a bit of luck, the counter of unique views of his/her work starts spinning\nvery quickly. In case of very high popularity, when displaying the current number of visits, full\naccuracy will not matter so much; we can reconcile with displaying a value that differs from the\nactual one by a few percent. It is completely sufficient that the accurate data — e.g. for monetization\npurposes — is available the next day, when we calculate it accurately using, for example, Apache Spark.</p>\n\n<p>Implementing such a counter of unique visitors into a site operating on huge data sets, we could\ntherefore consider using the HLL algorithm.</p>\n\n<p>Readers interested in a detailed description of the HLL algorithm are referred to a great article on\n<a href=\"http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation\">Damn Cool Algorithms post</a>.\nHowever, its most important features are worth noting here:</p>\n<ul>\n  <li>the results, although approximate, are deterministic,</li>\n  <li>the maximum possible error is known,</li>\n  <li>amount of memory used is fixed.</li>\n</ul>\n\n<p>The last two features are closely related and can be controlled: we can decrease the error level by increasing\nthe available memory limit and vice versa.\nThere are many ready-made implementations of the HLL algorithm available, so it’s worth reaching\nfor one of them and doing some experiments. I will use <a href=\"https://datasketches.apache.org/docs/HLL/HLL.html\">datasketches</a>\nand compare the memory consumption with the classic approach using the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. Moreover, I will add a third variant based\non a <code class=\"language-plaintext highlighter-rouge\">distinct</code> method from the Kotlin language, which — like the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> constructor — de-duplicates\nelements from the list.</p>\n\n<p>Below there is a code snippet of a simple program that determines the cardinal number of a set of numbers using <code class=\"language-plaintext highlighter-rouge\">HashSet</code>\nclass from Java language. In order to be able to run some trials, I’ve introduced a couple of basic parameters. The\ninput list consists of <code class=\"language-plaintext highlighter-rouge\">n</code> numbers, while using the <code class=\"language-plaintext highlighter-rouge\">f</code> parameter and the <code class=\"language-plaintext highlighter-rouge\">modulo</code> function I decide what\npart of the input list is unique. For example, for n=1 000 000 and f=0.1, the result will be a cardinal\nnumber equal to 100 000.</p>\n\n<p>Please note the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> constructor parameter. By default, when the constructor is empty - this class is\n<a href=\"https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/HashSet.html#%3Cinit%3E()\">initialized with the value 16</a>,\nwhich means that before adding the 17th element, memory reallocation must occur for next portion of elements, which takes time.\nTo eliminate this extra time I allocate in advance as much memory as needed.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">mod</span> <span class=\"p\">=</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">*</span> <span class=\"n\">f</span><span class=\"p\">).</span><span class=\"nf\">toLong</span><span class=\"p\">()</span>\n<span class=\"kd\">val</span> <span class=\"py\">set</span> <span class=\"p\">=</span> <span class=\"nc\">HashSet</span><span class=\"p\">&lt;</span><span class=\"nc\">Long</span><span class=\"p\">&gt;(</span><span class=\"n\">mod</span><span class=\"p\">.</span><span class=\"nf\">toInt</span><span class=\"p\">())</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">elapsed</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"mi\">0</span> <span class=\"n\">until</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">set</span><span class=\"p\">.</span><span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"p\">%</span> <span class=\"n\">mod</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"n\">cardinality</span> <span class=\"p\">=</span> <span class=\"k\">set</span><span class=\"p\">.</span><span class=\"n\">size</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>Two other programs do exactly the same thing: determine the cardinal number of a set of numbers, but one uses Kotlin\n<code class=\"language-plaintext highlighter-rouge\">distinct</code> method and the second one uses HLL algorithm. You can find full code of all three applications\non this <a href=\"https://github.com/mknasiecki/prob-alg-post\">repository</a>.</p>\n\n<p>All three programs, in addition to the result, also measure total execution time. Moreover, using\n<a href=\"https://openjdk.java.net/tools/svc/jconsole/\">jConsole</a> I am also able to measure the amount of memory used. I decided\nto measure the total memory used by the\nprograms, because measuring the size of the data structures is not a trivial task.</p>\n\n<p>We start by checking the variant n=1 000 000/f=0.25 as a result of which we should get a power of set\nequal 250 000. Let’s take a look at the results:</p>\n\n<p><em>n=1 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>250 000</td>\n      <td>250 000</td>\n      <td>249 979.9</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.01</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>71</td>\n      <td>106</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>42</td>\n      <td>73</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>In case of such a small set the deviation of the result of the HLL variant from the true value is far less than\n1%, while in this case you can already see the benefits of this method; the amount of memory used is\nhalf compared to the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> version and as much as 3 times less when compared to the\nversion using the Kotlin language function.</p>\n\n<p>It is worth pausing here for a moment to consider what is the reason for such a big difference in consumed memory.\nThe first two programs are based on collections of objects, thus storing in memory entire instances along with their references.\nThe HLL method, on the other hand, uses memory-efficient bit arrays that store data based on object hashes. This makes\nit insensitive to the original size of the processed data. It means that the benefits of using HLL increase with the\nmemory needed to store the objects you want to count. The results presented above would be even more spectacular if we\nused, for example, email addresses or IP addresses instead of numbers.</p>\n\n<p>During the next attempt we increase the value of the <code class=\"language-plaintext highlighter-rouge\">n</code> parameter tenfold:</p>\n\n<p><em>n=10 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>2 500 000</td>\n      <td>2 500 000</td>\n      <td>2 484 301.4</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>483</td>\n      <td>863</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>233</td>\n      <td>574</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The error value has increased slightly, while the difference in memory usage and the performance\ntime is even greater than before. Therefore, it is worthwhile to increase the size of the set again:</p>\n\n<p><em>n=100 000 000/f=0.25</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>25 000 000</td>\n      <td>25 000 000</td>\n      <td>25 301 157.2</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.2</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>3857</td>\n      <td>7718</td>\n      <td>1538</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>1800</td>\n      <td>5300</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Deviation from the correct result exceeded 1%; the times also went up, although they are still many\ntimes shorter compared to other variants. It’s worth noting that the amount of memory used has practically not changed.</p>\n\n<p>Now let’s see what happens when we change the second parameter, which determines the number of\nunique elements in the input set:</p>\n\n<p><em>n=10 000 000/f=0.5</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>5 000 000</td>\n      <td>5 000 000</td>\n      <td>5 067 045.2</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.34</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>467</td>\n      <td>914</td>\n      <td>183</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>420</td>\n      <td>753</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>n=10 000 000/f=0.75</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>distinct</th>\n      <th>HLL</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>cardinality</td>\n      <td>7 500 000</td>\n      <td>7 500 000</td>\n      <td>7 619 136.7</td>\n    </tr>\n    <tr>\n      <td>error [%]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.59</td>\n    </tr>\n    <tr>\n      <td>time [ms]</td>\n      <td>589</td>\n      <td>1187</td>\n      <td>191</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>616</td>\n      <td>843</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Again, the results clearly show the advantages of the HLL algorithm. With a relatively low error we\nsignificantly reduced the amount of memory used and the time required for calculations.\nAs you can see and as expected, the classical approach gives accurate results but it consumes a lot of\nmemory, while the solution using HLL brings results characterized by approx. 1% error, but in return\nwe use much less memory. A certain surprise for me is the poor result of the Kotlin <code class=\"language-plaintext highlighter-rouge\">distinct</code> function; I\nexpected results more similar to the variant based on the <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. Presumably the key difference is that it returns an instance\nof the <code class=\"language-plaintext highlighter-rouge\">List</code> class rather than <code class=\"language-plaintext highlighter-rouge\">HashSet</code>. This requires further investigation, which is beyond the scope of my considerations.</p>\n\n<p>The HLL algorithm is implemented in several solutions, including the aforementioned Elasticsearch,\nas well as in e.g. <a href=\"https://redis.com/redis-best-practices/counting/hyperloglog/\">Redis</a> and <a href=\"https://prestodb.io/docs/current/functions/hyperloglog.html\">Presto</a>. The above experiments clearly show that the approximate method, in case\nwe need to process huge amounts of data, is a good idea provided that we allow a result with a small\nerror.</p>\n\n<h2 id=\"memory-efficient-presence-test\">Memory-efficient presence test</h2>\n\n<p>It turns out that the HLL is not the only probabilistic algorithm available in popular databases —\nanother example of this approach is the Bloom Filter. This is an implementation of a memory-saving structure that is\nused in the so-called presence test. Let’s go back to our example with my cash: <code class=\"language-plaintext highlighter-rouge\">[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]</code>.\nImagine that we want to test whether there is a 100 value banknote in my wallet. In this case the answer is positive, but the test\nfor the 200 value banknote should be false, since there is no such a banknote in the wallet.</p>\n\n<p>Of course, we are able again to implement a solution to this problem by simply using the properties\nof the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class and the <code class=\"language-plaintext highlighter-rouge\">contains</code> method. However, similarly as in case of determining the\ncardinality — the memory requirement increases with the size of the dataset.\nAgain, the solution for this problem may be an approximate method.</p>\n\n<p>Similarly as in case of the HLL algorithm the Bloom Filter allows for some inaccuracy, and in this case\nthis means false positive results. This is because it can happen that the Bloom Filter finds that an element\nbelongs to a given set, while in fact it is not there. However, the opposite situation is not possible\nso if the Bloom Filter states that an element is not part of the set, it is certainly true. Referring this to\nour example with the content of my wallet, the Bloom Filter could therefore assure me that there was a\n200 value banknote in it, while standing at the checkout in a store it would turn out that,\nunfortunately, it is not there. What a pity…</p>\n\n<p>Before we move on to examine how this algorithm works, let’s consider where it could be useful. A\ntypical example is a recommendation system. Imagine we are designing a system intended to suggest\narticles for users to read, a feature common on social media sites. Such a system needs to store a\nlist of articles read by each user so that it does not suggest them again. It is easy to imagine that\nstoring these articles with each user in the classic way would quickly exhaust memory resources. If we\ndon’t use any data removal mechanism, the database will grow indefinitely. The discussed Bloom\nFilter fits perfectly here as it will allow us to save a lot of memory, although, one must consider\nconsequences of its limitations related to possible false results. It may happen that we will get false\ninformation that a certain article has already been read by someone, while in fact this is not true.\nConsequently, we will not offer that user to read the material. On the other hand, the opposite\nsituation is not possible: we will never display to a user a recommendation of an article that he/she\nhas already read.</p>\n\n<p>At this point it is worth checking how much we gain by accepting the inconvenience described\nabove. I have prepared two implementations of a program that adds to a set of values and then\nchecks if they are there.\nThe first program uses the classic approach — the <code class=\"language-plaintext highlighter-rouge\">HashSet</code> class, while the second uses the Bloom\nFilter available in the popular <a href=\"https://guava.dev/releases/20.0/api/docs/com/google/common/hash/BloomFilter.html\">guava</a> library.\nAgain, using jConsole we register for both programs the amount of memory used, and additionally — for the version with\nthe Bloom Filter we also check the\nnumber of false positives. This value can be easily controlled, as the maximum allowed false positive\nrate can be set in the API; for needs of the following tests we will set it to 1%.</p>\n\n<p>Moreover, we will measure the total time of adding values to the set and the total time of querying\nwhether there are values in the set.</p>\n\n<p>Same as before we will perform a number of tests using the following parameters: <code class=\"language-plaintext highlighter-rouge\">n</code> — the size of the set of\nnumbers, and <code class=\"language-plaintext highlighter-rouge\">f</code> — what part of it should be added to the set. The configuration n=1 000 000 and f=0.1 means\nthat the first 100 000 numbers out of 1 000 000 will be added to the set. So, in the first part, the program will\nadd 100 000 numbers to the set and then — in the second stage — it will perform a presence test\nby checking whether the numbers above 100 000 belong to the set. There is no point in checking the\nnumbers added to the set beforehand, because we know that Bloom Filters do not give false\nnegative results. On the other hand, if any number above 100 000 is found according to the Bloom Filter in\nthe set, we will consider it a false positive.</p>\n\n<p>Following code snippet presents fragment of the Bloom Filter variant:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">val</span> <span class=\"py\">insertions</span> <span class=\"p\">=</span> <span class=\"p\">(</span><span class=\"n\">n</span> <span class=\"p\">*</span> <span class=\"n\">f</span><span class=\"p\">).</span><span class=\"nf\">toInt</span><span class=\"p\">()</span>\n<span class=\"kd\">val</span> <span class=\"py\">filter</span> <span class=\"p\">=</span> <span class=\"nc\">BloomFilter</span><span class=\"p\">.</span><span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"nc\">Funnels</span><span class=\"p\">.</span><span class=\"nf\">integerFunnel</span><span class=\"p\">(),</span> <span class=\"n\">insertions</span><span class=\"p\">,</span> <span class=\"mf\">0.01</span><span class=\"p\">)</span>\n<span class=\"kd\">var</span> <span class=\"py\">falsePositives</span> <span class=\"p\">=</span> <span class=\"mi\">0</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">insertTime</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"mi\">0</span> <span class=\"n\">until</span> <span class=\"n\">insertions</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"n\">filter</span><span class=\"p\">.</span><span class=\"nf\">put</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">queryTime</span> <span class=\"p\">=</span> <span class=\"nf\">measureTimeMillis</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"k\">in</span> <span class=\"n\">insertions</span> <span class=\"n\">until</span> <span class=\"n\">n</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">filter</span><span class=\"p\">.</span><span class=\"nf\">mightContain</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">))</span> <span class=\"p\">{</span>\n            <span class=\"n\">falsePositives</span><span class=\"p\">++;</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">val</span> <span class=\"py\">fpRatio</span> <span class=\"p\">=</span> <span class=\"n\">falsePositives</span><span class=\"p\">/</span><span class=\"n\">n</span><span class=\"p\">.</span><span class=\"nf\">toDouble</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>Again — you can find full code of both programs on aforementioned <a href=\"https://github.com/mknasiecki/prob-alg-post\">repository</a>.</p>\n\n<p>Let’s start with the following configuration: n=10 000 000/f=0.1:</p>\n\n<p><em>n=10 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>81</td>\n      <td>293</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>82</td>\n      <td>846</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>94</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>As you can see, the Bloom Filter returned less than 1% false results, but — at the same time — it used three times\nless memory than HashSet variant. Unfortunately, the times of Bloom Filter’s version are significantly higher.\nLet’s check what happens when we increase the size of the input set:</p>\n\n<p><em>n=100 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>593</td>\n      <td>318</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>988</td>\n      <td>944</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>876</td>\n      <td>29</td>\n    </tr>\n  </tbody>\n</table>\n\n<p><em>n=500 000 000/f=0.1</em></p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Metric\\Variant</th>\n      <th>HashSet</th>\n      <th>Bloom filter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>error[%]</td>\n      <td>0</td>\n      <td>0.9</td>\n    </tr>\n    <tr>\n      <td>insert time [ms]</td>\n      <td>1975</td>\n      <td>1372</td>\n    </tr>\n    <tr>\n      <td>query time [ms]</td>\n      <td>4115</td>\n      <td>4923</td>\n    </tr>\n    <tr>\n      <td>memory [MB]</td>\n      <td>4400</td>\n      <td>81</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>The number of false positives is still below the preset 1%, the amount of memory used is still lower\nthan the classic implementation, and interestingly also the times of the probabilistic variant are\nlower, at least for inserting. Thus, it can be seen that along with the increase in the size of the data the benefit of this\nmethod increases.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>The above results clearly show that by accepting a small share of false answers, we can gain significant savings in memory usage.\nSimilarly to the HLL algorithm, the structure based on the Bloom Filters is available in many popular\ndatabases like <a href=\"https://redis.com/redis-best-practices/bloom-filter-pattern/\">Redis</a>,\n<a href=\"https://hbase.apache.org/2.2/devapidocs/org/apache/hadoop/hbase/util/BloomFilter.html\">HBase</a>\nor <a href=\"https://cassandra.apache.org/doc/latest/cassandra/operating/bloom_filters.html\">Cassandra</a>.</p>\n\n<p>The simple experiments we conducted showed that probabilistic algorithms can save a lot\nof memory, which is especially important if our database stores huge amounts of data. In such cases it\nis sometimes worth letting your database lie to you a little.</p>\n","contentSnippet":"One of the four fundamental features of transactional databases is durability. It says that once a\ntransaction is committed, the stored data remains available even if the database crashes. If we upload some information\ninto the database, we must be able to read it later, no matter what happens.\nIt is so elementary that we frequently don’t even think about it: if we save a record with the ’42’\nvalue in a database, we will get ’42’ every time we read that\nrecord, until the next modification. The durability concept can be generalized somewhat, by considering not only transactional\ndatabases but those that do not provide transactions. After all, in each of them, after a\ncorrect write we can be sure that the stored information is in the database and we have access\nto it.\nBut it turns out that there are databases that provide us with solutions making that the concept of durability —\neven in this generalized form — no longer so obvious. What would you say if we stored\n1 000 records in a database, and the database claimed that there were only 998 of them? Or, if we\ncreated a database storing sets of values and in some cases the database would claim that an\nelement was in that set, while in fact it was not? Seeing such a behavior many would probably start\nlooking for an error. However, behavior like this is not necessarily an error, as long as we use a database\nthat implements probabilistic algorithms and data structures. Solutions based on these methods allow some\ninaccuracy in the results, but in return they are able to provide us with great savings in the resources\nused. More interesting is that there is a good chance that you are already using such a DB.\nIn this post we will learn about two probability-based techniques, perform some experiments and\nconsider when it is worth using a database that lies to us a bit.\nFast cardinality aggregation\nSome time ago I had the opportunity to work on a service based on Elasticsearch. This service collects\nhuge amounts of data, which is later analyzed by our customer care specialists. One of the key elements to be analyzed\nis a simple aggregate — the number of unique occurrences of certain values. In mathematics, this\nquantity is called the power of the set or the cardinal number.\nThe easiest way to understand this is to use an example: imagine that I take out all the banknotes\nfrom my wallet and it turns out that I have 10 of them, with the following nominal values:\n\n[10, 20, 50, 20, 50, 100, 50, 20, 10, 10]\n\n\nIf we arranged them by value, we would end up collecting these 10\nbanknotes in four piles with values: [10, 20, 50, 100], so the cardinal number of the set containing my 10\nbanknotes equals: 4.\nElasticsearch has a special function: cardinality, which is used to determine the power of the set and we use\nthis function specifically to count unique occurrences that I mentioned earlier.\nIt may seem that counting unique occurrences of values is a trivial task.\nLet’s go back to our example with the banknotes. You can think of many ways to check how many\nunique values there are in this list, probably one of the simplest is to use the HashSet class. One of its main features is\nthat it de-duplicates the elements added to it, thus it stores only one occurrence of each.\nAfter adding 10 values of banknotes: [10, 20, 50, 20, 50, 100, 50, 20, 10, 10] to an instance of the HashSet\nclass, it will ultimately only store the values [10, 20, 50, 100] (not necessarily in that order, but it\ndoesn’t matter it this case). So all we need to do is check the size of this set and we have the result we were\nlooking for: 4.\nThis solution is simple and looks tempting, yet it has a certain drawback: the more unique elements the set stores,\nthe more memory our program needs. In an extreme case, when each added element is different from\nthe others, the memory complexity of this approach will be linear. This is bad news when we\nwant to operate on a large volume of data, because we will immediately use all available memory.\nIf, additionally, requests for the cardinal number come from\nclients with high intensity, and the input set contains billions of elements, it is easy to imagine that the\napproach described above has no chance of success.\nHow to address this issue? In such a situation we can switch to one of ingenious probabilistic algorithms. Their\nmain feature is that they give approximate rather than exact results. The huge advantage, on the\nother hand, is that they are much less resource-intensive.\nNear-optimal cardinality estimator\nOne such algorithm — HyperLogLog (HLL) — has been implemented in the aforementioned\nElasticsearch to build the cardinality function. It is used to count the unique values of a given field of\nan indexed document, and it does so with a certain approximation, using very little memory.\nInterestingly, you can control the accuracy of this approximation with a special parameter. This is\nbecause in addition to the field to be counted, the cardinality function also accepts a\nprecision_threshold argument, due to which we can specify how much inaccuracy we agree to, in\nexchange for less or more memory usage.\nObviously, in some cases even a small error is unacceptable. We must then abandon the probabilistic\napproach and look for another solution. However, for a sizable class of problems, certain\napproximation is completely sufficient. Imagine a video clip uploaded to a popular streaming service.\nIf the author of the clip has a bit of luck, the counter of unique views of his/her work starts spinning\nvery quickly. In case of very high popularity, when displaying the current number of visits, full\naccuracy will not matter so much; we can reconcile with displaying a value that differs from the\nactual one by a few percent. It is completely sufficient that the accurate data — e.g. for monetization\npurposes — is available the next day, when we calculate it accurately using, for example, Apache Spark.\nImplementing such a counter of unique visitors into a site operating on huge data sets, we could\ntherefore consider using the HLL algorithm.\nReaders interested in a detailed description of the HLL algorithm are referred to a great article on\nDamn Cool Algorithms post.\nHowever, its most important features are worth noting here:\nthe results, although approximate, are deterministic,\nthe maximum possible error is known,\namount of memory used is fixed.\nThe last two features are closely related and can be controlled: we can decrease the error level by increasing\nthe available memory limit and vice versa.\nThere are many ready-made implementations of the HLL algorithm available, so it’s worth reaching\nfor one of them and doing some experiments. I will use datasketches\nand compare the memory consumption with the classic approach using the HashSet. Moreover, I will add a third variant based\non a distinct method from the Kotlin language, which — like the HashSet constructor — de-duplicates\nelements from the list.\nBelow there is a code snippet of a simple program that determines the cardinal number of a set of numbers using HashSet\nclass from Java language. In order to be able to run some trials, I’ve introduced a couple of basic parameters. The\ninput list consists of n numbers, while using the f parameter and the modulo function I decide what\npart of the input list is unique. For example, for n=1 000 000 and f=0.1, the result will be a cardinal\nnumber equal to 100 000.\nPlease note the HashSet constructor parameter. By default, when the constructor is empty - this class is\ninitialized with the value 16,\nwhich means that before adding the 17th element, memory reallocation must occur for next portion of elements, which takes time.\nTo eliminate this extra time I allocate in advance as much memory as needed.\n\nval mod = (n * f).toLong()\nval set = HashSet<Long>(mod.toInt())\n\nval elapsed = measureTimeMillis {\n    for (i in 0 until n) {\n        set.add(i % mod)\n    }\n\n    cardinality = set.size\n}\n\n\nTwo other programs do exactly the same thing: determine the cardinal number of a set of numbers, but one uses Kotlin\ndistinct method and the second one uses HLL algorithm. You can find full code of all three applications\non this repository.\nAll three programs, in addition to the result, also measure total execution time. Moreover, using\njConsole I am also able to measure the amount of memory used. I decided\nto measure the total memory used by the\nprograms, because measuring the size of the data structures is not a trivial task.\nWe start by checking the variant n=1 000 000/f=0.25 as a result of which we should get a power of set\nequal 250 000. Let’s take a look at the results:\nn=1 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      250 000\n      250 000\n      249 979.9\n    \nerror [%]\n      0\n      0\n      0.01\n    \ntime [ms]\n      71\n      106\n      53\n    \nmemory [MB]\n      42\n      73\n      21\n    \nIn case of such a small set the deviation of the result of the HLL variant from the true value is far less than\n1%, while in this case you can already see the benefits of this method; the amount of memory used is\nhalf compared to the HashSet version and as much as 3 times less when compared to the\nversion using the Kotlin language function.\nIt is worth pausing here for a moment to consider what is the reason for such a big difference in consumed memory.\nThe first two programs are based on collections of objects, thus storing in memory entire instances along with their references.\nThe HLL method, on the other hand, uses memory-efficient bit arrays that store data based on object hashes. This makes\nit insensitive to the original size of the processed data. It means that the benefits of using HLL increase with the\nmemory needed to store the objects you want to count. The results presented above would be even more spectacular if we\nused, for example, email addresses or IP addresses instead of numbers.\nDuring the next attempt we increase the value of the n parameter tenfold:\nn=10 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      2 500 000\n      2 500 000\n      2 484 301.4\n    \nerror [%]\n      0\n      0\n      0.63\n    \ntime [ms]\n      483\n      863\n      189\n    \nmemory [MB]\n      233\n      574\n      21\n    \nThe error value has increased slightly, while the difference in memory usage and the performance\ntime is even greater than before. Therefore, it is worthwhile to increase the size of the set again:\nn=100 000 000/f=0.25\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      25 000 000\n      25 000 000\n      25 301 157.2\n    \nerror [%]\n      0\n      0\n      1.2\n    \ntime [ms]\n      3857\n      7718\n      1538\n    \nmemory [MB]\n      1800\n      5300\n      21\n    \nDeviation from the correct result exceeded 1%; the times also went up, although they are still many\ntimes shorter compared to other variants. It’s worth noting that the amount of memory used has practically not changed.\nNow let’s see what happens when we change the second parameter, which determines the number of\nunique elements in the input set:\nn=10 000 000/f=0.5\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      5 000 000\n      5 000 000\n      5 067 045.2\n    \nerror [%]\n      0\n      0\n      1.34\n    \ntime [ms]\n      467\n      914\n      183\n    \nmemory [MB]\n      420\n      753\n      21\n    \nn=10 000 000/f=0.75\nMetric\\Variant\n      HashSet\n      distinct\n      HLL\n    \ncardinality\n      7 500 000\n      7 500 000\n      7 619 136.7\n    \nerror [%]\n      0\n      0\n      1.59\n    \ntime [ms]\n      589\n      1187\n      191\n    \nmemory [MB]\n      616\n      843\n      26\n    \nAgain, the results clearly show the advantages of the HLL algorithm. With a relatively low error we\nsignificantly reduced the amount of memory used and the time required for calculations.\nAs you can see and as expected, the classical approach gives accurate results but it consumes a lot of\nmemory, while the solution using HLL brings results characterized by approx. 1% error, but in return\nwe use much less memory. A certain surprise for me is the poor result of the Kotlin distinct function; I\nexpected results more similar to the variant based on the HashSet. Presumably the key difference is that it returns an instance\nof the List class rather than HashSet. This requires further investigation, which is beyond the scope of my considerations.\nThe HLL algorithm is implemented in several solutions, including the aforementioned Elasticsearch,\nas well as in e.g. Redis and Presto. The above experiments clearly show that the approximate method, in case\nwe need to process huge amounts of data, is a good idea provided that we allow a result with a small\nerror.\nMemory-efficient presence test\nIt turns out that the HLL is not the only probabilistic algorithm available in popular databases —\nanother example of this approach is the Bloom Filter. This is an implementation of a memory-saving structure that is\nused in the so-called presence test. Let’s go back to our example with my cash: [10, 20, 50, 20, 50, 100, 50, 20, 10, 10].\nImagine that we want to test whether there is a 100 value banknote in my wallet. In this case the answer is positive, but the test\nfor the 200 value banknote should be false, since there is no such a banknote in the wallet.\nOf course, we are able again to implement a solution to this problem by simply using the properties\nof the HashSet class and the contains method. However, similarly as in case of determining the\ncardinality — the memory requirement increases with the size of the dataset.\nAgain, the solution for this problem may be an approximate method.\nSimilarly as in case of the HLL algorithm the Bloom Filter allows for some inaccuracy, and in this case\nthis means false positive results. This is because it can happen that the Bloom Filter finds that an element\nbelongs to a given set, while in fact it is not there. However, the opposite situation is not possible\nso if the Bloom Filter states that an element is not part of the set, it is certainly true. Referring this to\nour example with the content of my wallet, the Bloom Filter could therefore assure me that there was a\n200 value banknote in it, while standing at the checkout in a store it would turn out that,\nunfortunately, it is not there. What a pity…\nBefore we move on to examine how this algorithm works, let’s consider where it could be useful. A\ntypical example is a recommendation system. Imagine we are designing a system intended to suggest\narticles for users to read, a feature common on social media sites. Such a system needs to store a\nlist of articles read by each user so that it does not suggest them again. It is easy to imagine that\nstoring these articles with each user in the classic way would quickly exhaust memory resources. If we\ndon’t use any data removal mechanism, the database will grow indefinitely. The discussed Bloom\nFilter fits perfectly here as it will allow us to save a lot of memory, although, one must consider\nconsequences of its limitations related to possible false results. It may happen that we will get false\ninformation that a certain article has already been read by someone, while in fact this is not true.\nConsequently, we will not offer that user to read the material. On the other hand, the opposite\nsituation is not possible: we will never display to a user a recommendation of an article that he/she\nhas already read.\nAt this point it is worth checking how much we gain by accepting the inconvenience described\nabove. I have prepared two implementations of a program that adds to a set of values and then\nchecks if they are there.\nThe first program uses the classic approach — the HashSet class, while the second uses the Bloom\nFilter available in the popular guava library.\nAgain, using jConsole we register for both programs the amount of memory used, and additionally — for the version with\nthe Bloom Filter we also check the\nnumber of false positives. This value can be easily controlled, as the maximum allowed false positive\nrate can be set in the API; for needs of the following tests we will set it to 1%.\nMoreover, we will measure the total time of adding values to the set and the total time of querying\nwhether there are values in the set.\nSame as before we will perform a number of tests using the following parameters: n — the size of the set of\nnumbers, and f — what part of it should be added to the set. The configuration n=1 000 000 and f=0.1 means\nthat the first 100 000 numbers out of 1 000 000 will be added to the set. So, in the first part, the program will\nadd 100 000 numbers to the set and then — in the second stage — it will perform a presence test\nby checking whether the numbers above 100 000 belong to the set. There is no point in checking the\nnumbers added to the set beforehand, because we know that Bloom Filters do not give false\nnegative results. On the other hand, if any number above 100 000 is found according to the Bloom Filter in\nthe set, we will consider it a false positive.\nFollowing code snippet presents fragment of the Bloom Filter variant:\n\nval insertions = (n * f).toInt()\nval filter = BloomFilter.create(Funnels.integerFunnel(), insertions, 0.01)\nvar falsePositives = 0\n\nval insertTime = measureTimeMillis {\n    for (i in 0 until insertions) {\n        filter.put(i)\n    }\n}\n\nval queryTime = measureTimeMillis {\n    for (i in insertions until n) {\n        if (filter.mightContain(i)) {\n            falsePositives++;\n        }\n    }\n}\n\nval fpRatio = falsePositives/n.toDouble()\n\n\nAgain — you can find full code of both programs on aforementioned repository.\nLet’s start with the following configuration: n=10 000 000/f=0.1:\nn=10 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      81\n      293\n    \nquery time [ms]\n      82\n      846\n    \nmemory [MB]\n      94\n      30\n    \nAs you can see, the Bloom Filter returned less than 1% false results, but — at the same time — it used three times\nless memory than HashSet variant. Unfortunately, the times of Bloom Filter’s version are significantly higher.\nLet’s check what happens when we increase the size of the input set:\nn=100 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      593\n      318\n    \nquery time [ms]\n      988\n      944\n    \nmemory [MB]\n      876\n      29\n    \nn=500 000 000/f=0.1\nMetric\\Variant\n      HashSet\n      Bloom filter\n    \nerror[%]\n      0\n      0.9\n    \ninsert time [ms]\n      1975\n      1372\n    \nquery time [ms]\n      4115\n      4923\n    \nmemory [MB]\n      4400\n      81\n    \nThe number of false positives is still below the preset 1%, the amount of memory used is still lower\nthan the classic implementation, and interestingly also the times of the probabilistic variant are\nlower, at least for inserting. Thus, it can be seen that along with the increase in the size of the data the benefit of this\nmethod increases.\nSummary\nThe above results clearly show that by accepting a small share of false answers, we can gain significant savings in memory usage.\nSimilarly to the HLL algorithm, the structure based on the Bloom Filters is available in many popular\ndatabases like Redis,\nHBase\nor Cassandra.\nThe simple experiments we conducted showed that probabilistic algorithms can save a lot\nof memory, which is especially important if our database stores huge amounts of data. In such cases it\nis sometimes worth letting your database lie to you a little.","guid":"https://blog.allegro.tech/2022/10/probabilistic-algorithms.html","categories":["tech","performance","NoSQL"],"isoDate":"2022-10-03T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Example of modularization in Allegro Pay Android application","link":"https://blog.allegro.tech/2022/09/example-of-modularization-in-allegro-pay-android-application.html","pubDate":"Mon, 26 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Michał Kwiatek"],"photo":["https://blog.allegro.tech/img/authors/michal.kwiatek.jpg"],"url":["https://blog.allegro.tech/authors/michal.kwiatek"]}]},"content":"<p>Currently, in the Android world, the topic of modularization is very popular. Many bloggers describe their experiences\nwith it and analyze what <a href=\"https://developer.android.com/topic/modularization/patterns\">Google recommends</a>. Our team\nstarted the modularization process before it was hot. I will describe our reasons, decisions, problems and give you some\nadvice. We will see if modularization makes sense and what it brings to the table. I will also post some statistics\nshowing what it looked like before and after the modularization process.</p>\n\n<h2 id=\"some-theory\">Some theory</h2>\n\n<h3 id=\"module\">Module</h3>\n\n<blockquote>\n  <p>A <a href=\"https://developer.android.com/studio/projects#:~:text=inside%20your%20project.-,Modules,test%2C%20and%20debug%20each%20module\">module</a>\nis a collection of source files and build settings that allow you to divide your project into discrete units of\nfunctionality. Your project can have one or many modules, and one module may use another module as a dependency. You can\nindependently build, test, and debug each module.</p>\n</blockquote>\n\n<h3 id=\"background\">Background</h3>\n\n<p>Allegro Pay is a payment method on Allegro that allows you to postpone the payment by 30 days or divide it into smaller\nparts. People who use Allegro Pay know how many functionalities it has, those who don’t use it yet will know after\nreading this article. It started from 3 modules. At the time of writing this article the Allegro application for the\nAndroid platform consists of over 120 modules, 9 of which are maintained by Allegro Pay Team. In this quarter, we\nfocused on extracting several domains (features) from the main Allegro Pay module into separate, smaller and specialized\nmodules.</p>\n\n<h2 id=\"what-made-us-start-the-modularization-process\">What made us start the modularization process?</h2>\n\n<p>The main reason for the modularization process was the build time of one of these 3 modules — containing the entire\nAllegro Pay domain. Our internal monitoring tools showed that build times started to average 100 seconds, and at their\nworst point grew to just over 120 seconds. The module contains over 40k LoC (lines of code). In addition, we faced\nproblems when introducing changes, such as conflicts or the possibility of accidental modification of another\nfunctionality.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/before_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h3 id=\"cheat\">Cheat</h3>\n\n<p>I mention the build time for a reason. In our case, in a multi-module project, we use some Gradle instructions. Our\n<code class=\"language-plaintext highlighter-rouge\">gradle.properties</code> file looks something like this:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// some instructions</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">parallel</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">configureondemand</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"n\">org</span><span class=\"o\">.</span><span class=\"na\">gradle</span><span class=\"o\">.</span><span class=\"na\">caching</span> <span class=\"o\">=</span> <span class=\"kc\">true</span>\n<span class=\"c1\">// more instructions</span>\n</code></pre></div></div>\n\n<p>The first instruction\nenables <a href=\"https://docs.gradle.org/current/userguide/performance.html#parallel_execution\">parallelization</a>\nso that Gradle can perform more than one task at a time as long as the tasks are in different modules. The second one\nallows you to\n<a href=\"https://docs.gradle.org/current/userguide/multi_project_configuration_and_execution.html#sec:configuration_on_demand\">configure modules</a>\nthat are relevant only to the task you want, rather than configuring them all, which is the default behavior.\nImportantly, this instruction should be used for multi-module projects. And the last one is\n<a href=\"https://docs.gradle.org/current/userguide/build_cache.html\">caching</a>. It is „a cache mechanism that aims to save time\nby reusing outputs produced by other builds. The build cache works by storing (locally or remotely) build outputs and\nallowing builds to fetch these outputs from the cache when it is determined that inputs have not changed, avoiding the\nexpensive work of regenerating them.” By default, the build cache is disabled.</p>\n\n<h2 id=\"refinement-decisions-and-plans\">Refinement, decisions and plans</h2>\n\n<p>At one of the weekly meetings, we discussed how to solve the problem of the growing module and the increasing number of\ndependencies and functionalities. We decided that the best way would be to extract several domains (features) into\nseparate modules. Every new module should contain the implemented part of the domain that it represents according to the\nname and a small contract module that can be attached to other modules in order to provide them with the implemented\nfunctionality. So, we have planned the following modules:</p>\n\n<ol>\n  <li>ais (a banking service that isn’t relevant in the context of this article) with contract module,</li>\n  <li>common,</li>\n  <li>consolidation with contract module,</li>\n  <li>onboarding with contract module,</li>\n  <li>overpayment with contract module,</li>\n  <li>repayment with contract module.</li>\n</ol>\n\n<h2 id=\"contract\">Contract</h2>\n\n<p>The contract is a special module containing all the necessary interfaces, classes and methods that allow you to use the\nfunctionality in other places in an easy way. It is defined inside the module containing the functionality\nimplementation. It should be emphasized here that the implementation module can only be based on a contract. This\nsolution means that every developer working on the project knows where to find the necessary information and interfaces\nto run any feature.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">interface</span> <span class=\"nc\">AllegroPaySomeProcessHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">createSomeIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nc\">Context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">:</span> <span class=\"nc\">OtherData</span><span class=\"p\">):</span> <span class=\"nc\">Intent</span>\n\n    <span class=\"k\">fun</span> <span class=\"nf\">observeSomeResult</span><span class=\"p\">():</span> <span class=\"nc\">Observable</span><span class=\"p\">&lt;</span><span class=\"nc\">SomeResultEvent</span><span class=\"p\">&gt;</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">internal</span> <span class=\"kd\">class</span> <span class=\"nc\">AllegroPaySomeProcessHandlerImpl</span> <span class=\"p\">:</span> <span class=\"nc\">AllegroPaySomeProcessHandler</span> <span class=\"p\">{</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">createSomeIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">:</span> <span class=\"nc\">Context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">:</span> <span class=\"nc\">OtherData</span><span class=\"p\">):</span> <span class=\"nc\">Intent</span> <span class=\"p\">=</span>\n        <span class=\"nc\">SomeActivity</span><span class=\"p\">.</span><span class=\"nf\">getIntent</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">,</span> <span class=\"n\">someId</span><span class=\"p\">,</span> <span class=\"n\">otherData</span><span class=\"p\">)</span>\n\n    <span class=\"k\">override</span> <span class=\"k\">fun</span> <span class=\"nf\">observeSomeResult</span><span class=\"p\">():</span> <span class=\"nc\">Observable</span><span class=\"p\">&lt;</span><span class=\"nc\">SomeResultEvent</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n        <span class=\"nc\">DataBus</span><span class=\"p\">.</span><span class=\"nf\">listen</span><span class=\"p\">(</span><span class=\"nc\">SomeResultEvent</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">.</span><span class=\"n\">java</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>The above example shows what one of the assumptions of object-oriented programming — encapsulation — looks like in\npractice. The <em>AllegroPaySomeProcessHandler</em> interface provides two methods, one of them creates\nthe <a href=\"https://developer.android.com/reference/android/content/Intent\">Intent</a> necessary to run the process, and the other\nobserves its result. The exact implementation is hidden in an internal class, not accessible from the contract module.\nEvery change of interface implementation is transparent to contract clients. Example of how to declare a dependency on a\ncontract:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nf\">dependencies</span> <span class=\"p\">{</span>\n    <span class=\"n\">implementation</span> <span class=\"nf\">project</span> <span class=\"p\">(</span><span class=\"err\">'</span><span class=\"p\">:</span><span class=\"n\">allegropay-some</span><span class=\"p\">:</span><span class=\"n\">contract</span><span class=\"err\">'</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"tool\">Tool</h2>\n\n<p>The Allegro application consists of many modules and it is important to provide programmers with the right tools to work\neffectively. In the organization, the delivery of this type of tools is handled by the core team. A tool that allows us\nto check whether our module meets the requirement set for it is\nthe <a href=\"https://github.com/jraska/modules-graph-assert\">Module Graph Assert</a>. It is a Gradle plugin which „helps keep your\nmodule graph healthy and lean.” This tool defines the types of modules that are allowed in the application, the\ndependencies between them and the height of the dependency tree. The following types are defined in the Allegro\napplication: <em>App</em>, <em>Feature</em>, <em>Contract</em>, <em>Library</em>, <em>Util</em> and <em>NeedsMigration</em>. The last type tells us that the\nmodule still requires work from its owners and appropriate adaptation to one of the other types. We can also define\nallowed and restricted dependencies between modules, e.g. a contract may depend only on another contract or a module\nmarked as a feature depends only on the contract or library. Allegro app configuration:</p>\n\n<div class=\"language-groovy highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">moduleGraphAssert</span> <span class=\"o\">{</span>\n    <span class=\"n\">maxHeight</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n    <span class=\"n\">allowed</span> <span class=\"o\">=</span> <span class=\"o\">[</span>\n        <span class=\"s1\">'App -&gt; Feature'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Library'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Util'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'App -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Library'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Util'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Feature -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Contract -&gt; Contract'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'NeedsMigration -&gt; .*'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'.* -&gt; NeedsMigration'</span><span class=\"o\">,</span>\n    <span class=\"o\">]</span>\n    <span class=\"n\">restricted</span> <span class=\"o\">=</span> <span class=\"o\">[</span>\n        <span class=\"s1\">'Contract -X&gt; NeedsMigration'</span><span class=\"o\">,</span>\n        <span class=\"s1\">'Library -X&gt; .*'</span>\n    <span class=\"o\">]</span>\n<span class=\"o\">}</span>\n</code></pre></div></div>\n\n<h2 id=\"initial-modules\">Initial modules</h2>\n\n<p>The first separated feature module was overpayment. We immediately prepared a common module containing functionalities\nused in more than one Allegro Pay module. The contract that is shown earlier contains one method returning an Intent\nneeded to run the overpayment process. The feature module includes user-visible screens, use cases and network\ncommunication. Several thousand lines of code were added to this module and the time needed to build the main Allegro\nPay module was shortened. At that time, the build time of the main module was around 87.5 seconds, common and\noverpayment modules around 10.5 seconds.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/first_modules_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"following-modules\">Following modules</h2>\n\n<p>In the next stages, we separated the ais, consolidation and repayment modules. The current values of the build times of\nindividual modules are around 33.7 seconds for the Allegro Pay main module, 13.4 seconds for the ais, 12 seconds for the\nconsolidation, 10.6 seconds for the repayment. The extraction process was analogous to that of the first module.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/few_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"onboarding-module\">Onboarding module</h2>\n\n<p>This module was the most challenging and possibly the most time consuming. This was due to the combination of the\nprocess being available from multiple screens in different modules and ensuring unchanged functionality. During this\nmodularization process, we discovered the possibility of optimizing and reducing the amount of code. This module\ncontains approximately 10k LoC and the build time is less than 20 seconds. It is a really huge module.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/onboarding_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"other-two-modules\">Other two modules</h2>\n\n<p>If you remember, I mentioned three modules at the beginning of this text. So far, I have described the division of the\nlargest module. Let me now describe others in more detail. The first is the special analytical module. Includes an\nexternal library and a small contract. It was created at the same time as the main Allegro Pay module. The current value\nof the build time is 3 seconds and the module has more than 150 lines of code.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/sms_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<p>The second is the SMS verification module. It contains a functionality that allows users to authorize operations by\nproviding SMS code. Currently, it is used in the processes of buying, consolidation, onboarding and overpayment. We only\nwrote a contract here, which provides a universal and easy interface. The build time is approximately 9 seconds and the\nmodule contains almost 2k lines of code.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/sa_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n\n<h2 id=\"fin\">Fin</h2>\n\n<p>Probably for some of you, the division method used may be associated with the Latin term <em>divide et impera</em>. This\nparadigm of algorithm design could also be used in the modularization process by dividing one large module into several\nsmaller ones, each specialized in one task. The use of the concept of this paradigm, encapsulation by creating a\ncontract and Gradle configuration allowed to significantly reduce the build time and speed up the development of the\napplication. This solution introduces consistency in the module and decreases the possibility of introducing a\nregression by encapsulating each individual domain. Also the problem with the redundant conflicts has been minimalized.\nAfter the implementation of the modules described above, the main module containing the Allegro Pay responsibilities has\nshrunk significantly, and now contains around 18.4k LoC (which means it was reduced by half). In addition,\nmodularization will allow us to add new features and extend the existing ones in an easier and safer way. It was an\ninteresting challenge from a technical point of view.\n<img src=\"/img/articles/2022-09-26-example-of-modularization-in-allegro-pay-android-application/after_both.png\" alt=\"Build time in seconds and LoC.\" /></p>\n","contentSnippet":"Currently, in the Android world, the topic of modularization is very popular. Many bloggers describe their experiences\nwith it and analyze what Google recommends. Our team\nstarted the modularization process before it was hot. I will describe our reasons, decisions, problems and give you some\nadvice. We will see if modularization makes sense and what it brings to the table. I will also post some statistics\nshowing what it looked like before and after the modularization process.\nSome theory\nModule\nA module\nis a collection of source files and build settings that allow you to divide your project into discrete units of\nfunctionality. Your project can have one or many modules, and one module may use another module as a dependency. You can\nindependently build, test, and debug each module.\nBackground\nAllegro Pay is a payment method on Allegro that allows you to postpone the payment by 30 days or divide it into smaller\nparts. People who use Allegro Pay know how many functionalities it has, those who don’t use it yet will know after\nreading this article. It started from 3 modules. At the time of writing this article the Allegro application for the\nAndroid platform consists of over 120 modules, 9 of which are maintained by Allegro Pay Team. In this quarter, we\nfocused on extracting several domains (features) from the main Allegro Pay module into separate, smaller and specialized\nmodules.\nWhat made us start the modularization process?\nThe main reason for the modularization process was the build time of one of these 3 modules — containing the entire\nAllegro Pay domain. Our internal monitoring tools showed that build times started to average 100 seconds, and at their\nworst point grew to just over 120 seconds. The module contains over 40k LoC (lines of code). In addition, we faced\nproblems when introducing changes, such as conflicts or the possibility of accidental modification of another\nfunctionality.\n\nCheat\nI mention the build time for a reason. In our case, in a multi-module project, we use some Gradle instructions. Our\ngradle.properties file looks something like this:\n\n// some instructions\norg.gradle.parallel = true\norg.gradle.configureondemand = true\norg.gradle.caching = true\n// more instructions\n\n\nThe first instruction\nenables parallelization\nso that Gradle can perform more than one task at a time as long as the tasks are in different modules. The second one\nallows you to\nconfigure modules\nthat are relevant only to the task you want, rather than configuring them all, which is the default behavior.\nImportantly, this instruction should be used for multi-module projects. And the last one is\ncaching. It is „a cache mechanism that aims to save time\nby reusing outputs produced by other builds. The build cache works by storing (locally or remotely) build outputs and\nallowing builds to fetch these outputs from the cache when it is determined that inputs have not changed, avoiding the\nexpensive work of regenerating them.” By default, the build cache is disabled.\nRefinement, decisions and plans\nAt one of the weekly meetings, we discussed how to solve the problem of the growing module and the increasing number of\ndependencies and functionalities. We decided that the best way would be to extract several domains (features) into\nseparate modules. Every new module should contain the implemented part of the domain that it represents according to the\nname and a small contract module that can be attached to other modules in order to provide them with the implemented\nfunctionality. So, we have planned the following modules:\nais (a banking service that isn’t relevant in the context of this article) with contract module,\ncommon,\nconsolidation with contract module,\nonboarding with contract module,\noverpayment with contract module,\nrepayment with contract module.\nContract\nThe contract is a special module containing all the necessary interfaces, classes and methods that allow you to use the\nfunctionality in other places in an easy way. It is defined inside the module containing the functionality\nimplementation. It should be emphasized here that the implementation module can only be based on a contract. This\nsolution means that every developer working on the project knows where to find the necessary information and interfaces\nto run any feature.\n\ninterface AllegroPaySomeProcessHandler {\n\n    fun createSomeIntent(context: Context, someId: String, otherData: OtherData): Intent\n\n    fun observeSomeResult(): Observable<SomeResultEvent>\n}\n\n\n\ninternal class AllegroPaySomeProcessHandlerImpl : AllegroPaySomeProcessHandler {\n\n    override fun createSomeIntent(context: Context, someId: String, otherData: OtherData): Intent =\n        SomeActivity.getIntent(context, someId, otherData)\n\n    override fun observeSomeResult(): Observable<SomeResultEvent> =\n        DataBus.listen(SomeResultEvent::class.java)\n}\n\n\nThe above example shows what one of the assumptions of object-oriented programming — encapsulation — looks like in\npractice. The AllegroPaySomeProcessHandler interface provides two methods, one of them creates\nthe Intent necessary to run the process, and the other\nobserves its result. The exact implementation is hidden in an internal class, not accessible from the contract module.\nEvery change of interface implementation is transparent to contract clients. Example of how to declare a dependency on a\ncontract:\n\ndependencies {\n    implementation project (':allegropay-some:contract')\n}\n\n\nTool\nThe Allegro application consists of many modules and it is important to provide programmers with the right tools to work\neffectively. In the organization, the delivery of this type of tools is handled by the core team. A tool that allows us\nto check whether our module meets the requirement set for it is\nthe Module Graph Assert. It is a Gradle plugin which „helps keep your\nmodule graph healthy and lean.” This tool defines the types of modules that are allowed in the application, the\ndependencies between them and the height of the dependency tree. The following types are defined in the Allegro\napplication: App, Feature, Contract, Library, Util and NeedsMigration. The last type tells us that the\nmodule still requires work from its owners and appropriate adaptation to one of the other types. We can also define\nallowed and restricted dependencies between modules, e.g. a contract may depend only on another contract or a module\nmarked as a feature depends only on the contract or library. Allegro app configuration:\n\nmoduleGraphAssert {\n    maxHeight = 5\n    allowed = [\n        'App -> Feature',\n        'App -> Library',\n        'App -> Util',\n        'App -> Contract',\n        'Feature -> Library',\n        'Feature -> Util',\n        'Feature -> Contract',\n        'Contract -> Contract',\n        'NeedsMigration -> .*',\n        '.* -> NeedsMigration',\n    ]\n    restricted = [\n        'Contract -X> NeedsMigration',\n        'Library -X> .*'\n    ]\n}\n\n\nInitial modules\nThe first separated feature module was overpayment. We immediately prepared a common module containing functionalities\nused in more than one Allegro Pay module. The contract that is shown earlier contains one method returning an Intent\nneeded to run the overpayment process. The feature module includes user-visible screens, use cases and network\ncommunication. Several thousand lines of code were added to this module and the time needed to build the main Allegro\nPay module was shortened. At that time, the build time of the main module was around 87.5 seconds, common and\noverpayment modules around 10.5 seconds.\n\nFollowing modules\nIn the next stages, we separated the ais, consolidation and repayment modules. The current values of the build times of\nindividual modules are around 33.7 seconds for the Allegro Pay main module, 13.4 seconds for the ais, 12 seconds for the\nconsolidation, 10.6 seconds for the repayment. The extraction process was analogous to that of the first module.\n\nOnboarding module\nThis module was the most challenging and possibly the most time consuming. This was due to the combination of the\nprocess being available from multiple screens in different modules and ensuring unchanged functionality. During this\nmodularization process, we discovered the possibility of optimizing and reducing the amount of code. This module\ncontains approximately 10k LoC and the build time is less than 20 seconds. It is a really huge module.\n\nOther two modules\nIf you remember, I mentioned three modules at the beginning of this text. So far, I have described the division of the\nlargest module. Let me now describe others in more detail. The first is the special analytical module. Includes an\nexternal library and a small contract. It was created at the same time as the main Allegro Pay module. The current value\nof the build time is 3 seconds and the module has more than 150 lines of code.\n\nThe second is the SMS verification module. It contains a functionality that allows users to authorize operations by\nproviding SMS code. Currently, it is used in the processes of buying, consolidation, onboarding and overpayment. We only\nwrote a contract here, which provides a universal and easy interface. The build time is approximately 9 seconds and the\nmodule contains almost 2k lines of code.\n\nFin\nProbably for some of you, the division method used may be associated with the Latin term divide et impera. This\nparadigm of algorithm design could also be used in the modularization process by dividing one large module into several\nsmaller ones, each specialized in one task. The use of the concept of this paradigm, encapsulation by creating a\ncontract and Gradle configuration allowed to significantly reduce the build time and speed up the development of the\napplication. This solution introduces consistency in the module and decreases the possibility of introducing a\nregression by encapsulating each individual domain. Also the problem with the redundant conflicts has been minimalized.\nAfter the implementation of the modules described above, the main module containing the Allegro Pay responsibilities has\nshrunk significantly, and now contains around 18.4k LoC (which means it was reduced by half). In addition,\nmodularization will allow us to add new features and extend the existing ones in an easier and safer way. It was an\ninteresting challenge from a technical point of view.","guid":"https://blog.allegro.tech/2022/09/example-of-modularization-in-allegro-pay-android-application.html","categories":["tech","kotlin","mobile","android","modularization","gradle","allegro-pay"],"isoDate":"2022-09-25T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"How to efficiently write millions of records in the cloud and not go bankrupt — an Azure CosmosDB case study","link":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","pubDate":"Tue, 13 Sep 2022 00:00:00 +0200","authors":{"author":[{"name":["Kamil Starczak"],"photo":["https://blog.allegro.tech/img/authors/kamil.starczak.jpg"],"url":["https://blog.allegro.tech/authors/kamil.starczak"]}]},"content":"<p>Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with <a href=\"https://azure.microsoft.com/services/cosmos-db/\">Azure\nCosmos DB</a> and batch processing.</p>\n\n<h2 id=\"our-story\">Our story</h2>\n\n<p>At Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.</p>\n\n<p>In this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:</p>\n\n<ul>\n  <li>The overall time of such a batch operation cannot exceed 5 minutes per 1 million records.</li>\n  <li>The processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.</li>\n  <li>The solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.</li>\n  <li>The solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.</li>\n</ul>\n\n<p>The outcomes of this case study are published as an open source repository (see <a href=\"#our-library\">Our library</a>).</p>\n\n<h2 id=\"cosmos-db--the-basics\">Cosmos DB — the basics</h2>\n\n<p>Before going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the <a href=\"#database-utilization\">Database utilization</a> chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (<a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/#features\">source</a>). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.</p>\n\n<p>How does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>foreach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n</code></pre></div></div>\n\n<p>For each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.</p>\n\n<p>What’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.</p>\n\n<h2 id=\"cosmos-db--scaling-and-provisioning\">Cosmos DB — scaling and provisioning</h2>\n\n<p>Cosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img01.png\" alt=\"Cosmos DB Request Units overview\" /></p>\n\n<p>Source: <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\">https://docs.microsoft.com/en-us/azure/cosmos-db/request-units</a></p>\n\n<p>But what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.</p>\n\n<p>At this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.</p>\n\n<h3 id=\"manual\">Manual</h3>\n\n<p>In the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.</p>\n\n<p>What happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img02.png\" alt=\"Manual mode visualized\" /></p>\n\n<h3 id=\"autoscale\">Autoscale</h3>\n\n<p>The second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img03.png\" alt=\"Autoscale mode visualized\" /></p>\n\n<p>What’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.</p>\n\n<p>Nevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.</p>\n\n<h3 id=\"serverless\">Serverless</h3>\n\n<p>The last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img04.png\" alt=\"Serverless mode visualized\" /></p>\n\n<p>Unfortunately, if we read the docs, we can find some additional information:</p>\n\n<ul>\n  <li>The maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.</li>\n  <li>The guarantees for the operation latencies are worse — 30ms instead of 10ms.</li>\n  <li>Serverless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.</li>\n  <li>Moreover, the maximum throughput during a single second is 5000 RUs.</li>\n</ul>\n\n<p>As we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.</p>\n\n<p>To sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.</p>\n\n<h2 id=\"database-utilization\">Database utilization</h2>\n\n<p>Let’s go back to the sample code I was running.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>Foreach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n</code></pre></div></div>\n\n<p>It processed 50k records in about 10 minutes. How loaded was the database?</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img05.png\" alt=\"Normalized RU Consumption metric\" /></p>\n\n<p>Normalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.</p>\n\n<p>If we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.</p>\n\n<p>This time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.</p>\n\n<h3 id=\"ru-limiter\">RU limiter</h3>\n\n<p>Is there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img06.png\" alt=\"RU limiter visualized\" /></p>\n\n<p>The mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img07.png\" alt=\"Total Request Units metric\" /></p>\n\n<h3 id=\"partition-key-ranges\">Partition key ranges</h3>\n\n<p>It almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img08.png\" alt=\"Normalized RU Consumption metric\" /></p>\n\n<p>Something is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled <code class=\"language-plaintext highlighter-rouge\">PartitionKeyRangeId</code>. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img09.png\" alt=\"Normalized RU Consumption metric split by Partition Key ranges\" /></p>\n\n<p>If we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.</p>\n\n<h3 id=\"a-bit-of-reverse-engineering\">A bit of reverse engineering</h3>\n\n<p>Is there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.</p>\n\n<h2 id=\"the-autoscaler-auto-scaler\">The “Autoscaler auto scaler”</h2>\n\n<p>The problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.</p>\n\n<p>The one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.</p>\n\n<p>But what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).</p>\n\n<p><img src=\"/img/articles/2022-09-13-azure-cosmosdb-case-study/img10.png\" alt=\"Autoscaler visualized during batch processing\" /></p>\n\n<p>With this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.</p>\n\n<p>This way, we have fulfilled all the requirements that were set at the beginning:</p>\n\n<ul>\n  <li>Batch processing time — 5 minutes per 1 million records.</li>\n  <li>No risk of starving other processes, thanks to the RU limiter.</li>\n  <li>Cost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.</li>\n  <li>Scalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.</li>\n</ul>\n\n<h2 id=\"our-library\">Our library</h2>\n\n<p>The outcomes of this work have been published as an opensource .NET library on our GitHub page:\n<a href=\"https://github.com/allegro/cosmosdb-utils/tree/main/src/Allegro.CosmosDb.BatchUtilities\">Allegro.CosmosDb.BatchUtilities</a>.\nFeel free to use it or even contribute new features.</p>\n\n<h2 id=\"conclusions\">Conclusions</h2>\n\n<p>And here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:</p>\n\n<ul>\n  <li>Cosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.</li>\n  <li>Cosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.</li>\n  <li>You must pay close attention to the costs, as it is very easy to get high bills by misusing the service.</li>\n  <li>It’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.</li>\n</ul>\n","contentSnippet":"Cloud providers like to brag about high availability and unlimited scaling of their services – and they are correct,\nas these features are indeed significant advantages of cloud solutions. Their computational power is so high that for\nmost use cases, it’s almost unlimited. In this blog post, I would like to tell you about our experiences with Azure\nCosmos DB and batch processing.\nOur story\nAt Allegro Pay we are taking advantage of Azure’s no-SQL database, Cosmos DB. It does a great job when it comes to\nhandling operations on individual records — let’s say, fetching specific user’s data or modifying it. But what if we\nwanted to change the status of 10 million users based on some external analytic query? What’s more, we want it neither\nto last a couple of hours nor to cost us a little fortune. Actually, we may even want to run such operations on a daily\nbasis.\nIn this blog post, I want to focus on the technical aspect of this challenge rather than diving deep into the business\nscenario. So let’s specify our technical requirements explicitly:\nThe overall time of such a batch operation cannot exceed 5 minutes per 1 million records.\nThe processing cannot starve other operations that are being run on the database at the same time. The batches will\nbe executed from time to time, but the database still needs to be able to handle regular traffic that is generated by\nusers’ requests.\nThe solution must be cost-effective. The problem with the cloud is not making a solution that is scalable and fast,\nit’s making it so at a reasonable price. All these features that cloud providers brag about do come at a cost.\nThe solution must be scalable to handle the increasing size of the database. Today we are talking about writing 10\nmillion records, but if in one year we will be writing 100 million, all these requirements should still be met — of\ncourse, not at an exponentially higher price.\nThe outcomes of this case study are published as an open source repository (see Our library).\nCosmos DB — the basics\nBefore going into detail, let’s look at the basic concepts of Cosmos DB. If you are familiar with this service and its\nprovisioning modes, you may want to jump directly to the Database utilization chapter. As\nalready mentioned, Cosmos DB is a no-SQL database available in the Azure cloud. Some of its core features are unlimited\nautomatic scaling and guaranteed read and write latencies at any\nscale (source). If we compare them with the previously\nset requirements, it seems like Cosmos DB is a perfect choice. It scales automatically, so the database should scale\nitself up during batch processing. Besides, the “guaranteed latencies” feature may suggest that the response times\nshould not increase under heavy load, and the processing should be fast.\nHow does it look in reality? Let’s take a look at a quick experiment. I created the most naive implementation of a batch\nupdate process. Its pseudocode may look like this:\n\nforeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nFor each batch record, we first fetch it from the database, then execute some logic that modifies it, and at last, save\nit in the database. The Cosmos’ API is quite simple, allowing us to perform simple atomic operations, such as getting a\nrecord by an ID, updating, inserting etc. It also allows querying through multiple APIs, such as SQL, MongoDB,\nCassandra, Gremlin or Azure Table API, which is out of this document’s scope.\nWhat’s the result of executing this code? It processed 50k records in about 10 minutes. This doesn’t seem too long, but if\nwe estimate the time needed to process a million records, that would be more than 3 hours. Or even worse, if we think\nabout processing tens or hundreds of millions, it becomes almost impossible. But that’s not all — looking at the Cosmos\nDB metrics, I noticed that the database utilization was as low as about 6%. To explain what exactly it means, I will\nfirst talk about how Cosmos DB scales and how it calculates the costs.\nCosmos DB — scaling and provisioning\nCosmos DB uses so-called Request Units to calculate resource utilization. They represent a normalised operation cost in\nterms of CPU, memory and IO needed to execute the request. This way, we don’t need to care about physical (or virtual)\nmachines that are being used or their parameters — the database size and the operation costs are always expressed in\nRUs. Microsoft estimates a single read operation of a 1KB item as 1 RU and other operations’ cost correspondingly more.\n\nSource: https://docs.microsoft.com/en-us/azure/cosmos-db/request-units\nBut what does “correspondingly more” mean exactly? Microsoft does not precisely define this as it depends on multiple\nfactors — such as item’s size, index configuration, query complexity, etc. We do not know how many RUs the operation\nwill consume until we actually execute it. Luckily, every response from Cosmos DB contains the operation’s cost inside\nthe headers. What’s more, RU consumption is quite repeatable. For example, if one write operation has previously\ncost 5 RUs, and we execute the same request on a similar item, we can presume that it will also cost 5 RUs. Of course,\nit may change in time — along with the increasing database size, RU consumption may also increase.\nAt this point, the question is: how do all these affect the price of the service, and how many of these RU units are we\nactually able to use? Cosmos DB offers us three so-called provisioning modes, which determine how Azure scales the\ndatabase and bills us for the consumed resources.\nManual\nIn the manual mode (aka “provisioned throughput”), we declare how many RUs we are going to consume per second — the\nhigher we set this limit, the higher the price. The minimal value is 400 RU/s which converts to around 20 euros per\nmonth. This can be increased at any time if needed, but we will pay more. The billing is per hour, so we pay for the\nhighest configured value during a single wall-clock hour.\nWhat happens if we try to exceed this declared value? Some of the requests will be rejected with HTTP status code 429\n(Too Many Requests) — Cosmos DB will throttle the traffic so that the actual sum of the consumed RU in each second does\nnot exceed the configured limit.\n\nAutoscale\nThe second mode is autoscale. As the name suggests, it will automatically scale based on the current load, that is, the\nactually consumed RUs, but not higher than the configured limit. To be precise, autoscale mode can scale the\ndatabase up to 10 times. For example, if we configure the max autoscale limit to 4000 RU/s, then the basic available RU\nlimit will be 400 RU/s, which converts to 20 euros per month. If we try to consume more, it will automatically scale up\nto 4000 RU/s, which converts to 200 euros per month. The bill at the end of the month will range between 20 and 200\neuros, depending on how many times and how much the database needed to scale.\n\nWhat’s the catch? We can easily set the max autoscale throughput to any value we want, but we will not always be able to\nreturn to the previous value. In fact, we can only decrease it to 1/10 of the maximum value we ever set. For example,\nif we set the database to autoscale in the range of 6k-60k RU/s, the lowest we can go back with is 600-6k RU/s.\nNevertheless, this mode sounds quite promising. As the requirements state, we want to put a high load on the database\nfrom time to time without affecting other processes. It seems that autoscale mode can be useful for this use case.\nServerless\nThe last mode is serverless. It’s rather straightforward — at the end of the month, we pay for the exact number of RUs\nthat we have consumed. No need to declare anything, no need to scale. A million RUs cost around 25 euro cents. This may\nsound tempting. We can calculate how much it costs to process a million records, estimate how many we process during\na month, and when we put it together, it may look like the final price is not even very high.\n\nUnfortunately, if we read the docs, we can find some additional information:\nThe maximum storage for the Serverless Cosmos DB is 50 GB. For a big production database of a high-scale service, such\nas Allegro Pay — it is simply not enough.\nThe guarantees for the operation latencies are worse — 30ms instead of 10ms.\nServerless mode is incompatible with High Availability settings and cannot be replicated in another Azure region.\nMoreover, the maximum throughput during a single second is 5000 RUs.\nAs we can see, the more we learn about the Serverless mode, the more evident it seems that it’s not intended for\napplications in production. Even Microsoft suggests that this mode is best suited for the development or test databases\nand new services with low throughput.\nTo sum up, Cosmos offers us three interesting options when it comes to scaling that seem pretty simple to use. But if we\ndig deeper, there are quite a few catches.\nDatabase utilization\nLet’s go back to the sample code I was running.\n\nForeach record\n{\n    Item = CosmosClient.Get(record.ID)\n    ProcessChange(Item)\n    CosmosClient.Update(Item)\n}\n\n\nIt processed 50k records in about 10 minutes. How loaded was the database?\n\nNormalized RU Consumption shows the percentage of the database load, which at this time was scaled up to 4000 RU/s.\nIts utilization was only around 6% during the batch processing. It’s a bit low and it obviously could take more load.\nIf we look back at the code I was running, it’s easy to see that it’s lacking one important thing — parallelism. The\noperations are executed one after another synchronously, which makes it impossible to fully utilize the database.\nSending the requests in parallel is a simple optimization that obviously comes to mind. Let’s see what happens if we run\nthe code with parallelism added.\nThis time, with the database scaled up to 40k RU/s, the processing of 1 million records took 8 minutes. What’s more, the\ndatabase utilization was reaching 100%. This may look very promising, but hang on a minute — running at 100% database\nusage means that we are on the edge of throttling. I checked the logs and it actually happened — some of the requests\nwere being throttled and retried. What if some other operation would try to access the database in the meantime, for\nexample customer’s purchase process? It could easily be throttled and rejected or at least delayed by the retries.\nRU limiter\nIs there anything we can do to make this solution fulfil the previously set requirements? Let’s think about it. We know\nhow many RUs we consume (Cosmos DB is providing this information in the response headers), and we know how high we\nscaled the database… Then why not try and precisely control the flow of outgoing requests, aiming at a specific RU/s\nusage? This is what we have done at Allegro Pay — we have built our own RU limiter, as we called it. In order to do\nthat, we implemented a simple counter that tracks RUs consumed in a given interval. Using this counter, we can limit the\noutgoing requests so that RU limit is not exceeded in any second, but instead wait until the next second before\nreleasing the queued requests.\n\nThe mechanism sounds pretty simple, doesn’t it? And here is how it worked. I ran another test, this time with RU limiter\nset to 32k RU/s. Although the requests were being limited, the processing of 1 million records took only 5 minutes this\ntime and no request was throttled. Below we can see the Total Request Units metric during the test. The consumption was\nalmost precisely 1,92 mln RU / minute, which gives us 32k RU/s — exactly as the RU limiter was configured.\n\nPartition key ranges\nIt almost looks as if we could wrap up and call it a day. But let’s take another look at the Normalized RU\nConsumption metric.\n\nSomething is not right here. With the database scaled up to 40k RU/s and the consumption rate of precisely 32k RU/s\n(confirmed with the Total Request Units metric), the database utilization should be around 80%, not 100%. What exactly\nis happening here? If we dig deeper into the documentation or just look around the metrics, we could discover something\ncalled PartitionKeyRangeId. And what is the partition key range? Every item stored in a Cosmos DB collection has its\nPartitionKey — a key used by Cosmos to partition the data. In our case, that could for example be an Allegro user\nidentifier. The partition key passed to the API is hashed, so that the distribution of partition keys is even. As the\ndatabase grows, Cosmos DB automatically splits it into partitions. It does it using the partition key ranges — items\nfrom each range make up a physical partition. The problem is that these ranges are not always of equal size — they usually\nare, but there are periods when Cosmos has just split some of the partitions, but has not yet split others. Below is\nthe Normalized RU Consumption metric split by partitions.\n\nIf we dig into the documentation even further it turns out that the 40k RU/s that we configured as the provisioned\nthroughput is equally split between the partitions — even if their sizes are not equal. Odds are that even if we consume\nup to 40k RU/s in total, we are still overloading some of the partitions. If at that moment we received a request from\na customer whose ID falls into that partition key range, the request could be throttled.\nA bit of reverse engineering\nIs there anything that could be done to limit RU consumption per partition? Well, technically yes. If we knew the\npartition key hashing mechanism that Cosmos DB is using and knew the exact partition ranges that our database is\ncurrently split into, we could count the RU limits not per the whole database, but per each partition. The good news is\nthat this is indeed possible, as the hashing is done on the client side, inside the CosmosDB SDK, which is open source.\nThe bad news is that probably we don’t want to do that, except maybe out of academic curiosity. In fact, I implemented\nsuch a partition-based RU limiter and it worked like a charm. But would I use that in production? Absolutely not. Copy\npasting and making a dependency on some internal implementation of the database, which may change at any time (well,\nprobably with some backward compatibility, because that would also break the SDK) does not sound like a production-ready\nsolution or something that my colleagues at Allegro Pay would approve in a code review.\nThe “Autoscaler auto scaler”\nThe problem of the uneven partition key ranges persists, but is there any decent solution? Well, probably just one — to\nscale the database so far up, that we always have some RUs buffer. If we use autoscale mode and set the Max\nAutoscale Throughput high enough, we may on one hand not overpay during periods when the partition distribution is\nuneven, and on the other hand, not risk overloading some of the partitions when it happens.\nThe one last catch is that, as already mentioned, Cosmos DB in autoscale mode can only scale up to 10x. If we configure\nthe Max Autoscale Throughput to 60k RU/s, then the lowest it will scale down is 6k RU/s, costing us at least 300 euros a\nmonth, and every processed batch tops the bill up. Is it much for a company such as Allegro? Probably not, but let’s say\nwe do not have a single database like that, but tens, maybe even hundreds? It turns out the game is worth it.\nBut what if we increase the Max Autoscale Throughput value up to 60k RU/s only just before the batch processing has\nstarted? This is exactly what we did. Fortunately, Microsoft has given us the possibility to change the max throughput\nusing not only the Azure Portal, but also through the API. This way we can automatically scale up when the batch is\nstarting, and scale back down when the batch processing has finished. All we need to remember is that after rising the\nMax Autoscale Throughput, we can only go 10x lower. If we scale up to 60k RU/s — we can go back just to 6k RU/s Max\nAutoscale Throughput (meaning Cosmos will be scaled in range of 600-6000k RU/s).\n\nWith this one simple trick, we created an “Autoscaler auto scaler”, as we automatically scale the Cosmos DB’s Autoscaler\nrange and achieve in turn the possibility to scale 100x times instead of just 10x. When the traffic on the database is\nat its minimum, we operate at just 600 RU/s, and during the batch processing, we go up to 60k RU/s, maintaining a buffer\nhigh enough that there is no risk of throttling.\nThis way, we have fulfilled all the requirements that were set at the beginning:\nBatch processing time — 5 minutes per 1 million records.\nNo risk of starving other processes, thanks to the RU limiter.\nCost-effectiveness — thanks to the developed autoscaler we only pay for what we actually need.\nScalability — we can easily scale the solution up by scaling the database and if needed, also the number of batch\nprocessing service replicas. Although this will eventually increase the minimal throughput we can go back to, but with\nthe increasing scale, the minimal traffic on the database will also grow — and we can scale even 100x.\nOur library\nThe outcomes of this work have been published as an opensource .NET library on our GitHub page:\nAllegro.CosmosDb.BatchUtilities.\nFeel free to use it or even contribute new features.\nConclusions\nAnd here we are, at the end of the story. We have reached the intended goal, but there were a few plot twists and\nsurprises on the way. To sum it up, I would like to point out a few aspects of working with Cosmos DB or with almost any\ncloud service in general:\nCosmos DB (and the cloud in general) gives predictable costs as long as we get time to know it and study the\ndocumentation. Sometimes we may even need a PoC or some quick experiment because the documentation does not say\neverything or is not precise.\nCosmos DB gives precise control over the database scaling, but again — we need to get to know how exactly it works\nfirst.\nYou must pay close attention to the costs, as it is very easy to get high bills by misusing the service.\nIt’s worth making data-based decisions — do the PoCs, and experiments and watch the metrics. This is exactly what we\ndid here to get to the final and optimal solution.","guid":"https://blog.allegro.tech/2022/09/azure-cosmosdb-case-study.html","categories":["tech","cloud","azure","cosmosdb"],"isoDate":"2022-09-12T22:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999863248855","name":"Mobile Software Engineer (iOS) - Technology Consumer Experience","uuid":"4d644c7d-0288-4925-8699-6d0bd8102ce8","jobAdId":"7e557424-f661-46de-ab6d-f77408e01b2e","refNumber":"REF3121F","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2022-11-07T10:40:53.447Z","location":{"city":"Poznań, Warszawa, Kraków, Wrocław, Łódź, Gdańsk","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"faf591d7-af03-4d05-b67e-dba247924756","valueLabel":"IT - Other"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999863248855","creator":{"name":"Agnieszka Adamus"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999863248950","name":"Mobile Software Engineer (iOS) - Technology Consumer Experience","uuid":"099d7426-100e-4d45-98dd-c10e16f69c3f","jobAdId":"01a29148-b7ef-444e-bc6e-f938594bcff8","refNumber":"REF3121F","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2022-11-07T10:40:29.707Z","location":{"city":"Poznań, Warszawa, Kraków, Wrocław, Łódź, Gdańsk","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"associate","label":"Associate"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"faf591d7-af03-4d05-b67e-dba247924756","valueLabel":"IT - Other"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999863248950","creator":{"name":"Agnieszka Adamus"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999859214411","name":"Software Engineer (Java/Kotlin) - Data&AI","uuid":"aa1c7c1a-2c9a-4efa-a985-6eaa99aaeef6","jobAdId":"738b7495-98e2-4253-a74c-53b068bc137d","refNumber":"REF3093P","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2022-10-21T12:30:37.355Z","location":{"city":"Poznań, Warszawa, Toruń, Wrocław, Gdańsk, Łódź","region":"","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"business_development","label":"Business Development"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Java, Kotlin, Data, Artificial Intelligence, Inżynier, Engineer, Software Engineer, Developer, Dev, Programmer, Programista"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999859214411","creator":{"name":"Natalia Glińska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999859201866","name":"Senior Software Engineer (Java/Kotlin) - Data&AI","uuid":"bbfbcb17-f442-4398-9bee-33805bea0dca","jobAdId":"e648593b-afec-4ce6-90d6-6b68533d319e","refNumber":"REF3093P","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2022-10-21T11:46:34.029Z","location":{"city":"Poznań, Warszawa, Toruń, Wrocław, Gdańsk, Łódź","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"business_development","label":"Business Development"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Java, Kotlin, Data, Artificial Intelligence, Inżynier, Engineer, Software Engineer, Developer, Dev, Programmer, Programista"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999859201866","creator":{"name":"Natalia Glińska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999859202601","name":"Senior Software Engineer (Big Data Team) - Data & AI","uuid":"e785de34-4916-4fe8-877c-b438d7bb3ab4","jobAdId":"d17f7f91-ac82-4278-a168-828b5396ec38","refNumber":"REF3095Y","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2022-10-21T11:44:30.939Z","location":{"city":"Poznań, Warszawa, Kraków, Toruń, Wrocław, Gdańsk, Łódź","region":"","country":"pl","address":"","postalCode":"","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572770","label":"IT - Software Development"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Kraków","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"Łódź","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Additional Locations","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Yes"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toruń","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gdańsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Recruitment Process","valueId":"ed4682c7-33c9-41c2-8d13-428ed39046f5","valueLabel":"Tech. Engineer - IC"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wrocław","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572770","valueLabel":"IT - Software Development"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Poznań","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"Software Engineer, Inżynier, Developer, Programista, Java, Scala, Kotlin, Big Data, Hadoop, Spark"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999859202601","creator":{"name":"Natalia Glińska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1667908912000,"duration":5400000,"id":"289621472","name":"Allegro Tech Live #31 - Frontend: reporting i optymalizacje","date_in_series_pattern":false,"status":"upcoming","time":1669309200000,"local_date":"2022-11-24","local_time":"18:00","updated":1667910703000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":18,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/289621472/","description":"**➡ Rejestracja:** https://app.evenea.pl/event/allegro-tech-talk-31 **Allegro Tech Live** to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym…","visibility":"public","member_pay_fee":false},{"created":1664275530000,"duration":5400000,"id":"288748190","name":"Allegro Tech Live #30 - Dług technologiczny - jak go spłacić i nie zbankrutować","date_in_series_pattern":false,"status":"past","time":1665676800000,"local_date":"2022-10-13","local_time":"18:00","updated":1665684962000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":58,"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/288748190/","description":"**➡ Rejestracja:** [https://app.evenea.pl/event/allegro-tech-talk-30](https://app.evenea.pl/event/allegro-tech-talk-30) UWAGA: Z różnych powodów (mniej lub bardziej zależnych od nas) czwartkowe spotkanie Allegro Tech Talk przenosimy w pełni do świata online. Chociaż…","visibility":"public","member_pay_fee":false},{"created":1657193453000,"duration":7200000,"id":"287035383","name":"Allegro Tech Labs #10 Online: Poskromić stan w React","date_in_series_pattern":false,"status":"past","time":1658934000000,"local_date":"2022-07-27","local_time":"17:00","updated":1658944632000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":26,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/287035383/","description":"❗NA WYDARZENIE OBOWIĄZUJE REJESTRACJA: Liczba miejsc jest organiczona: [https://app.evenea.pl/event/allegro-tech-labs-10/](https://app.evenea.pl/event/allegro-tech-labs-10/?fbclid=IwAR1Zj3sIcfx3WEWiFfS_hgiW6BJQD6stYouSGuSqfxDq9YVeom8fTFcrE1Q) ❗ **Allegro Tech Labs** to w 100% zdalna odsłona naszych stacjonarnych spotkań warsztatowych. Zazwyczaj spotykaliśmy się…","visibility":"public","member_pay_fee":false},{"created":1655131243000,"duration":5400000,"id":"286545395","name":"Allegro Tech Live #29 - Wyzwania Product Managera","date_in_series_pattern":false,"status":"past","time":1656604800000,"local_date":"2022-06-30","local_time":"18:00","updated":1656612323000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":88,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/286545395/","description":"Allegro Tech Live to w 100% zdalna odsłona naszych stacjonarnych meetupów Allegro Tech Talks. Zazwyczaj spotykaliśmy się w naszych biurach, ale tym razem to my…","how_to_find_us":"https://app.evenea.pl/event/allegro-tech-live-29/","visibility":"public","member_pay_fee":false}],"podcasts":[{"title":"S03E06 - Bartosz Wojtkiewicz - Tłumaczenia na Allegro","link":"https://podcast.allegro.tech/tlumaczenia-na-allegro/","pubDate":"Thu, 03 Nov 2022 00:00:00 GMT","content":"W jaki sposób Allegro stało się platformą wielojęzyczną? Które treści tłumaczymy z pomocą tłumaczy, a które maszynowo? Jak działa system, który automatyzuje zarządzanie komunikacją pomiędzy zespołami developerskimi a profesjonalnymi tłumaczami? Jak dbamy o jakość tłumaczeń i badamy działanie silnika tłumaczeniowego? Do czego wykorzystujemy nasze serwery w nocy oraz co to ma wspólnego z tłumaczeniami? Na te i inne pytania odpowiada Bartosz Wojtkiewicz - Principal Software Engineer w Allegro w szóstym odcinku Allegro Tech Podcast.","contentSnippet":"W jaki sposób Allegro stało się platformą wielojęzyczną? Które treści tłumaczymy z pomocą tłumaczy, a które maszynowo? Jak działa system, który automatyzuje zarządzanie komunikacją pomiędzy zespołami developerskimi a profesjonalnymi tłumaczami? Jak dbamy o jakość tłumaczeń i badamy działanie silnika tłumaczeniowego? Do czego wykorzystujemy nasze serwery w nocy oraz co to ma wspólnego z tłumaczeniami? Na te i inne pytania odpowiada Bartosz Wojtkiewicz - Principal Software Engineer w Allegro w szóstym odcinku Allegro Tech Podcast.","guid":"https://podcast.allegro.tech/tlumaczenia-na-allegro/","isoDate":"2022-11-03T00:00:00.000Z"},{"title":"S03E05 - Riccardo Belluzzo - O Machine Learning Research w Allegro","link":"https://podcast.allegro.tech/o-machine-learning-research-w-allegro/","pubDate":"Thu, 20 Oct 2022 00:00:00 GMT","content":"Jak wygląda praca Machine Learning Research Engineera w Allegro? Jak udaje nam się efektywnie operować na ogromnej ilości danych oraz milionach parametrów? Czym są modele językowe trenowane na korpusie danych pochodzących z Allegro i jaki mają wpływ na realizowane projekty? Jak współpracujemy ze środowiskiem naukowym? I wreszcie - gdzie i w jakiej formie można znaleźć wiedzę, którą dzieli się nasz zespół Machine Learning Research? O tym wszystkim rozmawialiśmy z kolejnym gościem Allegro Tech Podcast, czyli Riccardo Belluzzo, który pracuje jako Machine Learning Research Engineer w Allegro.","contentSnippet":"Jak wygląda praca Machine Learning Research Engineera w Allegro? Jak udaje nam się efektywnie operować na ogromnej ilości danych oraz milionach parametrów? Czym są modele językowe trenowane na korpusie danych pochodzących z Allegro i jaki mają wpływ na realizowane projekty? Jak współpracujemy ze środowiskiem naukowym? I wreszcie - gdzie i w jakiej formie można znaleźć wiedzę, którą dzieli się nasz zespół Machine Learning Research? O tym wszystkim rozmawialiśmy z kolejnym gościem Allegro Tech Podcast, czyli Riccardo Belluzzo, który pracuje jako Machine Learning Research Engineer w Allegro.","guid":"https://podcast.allegro.tech/o-machine-learning-research-w-allegro/","isoDate":"2022-10-20T00:00:00.000Z"},{"title":"S03E04 - Jakub Westfalewski, Paweł Wolak - O rekrutacji developerów w Allegro","link":"https://podcast.allegro.tech/o-rekrutacji-developer%C3%B3w-w-allegro/","pubDate":"Thu, 06 Oct 2022 00:00:00 GMT","content":"Jak zacząć pracę w Allegro i dlaczego #dobrzetubyć? Jak wygląda proces rekrutacji na stanowiska developerskie? Jak przygotować się do udziału w naszej rekrutacji i kim są “finiszujący perfekcjoniści”? Czym jest i za co odpowiada Hiring Squad? Jakie wymagania trzeba spełnić, aby być senior developerem, a jakie, żeby zostać juniorem? Na te i inne pytania odpowiadają Jakub Westfalewski - Team Manager zespołu zajmującego się procesem zakupowym i pozakupowym oraz Paweł Wolak - Senior Front-end Software Engineer w Allegro.","contentSnippet":"Jak zacząć pracę w Allegro i dlaczego #dobrzetubyć? Jak wygląda proces rekrutacji na stanowiska developerskie? Jak przygotować się do udziału w naszej rekrutacji i kim są “finiszujący perfekcjoniści”? Czym jest i za co odpowiada Hiring Squad? Jakie wymagania trzeba spełnić, aby być senior developerem, a jakie, żeby zostać juniorem? Na te i inne pytania odpowiadają Jakub Westfalewski - Team Manager zespołu zajmującego się procesem zakupowym i pozakupowym oraz Paweł Wolak - Senior Front-end Software Engineer w Allegro.","guid":"https://podcast.allegro.tech/o-rekrutacji-developer%C3%B3w-w-allegro/","isoDate":"2022-10-06T00:00:00.000Z"},{"title":"S03E03 - Paweł Marcinkowski - O Data & AI w Allegro Pay","link":"https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/","pubDate":"Thu, 22 Sep 2022 00:00:00 GMT","content":"Jak zbudowany jest obszar Data & AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data & AI w Allegro Pay.","contentSnippet":"Jak zbudowany jest obszar Data & AI w Allegro Pay i jak (współ)pracują w nim ze sobą poszczególne role oraz zespoły? Jak działa decision engine, kluczowy komponent, od którego zależy sukces Allegro Pay? Jak wyglądałby proces wprowadzenia zupełnie nowej funkcjonalności lub nowego produktu w Allegro Pay? Kim jest i za co odpowiada Data Product Manager? Jak w modelach Machine Learning do predykcji ryzyka kredytowego Allegro Pay wykorzystuje kontekst otoczenia? Na te i inne pytania związane z pracą w największym fintechu w Europie Środkowej odpowiada Paweł Marcinkowski - lider obszaru Data & AI w Allegro Pay.","guid":"https://podcast.allegro.tech/o-data-i-ai-w-allegro-pay/","isoDate":"2022-09-22T00:00:00.000Z"}]},"__N_SSG":true}