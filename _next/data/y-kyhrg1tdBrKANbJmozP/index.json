{"pageProps":{"posts":[{"title":"Shrinking the size of a monorepo","link":"https://blog.allegro.tech/2022/01/shrinking-size-of-monorepo.html","pubDate":"Wed, 12 Jan 2022 00:00:00 +0100","authors":{"author":[{"name":["Maciej Piotrowski"],"photo":["https://blog.allegro.tech/img/authors/maciej.piotrowski.jpg"],"url":["https://blog.allegro.tech/authors/maciej.piotrowski"]}]},"content":"<p>The source code of Allegro iOS app for buyers used to be divided into separate modules hosted in multiple repositories\n(polyrepo). The\nsource code was migrated to a monorepo a few years back along with the history of all repos that constituted the app.\nUpdating source code of a module on one repository could affect another module hosted on a separate repository.\nVersioning modules and propagation of dependency update led to long release process of the entire application.\nOur main\nrepository for the iOS application thus became our monorepo. After 9 years of development of the app the repo size has\ngrown\nenormously and the <code class=\"language-plaintext highlighter-rouge\">git clone</code> command became a nightmare taking too much time. We had a possibility to shrink the\nproject size during the\nmigration from an on-premise to an external git repo hosting provider.</p>\n\n<h2 id=\"monorepo-scale\">Monorepo scale</h2>\n\n<h3 id=\"general-repo-scale\">General repo scale</h3>\n\n<p>The General scale of our old repository was as follows:</p>\n\n<ul>\n  <li>almost 9 years of history with <strong>91k +</strong> commits</li>\n  <li><strong>440k</strong> <a href=\"https://en.wikipedia.org/wiki/Binary_large_object\">BLOBs</a> were stored in the repo: multiple <code class=\"language-plaintext highlighter-rouge\">.png</code> and\n<code class=\"language-plaintext highlighter-rouge\">.jpeg</code> files, 3rd-party frameworks and toolset binaries. The unpacked size of the BLOBs would add up to <strong>36 GB</strong> and\nthe biggest BLOB stored was <strong>100+ MB</strong></li>\n  <li><strong>680k</strong> git <em>tree</em> objects</li>\n  <li>the unpacked repo size on the main development branch was <strong>8+ GB</strong> where the <code class=\"language-plaintext highlighter-rouge\">.git</code> dir size after a clone was\n<strong>7+ GB</strong>. The <code class=\"language-plaintext highlighter-rouge\">.git</code> directory contained compressed <code class=\"language-plaintext highlighter-rouge\">.png</code> (<strong>4.5+ GB</strong> ü§Ø) and <code class=\"language-plaintext highlighter-rouge\">.pbxproj</code> (<strong>600+ MB</strong>) files</li>\n</ul>\n\n<h3 id=\"new-repo-scale\">New repo scale</h3>\n\n<p>After the migration and history rewrite we shrank the repo size to:</p>\n\n<ul>\n  <li><strong>71k +</strong> commits</li>\n  <li><strong>230k</strong> <a href=\"https://en.wikipedia.org/wiki/Binary_large_object\">BLOBs</a>, where all BLOBs unpacked would add up to\n<strong>1.6 GB</strong> - this\nnumber also includes size of source code files, whereas assets and binaries were migrated to an external storage</li>\n  <li><strong>455k+</strong> git <em>tree</em> objects</li>\n</ul>\n\n<h2 id=\"how-did-we-do-it\">How did we do it?</h2>\n\n<p>The history-rewrite process required proper planning and a few steps:</p>\n<ol>\n  <li>Analysis of the old repo contents and its history</li>\n  <li>Creating a reproducible procedure for the history rewrite</li>\n  <li>Dry running the procedure to test out the process</li>\n  <li>Planning and scheduling activities necessary to migrate the repo</li>\n  <li>Proper communication about the process to stakeholders (a.k.a. developers üë©‚Äçüíªüë®‚Äçüíª)</li>\n  <li>The actual migration</li>\n</ol>\n\n<h3 id=\"the-repo-analysis\">The repo analysis</h3>\n\n<p>Goals:</p>\n<ul>\n  <li>find items that can be removed from the history</li>\n  <li>select items that can be migrated to an external storage</li>\n</ul>\n\n<p>We used tools such as <a href=\"https://github.com/github/git-sizer\">git-sizer</a> and\n<a href=\"https://github.com/newren/git-filter-repo\">git-filter-repo tool</a> to\nget information about types of files stored in the repository. If you wanted to do the same the workshops from\n<a href=\"https://githubuniverse.com/professional-services-workshop-2-how-to-keep-git-monorepos-manageable/\">GitHub Universe</a> and\n<a href=\"https://github.com/githubuniverseworkshops/grafting-monorepos/issues/2\">the scripts introduced there</a> might be a good\nstarting point.</p>\n\n<p>From the analysis we were able to select the following items for complete removal from the history:</p>\n\n<ul>\n  <li>deleted dirs and files</li>\n  <li>unwanted paths: e.g. <code class=\"language-plaintext highlighter-rouge\">Pods/</code>, invalid symlinks causing deep nesting of paths</li>\n  <li>unwanted history of paths: e.g. <code class=\"language-plaintext highlighter-rouge\">Vendor</code> for storing 3rd party dependencies or <code class=\"language-plaintext highlighter-rouge\">Toolset</code> with binaries</li>\n  <li>unwanted files: .e.g <code class=\"language-plaintext highlighter-rouge\">.pbxproj</code> that can be generated by <a href=\"https://github.com/yonaskolb/XcodeGen\">XcodeGen</a> and their\nhistory is meaningless (<strong>600+ MB</strong> savings in our case), history of BLOB files such as <code class=\"language-plaintext highlighter-rouge\">.jpg</code>, <code class=\"language-plaintext highlighter-rouge\">.png</code>, <code class=\"language-plaintext highlighter-rouge\">.a</code>,\n<code class=\"language-plaintext highlighter-rouge\">.dylib</code>, <code class=\"language-plaintext highlighter-rouge\">.pdf</code>, <code class=\"language-plaintext highlighter-rouge\">.zip</code>, <code class=\"language-plaintext highlighter-rouge\">.mp4</code>, <code class=\"language-plaintext highlighter-rouge\">.json</code></li>\n</ul>\n\n<p>We decided to track BLOBs using <a href=\"https://git-lfs.github.com/\">Git LFS (Large File Storage)</a>. In our case the\nfollowing were a good use case for it:</p>\n<ul>\n  <li>large binary files <code class=\"language-plaintext highlighter-rouge\">.jpg</code>, <code class=\"language-plaintext highlighter-rouge\">.png</code>, <code class=\"language-plaintext highlighter-rouge\">.a</code>, <code class=\"language-plaintext highlighter-rouge\">.dylib</code>, <code class=\"language-plaintext highlighter-rouge\">.pdf</code>, <code class=\"language-plaintext highlighter-rouge\">.zip</code>, <code class=\"language-plaintext highlighter-rouge\">.mp4</code>, <code class=\"language-plaintext highlighter-rouge\">.json</code></li>\n  <li>framework binaries</li>\n  <li>toolset binaries</li>\n</ul>\n\n<h3 id=\"reproducible-procedure-and-dry-runs\">Reproducible procedure and dry runs</h3>\n\n<p>We created a script that contained all commands that removed redundant items from the history. To remove items we used\n<a href=\"https://github.com/newren/git-filter-repo\">git filter-repo</a> - it‚Äòs much more performant than git‚Äòs built-in\n <code class=\"language-plaintext highlighter-rouge\">git filter-branch</code> (do not use it!). Some examples of usage:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git filter-repo <span class=\"nt\">--invert-paths</span> <span class=\"nt\">--path</span> Pods/ <span class=\"nt\">--force</span>\ngit filter-repo <span class=\"nt\">--invert-paths</span> <span class=\"nt\">--paths-from-file</span> remove.txt <span class=\"nt\">--force</span>\ngit filter-repo <span class=\"nt\">--invert-paths</span> <span class=\"nt\">--path-glob</span> <span class=\"s1\">'*.pbxproj'</span> <span class=\"nt\">--force</span>\n\n</code></pre></div></div>\n\n<p>After the removal we restored the most-recent version of binaries, frameworks and BLOBs to the repo and we tracked them\nwith Git LFS:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>git lfs track <span class=\"s2\">\"*.png\"</span>\ngit lfs track <span class=\"s2\">\"Vendor/SomeSDK/SomeSDK.framework/SomeSDK\"</span>\n\n</code></pre></div></div>\n\n<p>We ran the script a few times to verify the output size of the repo. One crucial aspect after the run was to verify\nthat <strong>all plans</strong> on the CI (Continuous Integration) pass - we did it to check that the app still compiles, tests pass\nand no more files that the ones we had wanted were actually deleted.</p>\n\n<h3 id=\"communication\">Communication</h3>\n\n<p>The crucial aspect of introducing any change is communication. It‚Äòs good to prepare it in advance, have team members\nreview it. We used a few channels so that our devs would get important info about the migration and history rewrite\nthrough the channel that suited their working habits best (e-mails, instant messaging tool, dev forums).</p>\n\n<h2 id=\"some-final-thoughts\">Some final thoughts</h2>\n\n<p>It took a large amount of time to prepare the migration, understand the history of the repository and select proper\nitems and\nstrategies for the migration. The links here might be a good starting point if you wanted to rewrite histories of your\noverweight repos:</p>\n\n<ul>\n  <li><a href=\"https://www.youtube.com/watch?v=bk7akV8nyAM\">GitHub Universe Workshops</a> and corresponding\n<a href=\"https://github.com/githubuniverseworkshops/grafting-monorepos\">repository</a></li>\n  <li><a href=\"https://github.com/github/git-sizer\">git-sizer</a></li>\n  <li><a href=\"https://github.com/newren/git-filter-repo\">git filter-repo</a></li>\n</ul>\n\n<p>When creating a plan for the rewrite, remember to have a checklist that you can use to verify outcomes and to remember\nall the steps involved in the process. If the repository migration from one provider to another hosting provider is\nrequired execute it together with the history rewrite. Plan the rewrite for a time that folks would not want to\npush code to the repo. We used Friday evening, and yes, we had to fix some issues over the weekend - not everything\nwent smoothly.</p>\n\n<p>You can have a copy of your old repository in READ-ONLY mode on the servers - it will serve as a <strong>backup</strong> and will\ncontain the actual history.</p>\n","contentSnippet":"The source code of Allegro iOS app for buyers used to be divided into separate modules hosted in multiple repositories\n(polyrepo). The\nsource code was migrated to a monorepo a few years back along with the history of all repos that constituted the app.\nUpdating source code of a module on one repository could affect another module hosted on a separate repository.\nVersioning modules and propagation of dependency update led to long release process of the entire application.\nOur main\nrepository for the iOS application thus became our monorepo. After 9 years of development of the app the repo size has\ngrown\nenormously and the git clone command became a nightmare taking too much time. We had a possibility to shrink the\nproject size during the\nmigration from an on-premise to an external git repo hosting provider.\nMonorepo scale\nGeneral repo scale\nThe General scale of our old repository was as follows:\nalmost 9 years of history with 91k + commits\n440k BLOBs were stored in the repo: multiple .png and\n.jpeg files, 3rd-party frameworks and toolset binaries. The unpacked size of the BLOBs would add up to 36 GB and\nthe biggest BLOB stored was 100+ MB\n680k git tree objects\nthe unpacked repo size on the main development branch was 8+ GB where the .git dir size after a clone was\n7+ GB. The .git directory contained compressed .png (4.5+ GB ü§Ø) and .pbxproj (600+ MB) files\nNew repo scale\nAfter the migration and history rewrite we shrank the repo size to:\n71k + commits\n230k BLOBs, where all BLOBs unpacked would add up to\n1.6 GB - this\nnumber also includes size of source code files, whereas assets and binaries were migrated to an external storage\n455k+ git tree objects\nHow did we do it?\nThe history-rewrite process required proper planning and a few steps:\nAnalysis of the old repo contents and its history\nCreating a reproducible procedure for the history rewrite\nDry running the procedure to test out the process\nPlanning and scheduling activities necessary to migrate the repo\nProper communication about the process to stakeholders (a.k.a. developers üë©‚Äçüíªüë®‚Äçüíª)\nThe actual migration\nThe repo analysis\nGoals:\nfind items that can be removed from the history\nselect items that can be migrated to an external storage\nWe used tools such as git-sizer and\ngit-filter-repo tool to\nget information about types of files stored in the repository. If you wanted to do the same the workshops from\nGitHub Universe and\nthe scripts introduced there might be a good\nstarting point.\nFrom the analysis we were able to select the following items for complete removal from the history:\ndeleted dirs and files\nunwanted paths: e.g. Pods/, invalid symlinks causing deep nesting of paths\nunwanted history of paths: e.g. Vendor for storing 3rd party dependencies or Toolset with binaries\nunwanted files: .e.g .pbxproj that can be generated by XcodeGen and their\nhistory is meaningless (600+ MB savings in our case), history of BLOB files such as .jpg, .png, .a,\n.dylib, .pdf, .zip, .mp4, .json\nWe decided to track BLOBs using Git LFS (Large File Storage). In our case the\nfollowing were a good use case for it:\nlarge binary files .jpg, .png, .a, .dylib, .pdf, .zip, .mp4, .json\nframework binaries\ntoolset binaries\nReproducible procedure and dry runs\nWe created a script that contained all commands that removed redundant items from the history. To remove items we used\ngit filter-repo - it‚Äòs much more performant than git‚Äòs built-in\n git filter-branch (do not use it!). Some examples of usage:\n\ngit filter-repo --invert-paths --path Pods/ --force\ngit filter-repo --invert-paths --paths-from-file remove.txt --force\ngit filter-repo --invert-paths --path-glob '*.pbxproj' --force\n\n\n\nAfter the removal we restored the most-recent version of binaries, frameworks and BLOBs to the repo and we tracked them\nwith Git LFS:\n\ngit lfs track \"*.png\"\ngit lfs track \"Vendor/SomeSDK/SomeSDK.framework/SomeSDK\"\n\n\n\nWe ran the script a few times to verify the output size of the repo. One crucial aspect after the run was to verify\nthat all plans on the CI (Continuous Integration) pass - we did it to check that the app still compiles, tests pass\nand no more files that the ones we had wanted were actually deleted.\nCommunication\nThe crucial aspect of introducing any change is communication. It‚Äòs good to prepare it in advance, have team members\nreview it. We used a few channels so that our devs would get important info about the migration and history rewrite\nthrough the channel that suited their working habits best (e-mails, instant messaging tool, dev forums).\nSome final thoughts\nIt took a large amount of time to prepare the migration, understand the history of the repository and select proper\nitems and\nstrategies for the migration. The links here might be a good starting point if you wanted to rewrite histories of your\noverweight repos:\nGitHub Universe Workshops and corresponding\nrepository\ngit-sizer\ngit filter-repo\nWhen creating a plan for the rewrite, remember to have a checklist that you can use to verify outcomes and to remember\nall the steps involved in the process. If the repository migration from one provider to another hosting provider is\nrequired execute it together with the history rewrite. Plan the rewrite for a time that folks would not want to\npush code to the repo. We used Friday evening, and yes, we had to fix some issues over the weekend - not everything\nwent smoothly.\nYou can have a copy of your old repository in READ-ONLY mode on the servers - it will serve as a backup and will\ncontain the actual history.","guid":"https://blog.allegro.tech/2022/01/shrinking-size-of-monorepo.html","categories":["tech","ios","git","mobile","swift","objectivec"],"isoDate":"2022-01-11T23:00:00.000Z","thumbnail":"images/post-headers/ios.png"},{"title":"Evaluating performance of time series collections","link":"https://blog.allegro.tech/2021/12/performance-evaluation-of-timeseries.html","pubDate":"Mon, 20 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Micha≈Ç Knasiecki"],"photo":["https://blog.allegro.tech/img/authors/michal.knasiecki.jpg"],"url":["https://blog.allegro.tech/authors/michal.knasiecki"]}]},"content":"<p>A few years ago, I was working on a new version of <a href=\"https://allegro.tech\">Allegro</a> purchase ratings.\nIt was a time of a pretty large revolution in the rating system when we moved this product away from our monolith, also\nintroducing quite significant changes to the concept of purchase rating itself. Replacing the system of positive, neutral\nor negative rating, we introduced an approach based on ‚Äúthumbs up‚Äù and ‚Äúthumbs down‚Äù as well as the option to rate several\nelements of the purchase separately: the time and cost of delivery, or products‚Äô conformity with its description. The\nproduct-related revolution was accompanied by a major change in technology. Apart from transitioning towards the\nmicroservices architecture, we also decided to migrate data from a large relational database to MongoDB. There were many\nreasons for this decision: from the non-relational nature of our model, through the need for easier scaling, to the wish\nfor cost reduction. Upon completion of the works, we were for the most part content with the decision that we made. The\nnew solution was more user-friendly, easier to maintain and worked smoothly. The sole exception was aggregation queries,\nspecifically: determining the average of seller ratings in a specified period of time. While at the level of the 99th\npercentile times were very low, some queries were much slower. We spent a lot of time optimising both queries and the\ncode, and had to use some programming tricks to achieve satisfactory results. While we were able to solve our problems in\nthe end, the final conclusion was that the aggregation of data in large MongoDB collections is quite challenging.</p>\n\n<h2 id=\"to-the-rescue-time-series\">To the rescue: Time series</h2>\n<p>A new version of MongoDB, 5.0, has been recently launched. The list of changes included one that I found particularly\ninteresting: the time series collections. It is a method of effective storing and processing of time-ordered value series.\nA classic example for this case is measuring the temperature of air. These measurements are taken\nperiodically (for instance every hour), and their sequence forms time series. We then often review such data in an\nappropriate order, as well as calculate the maximum and minimum values, or the arithmetic mean. Therefore, in the said\ncase of use, a database must be highly efficient when saving the data, store records in a compact manner due to the\nlarge number thereof, and must quickly calculate aggregates. Although in the case of temperature readings database write\noperations are made on a regular basis, it turns out that in the case of time series it is not mandatory, and the only\nthing that truly matters is the presence of time. While reading about this topic, I instantly remembered my countless\nlate nights struggling with slow ratings aggregations. Therefore, I decided to explore this topic and see how the\nsolution works in practice.</p>\n\n<p>Before the release of MongoDB 5.0, the only way to efficiently process time series was to store pre-calculated\naggregates, or use a <a href=\"https://www.mongodb.com/blog/post/building-with-patterns-the-bucket-pattern\">bucket pattern</a>,\nwhich was obviously associated with additional work and complexity of the\ncode. Now, things have been made much easier as this additional complexity is covered by a convenient abstraction. In\nMongoDB, time series are not actual collections, but materialised views that cover physical collections. This\nabstraction is intended to simplify complicated operations based on ‚Äúbuckets‚Äù of documents. You can read more about the\nconcept of storing data in the new kind of collection on the MongoDB <a href=\"https://www.mongodb.com/developer/how-to/new-time-series-collections/\">official blog</a>.\nEveryone who is interested in using this solution should take a look at it. In my article, on the other hand, I would\nlike to verify whether the processing of time series is really as fast as promised by the authors.</p>\n\n<h2 id=\"data-preparation\">Data preparation</h2>\n<p>For the purposes of our considerations I will use a simple document describing the rating in the form of:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"rating\" : 2.0\n}\n</code></pre></div></div>\n\n<p>Attentive readers will immediately notice the absence of the field containing the ID of the seller whom the rating\nconcerns. It was done intentionally, otherwise I would have to\ncreate an additional index covering this field. However, I did not want to introduce any additional elements in my\nexperiment that could have any impact on the results. Let‚Äôs assume for this experiment that we are rating a\nrestaurant, not Allegro sellers, therefore all ratings in the collection concern the restaurant only.</p>\n\n<p>Now we can create two collections storing an identical set of data. One will be a standard collection, and the other will\nbe a time series. The time series collection has to be created manually by indicating the field specifying the time label:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">createCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ts</span><span class=\"dl\">\"</span><span class=\"p\">,</span> <span class=\"p\">{</span> <span class=\"na\">timeseries</span><span class=\"p\">:</span> <span class=\"p\">{</span> <span class=\"na\">timeField</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span> <span class=\"p\">}</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If you did not install Mongo 5.0 from scratch, but updated its previous version, you should make sure that\nit is set to an adequate level of compatibility. Otherwise, the above command will not create a time series collection.\nYou can check it with this command:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">adminCommand</span><span class=\"p\">(</span> <span class=\"p\">{</span> <span class=\"na\">getParameter</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"na\">featureCompatibilityVersion</span><span class=\"p\">:</span> <span class=\"mi\">1</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>If the returned value is less than 5.0, you need to issue:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">adminCommand</span><span class=\"p\">(</span> <span class=\"p\">{</span> <span class=\"na\">setFeatureCompatibilityVersion</span><span class=\"p\">:</span> <span class=\"dl\">\"</span><span class=\"s2\">5.0</span><span class=\"dl\">\"</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Upon creating a collection, it is also worth checking that a time series has actually been created:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">runCommand</span><span class=\"p\">(</span> <span class=\"p\">{</span> <span class=\"na\">listCollections</span><span class=\"p\">:</span> <span class=\"mf\">1.0</span> <span class=\"p\">}</span> <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We look for <code class=\"language-plaintext highlighter-rouge\">\"type\" : \"timeseries\"</code> entry:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n    \"name\" : \"coll-ts\",\n    \"type\" : \"timeseries\",\n    \"options\" : {\n        \"timeseries\" : {\n            \"timeField\" : \"timestamp\",\n            \"granularity\" : \"seconds\",\n            \"bucketMaxSpanSeconds\" : 3600\n        }\n    },\n    \"info\" : {\n        \"readOnly\" : false\n    }\n}\n</code></pre></div></div>\n\n<p>We will also create the second collection manually (although it is not necessary, because it would be created with the\nfirst INSERT command). We will want to check the speed of data search based on time field, so we will create a unique index:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">createCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ord</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">createIndex</span><span class=\"p\">({</span><span class=\"na\">timestamp</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">},</span> <span class=\"p\">{</span><span class=\"na\">unique</span><span class=\"p\">:</span> <span class=\"kc\">true</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<p>Let‚Äôs use the following scripts to fill both collections with 10 million documents:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// Save as fill-ts.js</span>\n<span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ts</span><span class=\"dl\">\"</span><span class=\"p\">).</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"kd\">var</span> <span class=\"nx\">startTime</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"nx\">startTime</span><span class=\"p\">.</span><span class=\"nx\">getTime</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"nx\">i</span> <span class=\"o\">*</span> <span class=\"mi\">60000</span><span class=\"p\">),</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">rating</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">floor</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">random</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n    <span class=\"p\">})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\">// Save as fill-ord.js</span>\n<span class=\"kd\">var</span> <span class=\"nx\">bulk</span> <span class=\"o\">=</span> <span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">coll-ord</span><span class=\"dl\">\"</span><span class=\"p\">).</span><span class=\"nx\">initializeUnorderedBulkOp</span><span class=\"p\">();</span>\n<span class=\"kd\">var</span> <span class=\"nx\">startTime</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"p\">(</span><span class=\"kd\">let</span> <span class=\"nx\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;</span> <span class=\"mi\">10000000</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">insert</span><span class=\"p\">({</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">(</span><span class=\"nx\">startTime</span><span class=\"p\">.</span><span class=\"nx\">getTime</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"nx\">i</span> <span class=\"o\">*</span> <span class=\"mi\">60000</span><span class=\"p\">),</span>\n       <span class=\"dl\">\"</span><span class=\"s2\">rating</span><span class=\"dl\">\"</span><span class=\"p\">:</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">floor</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">random</span><span class=\"p\">()</span> <span class=\"o\">*</span> <span class=\"mi\">4</span><span class=\"p\">)</span>\n    <span class=\"p\">})</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">bulk</span><span class=\"p\">.</span><span class=\"nx\">execute</span><span class=\"p\">();</span>\n</code></pre></div></div>\n\n<p>It is worth to measure the execution time of scripts to compare write times to both collections. For this purpose I use\n<code class=\"language-plaintext highlighter-rouge\">time</code> command and <code class=\"language-plaintext highlighter-rouge\">mongo</code> command-line client, <code class=\"language-plaintext highlighter-rouge\">test</code> is the name of my database.\nTo avoid network latency I perform the measurements on my laptop with a local instance of MongoDB version 5.0.3 running.</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill-ts.js\n</code></pre></div></div>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>fill-ord.js\n</code></pre></div></div>\n\n<p>As expected, the filling of the time series collection was faster: 3:30,11 vs 4:39,48. Such a difference can be essential\nif our system performs many write operations in a short period of time. At the very beginning of our experiments\ncollection of a new type comes to the forefront.</p>\n\n<p>Now when our collections already contain data, we can take a look at the size of files. On the\n<a href=\"https://docs.mongodb.com/manual/core/timeseries-collections/\">manual page</a> we can read that:</p>\n\n<blockquote>\n  <p>Compared to normal collections, storing time series data in time series collections improves query efficiency and\nreduces the disk usage</p>\n</blockquote>\n\n<p>Let‚Äôs find out how true that is.</p>\n\n<h2 id=\"a-closer-look-at-the-data\">A closer look at the data</h2>\n\n<p>In the first place, it is worth making sure that documents in both collections look the same:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({}).</span><span class=\"nx\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({}).</span><span class=\"nx\">limit</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>Both documents should be similar to this one:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"_id\" : ObjectId(\"6184126fc42d1ab73d5208e4\"),\n    \"rating\" : 2.0\n}\n</code></pre></div></div>\n\n<p>The documents have the same schema. In addition, the <code class=\"language-plaintext highlighter-rouge\">_id</code> key was automatically generated in both cases, although our\nfilling script did not contain them.</p>\n\n<p>Let‚Äôs move on to indexes now and use the commands: <code class=\"language-plaintext highlighter-rouge\">db.getCollection('coll-ord').getIndexes()</code> and <code class=\"language-plaintext highlighter-rouge\">db.getCollection('coll-ts').getIndexes()</code>\nto get the indexes of both collections.</p>\n\n<p>The normal collection has two indexes, one that was created automatically for the <code class=\"language-plaintext highlighter-rouge\">_id</code> key and the one that we created manually:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>// coll-ord:\n[\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"_id\" : 1\n        },\n        \"name\" : \"_id_\"\n    },\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"timestamp\" : 1.0\n        },\n        \"name\" : \"timestamp_1\",\n        \"unique\" : true\n    }\n]\n</code></pre></div></div>\n\n<p>What is interesting is that the time series collection has no index at all:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>// coll-ts:\n[]\n</code></pre></div></div>\n\n<p>The lack of the index for the <code class=\"language-plaintext highlighter-rouge\">_id</code> key of\ncourse means that, by default, the time series collection will have to perform the <code class=\"language-plaintext highlighter-rouge\">COLLSCAN</code> operation if we want to\nsearch documents based on <code class=\"language-plaintext highlighter-rouge\">_id</code>. The index is probably missing simply to save disc space and storage time,\nstemming from the assumption that the time series collections are mainly used to search based on time. The lack of\nthe index for the timestamp field is much more surprising. Does it mean that time-based searches in time series will\nalso cause <code class=\"language-plaintext highlighter-rouge\">COLLCSAN</code> and work slowly? The answer to this question can be found in the documentation:</p>\n<blockquote>\n  <p>The internal index for a time series collection is not displayed</p>\n</blockquote>\n\n<p>So, there actually is an index, but it is different from those created manually, and even from indexes created\nautomatically for the <code class=\"language-plaintext highlighter-rouge\">_id</code> key.\nAs I wrote in another <a href=\"/2021/10/comparing-mongodb-composite-indexes.html\">post</a>, indexes are not all the\nsame, so it‚Äôs worth taking a closer look at this one. Let‚Äôs check the query execution plans:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">executionStats</span><span class=\"dl\">'</span><span class=\"p\">)</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({</span><span class=\"dl\">\"</span><span class=\"s2\">timestamp</span><span class=\"dl\">\"</span> <span class=\"p\">:</span> <span class=\"nx\">ISODate</span><span class=\"p\">(</span><span class=\"dl\">\"</span><span class=\"s2\">2021-10-22T00:00:00.000Z</span><span class=\"dl\">\"</span><span class=\"p\">)}).</span><span class=\"nx\">explain</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">executionStats</span><span class=\"dl\">'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>//coll-ord\n\"winningPlan\" : {\n    \"stage\" : \"FETCH\",\n    \"inputStage\" : {\n        \"stage\" : \"IXSCAN\",\n        \"keyPattern\" : {\n            \"timestamp\" : 1.0\n        }\n    },\n    \"indexName\" : \"timestamp_1\",\n}\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>//coll-ts\n\"winningPlan\" : {\n    \"stage\" : \"COLLSCAN\",\n    \"filter\" : {\n        \"$and\" : [\n            {\n                \"_id\" : {\n                    \"$lte\" : ObjectId(\"6171ff00ffffffffffffffff\")\n                }\n            },\n            {\n                \"_id\" : {\n                    \"$gte\" : ObjectId(\"6171f0f00000000000000000\")\n                }\n            },\n            {\n                \"control.max.timestamp\" : {\n                    \"$_internalExprGte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            },\n            {\n                \"control.min.timestamp\" : {\n                    \"$_internalExprLte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            }\n        ]\n    },\n}\n</code></pre></div></div>\n\n<p>It turns out that while in the case of the regular collection the plan shows the use of the index, in the time series\ncollection we see the <code class=\"language-plaintext highlighter-rouge\">COLLSCAN</code> operation. It doesn‚Äôt mean that this operation is slow, though. The execution times of\nboth operations were similar. We will move on to a more detailed time comparison in a moment; for now we should only\nnote that the hidden index in the time series collection follows specific rules, it is not only invisible, but it also\ncannot be seen in the execution plan, although it clearly affects the speed of the search.</p>\n\n<p>And what happens if we add sorting?</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">find</span><span class=\"p\">({}).</span><span class=\"nx\">sort</span><span class=\"p\">({</span><span class=\"na\">timestamp</span><span class=\"p\">:</span> <span class=\"mi\">1</span><span class=\"p\">})</span>\n</code></pre></div></div>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>{\n \"ok\" : 0,\n \"errmsg\" : \"PlanExecutor error during aggregation :: caused by :: Sort exceeded memory limit of 104857600 bytes,\n but did not opt in to external sorting.\",\n \"code\" : 292,\n \"codeName\" : \"QueryExceededMemoryLimitNoDiskUseAllowed\"\n}\n</code></pre></div></div>\n\n<p>Surprise! The internal index for the time series collection does not have a sorting feature. This means that if we add\nthe sort clause to our query, the operation will take very long, or even fail because of exceeding the memory\nlimit. It is surprising because I did not find any information on this in the documentation. Therefore, if we plan to\nsort our data based on the field with time, we will need to index this field manually. It means, of\ncourse, that the benefits stemming from a lower disc usage and faster saving times will unfortunately diminish.</p>\n\n<p>Since we are talking about the use of disc space, let‚Äôs check the data size:</p>\n\n<div class=\"language-javascript highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ts</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">stats</span><span class=\"p\">()</span>\n<span class=\"nx\">db</span><span class=\"p\">.</span><span class=\"nx\">getCollection</span><span class=\"p\">(</span><span class=\"dl\">'</span><span class=\"s1\">coll-ord</span><span class=\"dl\">'</span><span class=\"p\">).</span><span class=\"nx\">stats</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>We will compare several fields:</p>\n\n<ul>\n  <li><code class=\"language-plaintext highlighter-rouge\">size</code>: data size before the compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">storageSize</code>: size of data after the compression,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">totalIndexSize</code>: size of indexes,</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">totalSize</code>: total size of data and indexes.</li>\n</ul>\n\n<p>Results are gathered in the table below (in bytes, space is thousand separator):</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Field</th>\n      <th style=\"text-align: right\">Normal collection</th>\n      <th style=\"text-align: right\">Time series collection</th>\n      <th style=\"text-align: center\">Diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>size</td>\n      <td style=\"text-align: right\">570 000 000</td>\n      <td style=\"text-align: right\">747 855 228</td>\n      <td style=\"text-align: center\">+ 31%</td>\n    </tr>\n    <tr>\n      <td>storageSize</td>\n      <td style=\"text-align: right\">175 407 104</td>\n      <td style=\"text-align: right\">119 410 688</td>\n      <td style=\"text-align: center\">-31%</td>\n    </tr>\n    <tr>\n      <td>totalIndexSize</td>\n      <td style=\"text-align: right\">232 701 952</td>\n      <td style=\"text-align: right\">0</td>\n      <td style=\"text-align: center\">-100%</td>\n    </tr>\n    <tr>\n      <td>totalSize</td>\n      <td style=\"text-align: right\">408 109 056</td>\n      <td style=\"text-align: right\">119 410 688</td>\n      <td style=\"text-align: center\">-70%</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>As you can see, raw data of the time series collection may take up more space, but after their compression the new-type\ncollection turns out to be the winner in the size-on-disc category. After adding the size of indexes created in the\nregular collection, the difference will be even greater. Therefore, we must admit that the way time series data are\npacked on the disc is impressive.</p>\n\n<p>Now it‚Äôs time to compare query execution times for both collections.</p>\n\n<h2 id=\"speed-is-all-that-matters\">Speed is all that matters</h2>\n\n<p>Using the script below (saved as <code class=\"language-plaintext highlighter-rouge\">gen-find.sh</code> file), I generated two files containing commands getting documents from\nboth collections based on the\ntime label:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for</span> <span class=\"o\">((</span>i <span class=\"o\">=</span> 1<span class=\"p\">;</span> i &lt;<span class=\"o\">=</span> <span class=\"nv\">$1</span><span class=\"p\">;</span> i++ <span class=\"o\">))</span><span class=\"p\">;</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n  <span class=\"nv\">t</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">date</span> <span class=\"nt\">-jv</span> +<span class=\"k\">${</span><span class=\"nv\">x</span><span class=\"k\">}</span>M <span class=\"nt\">-f</span> <span class=\"s2\">\"%Y-%m-%d %H:%M:%S\"</span> <span class=\"s2\">\"2021-10-22 0:00:00\"</span> +%Y-%m-%dT%H:%M:%S<span class=\"si\">)</span>\n  <span class=\"nb\">echo</span> <span class=\"s2\">\"db.getCollection('</span><span class=\"nv\">$2</span><span class=\"s2\">').find({'timestamp' : new ISODate('</span><span class=\"nv\">$t</span><span class=\"s2\">')})\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> find-<span class=\"nv\">$2</span>.js\n</code></pre></div></div>\n\n<p>The script takes as parameters: the number of queries, and the name of the collection that we want to search. I\ngenerated a million queries (it may take some time depending on your hardware, so you can start with a lower amount of\nqueries):</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./gen-find.sh 1000000 coll-ts\n./gen-find.sh 1000000 coll-ord\n</code></pre></div></div>\n\n<p>Then I checked the time of the execution of both query sequences using the command:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find-coll-ts.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>find-coll-ord.js\n</code></pre></div></div>\n\n<p>The standard collection was a bit slower: 16,854 for <code class=\"language-plaintext highlighter-rouge\">coll-ord</code> vs 16,038 for <code class=\"language-plaintext highlighter-rouge\">coll-ts</code>. Although the difference is small,\nanother point goes to time series: simple search is slightly faster than in the case of the regular collection.</p>\n\n<p>But we‚Äôre yet to discuss the most interesting part. Time series is primarily used for quick aggregate counting. Let‚Äôs\nsee what the comparison looks like when calculating the arithmetic mean in a given time interval.</p>\n\n<p>The script below (saved as <code class=\"language-plaintext highlighter-rouge\">gen-aggregate.sh</code>) creates a list of queries calculating the arithmetic mean of ratings for\na randomly selected six-hour interval:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c\">#!/bin/bash</span>\n<span class=\"nv\">RANDOM</span><span class=\"o\">=</span>42\n<span class=\"k\">for</span> <span class=\"o\">((</span>i <span class=\"o\">=</span> 1<span class=\"p\">;</span> i &lt;<span class=\"o\">=</span> <span class=\"nv\">$1</span><span class=\"p\">;</span> i++ <span class=\"o\">))</span><span class=\"p\">;</span>\n<span class=\"k\">do\n  </span><span class=\"nv\">x1</span><span class=\"o\">=</span><span class=\"k\">$((</span> <span class=\"nv\">$RANDOM</span> <span class=\"o\">%</span> <span class=\"m\">10000000</span><span class=\"k\">))</span>\n  <span class=\"nv\">x2</span><span class=\"o\">=</span><span class=\"k\">$((</span> x1 <span class=\"o\">+</span> <span class=\"m\">360</span><span class=\"k\">))</span>\n  <span class=\"nv\">t1</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">date</span> <span class=\"nt\">-jv</span> +<span class=\"k\">${</span><span class=\"nv\">x1</span><span class=\"k\">}</span>M <span class=\"nt\">-f</span> <span class=\"s2\">\"%Y-%m-%d %H:%M:%S\"</span> <span class=\"s2\">\"2021-10-22 0:00:00\"</span> +%Y-%m-%dT%H:%M:%S<span class=\"si\">)</span>\n  <span class=\"nv\">t2</span><span class=\"o\">=</span><span class=\"si\">$(</span><span class=\"nb\">date</span> <span class=\"nt\">-jv</span> +<span class=\"k\">${</span><span class=\"nv\">x2</span><span class=\"k\">}</span>M <span class=\"nt\">-f</span> <span class=\"s2\">\"%Y-%m-%d %H:%M:%S\"</span> <span class=\"s2\">\"2021-10-22 0:00:00\"</span> +%Y-%m-%dT%H:%M:%S<span class=\"si\">)</span>\n  <span class=\"nb\">echo</span> <span class=\"s2\">\"db.getCollection('</span><span class=\"nv\">$2</span><span class=\"s2\">').aggregate([{ </span><span class=\"se\">\\$</span><span class=\"s2\">match: { \"</span>timestamp<span class=\"s2\">\" : \"</span> <span class=\"se\">\\</span>\n    <span class=\"s2\">\"{</span><span class=\"se\">\\$</span><span class=\"s2\">gte:new ISODate('</span><span class=\"nv\">$t1</span><span class=\"s2\">'),</span><span class=\"se\">\\$</span><span class=\"s2\">lt:new ISODate('</span><span class=\"nv\">$t2</span><span class=\"s2\">')} } },\"</span> <span class=\"se\">\\</span>\n    <span class=\"s2\">\"{ </span><span class=\"se\">\\$</span><span class=\"s2\">group: { _id: null, avg: { </span><span class=\"se\">\\$</span><span class=\"s2\">avg: </span><span class=\"se\">\\\"\\$</span><span class=\"s2\">rating</span><span class=\"se\">\\\"</span><span class=\"s2\"> } } }])\"</span>\n<span class=\"k\">done</span> <span class=\"o\">&gt;&gt;</span> aggregate-<span class=\"nv\">$2</span>-<span class=\"nv\">$1</span>.js\n</code></pre></div></div>\n\n<p>I prepared three script sets with: 10K, 50K and 100K queries for both collections:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>./gen-aggregate.sh 10000 coll-ts\n./gen-aggregate.sh 50000 coll-ts\n./gen-aggregate.sh 100000 coll-ts\n\n./gen-aggregate.sh 10000 coll-ord\n./gen-aggregate.sh 50000 coll-ord\n./gen-aggregate.sh 100000 coll-ord\n</code></pre></div></div>\n\n<p>I made the measurements using following commands:</p>\n\n<div class=\"language-shell highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ts-10000.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ord-10000.js\n\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ts-50000.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ord-50000.js\n\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ts-100000.js\n<span class=\"nb\">time </span>mongo <span class=\"nb\">test </span>aggregate-coll-ord-100000.js\n\n</code></pre></div></div>\n\n<p>The results are shown in the table below:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: center\">Number of queries</th>\n      <th style=\"text-align: right\">Normal collection [min:sec]</th>\n      <th style=\"text-align: right\">Time series [min:sec]</th>\n      <th style=\"text-align: center\">Diff</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: center\">10000</td>\n      <td style=\"text-align: right\">0:21,947</td>\n      <td style=\"text-align: right\">0:16,835</td>\n      <td style=\"text-align: center\">-23%</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">50000</td>\n      <td style=\"text-align: right\">1:37,02</td>\n      <td style=\"text-align: right\">1:11,18</td>\n      <td style=\"text-align: center\">-26%</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: center\">100000</td>\n      <td style=\"text-align: right\">4:33,29</td>\n      <td style=\"text-align: right\">2:21,37</td>\n      <td style=\"text-align: center\">-48%</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Although there are far fewer queries this time than in the previous experiment, the differences in times are much\ngreater. It clearly proves that time series collections are indeed spreading their wings when we want to use aggregation\nqueries and have been developed mainly for this purpose. The reason is probably the sequential way of writing data,\nwhich shows a noticeable improvement when running ranged queries. The results clearly show also how much of an impact\nquery performance can have on an element that often doesn‚Äôt get the adequate attention: proper modelling the data.</p>\n\n<h2 id=\"limitations\">Limitations</h2>\n\n<p>Of course, time series collections also have their limitations and should not be used as a golden hammer.\nWe have already mentioned the first one ‚Äî the lack of the primary key index. It stems from the assumption\nthat searches will be primarily based on time, so there is no point in creating an index that will be useful for very\nfew people. Of course, we can create this index ourselves.</p>\n\n<p>It is also not possible to sort by time field, which is another inconvenience. If we want to have\nsorting queries, we have to create an additional index.</p>\n\n<p>Although these two missing indexes may seem to be an easy thing to fix, we must remember that it involves additional use\nof disc space as well as longer indexing time during the saving of the document, which means that the benefits\nstemming from the use of time series will be somewhat reduced.</p>\n\n<p>The third, and perhaps most import limitation is the immutability of the document. Once saved, documents cannot be\nupdated or deleted. The only way to delete data is to use <code class=\"language-plaintext highlighter-rouge\">drop()</code> command or to define retention for the collection\nusing the <code class=\"language-plaintext highlighter-rouge\">expireAfterSeconds</code> parameter, which, like TTL indexes, will automatically delete documents certain time\nafter creation.</p>\n\n<p>The lack of possibility to manipulate the saved documents will probably be the main reason why programmers will be\nhesitant to use time series. We should mention, however, that the authors of MongoDB will probably add the possibility\nto edit and delete documents in the future:</p>\n\n<blockquote>\n  <p>While we know some of these limitations may be impactful to your current use case, we promise we‚Äôre working on this\nright now and would love for you to provide your feedback!</p>\n</blockquote>\n\n<h2 id=\"summary\">Summary</h2>\n<p>Adding the ability to store time series in MongoDB is a step in the right direction. First tests show that in certain\ncases the new type of collections really does work better than the regular ones. They use less disc space, are faster at\nsaving and searching by the time. But high performance always comes at a cost, in this case: the cost of reduced flexibility.\nTherefore, the final decision to use time series should be preceded by an analysis of\nadvantages and disadvantages of both in particular cases. We should also hope that the authors of the database are\nworking on improving it and will soon eliminate most limitations.</p>\n","contentSnippet":"A few years ago, I was working on a new version of Allegro purchase ratings.\nIt was a time of a pretty large revolution in the rating system when we moved this product away from our monolith, also\nintroducing quite significant changes to the concept of purchase rating itself. Replacing the system of positive, neutral\nor negative rating, we introduced an approach based on ‚Äúthumbs up‚Äù and ‚Äúthumbs down‚Äù as well as the option to rate several\nelements of the purchase separately: the time and cost of delivery, or products‚Äô conformity with its description. The\nproduct-related revolution was accompanied by a major change in technology. Apart from transitioning towards the\nmicroservices architecture, we also decided to migrate data from a large relational database to MongoDB. There were many\nreasons for this decision: from the non-relational nature of our model, through the need for easier scaling, to the wish\nfor cost reduction. Upon completion of the works, we were for the most part content with the decision that we made. The\nnew solution was more user-friendly, easier to maintain and worked smoothly. The sole exception was aggregation queries,\nspecifically: determining the average of seller ratings in a specified period of time. While at the level of the 99th\npercentile times were very low, some queries were much slower. We spent a lot of time optimising both queries and the\ncode, and had to use some programming tricks to achieve satisfactory results. While we were able to solve our problems in\nthe end, the final conclusion was that the aggregation of data in large MongoDB collections is quite challenging.\nTo the rescue: Time series\nA new version of MongoDB, 5.0, has been recently launched. The list of changes included one that I found particularly\ninteresting: the time series collections. It is a method of effective storing and processing of time-ordered value series.\nA classic example for this case is measuring the temperature of air. These measurements are taken\nperiodically (for instance every hour), and their sequence forms time series. We then often review such data in an\nappropriate order, as well as calculate the maximum and minimum values, or the arithmetic mean. Therefore, in the said\ncase of use, a database must be highly efficient when saving the data, store records in a compact manner due to the\nlarge number thereof, and must quickly calculate aggregates. Although in the case of temperature readings database write\noperations are made on a regular basis, it turns out that in the case of time series it is not mandatory, and the only\nthing that truly matters is the presence of time. While reading about this topic, I instantly remembered my countless\nlate nights struggling with slow ratings aggregations. Therefore, I decided to explore this topic and see how the\nsolution works in practice.\nBefore the release of MongoDB 5.0, the only way to efficiently process time series was to store pre-calculated\naggregates, or use a bucket pattern,\nwhich was obviously associated with additional work and complexity of the\ncode. Now, things have been made much easier as this additional complexity is covered by a convenient abstraction. In\nMongoDB, time series are not actual collections, but materialised views that cover physical collections. This\nabstraction is intended to simplify complicated operations based on ‚Äúbuckets‚Äù of documents. You can read more about the\nconcept of storing data in the new kind of collection on the MongoDB official blog.\nEveryone who is interested in using this solution should take a look at it. In my article, on the other hand, I would\nlike to verify whether the processing of time series is really as fast as promised by the authors.\nData preparation\nFor the purposes of our considerations I will use a simple document describing the rating in the form of:\n\n{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"rating\" : 2.0\n}\n\n\nAttentive readers will immediately notice the absence of the field containing the ID of the seller whom the rating\nconcerns. It was done intentionally, otherwise I would have to\ncreate an additional index covering this field. However, I did not want to introduce any additional elements in my\nexperiment that could have any impact on the results. Let‚Äôs assume for this experiment that we are rating a\nrestaurant, not Allegro sellers, therefore all ratings in the collection concern the restaurant only.\nNow we can create two collections storing an identical set of data. One will be a standard collection, and the other will\nbe a time series. The time series collection has to be created manually by indicating the field specifying the time label:\n\ndb.createCollection(\"coll-ts\", { timeseries: { timeField: \"timestamp\" } } )\n\n\nIf you did not install Mongo 5.0 from scratch, but updated its previous version, you should make sure that\nit is set to an adequate level of compatibility. Otherwise, the above command will not create a time series collection.\nYou can check it with this command:\n\ndb.adminCommand( { getParameter: 1, featureCompatibilityVersion: 1 } )\n\n\nIf the returned value is less than 5.0, you need to issue:\n\ndb.adminCommand( { setFeatureCompatibilityVersion: \"5.0\" } )\n\n\nUpon creating a collection, it is also worth checking that a time series has actually been created:\n\ndb.runCommand( { listCollections: 1.0 } )\n\n\nWe look for \"type\" : \"timeseries\" entry:\n\n{\n    \"name\" : \"coll-ts\",\n    \"type\" : \"timeseries\",\n    \"options\" : {\n        \"timeseries\" : {\n            \"timeField\" : \"timestamp\",\n            \"granularity\" : \"seconds\",\n            \"bucketMaxSpanSeconds\" : 3600\n        }\n    },\n    \"info\" : {\n        \"readOnly\" : false\n    }\n}\n\n\nWe will also create the second collection manually (although it is not necessary, because it would be created with the\nfirst INSERT command). We will want to check the speed of data search based on time field, so we will create a unique index:\n\ndb.createCollection(\"coll-ord\")\ndb.getCollection('coll-ord').createIndex({timestamp: 1}, {unique: true})\n\n\nLet‚Äôs use the following scripts to fill both collections with 10 million documents:\n\n// Save as fill-ts.js\nvar bulk = db.getCollection(\"coll-ts\").initializeUnorderedBulkOp();\nvar startTime = new Date(\"2021-10-22T00:00:00.000Z\")\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({\n       \"timestamp\": new Date(startTime.getTime() + i * 60000),\n       \"rating\": Math.floor(1 + Math.random() * 4)\n    })\n}\nbulk.execute();\n\n\n\n// Save as fill-ord.js\nvar bulk = db.getCollection(\"coll-ord\").initializeUnorderedBulkOp();\nvar startTime = new Date(\"2021-10-22T00:00:00.000Z\")\nfor (let i = 0; i < 10000000; i++) {\n    bulk.insert({\n       \"timestamp\": new Date(startTime.getTime() + i * 60000),\n       \"rating\": Math.floor(1 + Math.random() * 4)\n    })\n}\nbulk.execute();\n\n\nIt is worth to measure the execution time of scripts to compare write times to both collections. For this purpose I use\ntime command and mongo command-line client, test is the name of my database.\nTo avoid network latency I perform the measurements on my laptop with a local instance of MongoDB version 5.0.3 running.\n\ntime mongo test fill-ts.js\n\n\n\ntime mongo test fill-ord.js\n\n\nAs expected, the filling of the time series collection was faster: 3:30,11 vs 4:39,48. Such a difference can be essential\nif our system performs many write operations in a short period of time. At the very beginning of our experiments\ncollection of a new type comes to the forefront.\nNow when our collections already contain data, we can take a look at the size of files. On the\nmanual page we can read that:\nCompared to normal collections, storing time series data in time series collections improves query efficiency and\nreduces the disk usage\nLet‚Äôs find out how true that is.\nA closer look at the data\nIn the first place, it is worth making sure that documents in both collections look the same:\n\ndb.getCollection('coll-ts').find({}).limit(1)\ndb.getCollection('coll-ord').find({}).limit(1)\n\n\nBoth documents should be similar to this one:\n\n{\n    \"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\"),\n    \"_id\" : ObjectId(\"6184126fc42d1ab73d5208e4\"),\n    \"rating\" : 2.0\n}\n\n\nThe documents have the same schema. In addition, the _id key was automatically generated in both cases, although our\nfilling script did not contain them.\nLet‚Äôs move on to indexes now and use the commands: db.getCollection('coll-ord').getIndexes() and db.getCollection('coll-ts').getIndexes()\nto get the indexes of both collections.\nThe normal collection has two indexes, one that was created automatically for the _id key and the one that we created manually:\n\n// coll-ord:\n[\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"_id\" : 1\n        },\n        \"name\" : \"_id_\"\n    },\n    {\n        \"v\" : 2,\n        \"key\" : {\n            \"timestamp\" : 1.0\n        },\n        \"name\" : \"timestamp_1\",\n        \"unique\" : true\n    }\n]\n\n\nWhat is interesting is that the time series collection has no index at all:\n\n// coll-ts:\n[]\n\n\nThe lack of the index for the _id key of\ncourse means that, by default, the time series collection will have to perform the COLLSCAN operation if we want to\nsearch documents based on _id. The index is probably missing simply to save disc space and storage time,\nstemming from the assumption that the time series collections are mainly used to search based on time. The lack of\nthe index for the timestamp field is much more surprising. Does it mean that time-based searches in time series will\nalso cause COLLCSAN and work slowly? The answer to this question can be found in the documentation:\nThe internal index for a time series collection is not displayed\nSo, there actually is an index, but it is different from those created manually, and even from indexes created\nautomatically for the _id key.\nAs I wrote in another post, indexes are not all the\nsame, so it‚Äôs worth taking a closer look at this one. Let‚Äôs check the query execution plans:\n\ndb.getCollection('coll-ord').find({\"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\")}).explain('executionStats')\ndb.getCollection('coll-ts').find({\"timestamp\" : ISODate(\"2021-10-22T00:00:00.000Z\")}).explain('executionStats')\n\n\n\n//coll-ord\n\"winningPlan\" : {\n    \"stage\" : \"FETCH\",\n    \"inputStage\" : {\n        \"stage\" : \"IXSCAN\",\n        \"keyPattern\" : {\n            \"timestamp\" : 1.0\n        }\n    },\n    \"indexName\" : \"timestamp_1\",\n}\n\n\n\n//coll-ts\n\"winningPlan\" : {\n    \"stage\" : \"COLLSCAN\",\n    \"filter\" : {\n        \"$and\" : [\n            {\n                \"_id\" : {\n                    \"$lte\" : ObjectId(\"6171ff00ffffffffffffffff\")\n                }\n            },\n            {\n                \"_id\" : {\n                    \"$gte\" : ObjectId(\"6171f0f00000000000000000\")\n                }\n            },\n            {\n                \"control.max.timestamp\" : {\n                    \"$_internalExprGte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            },\n            {\n                \"control.min.timestamp\" : {\n                    \"$_internalExprLte\" : ISODate(\"2021-10-22T00:00:00.000Z\")\n                }\n            }\n        ]\n    },\n}\n\n\nIt turns out that while in the case of the regular collection the plan shows the use of the index, in the time series\ncollection we see the COLLSCAN operation. It doesn‚Äôt mean that this operation is slow, though. The execution times of\nboth operations were similar. We will move on to a more detailed time comparison in a moment; for now we should only\nnote that the hidden index in the time series collection follows specific rules, it is not only invisible, but it also\ncannot be seen in the execution plan, although it clearly affects the speed of the search.\nAnd what happens if we add sorting?\n\ndb.getCollection('coll-ts').find({}).sort({timestamp: 1})\n\n\n\n{\n \"ok\" : 0,\n \"errmsg\" : \"PlanExecutor error during aggregation :: caused by :: Sort exceeded memory limit of 104857600 bytes,\n but did not opt in to external sorting.\",\n \"code\" : 292,\n \"codeName\" : \"QueryExceededMemoryLimitNoDiskUseAllowed\"\n}\n\n\nSurprise! The internal index for the time series collection does not have a sorting feature. This means that if we add\nthe sort clause to our query, the operation will take very long, or even fail because of exceeding the memory\nlimit. It is surprising because I did not find any information on this in the documentation. Therefore, if we plan to\nsort our data based on the field with time, we will need to index this field manually. It means, of\ncourse, that the benefits stemming from a lower disc usage and faster saving times will unfortunately diminish.\nSince we are talking about the use of disc space, let‚Äôs check the data size:\n\ndb.getCollection('coll-ts').stats()\ndb.getCollection('coll-ord').stats()\n\n\nWe will compare several fields:\nsize: data size before the compression,\nstorageSize: size of data after the compression,\ntotalIndexSize: size of indexes,\ntotalSize: total size of data and indexes.\nResults are gathered in the table below (in bytes, space is thousand separator):\nField\n      Normal collection\n      Time series collection\n      Diff\n    \nsize\n      570 000 000\n      747 855 228\n      + 31%\n    \nstorageSize\n      175 407 104\n      119 410 688\n      -31%\n    \ntotalIndexSize\n      232 701 952\n      0\n      -100%\n    \ntotalSize\n      408 109 056\n      119 410 688\n      -70%\n    \nAs you can see, raw data of the time series collection may take up more space, but after their compression the new-type\ncollection turns out to be the winner in the size-on-disc category. After adding the size of indexes created in the\nregular collection, the difference will be even greater. Therefore, we must admit that the way time series data are\npacked on the disc is impressive.\nNow it‚Äôs time to compare query execution times for both collections.\nSpeed is all that matters\nUsing the script below (saved as gen-find.sh file), I generated two files containing commands getting documents from\nboth collections based on the\ntime label:\n\n#!/bin/bash\nRANDOM=42\nfor ((i = 1; i <= $1; i++ ));\ndo\n  x=$(( $RANDOM % 10000000))\n  t=$(date -jv +${x}M -f \"%Y-%m-%d %H:%M:%S\" \"2021-10-22 0:00:00\" +%Y-%m-%dT%H:%M:%S)\n  echo \"db.getCollection('$2').find({'timestamp' : new ISODate('$t')})\"\ndone >> find-$2.js\n\n\nThe script takes as parameters: the number of queries, and the name of the collection that we want to search. I\ngenerated a million queries (it may take some time depending on your hardware, so you can start with a lower amount of\nqueries):\n\n./gen-find.sh 1000000 coll-ts\n./gen-find.sh 1000000 coll-ord\n\n\nThen I checked the time of the execution of both query sequences using the command:\n\ntime mongo test find-coll-ts.js\ntime mongo test find-coll-ord.js\n\n\nThe standard collection was a bit slower: 16,854 for coll-ord vs 16,038 for coll-ts. Although the difference is small,\nanother point goes to time series: simple search is slightly faster than in the case of the regular collection.\nBut we‚Äôre yet to discuss the most interesting part. Time series is primarily used for quick aggregate counting. Let‚Äôs\nsee what the comparison looks like when calculating the arithmetic mean in a given time interval.\nThe script below (saved as gen-aggregate.sh) creates a list of queries calculating the arithmetic mean of ratings for\na randomly selected six-hour interval:\n\n#!/bin/bash\nRANDOM=42\nfor ((i = 1; i <= $1; i++ ));\ndo\n  x1=$(( $RANDOM % 10000000))\n  x2=$(( x1 + 360))\n  t1=$(date -jv +${x1}M -f \"%Y-%m-%d %H:%M:%S\" \"2021-10-22 0:00:00\" +%Y-%m-%dT%H:%M:%S)\n  t2=$(date -jv +${x2}M -f \"%Y-%m-%d %H:%M:%S\" \"2021-10-22 0:00:00\" +%Y-%m-%dT%H:%M:%S)\n  echo \"db.getCollection('$2').aggregate([{ \\$match: { \"timestamp\" : \" \\\n    \"{\\$gte:new ISODate('$t1'),\\$lt:new ISODate('$t2')} } },\" \\\n    \"{ \\$group: { _id: null, avg: { \\$avg: \\\"\\$rating\\\" } } }])\"\ndone >> aggregate-$2-$1.js\n\n\nI prepared three script sets with: 10K, 50K and 100K queries for both collections:\n\n./gen-aggregate.sh 10000 coll-ts\n./gen-aggregate.sh 50000 coll-ts\n./gen-aggregate.sh 100000 coll-ts\n\n./gen-aggregate.sh 10000 coll-ord\n./gen-aggregate.sh 50000 coll-ord\n./gen-aggregate.sh 100000 coll-ord\n\n\nI made the measurements using following commands:\n\ntime mongo test aggregate-coll-ts-10000.js\ntime mongo test aggregate-coll-ord-10000.js\n\ntime mongo test aggregate-coll-ts-50000.js\ntime mongo test aggregate-coll-ord-50000.js\n\ntime mongo test aggregate-coll-ts-100000.js\ntime mongo test aggregate-coll-ord-100000.js\n\n\n\nThe results are shown in the table below:\nNumber of queries\n      Normal collection [min:sec]\n      Time series [min:sec]\n      Diff\n    \n10000\n      0:21,947\n      0:16,835\n      -23%\n    \n50000\n      1:37,02\n      1:11,18\n      -26%\n    \n100000\n      4:33,29\n      2:21,37\n      -48%\n    \nAlthough there are far fewer queries this time than in the previous experiment, the differences in times are much\ngreater. It clearly proves that time series collections are indeed spreading their wings when we want to use aggregation\nqueries and have been developed mainly for this purpose. The reason is probably the sequential way of writing data,\nwhich shows a noticeable improvement when running ranged queries. The results clearly show also how much of an impact\nquery performance can have on an element that often doesn‚Äôt get the adequate attention: proper modelling the data.\nLimitations\nOf course, time series collections also have their limitations and should not be used as a golden hammer.\nWe have already mentioned the first one ‚Äî the lack of the primary key index. It stems from the assumption\nthat searches will be primarily based on time, so there is no point in creating an index that will be useful for very\nfew people. Of course, we can create this index ourselves.\nIt is also not possible to sort by time field, which is another inconvenience. If we want to have\nsorting queries, we have to create an additional index.\nAlthough these two missing indexes may seem to be an easy thing to fix, we must remember that it involves additional use\nof disc space as well as longer indexing time during the saving of the document, which means that the benefits\nstemming from the use of time series will be somewhat reduced.\nThe third, and perhaps most import limitation is the immutability of the document. Once saved, documents cannot be\nupdated or deleted. The only way to delete data is to use drop() command or to define retention for the collection\nusing the expireAfterSeconds parameter, which, like TTL indexes, will automatically delete documents certain time\nafter creation.\nThe lack of possibility to manipulate the saved documents will probably be the main reason why programmers will be\nhesitant to use time series. We should mention, however, that the authors of MongoDB will probably add the possibility\nto edit and delete documents in the future:\nWhile we know some of these limitations may be impactful to your current use case, we promise we‚Äôre working on this\nright now and would love for you to provide your feedback!\nSummary\nAdding the ability to store time series in MongoDB is a step in the right direction. First tests show that in certain\ncases the new type of collections really does work better than the regular ones. They use less disc space, are faster at\nsaving and searching by the time. But high performance always comes at a cost, in this case: the cost of reduced flexibility.\nTherefore, the final decision to use time series should be preceded by an analysis of\nadvantages and disadvantages of both in particular cases. We should also hope that the authors of the database are\nworking on improving it and will soon eliminate most limitations.","guid":"https://blog.allegro.tech/2021/12/performance-evaluation-of-timeseries.html","categories":["tech","mongodb","performance","time series"],"isoDate":"2021-12-19T23:00:00.000Z","thumbnail":"images/post-headers/mongodb.png"},{"title":"Clean Architecture Story","link":"https://blog.allegro.tech/2021/12/clean-architecture-story.html","pubDate":"Mon, 13 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Micha≈Ç Kowalcze"],"photo":["https://blog.allegro.tech/img/authors/michal.kowalcze.jpg"],"url":["https://blog.allegro.tech/authors/michal.kowalcze"]}]},"content":"<p><a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">The Clean Architecture</a> concept has been\naround for some time and keeps surfacing in one place or another, yet it is not widely adopted. In this post I would\nlike to introduce this topic in a less conventional way: starting with customer‚Äôs needs and going through various\nstages to present a solution that is clean enough to satisfy concepts from the aforementioned blog (or\n<a href=\"https://www.goodreads.com/book/show/18043011-clean-architecture\">the book</a> with the same name).</p>\n\n<h2 id=\"the-perspective\">The perspective</h2>\n<p>Why do we need software architecture? What is it anyway? An extensive definition can be found in a place a bit unexpected\nfor an agile world ‚Äî an enterprise-architecture definition from <a href=\"https://en.wikipedia.org/wiki/The_Open_Group_Architecture_Framework\">TOGAF</a>:</p>\n\n<ul>\n  <li>The fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and\nin the principles of its design and evolution. (Source: ISO/IEC/IEEE 42010:2011)</li>\n  <li>The structure of components, their inter-relationships, and the principles and guidelines governing their design and\nevolution over time.</li>\n</ul>\n\n<p>And what do we need such a governing structure or shape for? Basically it allows us to make cost/time-efficient choices\nwhen it comes to development. And deployment. And operation. And maintenance.</p>\n\n<p>It also allows us to keep as many options open as possible, so our future choices are not limited by an overcommitment\nfrom the past.</p>\n\n<p>So ‚Äî we have our perspective defined. Let‚Äôs dive into a real-world problem!</p>\n\n<h2 id=\"the-challenge\">The challenge</h2>\n<p>You are a young, promising programmer sitting in a dorm and one afternoon a stranger appears. ‚ÄúI run a small company\nthat delivers packages from furniture shops to customers. I need a database that will allow reservation of slots. Is it\nsomething you are able to deliver?‚Äù ‚ÄúOf course!‚Äù ‚Äî what else could a young, promising programmer answer?</p>\n\n<h2 id=\"the-false-start\">The false start</h2>\n<p>The customer needs a database, so what can we start with? The database schema, of course! We can identify entities with\nease: a transport slot, a schedule, a user (we need some authentication, right?), a ‚Ä¶ something? Okay, perhaps it is\nnot the easiest way. So why don‚Äôt we start with something else?</p>\n\n<p>Let‚Äôs choose the technology to use! Let‚Äôs go with React frontend, Java+Spring backend, some SQL as persistence. To\npresent a clickable version to our customer we need some warm-up work to set up an environment, create a deployable\nservice version or GUI mockups, configure persistence and so on. In general: to pay attention to technical details ‚Äî\ncode necessary to set up something working, of which non-devs are usually not aware. It simply has to be done before we\nstart talking about nitty-gritty for business logic.</p>\n\n<h2 id=\"the-use-case-driven-approach\">The use-case-driven approach</h2>\n<p>What if instead of starting with what we already know ‚Äî how to visualize relationships, how to build a web-system ‚Äî we\nstarted with what we didn‚Äôt know? Simply ‚Äî by asking questions such as: How is the system going to be used? By whom?</p>\n\n<h2 id=\"use-cases\">Use cases</h2>\n<p>In other words ‚Äî what are the use cases for the system? Let‚Äôs define the challenge once more using high-level actors\nand interactions: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_cases.png\" alt=\"Use cases\" /> and pick the first\nrequired interaction: shop makes a reservation. What is required to make a reservation? Hmm, I think that it would be\ngood to get the current schedule in the first place. Why am I using ‚Äúget‚Äù instead of ‚Äúdisplay‚Äù? ‚ÄúDisplay‚Äù already\nsuggests a way of delivering output, when we hear ‚Äúdisplay‚Äù a computer screen comes to our minds, with a web\napplication. Single page web app, of course. ‚ÄúGet‚Äù is more neutral, it does not constrain our vision by a specific\npresentation method. Frankly ‚Äî is there anything wrong with delivering the current schedule over the phone, for\nexample?</p>\n\n<h3 id=\"getting-the-schedule\">Getting the schedule</h3>\n<p>So, we can start thinking about our schedule model ‚Äî let it be a single instance representing a day with slots inside.\nGreat, we have our entities! How to get one? Well, we need to check if there is already a stored schedule and if so\n‚Äî retrieve it from the repository. If the schedule is not available we have to create one. Based on‚Ä¶? Exactly ‚Äî we do\nnot know yet, all we can say is that it will probably be something flexible. Something to discuss with our customer\n‚Äî but this does not prevent us from going forward with our first use case. Logic is indeed simple:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">:</span> <span class=\"nc\">LocalDate</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">{</span>\n  <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">daySchedulerRepository</span><span class=\"p\">.</span><span class=\"k\">get</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n  <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">daySchedule</span> <span class=\"p\">!=</span> <span class=\"k\">null</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">daySchedule</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"kd\">val</span> <span class=\"py\">newSchedule</span> <span class=\"p\">=</span> <span class=\"n\">dayScheduleCreator</span><span class=\"p\">.</span><span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n  <span class=\"k\">return</span> <span class=\"n\">daySchedulerRepository</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">newSchedule</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/6dfeee53554a4ccf37e81aa50a2bd24af7e02cce\">GitHub</a>)</p>\n\n<p>And even with this simple logic we identified a hidden assumption regarding the schedule definition: that there is a\nrecipe for creating a daily schedule. What is more we can test retrieval of a schedule ‚Äî with definition of schedule\ncreator if required ‚Äî without any irrelevant details, like database, UI, framework and so on. Test only business rules,\nwithout unnecessary details.</p>\n\n<h2 id=\"reserving-the-slot\">Reserving the slot</h2>\n<p>To finish the reservation we have to add at least one more use case ‚Äî one for reservation of a free slot. Provided that\nwe re-use existing logic, the interaction is still simple:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">fun</span> <span class=\"nf\">reserve</span><span class=\"p\">(</span><span class=\"n\">slotId</span><span class=\"p\">:</span> <span class=\"nc\">SlotId</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">{</span>\n  <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">getScheduleUseCase</span><span class=\"p\">.</span><span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span> <span class=\"p\">=</span> <span class=\"n\">slotId</span><span class=\"p\">.</span><span class=\"n\">day</span><span class=\"p\">)</span>\n\n  <span class=\"kd\">val</span> <span class=\"py\">modifiedSchedule</span> <span class=\"p\">=</span> <span class=\"n\">daySchedule</span><span class=\"p\">.</span><span class=\"nf\">reserveSlot</span><span class=\"p\">(</span><span class=\"n\">slotId</span><span class=\"p\">.</span><span class=\"n\">index</span><span class=\"p\">)</span>\n\n  <span class=\"k\">return</span> <span class=\"n\">dayScheduleRepository</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">modifiedSchedule</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/7b7961b28107c3c89d40ce69a8383bf9f32337b0\">GitHub</a>)</p>\n\n<p>And, as we can see ‚Äî the slot reservation business rule (and constraint) is implemented at the domain model itself ‚Äî so\nwe are safe, that any other interaction, any other use case, is not going to break these rules. This approach also\nsimplifies testing, as business rules can be verified in separation from the use case interaction logic.</p>\n\n<h2 id=\"where-is-the-clean-architecture\">Where is the ‚ÄúClean Architecture‚Äù?</h2>\n<p>Let‚Äòs stop with business logic for a moment. We created quite thoughtful, extensible code for sure, but why are we\ntalking about ‚ÄúClean‚Äù architecture? We already used Domain-Driven Design and Hexagonal architecture concepts. Is there\nanything more? Imagine that another person is going to help us with implementation. She is not aware of the source code\nyet and simply would like to take a look at the codebase. And she sees: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_case_classes.png\" alt=\"Use case classes\" />\nIt looks like something to her, doesn‚Äòt it? A kind of reservation system! It is not yet another domain service with\nsome methods that have no clear connection with possible uses ‚Äî the list of classes itself describes what the system\ncan do.</p>\n\n<h2 id=\"the-first-assumption\">The first assumption</h2>\n<p>We have a mocked implementation as the schedule creator. It is OK to test logic at the unit test level, but not enough\nto run a prototype.</p>\n\n<p>After a short call with our customer we know more about the daily schedule ‚Äî there are six slots, two hours each,\nstarting at 8:oo a.m. We also know that this recipe for the daily schedule is very, very simple and it is going to be\nchanged soon (e.g. to accommodate for holidays, etc.). All these issues will be solved later, now we are at the\nprototype stage and our desired outcome is to have a working demo for our stranger.</p>\n\n<p>Where to put this simple implementation of the schedule creator? So far, the domain used an interface for that. Are we\ngoing to put an implementation of this interface to the infrastructure package and treat it as something outside the\ndomain? Certainly not! It is not complicated and this is part of the domain itself, we simply replace the mocked\nimplementation of the schedule creator with class specification.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">package</span> <span class=\"nn\">eu.kowalcze.michal.arch.clean.example.domain.model</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">DayScheduleCreator</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">create</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">:</span> <span class=\"nc\">LocalDate</span><span class=\"p\">):</span> <span class=\"nc\">DaySchedule</span> <span class=\"p\">=</span> <span class=\"nc\">DaySchedule</span><span class=\"p\">(</span>\n        <span class=\"n\">scheduleDay</span><span class=\"p\">,</span>\n        <span class=\"nf\">createStandardSlots</span><span class=\"p\">()</span>\n    <span class=\"p\">)</span>\n<span class=\"c1\">//...</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/2792fc31e98d76a610561636f03073dee73fbb47\">GitHub</a>)</p>\n\n<h2 id=\"the-prototype\">The prototype</h2>\n<p>I will not be original here ‚Äî for the first prototype version the REST API sounds like something reasonable. Do we care\nabout other infrastructure at the moment? Persistence? No! In the previous commits a map-based persistence layer is\nused for unit tests and this solution is good enough to start with. As long as the system is not restarted, of course.</p>\n\n<p>What is important at this stage? We are introducing an <strong>API</strong> ‚Äî this is a separate layer, so it is crucial to ensure\nthat domain classes are not exposed to the outside world ‚Äî and that we do not introduce a dependency on the API into\nthe domain.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">package</span> <span class=\"nn\">eu.kowalcze.michal.arch.clean.example.api</span>\n\n<span class=\"nd\">@Controller</span>\n<span class=\"kd\">class</span> <span class=\"nc\">GetScheduleEndpoint</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">getScheduleUseCase</span><span class=\"p\">:</span> <span class=\"nc\">GetScheduleUseCase</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n    <span class=\"nd\">@GetMapping</span><span class=\"p\">(</span><span class=\"s\">\"/schedules/{localDate}\"</span><span class=\"p\">)</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">getSchedules</span><span class=\"p\">(</span><span class=\"nd\">@PathVariable</span> <span class=\"n\">localDate</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">):</span> <span class=\"nc\">DayScheduleDto</span> <span class=\"p\">{</span>\n        <span class=\"kd\">val</span> <span class=\"py\">scheduleDay</span> <span class=\"p\">=</span> <span class=\"nc\">LocalDate</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">localDate</span><span class=\"p\">)</span>\n        <span class=\"kd\">val</span> <span class=\"py\">daySchedule</span> <span class=\"p\">=</span> <span class=\"n\">getScheduleUseCase</span><span class=\"p\">.</span><span class=\"nf\">getSchedule</span><span class=\"p\">(</span><span class=\"n\">scheduleDay</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">daySchedule</span><span class=\"p\">.</span><span class=\"nf\">toApi</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/b1d1c3fe3901d9328bdfaf560331d35131f8224b\">GitHub</a>)</p>\n\n<h2 id=\"the-abstractions\">The abstractions</h2>\n<h3 id=\"use-case\">Use Case</h3>\n<p>Checking the implementation of endpoints (see comments in the code) we can see that conceptually each endpoint executes\nlogic according to the same structure: <img src=\"/img/articles/2021-12-13-clean-architecture-story/use_case_flow.png\" alt=\"Use case flow\" />\nWell, why don‚Äôt we make some abstraction for this? Sounds like a crazy idea? Let‚Äòs check! Based on our code and the\ndiagram above we can identify the <code class=\"language-plaintext highlighter-rouge\">UseCase</code> abstraction ‚Äî something that takes some input (domain input, to be precise)\nand converts it to a (domain) output.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">interface</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">INPUT</span><span class=\"p\">):</span> <span class=\"nc\">OUTPUT</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/006811b49ae4531b96b300c964d3a66d725183bf\">GitHub</a>)</p>\n\n<h3 id=\"use-case-executor\">Use Case Executor</h3>\n<p>Great! We have use cases and I just realized that I would like to have an email in my inbox each time an exception is\nthrown ‚Äî and I do not want to depend on a spring-specific mechanism to do this. A common <code class=\"language-plaintext highlighter-rouge\">UseCaseExecutor</code> will be a\ngreat help to address this non-functional requirement.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">notificationGateway</span><span class=\"p\">:</span> <span class=\"nc\">NotificationGateway</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">INPUT</span><span class=\"p\">,</span> <span class=\"nc\">OUTPUT</span><span class=\"p\">&gt;,</span> <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">INPUT</span><span class=\"p\">):</span> <span class=\"nc\">OUTPUT</span> <span class=\"p\">{</span>\n        <span class=\"k\">try</span> <span class=\"p\">{</span>\n            <span class=\"k\">return</span> <span class=\"n\">useCase</span><span class=\"p\">.</span><span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">Exception</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"n\">notificationGateway</span><span class=\"p\">.</span><span class=\"nf\">notify</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">,</span> <span class=\"n\">e</span><span class=\"p\">)</span>\n            <span class=\"k\">throw</span> <span class=\"n\">e</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/54d3187aed94427bb60af9781d0eec573c8c8db0\">GitHub</a>)</p>\n\n<h3 id=\"framework-independent-response\">Framework-independent response</h3>\n<p>In order to handle the next requirements in our plan we have to change the logic a bit ‚Äî add the possibility of returning\nspring-specific response entities from the executor itself. To make our code reusable in a non-spring world (ktor,\nanyone?) we separated the plain executor from spring specific decorator, so that it is possible to use this code easily\nin other frameworks.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">data class</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;(</span>\n    <span class=\"kd\">val</span> <span class=\"py\">responseCode</span><span class=\"p\">:</span> <span class=\"nc\">Int</span><span class=\"p\">,</span>\n    <span class=\"kd\">val</span> <span class=\"py\">output</span><span class=\"p\">:</span> <span class=\"nc\">API_OUTPUT</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">class</span> <span class=\"nc\">SpringUseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">useCaseExecutor</span><span class=\"p\">:</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">,</span> <span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n        <span class=\"n\">toApiConversion</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">)</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span>\n    <span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"n\">useCaseExecutor</span><span class=\"p\">.</span><span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">useCase</span><span class=\"p\">,</span> <span class=\"n\">input</span><span class=\"p\">,</span> <span class=\"n\">toApiConversion</span><span class=\"p\">).</span><span class=\"nf\">toSpringResponse</span><span class=\"p\">()</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">private</span> <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;.</span><span class=\"nf\">toSpringResponse</span><span class=\"p\">():</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"nc\">API_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"nc\">ResponseEntity</span><span class=\"p\">.</span><span class=\"nf\">status</span><span class=\"p\">(</span><span class=\"n\">responseCode</span><span class=\"p\">).</span><span class=\"nf\">body</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/d44f7f993fab2e749e3048561b3ac4d3cff6fd88\">GitHub</a>)</p>\n\n<h3 id=\"handle-domain-exceptions\">Handle domain exceptions</h3>\n<p>Ooops. Our prototype is running and we observe exceptions resulting in HTTP 500 errors. It would be nice to convert\nthese to dedicated response codes in a reasonable way yet without using much of spring infrastructure, for simplified\nmaintenance (and possible future changes). This can be easily achieved by adding another parameter to use case\nexecution, like this:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kd\">class</span> <span class=\"nc\">UseCaseExecutor</span><span class=\"p\">(</span><span class=\"k\">private</span> <span class=\"kd\">val</span> <span class=\"py\">notificationGateway</span><span class=\"p\">:</span> <span class=\"nc\">NotificationGateway</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">fun</span> <span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;</span> <span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span><span class=\"p\">:</span> <span class=\"nc\">UseCase</span><span class=\"p\">&lt;</span><span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">input</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n        <span class=\"n\">toApiConversion</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">:</span> <span class=\"nc\">DOMAIN_OUTPUT</span><span class=\"p\">)</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;,</span>\n        <span class=\"n\">handledExceptions</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"nc\">ExceptionHandler</span><span class=\"p\">.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">Any</span><span class=\"p\">)?</span> <span class=\"p\">=</span> <span class=\"k\">null</span><span class=\"p\">,</span>\n    <span class=\"p\">):</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">{</span>\n\n        <span class=\"k\">try</span> <span class=\"p\">{</span>\n            <span class=\"kd\">val</span> <span class=\"py\">domainOutput</span> <span class=\"p\">=</span> <span class=\"n\">useCase</span><span class=\"p\">.</span><span class=\"nf\">apply</span><span class=\"p\">(</span><span class=\"n\">input</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"nf\">toApiConversion</span><span class=\"p\">(</span><span class=\"n\">domainOutput</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span> <span class=\"k\">catch</span> <span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">:</span> <span class=\"nc\">Exception</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"c1\">// conceptual logic</span>\n            <span class=\"kd\">val</span> <span class=\"py\">exceptionHandler</span> <span class=\"p\">=</span> <span class=\"nc\">ExceptionHandler</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span>\n            <span class=\"n\">handledExceptions</span><span class=\"o\">?.</span><span class=\"nf\">let</span> <span class=\"p\">{</span> <span class=\"n\">exceptionHandler</span><span class=\"p\">.</span><span class=\"nf\">handledExceptions</span><span class=\"p\">()</span> <span class=\"p\">}</span>\n            <span class=\"k\">return</span> <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">(</span><span class=\"n\">responseCodeIfExceptionIsHandled</span><span class=\"p\">,</span> <span class=\"n\">exceptionHandler</span><span class=\"p\">.</span><span class=\"n\">message</span> <span class=\"o\">?:</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">message</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/ac6763f19e2f3f61adc1f8b02bab6cb1e1a65c11\">GitHub</a>)</p>\n\n<h3 id=\"handle-dto-conversion-exceptions\">Handle DTO conversion exceptions</h3>\n<p>By simply replacing input with:</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">inputProvider</span><span class=\"p\">:</span> <span class=\"nc\">Any</span><span class=\"p\">.()</span> <span class=\"p\">-&gt;</span> <span class=\"nc\">DOMAIN_INPUT</span><span class=\"p\">,</span>\n</code></pre></div></div>\n\n<p>(full commit: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example/commit/a9ef4bb835977a4bd4a62eb754d8563340bd3d4e\">GitHub</a>)</p>\n\n<p>we are able to handle exceptions raised during creation of input domain objects in a uniform way, without any\nadditional try/catches at the endpoint level.</p>\n\n<h2 id=\"the-outcome\">The outcome</h2>\n\n<p>What is the result of our journey across some functional requirements and a bit more non-functional requirements? By\nlooking at the definition of an endpoint we have full documentation of its behaviour, including exceptions. Our code is\neasily portable to some different API (e.g. EJB), we have fully-auditable modifications, and we can exchange layers\nquite freely. Also analysis of whole service is simplified, as possible use cases are explicitely stated.</p>\n\n<div class=\"language-kotlin highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nd\">@PutMapping</span><span class=\"p\">(</span><span class=\"s\">\"/schedules/{localDate}/{index}\"</span><span class=\"p\">,</span> <span class=\"n\">produces</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s\">\"application/json\"</span><span class=\"p\">],</span> <span class=\"n\">consumes</span> <span class=\"p\">=</span> <span class=\"p\">[</span><span class=\"s\">\"application/json\"</span><span class=\"p\">])</span>\n<span class=\"k\">fun</span> <span class=\"nf\">getSchedules</span><span class=\"p\">(</span><span class=\"nd\">@PathVariable</span> <span class=\"n\">localDate</span><span class=\"p\">:</span> <span class=\"nc\">String</span><span class=\"p\">,</span> <span class=\"nd\">@PathVariable</span> <span class=\"n\">index</span><span class=\"p\">:</span> <span class=\"nc\">Int</span><span class=\"p\">):</span> <span class=\"nc\">ResponseEntity</span><span class=\"p\">&lt;</span><span class=\"err\">*</span><span class=\"p\">&gt;</span> <span class=\"p\">=</span>\n    <span class=\"n\">useCaseExecutor</span><span class=\"p\">.</span><span class=\"nf\">execute</span><span class=\"p\">(</span>\n        <span class=\"n\">useCase</span> <span class=\"p\">=</span> <span class=\"n\">reserveSlotUseCase</span><span class=\"p\">,</span>\n        <span class=\"n\">inputProvider</span> <span class=\"p\">=</span> <span class=\"p\">{</span> <span class=\"nc\">SlotId</span><span class=\"p\">(</span><span class=\"nc\">LocalDate</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">localDate</span><span class=\"p\">),</span> <span class=\"n\">index</span><span class=\"p\">)</span> <span class=\"p\">},</span>\n        <span class=\"n\">toApiConversion</span> <span class=\"p\">=</span> <span class=\"p\">{</span>\n            <span class=\"kd\">val</span> <span class=\"py\">dayScheduleDto</span> <span class=\"p\">=</span> <span class=\"n\">it</span><span class=\"p\">.</span><span class=\"nf\">toApi</span><span class=\"p\">()</span>\n            <span class=\"nc\">UseCaseApiResult</span><span class=\"p\">(</span><span class=\"nc\">HttpServletResponse</span><span class=\"p\">.</span><span class=\"nc\">SC_ACCEPTED</span><span class=\"p\">,</span> <span class=\"n\">dayScheduleDto</span><span class=\"p\">)</span>\n        <span class=\"p\">},</span>\n        <span class=\"n\">handledExceptions</span> <span class=\"p\">=</span> <span class=\"p\">{</span>\n            <span class=\"nf\">exception</span><span class=\"p\">(</span><span class=\"nc\">InvalidSlotIndexException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">,</span> <span class=\"nc\">UNPROCESSABLE_ENTITY</span><span class=\"p\">,</span> <span class=\"s\">\"INVALID-SLOT-ID\"</span><span class=\"p\">)</span>\n            <span class=\"nf\">exception</span><span class=\"p\">(</span><span class=\"nc\">SlotAlreadyReservedException</span><span class=\"o\">::</span><span class=\"k\">class</span><span class=\"p\">,</span> <span class=\"nc\">CONFLICT</span><span class=\"p\">,</span> <span class=\"s\">\"SLOT-ALREADY-RESERVED\"</span><span class=\"p\">)</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>(repository: <a href=\"https://github.com/michal-kowalcze/clean-architecture-example\">GitHub</a>)</p>\n\n<p>A simple evaluation of our solution with measures mentioned at the beginning:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th style=\"text-align: left\">Aspect</th>\n      <th style=\"text-align: left\">Evaluation</th>\n      <th style=\"text-align: center\">Has advantage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td style=\"text-align: left\">Development</td>\n      <td style=\"text-align: left\"><code class=\"language-plaintext highlighter-rouge\">UseCase</code> abstraction forces unification of approach across different teams in a more significant way than standard service approach.</td>\n      <td style=\"text-align: center\">‚úì</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Deployment</td>\n      <td style=\"text-align: left\">We did not consider deployment in our example. It certainly is not going to be different/harder than in case of hexagonal architecture.</td>\n      <td style=\"text-align: center\">¬†</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Operation</td>\n      <td style=\"text-align: left\">Use case-based approach reveals operation of the system, which reduces learning curve for both development and maintenance.</td>\n      <td style=\"text-align: center\">‚úì</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Maintenance</td>\n      <td style=\"text-align: left\">Entry threshold might be lower compared to hexagonal approach, as service is separated horizontally (into layers) and vertically (into use cases with common domain model).</td>\n      <td style=\"text-align: center\">‚úì</td>\n    </tr>\n    <tr>\n      <td style=\"text-align: left\">Keeping options open</td>\n      <td style=\"text-align: left\">Similar to hexagonal architecture approach.</td>\n      <td style=\"text-align: center\">¬†</td>\n    </tr>\n  </tbody>\n</table>\n\n<h3 id=\"tldr\">TL;DR</h3>\n<p>It is like hexagonal architecture with one additional dimension, composed of use cases, giving better insight into\noperations of a system and streamlining development and maintenance. Solution that was created during this narrative\nallows for creation of a self-documenting API endpoint.</p>\n\n<h2 id=\"high-level-overview\">High-level overview</h2>\n<p>With all this read we can switch our view to the high-level perspective:</p>\n\n<p><img src=\"/img/articles/2021-12-13-clean-architecture-story/clean_architecture_diagram.png\" alt=\"The Clean Architecture Diagram\" /></p>\n\n<p>and describe abstractions. Starting from the inside we have:</p>\n<ul>\n  <li><strong>Domain Model</strong>, <strong>Services</strong> and <strong>Gateways</strong>, which are responsible for defining\nbusiness rules for the domain.</li>\n  <li><strong>UseCase</strong>, which orchestrates execution of business rules.</li>\n  <li><strong>UseCaseExecutor</strong> providing common behavior for all use cases.</li>\n  <li><strong>API</strong> connecting service with the outside world.</li>\n  <li><strong>Implementation of gateways</strong>, which connects with other services or persistence providers.</li>\n  <li><strong>Configuration</strong>, responsible for gluing all elements together.</li>\n</ul>\n\n<p>I hope that you enjoy this simple story and find the concept of\n<a href=\"https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html\">the Clean Architecture</a> useful.\nThank you for reading!</p>\n","contentSnippet":"The Clean Architecture concept has been\naround for some time and keeps surfacing in one place or another, yet it is not widely adopted. In this post I would\nlike to introduce this topic in a less conventional way: starting with customer‚Äôs needs and going through various\nstages to present a solution that is clean enough to satisfy concepts from the aforementioned blog (or\nthe book with the same name).\nThe perspective\nWhy do we need software architecture? What is it anyway? An extensive definition can be found in a place a bit unexpected\nfor an agile world ‚Äî an enterprise-architecture definition from TOGAF:\nThe fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and\nin the principles of its design and evolution. (Source: ISO/IEC/IEEE 42010:2011)\nThe structure of components, their inter-relationships, and the principles and guidelines governing their design and\nevolution over time.\nAnd what do we need such a governing structure or shape for? Basically it allows us to make cost/time-efficient choices\nwhen it comes to development. And deployment. And operation. And maintenance.\nIt also allows us to keep as many options open as possible, so our future choices are not limited by an overcommitment\nfrom the past.\nSo ‚Äî we have our perspective defined. Let‚Äôs dive into a real-world problem!\nThe challenge\nYou are a young, promising programmer sitting in a dorm and one afternoon a stranger appears. ‚ÄúI run a small company\nthat delivers packages from furniture shops to customers. I need a database that will allow reservation of slots. Is it\nsomething you are able to deliver?‚Äù ‚ÄúOf course!‚Äù ‚Äî what else could a young, promising programmer answer?\nThe false start\nThe customer needs a database, so what can we start with? The database schema, of course! We can identify entities with\nease: a transport slot, a schedule, a user (we need some authentication, right?), a ‚Ä¶ something? Okay, perhaps it is\nnot the easiest way. So why don‚Äôt we start with something else?\nLet‚Äôs choose the technology to use! Let‚Äôs go with React frontend, Java+Spring backend, some SQL as persistence. To\npresent a clickable version to our customer we need some warm-up work to set up an environment, create a deployable\nservice version or GUI mockups, configure persistence and so on. In general: to pay attention to technical details ‚Äî\ncode necessary to set up something working, of which non-devs are usually not aware. It simply has to be done before we\nstart talking about nitty-gritty for business logic.\nThe use-case-driven approach\nWhat if instead of starting with what we already know ‚Äî how to visualize relationships, how to build a web-system ‚Äî we\nstarted with what we didn‚Äôt know? Simply ‚Äî by asking questions such as: How is the system going to be used? By whom?\nUse cases\nIn other words ‚Äî what are the use cases for the system? Let‚Äôs define the challenge once more using high-level actors\nand interactions:  and pick the first\nrequired interaction: shop makes a reservation. What is required to make a reservation? Hmm, I think that it would be\ngood to get the current schedule in the first place. Why am I using ‚Äúget‚Äù instead of ‚Äúdisplay‚Äù? ‚ÄúDisplay‚Äù already\nsuggests a way of delivering output, when we hear ‚Äúdisplay‚Äù a computer screen comes to our minds, with a web\napplication. Single page web app, of course. ‚ÄúGet‚Äù is more neutral, it does not constrain our vision by a specific\npresentation method. Frankly ‚Äî is there anything wrong with delivering the current schedule over the phone, for\nexample?\nGetting the schedule\nSo, we can start thinking about our schedule model ‚Äî let it be a single instance representing a day with slots inside.\nGreat, we have our entities! How to get one? Well, we need to check if there is already a stored schedule and if so\n‚Äî retrieve it from the repository. If the schedule is not available we have to create one. Based on‚Ä¶? Exactly ‚Äî we do\nnot know yet, all we can say is that it will probably be something flexible. Something to discuss with our customer\n‚Äî but this does not prevent us from going forward with our first use case. Logic is indeed simple:\n\nfun getSchedule(scheduleDay: LocalDate): DaySchedule {\n  val daySchedule = daySchedulerRepository.get(scheduleDay)\n  if (daySchedule != null) {\n    return daySchedule\n  }\n\n  val newSchedule = dayScheduleCreator.create(scheduleDay)\n  return daySchedulerRepository.save(newSchedule)\n}\n\n\n(full commit: GitHub)\nAnd even with this simple logic we identified a hidden assumption regarding the schedule definition: that there is a\nrecipe for creating a daily schedule. What is more we can test retrieval of a schedule ‚Äî with definition of schedule\ncreator if required ‚Äî without any irrelevant details, like database, UI, framework and so on. Test only business rules,\nwithout unnecessary details.\nReserving the slot\nTo finish the reservation we have to add at least one more use case ‚Äî one for reservation of a free slot. Provided that\nwe re-use existing logic, the interaction is still simple:\n\nfun reserve(slotId: SlotId): DaySchedule {\n  val daySchedule = getScheduleUseCase.getSchedule(scheduleDay = slotId.day)\n\n  val modifiedSchedule = daySchedule.reserveSlot(slotId.index)\n\n  return dayScheduleRepository.save(modifiedSchedule)\n}\n\n\n(full commit: GitHub)\nAnd, as we can see ‚Äî the slot reservation business rule (and constraint) is implemented at the domain model itself ‚Äî so\nwe are safe, that any other interaction, any other use case, is not going to break these rules. This approach also\nsimplifies testing, as business rules can be verified in separation from the use case interaction logic.\nWhere is the ‚ÄúClean Architecture‚Äù?\nLet‚Äòs stop with business logic for a moment. We created quite thoughtful, extensible code for sure, but why are we\ntalking about ‚ÄúClean‚Äù architecture? We already used Domain-Driven Design and Hexagonal architecture concepts. Is there\nanything more? Imagine that another person is going to help us with implementation. She is not aware of the source code\nyet and simply would like to take a look at the codebase. And she sees: \nIt looks like something to her, doesn‚Äòt it? A kind of reservation system! It is not yet another domain service with\nsome methods that have no clear connection with possible uses ‚Äî the list of classes itself describes what the system\ncan do.\nThe first assumption\nWe have a mocked implementation as the schedule creator. It is OK to test logic at the unit test level, but not enough\nto run a prototype.\nAfter a short call with our customer we know more about the daily schedule ‚Äî there are six slots, two hours each,\nstarting at 8:oo a.m. We also know that this recipe for the daily schedule is very, very simple and it is going to be\nchanged soon (e.g. to accommodate for holidays, etc.). All these issues will be solved later, now we are at the\nprototype stage and our desired outcome is to have a working demo for our stranger.\nWhere to put this simple implementation of the schedule creator? So far, the domain used an interface for that. Are we\ngoing to put an implementation of this interface to the infrastructure package and treat it as something outside the\ndomain? Certainly not! It is not complicated and this is part of the domain itself, we simply replace the mocked\nimplementation of the schedule creator with class specification.\n\npackage eu.kowalcze.michal.arch.clean.example.domain.model\n\nclass DayScheduleCreator {\n    fun create(scheduleDay: LocalDate): DaySchedule = DaySchedule(\n        scheduleDay,\n        createStandardSlots()\n    )\n//...\n}\n\n\n(full commit: GitHub)\nThe prototype\nI will not be original here ‚Äî for the first prototype version the REST API sounds like something reasonable. Do we care\nabout other infrastructure at the moment? Persistence? No! In the previous commits a map-based persistence layer is\nused for unit tests and this solution is good enough to start with. As long as the system is not restarted, of course.\nWhat is important at this stage? We are introducing an API ‚Äî this is a separate layer, so it is crucial to ensure\nthat domain classes are not exposed to the outside world ‚Äî and that we do not introduce a dependency on the API into\nthe domain.\n\npackage eu.kowalcze.michal.arch.clean.example.api\n\n@Controller\nclass GetScheduleEndpoint(private val getScheduleUseCase: GetScheduleUseCase) {\n\n    @GetMapping(\"/schedules/{localDate}\")\n    fun getSchedules(@PathVariable localDate: String): DayScheduleDto {\n        val scheduleDay = LocalDate.parse(localDate)\n        val daySchedule = getScheduleUseCase.getSchedule(scheduleDay)\n        return daySchedule.toApi()\n    }\n\n}\n\n\n(full commit: GitHub)\nThe abstractions\nUse Case\nChecking the implementation of endpoints (see comments in the code) we can see that conceptually each endpoint executes\nlogic according to the same structure: \nWell, why don‚Äôt we make some abstraction for this? Sounds like a crazy idea? Let‚Äòs check! Based on our code and the\ndiagram above we can identify the UseCase abstraction ‚Äî something that takes some input (domain input, to be precise)\nand converts it to a (domain) output.\n\ninterface UseCase<INPUT, OUTPUT> {\n    fun apply(input: INPUT): OUTPUT\n}\n\n\n(full commit: GitHub)\nUse Case Executor\nGreat! We have use cases and I just realized that I would like to have an email in my inbox each time an exception is\nthrown ‚Äî and I do not want to depend on a spring-specific mechanism to do this. A common UseCaseExecutor will be a\ngreat help to address this non-functional requirement.\n\nclass UseCaseExecutor(private val notificationGateway: NotificationGateway) {\n    fun <INPUT, OUTPUT> execute(useCase: UseCase<INPUT, OUTPUT>, input: INPUT): OUTPUT {\n        try {\n            return useCase.apply(input)\n        } catch (e: Exception) {\n            notificationGateway.notify(useCase, e)\n            throw e\n        }\n    }\n}\n\n\n(full commit: GitHub)\nFramework-independent response\nIn order to handle the next requirements in our plan we have to change the logic a bit ‚Äî add the possibility of returning\nspring-specific response entities from the executor itself. To make our code reusable in a non-spring world (ktor,\nanyone?) we separated the plain executor from spring specific decorator, so that it is possible to use this code easily\nin other frameworks.\n\ndata class UseCaseApiResult<API_OUTPUT>(\n    val responseCode: Int,\n    val output: API_OUTPUT,\n)\n\nclass SpringUseCaseExecutor(private val useCaseExecutor: UseCaseExecutor) {\n    fun <DOMAIN_INPUT, DOMAIN_OUTPUT, API_OUTPUT> execute(\n        useCase: UseCase<DOMAIN_INPUT, DOMAIN_OUTPUT>,\n        input: DOMAIN_INPUT,\n        toApiConversion: (domainOutput: DOMAIN_OUTPUT) -> UseCaseApiResult<API_OUTPUT>\n    ): ResponseEntity<API_OUTPUT> {\n        return useCaseExecutor.execute(useCase, input, toApiConversion).toSpringResponse()\n    }\n}\n\nprivate fun <API_OUTPUT> UseCaseApiResult<API_OUTPUT>.toSpringResponse(): ResponseEntity<API_OUTPUT> =\n    ResponseEntity.status(responseCode).body(output)\n\n\n(full commit: GitHub)\nHandle domain exceptions\nOoops. Our prototype is running and we observe exceptions resulting in HTTP 500 errors. It would be nice to convert\nthese to dedicated response codes in a reasonable way yet without using much of spring infrastructure, for simplified\nmaintenance (and possible future changes). This can be easily achieved by adding another parameter to use case\nexecution, like this:\n\nclass UseCaseExecutor(private val notificationGateway: NotificationGateway) {\n    fun <DOMAIN_INPUT, DOMAIN_OUTPUT> execute(\n        useCase: UseCase<DOMAIN_INPUT, DOMAIN_OUTPUT>,\n        input: DOMAIN_INPUT,\n        toApiConversion: (domainOutput: DOMAIN_OUTPUT) -> UseCaseApiResult<*>,\n        handledExceptions: (ExceptionHandler.() -> Any)? = null,\n    ): UseCaseApiResult<*> {\n\n        try {\n            val domainOutput = useCase.apply(input)\n            return toApiConversion(domainOutput)\n        } catch (e: Exception) {\n            // conceptual logic\n            val exceptionHandler = ExceptionHandler(e)\n            handledExceptions?.let { exceptionHandler.handledExceptions() }\n            return UseCaseApiResult(responseCodeIfExceptionIsHandled, exceptionHandler.message ?: e.message)\n        }\n    }\n}\n\n\n(full commit: GitHub)\nHandle DTO conversion exceptions\nBy simply replacing input with:\n\ninputProvider: Any.() -> DOMAIN_INPUT,\n\n\n(full commit: GitHub)\nwe are able to handle exceptions raised during creation of input domain objects in a uniform way, without any\nadditional try/catches at the endpoint level.\nThe outcome\nWhat is the result of our journey across some functional requirements and a bit more non-functional requirements? By\nlooking at the definition of an endpoint we have full documentation of its behaviour, including exceptions. Our code is\neasily portable to some different API (e.g. EJB), we have fully-auditable modifications, and we can exchange layers\nquite freely. Also analysis of whole service is simplified, as possible use cases are explicitely stated.\n\n@PutMapping(\"/schedules/{localDate}/{index}\", produces = [\"application/json\"], consumes = [\"application/json\"])\nfun getSchedules(@PathVariable localDate: String, @PathVariable index: Int): ResponseEntity<*> =\n    useCaseExecutor.execute(\n        useCase = reserveSlotUseCase,\n        inputProvider = { SlotId(LocalDate.parse(localDate), index) },\n        toApiConversion = {\n            val dayScheduleDto = it.toApi()\n            UseCaseApiResult(HttpServletResponse.SC_ACCEPTED, dayScheduleDto)\n        },\n        handledExceptions = {\n            exception(InvalidSlotIndexException::class, UNPROCESSABLE_ENTITY, \"INVALID-SLOT-ID\")\n            exception(SlotAlreadyReservedException::class, CONFLICT, \"SLOT-ALREADY-RESERVED\")\n        },\n    )\n\n\n(repository: GitHub)\nA simple evaluation of our solution with measures mentioned at the beginning:\nAspect\n      Evaluation\n      Has advantage\n    \nDevelopment\n      UseCase abstraction forces unification of approach across different teams in a more significant way than standard service approach.\n      ‚úì\n    \nDeployment\n      We did not consider deployment in our example. It certainly is not going to be different/harder than in case of hexagonal architecture.\n      ¬†\n    \nOperation\n      Use case-based approach reveals operation of the system, which reduces learning curve for both development and maintenance.\n      ‚úì\n    \nMaintenance\n      Entry threshold might be lower compared to hexagonal approach, as service is separated horizontally (into layers) and vertically (into use cases with common domain model).\n      ‚úì\n    \nKeeping options open\n      Similar to hexagonal architecture approach.\n      ¬†\n    \nTL;DR\nIt is like hexagonal architecture with one additional dimension, composed of use cases, giving better insight into\noperations of a system and streamlining development and maintenance. Solution that was created during this narrative\nallows for creation of a self-documenting API endpoint.\nHigh-level overview\nWith all this read we can switch our view to the high-level perspective:\n\nand describe abstractions. Starting from the inside we have:\nDomain Model, Services and Gateways, which are responsible for defining\nbusiness rules for the domain.\nUseCase, which orchestrates execution of business rules.\nUseCaseExecutor providing common behavior for all use cases.\nAPI connecting service with the outside world.\nImplementation of gateways, which connects with other services or persistence providers.\nConfiguration, responsible for gluing all elements together.\nI hope that you enjoy this simple story and find the concept of\nthe Clean Architecture useful.\nThank you for reading!","guid":"https://blog.allegro.tech/2021/12/clean-architecture-story.html","categories":["tech","architecture","clean-architecture","ddd","kotlin"],"isoDate":"2021-12-12T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"},{"title":"Which skills to choose in order to be more valuable","link":"https://blog.allegro.tech/2021/12/choose-your-skills.html","pubDate":"Thu, 09 Dec 2021 00:00:00 +0100","authors":{"author":[{"name":["Piotr Prusi≈Ñski"],"photo":["https://blog.allegro.tech/img/authors/piotr.prusinski.jpg"],"url":["https://blog.allegro.tech/authors/piotr.prusinski"]}]},"content":"<p>At some point in your career, you realize that it‚Äôs time to try to advance through the hierarchy. You think you are\ndoing a good job. You are constantly developing and learning something new. But at the same time, someone you know, with\nmuch less experience and knowledge than you, has long been higher up the hierarchy than you. Then you ask yourself:\n<em>what is wrong with me</em>? In my case, the answer turned out to be properly gathering the expectations concerning my skills\nand work.</p>\n\n<p>Below, you can read my insight about it. You will find out how you can steer your development, so that you become much\nmore valuable to your team, leader or company and how to combine all this with your talents and interests.</p>\n\n<h2 id=\"how-to-start-the-development\">How to start the development</h2>\n\n<p>As a member of a project team, you want to deliver a fully valuable solution. The key to success is to discover what‚Äôs\nbehind the word <em>valuable</em> to your client. Is it enough to gather requirements? Unfortunately not. To find out exactly\nwhat stands behind the word <em>valuable</em> you need to use a prototyping method, such as an iterative\n<a href=\"https://www.wikipedia.org/wiki/Minimum_viable_product\">MVP</a> (Minimum Viable Product) approach. For example, suppose you\nhave to paint your daughter‚Äôs room. You have several choices. You can choose the colour by yourself and paint the room\nwithout asking her for her opinion. The chance she will like the colour is small. So you will make a non-compliant\nproduct. You can bring her a sampler and ask her to choose a colour. The final colour when applied to the wall, may turn\nout to be different and then, the product will also not be as expected. You can also ask her about the colour and buy a\nfew samplers, paint a piece of wall with them and ask if any meet the requirements, if yes then paint the whole room\nwith the chosen colour, if not, buy another 5 colours and so on over and over until you get it right. This is the MVP\nprocess. First you gather requirements, then you prototype, which means you make a real product. You present it and\nreceive feedback (you learn) and decide what to do next. You repeat this process until you meet the client‚Äôs\nexpectations. This approach gives you the assurance that you have done exactly what was needed.</p>\n\n<p>Imagine that your skills and the work you do are products. We already know that to make a valuable product you need to\ngather requirements and prototype with the client. Your customer who cares most about your work and skills are the team,\nthe leader and the company you work for. However, what you do must also suit you. If you want your work and your\nknowledge to be of value to them, you need to gather their requirements first. Then quickly show the result and make\nsure it‚Äôs what they wanted. You are probably wondering what this prototyping is for in the case of your work? You have\ndone some work, it has been accepted so what more is there to talk about. There could be lots of reasons. You could have\nspent too much time on it, it‚Äôs even possible that what you did is wrong. You would have to put in a huge amount of time\nand work to improve it, which is why it was accepted as it is. Without prototyping, your work might appear to be of good\nquality to you, but the team might not be happy with it.</p>\n\n<p>Let‚Äôs start with requirements gathering. Unfortunately it is impossible to gather universal requirements for a given\nposition since these requirements are different at different companies and even teams. The same person who has certain\nskills and performs certain tasks may be a senior in one company, where in another, he or she may be a junior. In one of\nthe companies I worked for, a good programmer was required to know the products and the business, then the tenure at the\ncompany mattered as well and being a support for the business. Performing a large number of tasks was not so important.\nIn another company, there were people who knew their product very well and were an excellent support to the business.\nHowever, they did not perform their tasks on time and did not take on issues that required them to undertake a lot of\nresponsibility. They still had junior or mid-level status.</p>\n\n<p><img src=\"/img/articles/2021-12-09-choose-your-skills/expectations.png\" alt=\"Expectations\" />\nThis begs the question: how do I gather my skills requirements?</p>\n\n<h2 id=\"assess-yourself-with-some-evaluation-sheets\">Assess yourself with some evaluation sheets</h2>\n\n<p>At Allegro, in the case of programmers and testers, we have a list of expectations prepared within the leadership\ncommunity. With such a list, it is much easier to collect requirements and talk about your own development towards\npromotion (in the team or in the company).</p>\n\n<p>In my case, after working at Allegro for a while, I decided that it might be a good idea to finally get my development\non track. My leader asked me to assess myself using a requirements sheet. This allowed me to identify the current stage\nI was at. The same sheet about me was completed by my leader. Then we calibrated ourselves. This sheet is divided into 3\nmain parts:</p>\n\n<ul>\n  <li>Technical knowledge</li>\n  <li>Attitude</li>\n  <li>Product knowledge</li>\n</ul>\n\n<p>Each part has a list of skills, each with an extensive description. For example, one of the descriptions of technical\nknowledge looked like this:</p>\n\n<ul>\n  <li>You know your technological stack very well (systems, applications, languages, tools). Having a technical problem to\nsolve, you know what your technological abilities are, and you know how to use them to achieve your goal.</li>\n</ul>\n\n<p>I rated each skill on a scale of 1 to 5. Ideally, I would like each skill to be close to a 5. If I rated myself\nsomewhere at a 4 and my leader rated me a 3, during calibration, I found out that the requirements for that skill were\ndifferent from what I thought. He said, there were things I hadn‚Äôt used yet. To have a 4 at this point I need to know\nthem e.g. Cassandra database. With that I learned the first relevant requirement that I didn‚Äôt know about. After the\nwhole process of gathering requirements this way, I already knew what I needed to work on. Now all I had to do was\nselect a few of them and start working. But how to choose these requirements, which I will implement with passion and\nactually do them well?</p>\n\n<h2 id=\"how-to-choose-skills-to-improve\">How to choose skills to improve?</h2>\n\n<p>There are many strategies on how to choose skills to develop. All in all, you need to use the right method for the right\ntype of skill to get a good result. The first step is to get a mentor. Usually it will be your leader, but it can also\nbe an experienced team member, e.g. a senior.</p>\n\n<p>Leaders at Allegro gain their development knowledge from many available training sessions. You should find a similar\nperson in your environment. Such a person will help you on your way and will simulate the customer of the product that\nyou will be making. Show him or her a list of things you should develop. Ask them which are the most important for your\nteam. It is possible that the list of things will become very short. Now just pick two or three things and prepare\nprototypes. You can choose them intuitively or match them with your talents. Just take a Gallup\ntest (<a href=\"https://www.gallup.com/cliftonstrengths/en/home.aspx\">CliftonStrengths</a>) to discover them. This test will help\nyou choose a strategy for developing a particular skill. This test will work well for soft skills. For hard skills\na <a href=\"https://en.wikipedia.org/wiki/Lean_startup\">lean startup strategy</a> may be better. Below are two examples of what this\ncan look like in practice.</p>\n\n<h2 id=\"how-gallup-test-can-help-you-find-activities-that-improve-you\">How Gallup test can help you find activities that improve you</h2>\n\n<p>The <a href=\"https://www.gallup.com/cliftonstrengths/en/home.aspx\">Gallup test</a>\nhelps you find activities that you naturally enjoy doing. This natural response to certain types of information,\nsituations and people in the context of the Gallup test is called a talent. The test result is divided into four main\ndomains:</p>\n\n<ul>\n  <li>Executing</li>\n  <li>Influencing</li>\n  <li>Relationship Building</li>\n  <li>Strategic Thinking</li>\n</ul>\n\n<p>Some of these will have a direct impact on your work, others will not. For example a description of your character in\nthe context of building relationships with people will be interesting information and will certainly raise your\nawareness, but will not help us in our challenge. Other talents that come out of the challenge may fit into the skill\nyou are developing or show how you should develop it.</p>\n\n<p>Of course with these talents, the knowledge itself that you have a talent does not immediately translate into skills,\nfor example finding out that you have a talent for standing on your head, it does not mean that you will immediately\nstand on your head. You have to develop this talent, and you probably are already doing similar things because you\nsimply have a talent for it, and you enjoy it. So the Gallup test shows the things you should focus on. They are so\ngeneric that you can easily match them to your problems.</p>\n\n<p><img src=\"/img/articles/2021-12-09-choose-your-skills/gallup.png\" alt=\"Gallup\" /></p>\n\n<p>Here‚Äôs what it was like for me. I did the Gallup test, and it came out that I am a learner. I like to learn, and I\nshould share knowledge - which should make me happy. Comparing this talent to the list of skills I should develop, I\nfound out that I don‚Äôt share knowledge with others. I decided that a good form of knowledge transfer is to make\npresentations. I completed training in making presentations. I also estimated the time for preparation and set a\ndeadline. While preparing, I realized that this knowledge sharing strategy was not for me. I finished this presentation\nand decided to change my knowledge sharing strategy. Such a change in strategy is called a pivot. I discussed the\nproblem with my mentor and decided to make a pivot towards writing articles. Being in this situation you have two\nchoices: either you do a pivot, which means you change your strategy, or you persevere, which means you continue to\ndevelop using your current strategy, after getting proper feedback from your mentor and team of course. Let us now look\nat the second type of skill.</p>\n\n<h2 id=\"how-startup-method-can-help-you-develop-your-skills\">How startup method can help you develop your skills</h2>\n\n<p>Let us divide hard skills into two groups. In the first group there are skills you acquire while learning by yourself,\ne.g. a new programming language. For this you need internal motivation and discipline to achieve the expected high\nlevel. The second group is the one where you acquire skills automatically e.g. you use some framework in your project.\nFor the first group you can use the startup method.\n<img src=\"/img/articles/2021-12-09-choose-your-skills/startup.png\" alt=\"Startup\" />\nAn example scenario of what the process of developing hard skills might look like:</p>\n\n<ol>\n  <li>Choose one skill with your mentor (the order does not matter). Additionally, collect the requirements.</li>\n  <li>Estimate how long it will take you to develop that skill. Put it on your calendar.</li>\n  <li>Together with your mentor, set a first stage, i.e. the first prototype of your work and work on it for e.g. 2-3\nweeks.</li>\n  <li>After this time, show what you have done to your mentor or the whole team.</li>\n  <li>Draw conclusions. Collect feedback. Do you like doing it? Did the result meet their expectations?</li>\n  <li>If not, make a pivot to another strategy or take another skill. If so, improve what you collected during the feedback\nand keep developing.</li>\n</ol>\n\n<p>In the worst case scenario you will go through all the skills. You will then have a fairly broad knowledge base. I have\nnever heard the opinion that someone learned something and now regrets it. In a positive scenario you will probably find\nsomething you do well and enjoy it.</p>\n\n<p>We will now look at the second group of skills, which you develop automatically while working on a project. Here again,\nyou should first consult with your mentor on the best strategy to follow. There are two strategies in such cases: to\ngain deep knowledge in one of these skills or to learn a little about all of them. The mentor should know what the team\nexpects from a specialist in the given position. This automatically affects the implementation of the tasks in the\nproject:</p>\n\n<ol>\n  <li>You perform tasks only in the domain you know and after some time you become a specialist in this area, but you do\nnot take tasks from another domain that is new to you. Your value is the speed of task completion.</li>\n  <li>You always take tasks from a new domain, you are brave and not afraid of challenges. You will then have a broad\ngeneral knowledge of all products but will complete tasks much more slowly.</li>\n</ol>\n\n<p>This is just one example. It is important that the mentor is clear about his requirements and that he gives you feedback\nafter you have done a few of these tasks to make sure that this is what you wanted.</p>\n\n<h2 id=\"pivot-or-persevere\">Pivot or persevere</h2>\n\n<p>Finally, let me explain the startup terms for pivot. The term pivot means to change from one strategy to another. For\nexample, I did one pivot during my development process. I was developing the skill of knowledge sharing. I had several\nstrategies to choose from:</p>\n\n<ul>\n  <li>Making presentations</li>\n  <li>Writing articles</li>\n  <li>Training preparation</li>\n</ul>\n\n<p>I made a presentation and after collecting feedback I had to make a <code class=\"language-plaintext highlighter-rouge\">pivot</code> and changed my strategy to another one. I\nchose to share knowledge by writing articles. This article is the result of that. If this strategy works for me then I\nwill do a <code class=\"language-plaintext highlighter-rouge\">persevere</code> and continue using the chosen strategy.</p>\n\n<h2 id=\"summary\">Summary</h2>\n\n<p>If you feel that you should already be in a higher position it is likely that the expectations for your job are quite\ndifferent than you think. To change this you should first find a mentor and then, with their help, gather the\nrequirements. A good place to start is with a list of requirements or a list of skills to begin discussing with your\nmentor. From this list you should select a few skills and then choose appropriate strategies for their development. To\nmake it easier for you to choose a strategy, do a Gallup test. Additionally, you can use the startup MVP approach.\nUltimately, the key to success is to gather requirements from your leader, team or entire company and then show the\nresults as quickly as possible. If the chosen skill or strategy doesn‚Äôt suit you or the feedback collected is strongly\nnegative, don‚Äôt worry, make a pivot and choose something else. After a few iterations you will definitely achieve your\ngoal.</p>\n","contentSnippet":"At some point in your career, you realize that it‚Äôs time to try to advance through the hierarchy. You think you are\ndoing a good job. You are constantly developing and learning something new. But at the same time, someone you know, with\nmuch less experience and knowledge than you, has long been higher up the hierarchy than you. Then you ask yourself:\nwhat is wrong with me? In my case, the answer turned out to be properly gathering the expectations concerning my skills\nand work.\nBelow, you can read my insight about it. You will find out how you can steer your development, so that you become much\nmore valuable to your team, leader or company and how to combine all this with your talents and interests.\nHow to start the development\nAs a member of a project team, you want to deliver a fully valuable solution. The key to success is to discover what‚Äôs\nbehind the word valuable to your client. Is it enough to gather requirements? Unfortunately not. To find out exactly\nwhat stands behind the word valuable you need to use a prototyping method, such as an iterative\nMVP (Minimum Viable Product) approach. For example, suppose you\nhave to paint your daughter‚Äôs room. You have several choices. You can choose the colour by yourself and paint the room\nwithout asking her for her opinion. The chance she will like the colour is small. So you will make a non-compliant\nproduct. You can bring her a sampler and ask her to choose a colour. The final colour when applied to the wall, may turn\nout to be different and then, the product will also not be as expected. You can also ask her about the colour and buy a\nfew samplers, paint a piece of wall with them and ask if any meet the requirements, if yes then paint the whole room\nwith the chosen colour, if not, buy another 5 colours and so on over and over until you get it right. This is the MVP\nprocess. First you gather requirements, then you prototype, which means you make a real product. You present it and\nreceive feedback (you learn) and decide what to do next. You repeat this process until you meet the client‚Äôs\nexpectations. This approach gives you the assurance that you have done exactly what was needed.\nImagine that your skills and the work you do are products. We already know that to make a valuable product you need to\ngather requirements and prototype with the client. Your customer who cares most about your work and skills are the team,\nthe leader and the company you work for. However, what you do must also suit you. If you want your work and your\nknowledge to be of value to them, you need to gather their requirements first. Then quickly show the result and make\nsure it‚Äôs what they wanted. You are probably wondering what this prototyping is for in the case of your work? You have\ndone some work, it has been accepted so what more is there to talk about. There could be lots of reasons. You could have\nspent too much time on it, it‚Äôs even possible that what you did is wrong. You would have to put in a huge amount of time\nand work to improve it, which is why it was accepted as it is. Without prototyping, your work might appear to be of good\nquality to you, but the team might not be happy with it.\nLet‚Äôs start with requirements gathering. Unfortunately it is impossible to gather universal requirements for a given\nposition since these requirements are different at different companies and even teams. The same person who has certain\nskills and performs certain tasks may be a senior in one company, where in another, he or she may be a junior. In one of\nthe companies I worked for, a good programmer was required to know the products and the business, then the tenure at the\ncompany mattered as well and being a support for the business. Performing a large number of tasks was not so important.\nIn another company, there were people who knew their product very well and were an excellent support to the business.\nHowever, they did not perform their tasks on time and did not take on issues that required them to undertake a lot of\nresponsibility. They still had junior or mid-level status.\n\nThis begs the question: how do I gather my skills requirements?\nAssess yourself with some evaluation sheets\nAt Allegro, in the case of programmers and testers, we have a list of expectations prepared within the leadership\ncommunity. With such a list, it is much easier to collect requirements and talk about your own development towards\npromotion (in the team or in the company).\nIn my case, after working at Allegro for a while, I decided that it might be a good idea to finally get my development\non track. My leader asked me to assess myself using a requirements sheet. This allowed me to identify the current stage\nI was at. The same sheet about me was completed by my leader. Then we calibrated ourselves. This sheet is divided into 3\nmain parts:\nTechnical knowledge\nAttitude\nProduct knowledge\nEach part has a list of skills, each with an extensive description. For example, one of the descriptions of technical\nknowledge looked like this:\nYou know your technological stack very well (systems, applications, languages, tools). Having a technical problem to\nsolve, you know what your technological abilities are, and you know how to use them to achieve your goal.\nI rated each skill on a scale of 1 to 5. Ideally, I would like each skill to be close to a 5. If I rated myself\nsomewhere at a 4 and my leader rated me a 3, during calibration, I found out that the requirements for that skill were\ndifferent from what I thought. He said, there were things I hadn‚Äôt used yet. To have a 4 at this point I need to know\nthem e.g. Cassandra database. With that I learned the first relevant requirement that I didn‚Äôt know about. After the\nwhole process of gathering requirements this way, I already knew what I needed to work on. Now all I had to do was\nselect a few of them and start working. But how to choose these requirements, which I will implement with passion and\nactually do them well?\nHow to choose skills to improve?\nThere are many strategies on how to choose skills to develop. All in all, you need to use the right method for the right\ntype of skill to get a good result. The first step is to get a mentor. Usually it will be your leader, but it can also\nbe an experienced team member, e.g. a senior.\nLeaders at Allegro gain their development knowledge from many available training sessions. You should find a similar\nperson in your environment. Such a person will help you on your way and will simulate the customer of the product that\nyou will be making. Show him or her a list of things you should develop. Ask them which are the most important for your\nteam. It is possible that the list of things will become very short. Now just pick two or three things and prepare\nprototypes. You can choose them intuitively or match them with your talents. Just take a Gallup\ntest (CliftonStrengths) to discover them. This test will help\nyou choose a strategy for developing a particular skill. This test will work well for soft skills. For hard skills\na lean startup strategy may be better. Below are two examples of what this\ncan look like in practice.\nHow Gallup test can help you find activities that improve you\nThe Gallup test\nhelps you find activities that you naturally enjoy doing. This natural response to certain types of information,\nsituations and people in the context of the Gallup test is called a talent. The test result is divided into four main\ndomains:\nExecuting\nInfluencing\nRelationship Building\nStrategic Thinking\nSome of these will have a direct impact on your work, others will not. For example a description of your character in\nthe context of building relationships with people will be interesting information and will certainly raise your\nawareness, but will not help us in our challenge. Other talents that come out of the challenge may fit into the skill\nyou are developing or show how you should develop it.\nOf course with these talents, the knowledge itself that you have a talent does not immediately translate into skills,\nfor example finding out that you have a talent for standing on your head, it does not mean that you will immediately\nstand on your head. You have to develop this talent, and you probably are already doing similar things because you\nsimply have a talent for it, and you enjoy it. So the Gallup test shows the things you should focus on. They are so\ngeneric that you can easily match them to your problems.\n\nHere‚Äôs what it was like for me. I did the Gallup test, and it came out that I am a learner. I like to learn, and I\nshould share knowledge - which should make me happy. Comparing this talent to the list of skills I should develop, I\nfound out that I don‚Äôt share knowledge with others. I decided that a good form of knowledge transfer is to make\npresentations. I completed training in making presentations. I also estimated the time for preparation and set a\ndeadline. While preparing, I realized that this knowledge sharing strategy was not for me. I finished this presentation\nand decided to change my knowledge sharing strategy. Such a change in strategy is called a pivot. I discussed the\nproblem with my mentor and decided to make a pivot towards writing articles. Being in this situation you have two\nchoices: either you do a pivot, which means you change your strategy, or you persevere, which means you continue to\ndevelop using your current strategy, after getting proper feedback from your mentor and team of course. Let us now look\nat the second type of skill.\nHow startup method can help you develop your skills\nLet us divide hard skills into two groups. In the first group there are skills you acquire while learning by yourself,\ne.g. a new programming language. For this you need internal motivation and discipline to achieve the expected high\nlevel. The second group is the one where you acquire skills automatically e.g. you use some framework in your project.\nFor the first group you can use the startup method.\n\nAn example scenario of what the process of developing hard skills might look like:\nChoose one skill with your mentor (the order does not matter). Additionally, collect the requirements.\nEstimate how long it will take you to develop that skill. Put it on your calendar.\nTogether with your mentor, set a first stage, i.e. the first prototype of your work and work on it for e.g. 2-3\nweeks.\nAfter this time, show what you have done to your mentor or the whole team.\nDraw conclusions. Collect feedback. Do you like doing it? Did the result meet their expectations?\nIf not, make a pivot to another strategy or take another skill. If so, improve what you collected during the feedback\nand keep developing.\nIn the worst case scenario you will go through all the skills. You will then have a fairly broad knowledge base. I have\nnever heard the opinion that someone learned something and now regrets it. In a positive scenario you will probably find\nsomething you do well and enjoy it.\nWe will now look at the second group of skills, which you develop automatically while working on a project. Here again,\nyou should first consult with your mentor on the best strategy to follow. There are two strategies in such cases: to\ngain deep knowledge in one of these skills or to learn a little about all of them. The mentor should know what the team\nexpects from a specialist in the given position. This automatically affects the implementation of the tasks in the\nproject:\nYou perform tasks only in the domain you know and after some time you become a specialist in this area, but you do\nnot take tasks from another domain that is new to you. Your value is the speed of task completion.\nYou always take tasks from a new domain, you are brave and not afraid of challenges. You will then have a broad\ngeneral knowledge of all products but will complete tasks much more slowly.\nThis is just one example. It is important that the mentor is clear about his requirements and that he gives you feedback\nafter you have done a few of these tasks to make sure that this is what you wanted.\nPivot or persevere\nFinally, let me explain the startup terms for pivot. The term pivot means to change from one strategy to another. For\nexample, I did one pivot during my development process. I was developing the skill of knowledge sharing. I had several\nstrategies to choose from:\nMaking presentations\nWriting articles\nTraining preparation\nI made a presentation and after collecting feedback I had to make a pivot and changed my strategy to another one. I\nchose to share knowledge by writing articles. This article is the result of that. If this strategy works for me then I\nwill do a persevere and continue using the chosen strategy.\nSummary\nIf you feel that you should already be in a higher position it is likely that the expectations for your job are quite\ndifferent than you think. To change this you should first find a mentor and then, with their help, gather the\nrequirements. A good place to start is with a list of requirements or a list of skills to begin discussing with your\nmentor. From this list you should select a few skills and then choose appropriate strategies for their development. To\nmake it easier for you to choose a strategy, do a Gallup test. Additionally, you can use the startup MVP approach.\nUltimately, the key to success is to gather requirements from your leader, team or entire company and then show the\nresults as quickly as possible. If the chosen skill or strategy doesn‚Äôt suit you or the feedback collected is strongly\nnegative, don‚Äôt worry, make a pivot and choose something else. After a few iterations you will definitely achieve your\ngoal.","guid":"https://blog.allegro.tech/2021/12/choose-your-skills.html","categories":["tech","education","skills","startup","lean startup","mvp"],"isoDate":"2021-12-08T23:00:00.000Z","thumbnail":"images/post-headers/default.jpg"}],"jobs":[{"id":"743999785422127","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"229d607a-333b-431b-9abe-78137730f5fd","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:56:17.000Z","location":{"city":"Warszawa, Krak√≥w, Pozna≈Ñ, Toru≈Ñ, Wroc≈Çaw, Gda≈Ñsk, Katowice, ≈Å√≥d≈∫, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Krak√≥w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"≈Å√≥d≈∫","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Wiƒôcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toru≈Ñ","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gda≈Ñsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"B≈Çonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wroc≈Çaw","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Pozna≈Ñ","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785422127","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999785421861","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"a6b2b59e-28e3-4bfa-89ab-b13ab97f06c8","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-11-08T09:54:52.000Z","location":{"city":"Warszawa, Pozna≈Ñ, Krak√≥w, Toru≈Ñ, Wroc≈Çaw, Gda≈Ñsk, Katowice, ≈Å√≥d≈∫, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Krak√≥w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Wiƒôcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toru≈Ñ","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"B≈Çonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wroc≈Çaw","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Pozna≈Ñ","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999785421861","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"en","label":"English","labelNative":"English (US)"}},{"id":"743999779448775","name":"Research Engineer - Machine Learning (Reinforcement Learning)","uuid":"c8e577cc-c93a-43e7-8e73-e430989798d7","refNumber":"REF2881V","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:36.000Z","location":{"city":"Warszawa, Krak√≥w, Pozna≈Ñ, Toru≈Ñ, Wroc≈Çaw, Gda≈Ñsk, Katowice, ≈Å√≥d≈∫, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"606235fe248e6f5bea0815ed","fieldLabel":"Katowice","valueId":"185eb5a9-b884-4ee8-8ebc-0e5f3e852b27","valueLabel":"Tak"},{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Krak√≥w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"606235bcefbac7156d6a470a","fieldLabel":"≈Å√≥d≈∫","valueId":"7d33e23d-3fa7-4d7d-86ae-7d7caff54fa9","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Wiƒôcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toru≈Ñ","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"6165609ee6b46b6506c66b63","fieldLabel":"Gda≈Ñsk","valueId":"cde0f8e7-5c9d-4d78-9f5c-e1c17ee499a8","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"B≈Çonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wroc≈Çaw","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"61656102a169ed164d546c31","fieldLabel":"Lublin","valueId":"02d54f00-48b9-4669-b7b3-10c16ac4bada","valueLabel":"Tak"},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Pozna≈Ñ","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, Machine Learning, Python, Deep Learning, AI, Artificial Intelligence"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448775","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779448676","name":"Research Engineer - Machine Learning (Ranking and Recommendations)","uuid":"7cb35dfc-f53c-4b51-81ac-61b683060f4c","refNumber":"REF2990T","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-14T10:29:00.000Z","location":{"city":"Warszawa, Pozna≈Ñ, Krak√≥w, Toru≈Ñ, Wroc≈Çaw, Gda≈Ñsk, Katowice, ≈Å√≥d≈∫, Lublin","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572821","label":"IT - Machine Learning"},"function":{"id":"information_technology","label":"Information Technology"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c1575ee4b01d4b19ddf797","fieldLabel":"Krak√≥w","valueId":"2cfcfcc1-da44-43f7-8eaa-3907faf2797c","valueLabel":"Tak"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Wiƒôcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c1576ee4b0614667d59732","fieldLabel":"Toru≈Ñ","valueId":"987e8884-bad1-4a8f-b149-99e4184cc221","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"},{"fieldId":"60cb31a9c87e511299a3a050","fieldLabel":"Department II","valueId":"b8a4596e-d9ce-42bb-8de5-10995e9ccf99","valueLabel":"IT - Machine Learning"},{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c15788e4b0614667d59733","fieldLabel":"B≈Çonie","valueId":"25f6cb8c-81b3-434a-93ec-6dc851d5808d","valueLabel":"Nie"},{"fieldId":"58c15798e4b01d4b19ddf79b","fieldLabel":"Wroc≈Çaw","valueId":"64818201-cdba-422e-8e8c-8ec633b0d327","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572821","valueLabel":"IT - Machine Learning"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Pozna≈Ñ","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"5cdab2c84cedfd0006e7758c","fieldLabel":"Key words","valueLabel":"ML, AI, Ranking, Research, Machine Learning"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779448676","creator":{"name":"Maciej Matwiejczyk"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}},{"id":"743999779015147","name":"Data Analyst (Consumer Analytics)","uuid":"57b6c274-611c-402d-aeef-6b2753039c68","refNumber":"REF2827C","company":{"identifier":"Allegro","name":"Allegro"},"releasedDate":"2021-10-12T13:32:35.000Z","location":{"city":"Warszawa, Pozna≈Ñ","country":"pl","remote":false},"industry":{"id":"internet","label":"Internet"},"department":{"id":"2572804","label":"IT - Analytics & Consulting"},"function":{"id":"analyst","label":"Analyst"},"typeOfEmployment":{"label":"Full-time"},"experienceLevel":{"id":"mid_senior_level","label":"Mid-Senior Level"},"customField":[{"fieldId":"58c15608e4b01d4b19ddf790","fieldLabel":"Proces rekrutacji","valueId":"c807eec2-8a53-4b55-b7c5-c03180f2059b","valueLabel":"(Archive) IT Allegro"},{"fieldId":"COUNTRY","fieldLabel":"Country","valueId":"pl","valueLabel":"Poland"},{"fieldId":"58c158e9e4b0614667d5973e","fieldLabel":"Wiƒôcej lokalizacji","valueId":"3a186e8d-a82c-4955-af81-fcef9a63928a","valueLabel":"Tak"},{"fieldId":"58c13159e4b01d4b19ddf729","fieldLabel":"Department","valueId":"2572804","valueLabel":"IT - Analytics & Consulting"},{"fieldId":"58c13159e4b01d4b19ddf728","fieldLabel":"Brands","valueId":"4ccb4fab-6c3f-4ed0-9140-8533fe17447f","valueLabel":"Allegro.pl sp. z o.o."},{"fieldId":"58c156b9e4b01d4b19ddf792","fieldLabel":"Pozna≈Ñ","valueId":"ac20917b-cb6a-4280-aff3-4fae2532a33e","valueLabel":"Tak"},{"fieldId":"58c156e6e4b01d4b19ddf793","fieldLabel":"Warszawa","valueId":"6a428533-1586-4372-89ec-65fb45366363","valueLabel":"Tak"}],"ref":"https://api.smartrecruiters.com/v1/companies/allegro/postings/743999779015147","creator":{"name":"Ada Lata≈Ñska"},"language":{"code":"pl","label":"Polish","labelNative":"polski"}}],"events":[{"created":1642501324000,"duration":5400000,"id":"283335598","name":"Allegro Tech Live #24 - Automatyzacja i us≈Çugi biznesowe w Allegro","date_in_series_pattern":false,"status":"upcoming","time":1643907600000,"local_date":"2022-02-03","local_time":"18:00","updated":1642501324000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":9,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/283335598/","description":"REJESTRACJA NA WYDARZENIE ---&gt; https://app.evenea.pl/event/allegro-tech-live-24/ Allegro Tech Live to w 100% zdalna ods≈Çona naszych stacjonarnych meetup√≥w Allegro Tech Talks. Kiedy≈õ spotykali≈õmy siƒô w naszych biurach,‚Ä¶","how_to_find_us":"https://app.evenea.pl/event/allegro-tech-live-24/","visibility":"public","member_pay_fee":false},{"created":1638356820000,"duration":7200000,"id":"282421464","name":"Allegro Tech Labs #9 Online: System design workshop","date_in_series_pattern":false,"status":"past","time":1639501200000,"local_date":"2021-12-14","local_time":"18:00","updated":1639514166000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":62,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/282421464/","description":"‚ùóNA WYDARZENIE OBOWIƒÑZUJE REJESTRACJA: Liczba miejsc jest organiczona: https://app.evenea.pl/event/allegro-tech-labs-9/ ‚ùó To ju≈º druga edycja naszych warsztat√≥w System Design Workshop! Czy chcesz poznaƒá tajniki tworzenia system√≥w?‚Ä¶","how_to_find_us":"https://app.evenea.pl/event/allegro-tech-labs-9/","visibility":"public","member_pay_fee":false},{"created":1635344914000,"duration":7200000,"id":"281692274","name":"Allegro Tech Live #23 - Przygody backendowc√≥w w C#","date_in_series_pattern":false,"status":"past","time":1636045200000,"local_date":"2021-11-04","local_time":"18:00","updated":1636056125000,"utc_offset":3600000,"waitlist_count":0,"yes_rsvp_count":29,"is_online_event":false,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281692274/","description":"------ Rejestracja: https://app.evenea.pl/event/allegro-tech-live-23/------- Allegro Tech Live to w 100% zdalna ods≈Çona naszych stacjonarnych meetup√≥w Allegro Tech Talks. Kiedy≈õ spotykali≈õmy siƒô w naszych biurach, a teraz‚Ä¶","visibility":"public","member_pay_fee":false},{"created":1634290537000,"duration":5400000,"id":"281441586","name":"Allegro Tech Live #22 - Jak wyglƒÖda codzienno≈õƒá lidera w Allegro?","date_in_series_pattern":false,"status":"past","time":1634832000000,"local_date":"2021-10-21","local_time":"18:00","updated":1634841007000,"utc_offset":7200000,"waitlist_count":0,"yes_rsvp_count":67,"venue":{"id":26906060,"name":"Online event","repinned":false,"country":"","localized_country_name":""},"is_online_event":true,"group":{"created":1425052059000,"name":"allegro Tech","id":18465254,"join_mode":"open","lat":52.2599983215332,"lon":21.020000457763672,"urlname":"allegrotech","who":"Techs","localized_location":"Warsaw, Poland","state":"","country":"pl","region":"en_US","timezone":"Europe/Warsaw"},"link":"https://www.meetup.com/allegrotech/events/281441586/","description":"!!!! Rejestracja: https://app.evenea.pl/event/allegro-tech-live-22/ !!!! Allegro Tech Live to w 100% zdalna ods≈Çona naszych stacjonarnych meetup√≥w Allegro Tech Talks. Zazwyczaj spotykali≈õmy siƒô w naszych biurach, ale‚Ä¶","visibility":"public","member_pay_fee":false}],"podcasts":[{"creator":{"name":["Piotr Betkier"]},"title":"Rola architekta w Allegro","link":"https://podcast.allegro.tech/rola_architekta_w_allegro","pubDate":"Wed, 16 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Betkier"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8712218.mp3","type":"audio/mpeg"},"content":"Od kodowania do tworzenia strategii technicznej... Jak wyglƒÖda rola architekta w Allegro? Ile takich os√≥b pracuje w naszej firmie i dlaczego ta rola jest tak r√≥≈ºnorodna? Czym jest Andamio i jak rozwijamy naszƒÖ platformƒô ‚Äì o tym wszystkim opowie Piotr Betkier ‚Äì In≈ºynier, Architekt Platformy Technicznej w Allegro oraz tw√≥rca piosenek o IT :)","contentSnippet":"Od kodowania do tworzenia strategii technicznej... Jak wyglƒÖda rola architekta w Allegro? Ile takich os√≥b pracuje w naszej firmie i dlaczego ta rola jest tak r√≥≈ºnorodna? Czym jest Andamio i jak rozwijamy naszƒÖ platformƒô ‚Äì o tym wszystkim opowie Piotr Betkier ‚Äì In≈ºynier, Architekt Platformy Technicznej w Allegro oraz tw√≥rca piosenek o IT :)","guid":"https://podcast.allegro.tech/rola_architekta_w_allegro","isoDate":"2021-06-16T00:00:00.000Z","itunes":{"author":"Piotr Betkier","summary":"Od kodowania do tworzenia strategii technicznej... Jak wyglƒÖda rola architekta w Allegro? Ile takich os√≥b pracuje w naszej firmie i dlaczego ta rola jest tak r√≥≈ºnorodna? Czym jest Andamio i jak rozwijamy naszƒÖ platformƒô ‚Äì o tym wszystkim opowie Piotr Betkier ‚Äì In≈ºynier, Architekt Platformy Technicznej w Allegro oraz tw√≥rca piosenek o IT :)","explicit":"false"}},{"creator":{"name":["Piotr Micho≈Ñski"]},"title":"Infrastruktura Allegro","link":"https://podcast.allegro.tech/infrastruktura_Allegro","pubDate":"Tue, 01 Jun 2021 00:00:00 GMT","author":{"name":["Piotr Micho≈Ñski"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8623783-sezon-ii-11-infrastruktura-allegro-piotr-michonski.mp3","type":"audio/mpeg"},"content":"Jak jest zbudowane ≈õrodowisko uruchomienia aplikacji Allegro? Jak dzia≈ÇajƒÖ serwerownie firmy i ile ich potrzeba, a kt√≥re elementy Allegro dzia≈ÇajƒÖ w chmurze publicznej? Jak przebiega≈Ça transformacja w Allegro i co zmienia≈Ço siƒô przez lata? Jak wzrost biznesu wp≈Çywa na wielko≈õƒá infrastruktury i jak infrastruktura Allegro odczu≈Ça przyj≈õcie pandemii? O tym, a tak≈ºe o rozwoju lider√≥w technologii w Allegro oraz o historii powstania d≈ºingla do naszych podcast√≥w, opowie Piotr Micho≈Ñski - menad≈ºer Zespo≈Ç√≥w tworzƒÖcych infrastrukturƒô Allegro.","contentSnippet":"Jak jest zbudowane ≈õrodowisko uruchomienia aplikacji Allegro? Jak dzia≈ÇajƒÖ serwerownie firmy i ile ich potrzeba, a kt√≥re elementy Allegro dzia≈ÇajƒÖ w chmurze publicznej? Jak przebiega≈Ça transformacja w Allegro i co zmienia≈Ço siƒô przez lata? Jak wzrost biznesu wp≈Çywa na wielko≈õƒá infrastruktury i jak infrastruktura Allegro odczu≈Ça przyj≈õcie pandemii? O tym, a tak≈ºe o rozwoju lider√≥w technologii w Allegro oraz o historii powstania d≈ºingla do naszych podcast√≥w, opowie Piotr Micho≈Ñski - menad≈ºer Zespo≈Ç√≥w tworzƒÖcych infrastrukturƒô Allegro.","guid":"https://podcast.allegro.tech/infrastruktura_Allegro","isoDate":"2021-06-01T00:00:00.000Z","itunes":{"author":"Piotr Micho≈Ñski","summary":"Jak jest zbudowane ≈õrodowisko uruchomienia aplikacji Allegro? Jak dzia≈ÇajƒÖ serwerownie firmy i ile ich potrzeba, a kt√≥re elementy Allegro dzia≈ÇajƒÖ w chmurze publicznej? Jak przebiega≈Ça transformacja w Allegro i co zmienia≈Ço siƒô przez lata? Jak wzrost biznesu wp≈Çywa na wielko≈õƒá infrastruktury i jak infrastruktura Allegro odczu≈Ça przyj≈õcie pandemii? O tym, a tak≈ºe o rozwoju lider√≥w technologii w Allegro oraz o historii powstania d≈ºingla do naszych podcast√≥w, opowie Piotr Micho≈Ñski - menad≈ºer Zespo≈Ç√≥w tworzƒÖcych infrastrukturƒô Allegro.","explicit":"false"}},{"creator":{"name":["Dariusz Eliasz"]},"title":"Praca architekta ekosystemu big data w Allegro","link":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","pubDate":"Thu, 20 May 2021 00:00:00 GMT","author":{"name":["Dariusz Eliasz"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8554742-sezon-ii-10-przetwarzanie-danych-w-allegro-dariusz-eliasz.mp3","type":"audio/mpeg"},"content":"Jak wyglƒÖda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zesp√≥≈Ç odpowiedzialny za narzƒôdzia i infrastrukturƒô dla przetwarzania danych? Kiedy mo≈ºemy m√≥wiƒá o du≈ºych danych i ile petabajt√≥w przetwarza Allegro? SkƒÖd pochodzƒÖ dane Allegro i dlaczego jest ich tak du≈ºo oraz z jakiego powodu dopiero teraz przenosimy siƒô do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz ‚Äì Team Manager & Platform Architect w Allegro.","contentSnippet":"Jak wyglƒÖda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zesp√≥≈Ç odpowiedzialny za narzƒôdzia i infrastrukturƒô dla przetwarzania danych? Kiedy mo≈ºemy m√≥wiƒá o du≈ºych danych i ile petabajt√≥w przetwarza Allegro? SkƒÖd pochodzƒÖ dane Allegro i dlaczego jest ich tak du≈ºo oraz z jakiego powodu dopiero teraz przenosimy siƒô do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz ‚Äì Team Manager & Platform Architect w Allegro.","guid":"https://podcast.allegro.tech/praca_architekta_ekosystemu_big_data_w_Allegro","isoDate":"2021-05-20T00:00:00.000Z","itunes":{"author":"Dariusz Eliasz","summary":"Jak wyglƒÖda praca architekta ekosystemu big data w Allegro? Jakie zadania realizuje nasz zesp√≥≈Ç odpowiedzialny za narzƒôdzia i infrastrukturƒô dla przetwarzania danych? Kiedy mo≈ºemy m√≥wiƒá o du≈ºych danych i ile petabajt√≥w przetwarza Allegro? SkƒÖd pochodzƒÖ dane Allegro i dlaczego jest ich tak du≈ºo oraz z jakiego powodu dopiero teraz przenosimy siƒô do chmury? O tym wszystkim opowie zdobywca statuetki Allegro Tech Hero - Dariusz Eliasz ‚Äì Team Manager & Platform Architect w Allegro.","explicit":"false"}},{"creator":{"name":["Bartosz Ga≈Çek"]},"title":"Od in≈ºyniera do lidera w Allegro","link":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","pubDate":"Thu, 06 May 2021 00:00:00 GMT","author":{"name":["Bartosz Ga≈Çek"]},"enclosure":{"url":"https://www.buzzsprout.com/887914/8455586-sezon-ii-9-od-inzyniera-do-lidera-w-allegro-bartosz-galek.mp3","type":"audio/mpeg"},"content":"Czym jest Opbox i jakie wyzwania przed nim stojƒÖ? Jak w Allegro anga≈ºujemy siƒô w rozw√≥j kultury Open Source? Ile mamy projekt√≥w na GitHubie i jak ≈õwiƒôtujemy Hacktoberfest? W jaki spos√≥b mo≈ºna rozwinƒÖƒá siƒô od in≈ºyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Ga≈Çek, Team Leader w Allegro.","contentSnippet":"Czym jest Opbox i jakie wyzwania przed nim stojƒÖ? Jak w Allegro anga≈ºujemy siƒô w rozw√≥j kultury Open Source? Ile mamy projekt√≥w na GitHubie i jak ≈õwiƒôtujemy Hacktoberfest? W jaki spos√≥b mo≈ºna rozwinƒÖƒá siƒô od in≈ºyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Ga≈Çek, Team Leader w Allegro.","guid":"https://podcast.allegro.tech/od_inzyniera_do_lidera_w_allegro","isoDate":"2021-05-06T00:00:00.000Z","itunes":{"author":"Bartosz Ga≈Çek","summary":"Czym jest Opbox i jakie wyzwania przed nim stojƒÖ? Jak w Allegro anga≈ºujemy siƒô w rozw√≥j kultury Open Source? Ile mamy projekt√≥w na GitHubie i jak ≈õwiƒôtujemy Hacktoberfest? W jaki spos√≥b mo≈ºna rozwinƒÖƒá siƒô od in≈ºyniera do lidera? Na te pytania w najnowszym Allegro Tech Podcast odpowie Bartek Ga≈Çek, Team Leader w Allegro.","explicit":"false"}}]},"__N_SSG":true}